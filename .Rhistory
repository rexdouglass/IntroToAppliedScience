x_bar
reticulate::repl_python()
library(DBI)
# Create an ephemeral in-memory RSQLite database
#con <- dbConnect(RSQLite::SQLite(), dbname = ":memory:")
#dbListTables(con)
#dbWriteTable(con, "mtcars", mtcars)
#dbListTables(con)
#Configuration failed because libpq was not found. Try installing:
#* deb: libpq-dev libssl-dev (Debian, Ubuntu, etc)
#install.packages('RPostgres')
#remotes::install_github("r-dbi/RPostgres")
#Took forever because my file permissions were broken
#pg_lsclusters
require(RPostgres)
# Connect to the default postgres database
#I had to follow these instructions and create both a username and database that matched my ubuntu name
#https://www.digitalocean.com/community/tutorials/how-to-install-postgresql-on-ubuntu-20-04-quickstart
con <- dbConnect(RPostgres::Postgres())
reticulate::repl_python()
x = c(1,2,3,4)
x
#Algorithm
x_bar = sum(x, na.rm=T)/length(x)
x_bar
#Base Function
x_bar = mean(x, na.rm=T)
x_bar
reticulate::repl_python()
library(DBI)
# Create an ephemeral in-memory RSQLite database
#con <- dbConnect(RSQLite::SQLite(), dbname = ":memory:")
#dbListTables(con)
#dbWriteTable(con, "mtcars", mtcars)
#dbListTables(con)
#Configuration failed because libpq was not found. Try installing:
#* deb: libpq-dev libssl-dev (Debian, Ubuntu, etc)
#install.packages('RPostgres')
#remotes::install_github("r-dbi/RPostgres")
#Took forever because my file permissions were broken
#pg_lsclusters
require(RPostgres)
# Connect to the default postgres database
#I had to follow these instructions and create both a username and database that matched my ubuntu name
#https://www.digitalocean.com/community/tutorials/how-to-install-postgresql-on-ubuntu-20-04-quickstart
con <- dbConnect(RPostgres::Postgres())
x = c(1,2,3,4)
x
#Algorithm
x_bar = sum(x, na.rm=T)/length(x)
x_bar
#Base Function
x_bar = mean(x, na.rm=T)
x_bar
reticulate::repl_python()
library(DBI)
# Create an ephemeral in-memory RSQLite database
#con <- dbConnect(RSQLite::SQLite(), dbname = ":memory:")
#dbListTables(con)
#dbWriteTable(con, "mtcars", mtcars)
#dbListTables(con)
#Configuration failed because libpq was not found. Try installing:
#* deb: libpq-dev libssl-dev (Debian, Ubuntu, etc)
#install.packages('RPostgres')
#remotes::install_github("r-dbi/RPostgres")
#Took forever because my file permissions were broken
#pg_lsclusters
require(RPostgres)
# Connect to the default postgres database
#I had to follow these instructions and create both a username and database that matched my ubuntu name
#https://www.digitalocean.com/community/tutorials/how-to-install-postgresql-on-ubuntu-20-04-quickstart
con <- dbConnect(RPostgres::Postgres())
x = c(1,2,3,4)
x
#Algorithm
x_bar = sum(x, na.rm=T)/length(x)
x_bar
#Base Function
x_bar = mean(x, na.rm=T)
x_bar
reticulate::repl_python()
library(DBI)
# Create an ephemeral in-memory RSQLite database
#con <- dbConnect(RSQLite::SQLite(), dbname = ":memory:")
#dbListTables(con)
#dbWriteTable(con, "mtcars", mtcars)
#dbListTables(con)
#Configuration failed because libpq was not found. Try installing:
#* deb: libpq-dev libssl-dev (Debian, Ubuntu, etc)
#install.packages('RPostgres')
#remotes::install_github("r-dbi/RPostgres")
#Took forever because my file permissions were broken
#pg_lsclusters
require(RPostgres)
# Connect to the default postgres database
#I had to follow these instructions and create both a username and database that matched my ubuntu name
#https://www.digitalocean.com/community/tutorials/how-to-install-postgresql-on-ubuntu-20-04-quickstart
con <- dbConnect(RPostgres::Postgres())
x = c(1,2,3,4)
x
#Algorithm
x_bar = sum(x, na.rm=T)/length(x)
x_bar
#Base Function
x_bar = mean(x, na.rm=T)
x_bar
reticulate::repl_python()
library(DBI)
# Create an ephemeral in-memory RSQLite database
#con <- dbConnect(RSQLite::SQLite(), dbname = ":memory:")
#dbListTables(con)
#dbWriteTable(con, "mtcars", mtcars)
#dbListTables(con)
#Configuration failed because libpq was not found. Try installing:
#* deb: libpq-dev libssl-dev (Debian, Ubuntu, etc)
#install.packages('RPostgres')
#remotes::install_github("r-dbi/RPostgres")
#Took forever because my file permissions were broken
#pg_lsclusters
require(RPostgres)
# Connect to the default postgres database
#I had to follow these instructions and create both a username and database that matched my ubuntu name
#https://www.digitalocean.com/community/tutorials/how-to-install-postgresql-on-ubuntu-20-04-quickstart
con <- dbConnect(RPostgres::Postgres())
df=data.frame(a=c(1,2,3,4), b=c('A','B','C','D'))
df
reticulate::repl_python()
df=data.frame(a=c(1,2,3,4), b=c('a','b','c','d'))
df
reticulate::repl_python()
library(DBI)
# Create an ephemeral in-memory RSQLite database
#con <- dbConnect(RSQLite::SQLite(), dbname = ":memory:")
#dbListTables(con)
#dbWriteTable(con, "mtcars", mtcars)
#dbListTables(con)
#Configuration failed because libpq was not found. Try installing:
#* deb: libpq-dev libssl-dev (Debian, Ubuntu, etc)
#install.packages('RPostgres')
#remotes::install_github("r-dbi/RPostgres")
#Took forever because my file permissions were broken
#pg_lsclusters
require(RPostgres)
# Connect to the default postgres database
#I had to follow these instructions and create both a username and database that matched my ubuntu name
#https://www.digitalocean.com/community/tutorials/how-to-install-postgresql-on-ubuntu-20-04-quickstart
con <- dbConnect(RPostgres::Postgres())
as.array(letters)
dim(as.array(letters))
example_tensor= array(c(1,2,3),('A','B','C'))
example_tensor= array(c(1,2,3),c('A','B','C'))
?array
array(c(1,2,3),c('A','B','C'))
example_tensor= array(c(1,2,3,"A","B","C"),dim=c(1,2))
example_tensor
example_tensor= array(c(1,2,3,"A","B","C"),dim=c(2,3))
example_tensor= array(c(1,2,3,"A","B","C"),dim=c(2,3))
example_tensor
example_tensor= array(c(1,2,3,4,"A","B","C","D","+","-","*","/"),dim=c(2,3,2))
example_tensor
example_tensor
example_tensor[1,,]
example_tensor[,1,]
example_matrix <- matrix(c(1,2,3,4,"A","B","C","D"), nrow = 2, ncol = 4, byrow = TRUE,
dimnames = list(c("row1", "row2"),
c("C.1", "C.2")))
example_matrix <- matrix(c(1,2,3,4,"A","B","C","D"), nrow = 2, ncol = 4, byrow = TRUE,
dimnames = list(c("row1", "row2"),
c("C.1", "C.2", "C.3", "C.4")))
example_matrix
example_vector <- vector(c(1,2,3,4))
example_vector <- as.vector(c(1,2,3,4))
example_vector
class(example_vector)
class(example_matrix)
example_vector <- as.vector(c(1,2,3,4))
example_vector
class(example_vector)
example_matrix <- matrix(c(1,2,3,4,"A","B","C","D"), nrow = 2, ncol = 4, byrow = TRUE,
dimnames = list(c("row1", "row2"),
c("C.1", "C.2", "C.3", "C.4")))
example_matrix
class(example_matrix)
example_tensor= array(c(1,2,3,4,"A","B","C","D","+","-","*","/"),dim=c(2,3,2,2))
example_tensor
class(example_tensor)
library(DBI)
# Create an ephemeral in-memory RSQLite database
#con <- dbConnect(RSQLite::SQLite(), dbname = ":memory:")
#dbListTables(con)
#dbWriteTable(con, "mtcars", mtcars)
#dbListTables(con)
#Configuration failed because libpq was not found. Try installing:
#* deb: libpq-dev libssl-dev (Debian, Ubuntu, etc)
#install.packages('RPostgres')
#remotes::install_github("r-dbi/RPostgres")
#Took forever because my file permissions were broken
#pg_lsclusters
require(RPostgres)
# Connect to the default postgres database
#I had to follow these instructions and create both a username and database that matched my ubuntu name
#https://www.digitalocean.com/community/tutorials/how-to-install-postgresql-on-ubuntu-20-04-quickstart
con <- dbConnect(RPostgres::Postgres())
reticulate::repl_python()
example_vector <- as.vector(c(1,2,3,4))
example_vector
class(example_vector)
example_matrix <- matrix(c(1,2,3,4,"A","B","C","D"), nrow = 2, ncol = 4, byrow = TRUE,
dimnames = list(c("row1", "row2"),
c("C.1", "C.2", "C.3", "C.4")))
example_matrix
class(example_matrix)
example_tensor= array(c(1,2,3,4,"A","B","C","D","+","-","*","/"),dim=c(2,3,2,2))
example_tensor
class(example_tensor)
library(DBI)
# Create an ephemeral in-memory RSQLite database
#con <- dbConnect(RSQLite::SQLite(), dbname = ":memory:")
#dbListTables(con)
#dbWriteTable(con, "mtcars", mtcars)
#dbListTables(con)
#Configuration failed because libpq was not found. Try installing:
#* deb: libpq-dev libssl-dev (Debian, Ubuntu, etc)
#install.packages('RPostgres')
#remotes::install_github("r-dbi/RPostgres")
#Took forever because my file permissions were broken
#pg_lsclusters
require(RPostgres)
# Connect to the default postgres database
#I had to follow these instructions and create both a username and database that matched my ubuntu name
#https://www.digitalocean.com/community/tutorials/how-to-install-postgresql-on-ubuntu-20-04-quickstart
con <- dbConnect(RPostgres::Postgres())
reticulate::repl_python()
install.packages("vembedr")
#install.packages("vembedr")
vembedr::embed_url("https://www.youtube.com/watch?v=cbAOfPdsTSo")
?vembedr::embed_url
install.packages('V8')
#library(V8)
#cx <- v8()
#cx$source("/home/skynet3/Downloads/twitter-2022-11-08-24fa1be25590defacf8df1fe45d475a1b82d1e6d6223a7c9cb10e61deff3c19d/data/tweets.js") # now the variable 'data' is defined #in V8
#cx$get("data")
file="/home/skynet3/Downloads/twitter-2022-11-08-24fa1be25590defacf8df1fe45d475a1b82d1e6d6223a7c9cb10e61deff3c19d/data/tweets.js"
library(jsonlite)
df <- fromJSON(file)
names(df)
library(tidyverse)
tweets <- df$tweet %>% janitor::clean_names()
dim(tweets)
names(tweets)
tweets$rex_rank <- as.numeric(tweets$retweet_count) + as.numeric(tweets$favorite_count)
extended_entities <- tweets$extended_entities$media
dim(extended_entities)
extended_entities <- tweets$entities$urls
dim(extended_entities)
tweets$url <- sapply(tweets$entities$urls, FUN=function(x) { ifelse(is.null(x$expanded_url), NA, x$expanded_url) } ) %>% unlist()
tweets_url <- tweets %>% filter(!is.na(url))
dim(tweets_url)
View(tweets_url)
tweets_url_small <- tweets_url %>%
dplyr::select(created_at, full_text, rex_rank, url) %>%
filter(!url %>% str_detect("twitter")) %>%
arrange(desc(rex_rank), url)
dim(tweets_url_small)
tweets_url_small %>% write.csv("/mnt/8tb_a/rwd_github_private/IntroToAppliedScience/git_ignore/temp.csv")
tweets_url_labeled <- read.csv("/mnt/8tb_a/rwd_github_private/IntroToAppliedScience/git_ignore/temp_labeled.csv") #%>% filter(keep %in% 1)
dim(tweets_url_labeled)
tweets_url_labeled_bk <- tweets_url_labeled
dim(tweets_url_labeled_bk)
tweets_url_labeled %>%
arrange(desc(rex_rank), url)  %>% filter(!duplicated(url)) %>%
filter(!url %>% str_detect("covid")) %>%
#dim() %>%
write.csv("/mnt/8tb_a/rwd_github_private/IntroToAppliedScience/git_ignore/temp_labeled.csv", row.names = FALSE)
tweets_url_labeled <- read.csv("/mnt/8tb_a/rwd_github_private/IntroToAppliedScience/git_ignore/temp_labeled.csv") #%>% filter(keep %in% 1)
dim(tweets_url_labeled)
for(url in tweets_url_labeled$url[3001:3100]){browseURL(url=url)}
for(url in tweets_url_labeled$url[3101:3200]){browseURL(url=url)}
for(url in tweets_url_labeled$url[3201:3300]){browseURL(url=url)}
for(url in tweets_url_labeled$url[3301:3400]){browseURL(url=url)}
for(url in tweets_url_labeled$url[3301:3400]){browseURL(url=url)}
Can transparency undermine peer review? A simulation model of scientist behavior under open peer review
Exploratory Data Analysis with R (2020) will overview tools and best practices in R to accomplish all the best steps of the data analysis process.
for(url in tweets_url_labeled$url[3401:3500]){browseURL(url=url)}
for(url in tweets_url_labeled$url[3501:3600]){browseURL(url=url)}
for(url in tweets_url_labeled$url[3601:3700]){browseURL(url=url)}
install.packages('V8')
#library(V8)
#cx <- v8()
#cx$source("/home/skynet3/Downloads/twitter-2022-11-08-24fa1be25590defacf8df1fe45d475a1b82d1e6d6223a7c9cb10e61deff3c19d/data/tweets.js") # now the variable 'data' is defined #in V8
#cx$get("data")
file="/home/skynet3/Downloads/twitter-2022-11-08-24fa1be25590defacf8df1fe45d475a1b82d1e6d6223a7c9cb10e61deff3c19d/data/tweets.js"
library(jsonlite)
df <- fromJSON(file)
names(df)
library(tidyverse)
tweets <- df$tweet %>% janitor::clean_names()
dim(tweets)
names(tweets)
tweets$rex_rank <- as.numeric(tweets$retweet_count) + as.numeric(tweets$favorite_count)
extended_entities <- tweets$extended_entities$media
dim(extended_entities)
extended_entities <- tweets$entities$urls
dim(extended_entities)
tweets$url <- sapply(tweets$entities$urls, FUN=function(x) { ifelse(is.null(x$expanded_url), NA, x$expanded_url) } ) %>% unlist()
tweets_url <- tweets %>% filter(!is.na(url))
dim(tweets_url)
View(tweets_url)
tweets_url_small <- tweets_url %>%
dplyr::select(created_at, full_text, rex_rank, url) %>%
filter(!url %>% str_detect("twitter")) %>%
arrange(desc(rex_rank), url)
dim(tweets_url_small)
tweets_url_small %>% write.csv("/mnt/8tb_a/rwd_github_private/IntroToAppliedScience/git_ignore/temp.csv")
tweets_url_labeled <- read.csv("/mnt/8tb_a/rwd_github_private/IntroToAppliedScience/git_ignore/temp_labeled.csv") #%>% filter(keep %in% 1)
dim(tweets_url_labeled)
tweets_url_labeled_bk <- tweets_url_labeled
dim(tweets_url_labeled_bk)
tweets_url_labeled %>%
arrange(desc(rex_rank), url)  %>% filter(!duplicated(url)) %>%
filter(!url %>% str_detect("covid")) %>%
#dim() %>%
write.csv("/mnt/8tb_a/rwd_github_private/IntroToAppliedScience/git_ignore/temp_labeled.csv", row.names = FALSE)
tweets_url_labeled <- read.csv("/mnt/8tb_a/rwd_github_private/IntroToAppliedScience/git_ignore/temp_labeled.csv") #%>% filter(keep %in% 1)
dim(tweets_url_labeled)
for(url in tweets_url_labeled$url[3701:3800]){browseURL(url=url)}
for(url in tweets_url_labeled$url[3801:3900]){browseURL(url=url)}
for(url in tweets_url_labeled$url[3901:4000]){browseURL(url=url)}
for(url in tweets_url_labeled$url[4001:4200]){browseURL(url=url)}
for(url in tweets_url_labeled$url[4201:4400]){browseURL(url=url)}
for(url in tweets_url_labeled$url[4401:4600]){browseURL(url=url)}
for(url in tweets_url_labeled$url[4401:4500]){browseURL(url=url)}
Benign Overfitting in Linear Regression
for(url in tweets_url_labeled$url[4501:4600]){browseURL(url=url)}
bookmark_border
for(url in tweets_url_labeled$url[4601:4800]){browseURL(url=url)}
for(url in tweets_url_labeled$url[4801:5000]){browseURL(url=url)}
for(url in tweets_url_labeled$url[5001:5200]){browseURL(url=url)}
tidypolars is a data frame library built on top of the blazingly fast polars library that gives access to methods and functions familiar to R tidyverse users.
for(url in tweets_url_labeled$url[5201:5400]){browseURL(url=url)}
charlatan makes fake data, inspired from and borrowing some code from Python's faker (https://github.com/joke2k/faker)
for(url in tweets_url_labeled$url[5401:5600]){browseURL(url=url)}
for(url in tweets_url_labeled$url[5601:5800]){browseURL(url=url)}
for(url in tweets_url_labeled$url[5801:6000]){browseURL(url=url)}
for(url in tweets_url_labeled$url[5801:6000]){browseURL(url=url)}
The Precision-Recall Plot Is More Informative than the ROC Plot When Evaluating Binary Classifiers on Imbalanced Datasets
for(url in tweets_url_labeled$url[6001:6200]){browseURL(url=url)}
for(url in tweets_url_labeled$url[6201:6400]){browseURL(url=url)}
for(url in tweets_url_labeled$url[6401:6600]){browseURL(url=url)}
for(url in tweets_url_labeled$url[6601:6800]){browseURL(url=url)}
for(url in tweets_url_labeled$url[6801:7000]){browseURL(url=url)}
for(url in tweets_url_labeled$url[7001:7200]){browseURL(url=url)}
length(tweets_url_labeled$url)
for(url in tweets_url_labeled$url[7201:7400]){browseURL(url=url)}
for(url in tweets_url_labeled$url[7401:7600]){browseURL(url=url)}
for(url in tweets_url_labeled$url[7401:7600]){browseURL(url=url)}
for(url in tweets_url_labeled$url[7601:7800]){browseURL(url=url)}
length(tweets_url_labeled$url)
files <- list.files(path="/mnt/8tb_a/rwd_github_private/IntroToAppliedScience/", pattern="*.qmd", full.names=TRUE, recursive=FALSE)
library(stringr)
url_pattern <- "http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+"
files
file="/mnt/8tb_a/rwd_github_private/IntroToAppliedScience//lit.qmd"
file_txts <- file %>% readLines()
?str_extract
urls <- str_extract_all(file_txts, url_pattern)
urls
urls_vec <- urls %>% unlist()
urls_vec
file_txts <-  sapply(file, readLines())
file_txts <-  lapply(file, readLines())
length(file_txts)
libraty(data.table)
?fread
library(data.table)
?fread
files
file
file_txts <-  file %>% lapply(fread(sep=NULL, sep2=NULL, header=F)) #slower than you'd like
file_txts <-  lapply(file, fread(sep=NULL, sep2=NULL, header=F)) #slower than you'd like
file_txts <-  lapply(file, readLines()) #slower than you'd like #fread(sep=NULL, sep2=NULL, header=F)
file_txts
length(file_txts)
file_txts <- NA
library("readr")
?read_lines
file_txts <-  lapply(file, read_lines()) #slower than you'd like #fread(sep=NULL, sep2=NULL, header=F)
file_txts <-  read_lines(file)
file_txts <- file %>% read_lines() #slower than you'd like #fread(sep=NULL, sep2=NULL, header=F)
len(file_txts)
length(file_txts)
file_txts <- lapply( files,  read_lines() ) #slower than you'd like #fread(sep=NULL, sep2=NULL, header=F)
file_txts <- list()
for(file in files){
file_txts[[file]] <- read_lines(file)
}
length(file_txts)
urls <- str_extract_all(file_txts %>% unlist, url_pattern)
urls_vec <- urls %>% unlist()
length(urls_vec)
urls_vec
library(data.table)
library("readr")
files <- list.files(path="/mnt/8tb_a/rwd_github_private/IntroToAppliedScience/", pattern="*.qmd", full.names=TRUE, recursive=FALSE)
file="/mnt/8tb_a/rwd_github_private/IntroToAppliedScience//lit.qmd"
file_txts <- list()
for(file in files){
file_txts[[file]] <- read_lines(file)
}
length(file_txts)
library(stringr)
url_pattern <- "http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+"
urls <- str_extract_all(file_txts %>% unlist, url_pattern)
urls_vec <- urls %>% unlist() %>% unique()
length(urls_vec) #1219
urls_vec
urls_vec %>% writeLines("/mnt/8tb_a/rwd_github_private/IntroToAppliedScience/urls.html")
urls_vec %>% paste("\n") %>% writeLines("/mnt/8tb_a/rwd_github_private/IntroToAppliedScience/urls.html")
urls_vec %>% paste("<br>") %>% writeLines("/mnt/8tb_a/rwd_github_private/IntroToAppliedScience/urls.html")
length(urls_vec)
for(url in tweets_url_labeled$url[1:100]){browseURL(url=url)}
for(url in tweets_url_labeled$url[1:100]){browseURL(url=url)}
for(url in urls_vec[1:100]){browseURL(url=url)}
for(url in urls_vec[1:100]){browseURL(url=url)}
urls_vec[1:100]
url_pattern <- "[(]http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))[)]+"
urls <- str_extract_all(file_txts %>% unlist, url_pattern)
urls_vec <- urls %>% unlist() %>% unique()
length(urls_vec) #1219
urls_vec %>% paste("<br>") %>% writeLines("/mnt/8tb_a/rwd_github_private/IntroToAppliedScience/urls.html")
length(urls_vec)
url_pattern <- "http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+"
urls <- str_extract_all(file_txts %>% unlist, url_pattern)
urls_vec <- urls %>% unlist() %>% unique()
length(urls_vec) #1219
urls_vec
urls_vec[1:100]
url_pattern <- "https?:\/\/(www\.)?[-a-zA-Z0-9@:%._\+~#=]{1,256}\.[a-zA-Z0-9()]{1,6}\b([-a-zA-Z0-9()@:%_\+.~#?&//=]*)"
urls_vec
urls_vec <- urls %>% unlist() %>% str_replace(")$","") %>% unique()
urls_vec <- urls %>% unlist() %>% str_replace("\\\\)$","") %>% unique()
urls_vec <- urls %>% unlist() %>% str_replace("\\)$","") %>% unique()
urls_vec[1:100]
for(url in urls_vec[1:100]){browseURL(url=url)}
for(url in urls_vec[101:200]){browseURL(url=url)}
for(url in urls_vec[201:300]){browseURL(url=url)}
for(url in urls_vec[301:400]){browseURL(url=url)}
for(url in urls_vec[301:330]){browseURL(url=url)}
for(url in urls_vec[401:500]){browseURL(url=url)}
for(url in urls_vec[501:600]){browseURL(url=url)}
for(url in urls_vec[601:700]){browseURL(url=url)}
for(url in urls_vec[601:700]){browseURL(url=url)}
for(url in urls_vec[701:800]){browseURL(url=url)}
length(urls_vec)
for(url in urls_vec[801:900]){browseURL(url=url)}
for(url in urls_vec[901:1000]){browseURL(url=url)}
for(url in urls_vec[1001:1100]){browseURL(url=url)}
for(url in urls_vec[1001:1100]){browseURL(url=url)}
for(url in urls_vec[1201:1300]){browseURL(url=url)}
for(url in urls_vec[1101:1200]){browseURL(url=url)}
scraped <- read_csv("/home/skynet3/Documents/IntroToAppliedScience.csv")
View(scraped)
url_missing <- setdiff(urls_vec, scraped$Url)
length(url_missing)
for(url in url_missing[1:100]){browseURL(url=url)}
for(url in url_missing[101:200]){browseURL(url=url)}
for(url in url_missing[201:300]){browseURL(url=url)}
for(url in url_missing[301:400]){browseURL(url=url)}
for(url in url_missing[401:500]){browseURL(url=url)}
for(url in url_missing[501:600]){browseURL(url=url)}
for(url in url_missing[601:700]){browseURL(url=url)}
scraped <- read_csv("/home/skynet3/Documents/IntroToAppliedScience.csv")
url_missing <- setdiff(urls_vec, scraped$Url)
length(url_missing) #698
scraped <- read_csv("/home/skynet3/Documents/IntroToAppliedScience.csv")
url_missing <- setdiff(urls_vec, scraped$Url)
length(url_missing) #698 #oh wow only missed 98
url_missing
vector[!vector %>% str_detect('arxiv')]
url_missing[!url_missing %>% str_detect('arxiv')]
url_missing[!url_missing %>% str_detect('arxiv')] #322 that didn't pull
url_missing[!url_missing %>% str_detect('arxiv|wikipedia')] #322 that didn't pull
url_missing[!url_missing %>% str_detect('arxiv|wikipedia')] %>% sort() #322 that didn't pull
url_missing[!url_missing %>% str_detect('arxiv|wikipedia|tandfonline')] %>% sort() #322 that didn't pull
url_missing[!url_missing %>% str_detect('arxiv|wikipedia|tandfonline|ssrn|ncbi|onlinelibrary|sagepub|iomedcentral|elifesciences|academic')] %>% sort() #322 that didn't pull
View(scraped)
scraped$`Abstract Note` %>% is.na() %>% table()
scraped <- read_csv("/home/skynet3/Documents/IntroToAppliedScience.csv") %>% janitor::lubridate()
scraped <- read_csv("/home/skynet3/Documents/IntroToAppliedScience.csv") %>% janitor::clean_names()
scraped %>% write_csv("/home/skynet3/Documents/IntroToAppliedScience_tolabel.csv")
temp <- fromJSON("/home/skynet3/Documents/IntroToAppliedScience.json")
temp
class(temp)
View(temp)
temp <- fromJSON("/home/skynet3/Documents/IntroToAppliedScience.json")
temp %>% janitor::clean_names() %>% write_csv("/home/skynet3/Documents/IntroToAppliedScience_tolabel.csv")
temp %>% flatten() %>% janitor::clean_names() %>% write_csv("/home/skynet3/Documents/IntroToAppliedScience_tolabel.csv")
class(temp)
temp %>% flatten()
?flatten
temp %>% jsonlite::flatten() %>% janitor::clean_names() %>% write_csv("/home/skynet3/Documents/IntroToAppliedScience_tolabel.csv")
scraped %>%  janitor::clean_names() %>% write_csv("/home/skynet3/Documents/IntroToAppliedScience_tolabel2.csv")
scraped %>%  janitor::clean_names() %>% names()
combined <- temp %>%  janitor::clean_names() %>% dplyr::select(doi, url, title, citation_key) %>%
left_join(scraped)
combined <- scraped %>%
left_join(temp %>%  janitor::clean_names() %>% dplyr::select(doi, url, title, citation_key) )
dim(combined)
combined$citation_key
is.na(combined$citation_key)
table(is.na(combined$citation_key))
combined %>% write_csv("/home/skynet3/Documents/IntroToAppliedScience_tolabel3.csv")
