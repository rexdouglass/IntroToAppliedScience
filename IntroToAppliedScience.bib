@online{11Documentation,
  title = {3.11.0 {{Documentation}}},
  url = {https://docs.python.org/3/},
  urldate = {2022-11-11},
  file = {/home/skynet3/Zotero/storage/I6UGSUPQ/3.html}
}

@online{201101808Https,
  title = {[2011.01808](Https:/Arxiv.Org/Abs/2011.01808] {{Article}} Identifier Not Recognized},
  url = {https://arxiv.org/abs/2011.01808](https://arxiv.org/abs/2011.01808},
  urldate = {2022-11-11},
  file = {/home/skynet3/Zotero/storage/J8ASI7U6/2011.html}
}

@online{20RegressionOther,
  ids = {20RegressionOthera},
  title = {\#20 {{Regression}} and {{Other Stories}}, with {{Andrew Gelman}}, {{Jennifer Hill}} \& {{Aki Vehtari}}},
  url = {https://learnbayesstats.com/episode/20-regression-and-other-stories-with-andrew-gelman-jennifer-hill-aki-vehtari},
  urldate = {2022-11-11},
  abstract = {Once upon a time, there was an enchanted book filled with hundreds of little plots, applied examples and linear regressions — the prettiest creature t...},
  langid = {british},
  file = {/home/skynet3/Zotero/storage/4TLX7YMN/20-regression-and-other-stories-with-andrew-gelman-jennifer-hill-aki-vehtari.html}
}

@online{210802497Https,
  title = {[2108.02497](Https:/Arxiv.Org/Abs/2108.02497] {{Article}} Identifier Not Recognized},
  url = {https://arxiv.org/abs/2108.02497](https://arxiv.org/abs/2108.02497},
  urldate = {2022-11-11},
  file = {/home/skynet3/Zotero/storage/Y9JFIDGZ/2108.html}
}

@online{22FreeData,
  title = {22 {{Free Data Science Books}}},
  url = {http://www.wzchen.com/data-science-books},
  urldate = {2022-11-11},
  abstract = {22 free data science books for the aspirational data scientist, covering statistics, Python, machine learning, the data science process, and more.},
  langid = {american},
  organization = {{Hi. I'm William Chen.}},
  file = {/home/skynet3/Zotero/storage/Z9LPXRPV/data-science-books.html}
}

@online{32DowhyIcml,
  ids = {32DowhyIcmla},
  title = {32\_dowhy\_icml\_causal\_assumptions\_workshop\_2021\_final.Pdf},
  url = {https://drive.google.com/file/d/1i81CnMd683A788RYtEb8KSowhhPJn3Z6/view?usp=embed_facebook},
  urldate = {2022-11-11},
  organization = {{Google Docs}},
  file = {/home/skynet3/Zotero/storage/6N6H92NZ/view.html}
}

@video{3blue1brownCrossProductsLight2016,
  title = {Cross Products in the Light of Linear Transformations | {{Chapter}} 11, {{Essence}} of Linear Algebra},
  editor = {{3Blue1Brown}},
  date = {2016-08-31},
  url = {https://www.youtube.com/watch?v=BaM7OCEm3G0},
  urldate = {2022-11-11},
  editortype = {director}
}

@video{3blue1brownDotProductsDuality2016,
  title = {Dot Products and Duality | {{Chapter}} 9, {{Essence}} of Linear Algebra},
  editor = {{3Blue1Brown}},
  date = {2016-08-24},
  url = {https://www.youtube.com/watch?v=LyGKycYT2v0},
  urldate = {2022-11-11},
  editortype = {director}
}

@video{3blue1brownLinearCombinationsSpan2016,
  title = {Linear Combinations, Span, and Basis Vectors | {{Chapter}} 2, {{Essence}} of Linear Algebra},
  editor = {{3Blue1Brown}},
  date = {2016-08-06},
  url = {https://www.youtube.com/watch?v=k7RM-ot2NWY},
  urldate = {2022-11-11},
  editortype = {director}
}

@video{3blue1brownLinearTransformationsMatrices2016,
  title = {Linear Transformations and Matrices | {{Chapter}} 3, {{Essence}} of Linear Algebra},
  editor = {{3Blue1Brown}},
  date = {2016-08-07},
  url = {https://www.youtube.com/watch?v=kYB8IZa5AuE},
  urldate = {2022-11-11},
  editortype = {director}
}

@video{3blue1brownVectorsChapterEssence2016,
  title = {Vectors | {{Chapter}} 1, {{Essence}} of Linear Algebra},
  editor = {{3Blue1Brown}},
  date = {2016-08-05},
  url = {https://www.youtube.com/watch?v=fNk_zzaMoSs},
  urldate = {2022-11-11},
  editortype = {director}
}

@online{403Forbidden,
  ids = {403Forbiddena,403Forbiddenb},
  title = {403 {{Forbidden}}},
  url = {https://people.cs.uct.ac.za/},
  urldate = {2022-11-11},
  file = {/home/skynet3/Zotero/storage/D693XB9D/www2.math.uu.se.html;/home/skynet3/Zotero/storage/XJMDQZ69/www2.math.uu.se.html}
}

@online{500,
  title = {500},
  url = {https://math.papers.bar/paper/cc98597a2c2e11edaa66a71c10a887e7},
  urldate = {2022-11-11},
  file = {/home/skynet3/Zotero/storage/8ISKJ5VL/cc98597a2c2e11edaa66a71c10a887e7.html}
}

@misc{abadieStatisticalNonSignificance2018,
  title = {On {{Statistical Non-Significance}}},
  author = {Abadie, Alberto},
  date = {2018-03-01},
  number = {arXiv:1803.00609},
  eprint = {1803.00609},
  eprinttype = {arxiv},
  primaryclass = {stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1803.00609},
  url = {http://arxiv.org/abs/1803.00609},
  urldate = {2022-11-11},
  abstract = {Significance tests are probably the most extended form of inference in empirical research, and significance is often interpreted as providing greater informational content than non-significance. In this article we show, however, that rejection of a point null often carries very little information, while failure to reject may be highly informative. This is particularly true in empirical contexts where data sets are large and where there are rarely reasons to put substantial prior probability on a point null. Our results challenge the usual practice of conferring point null rejections a higher level of scientific significance than non-rejections. In consequence, we advocate a visible reporting and discussion of non-significant results in empirical practice.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Applications,Statistics - Methodology,Statistics - Other Statistics},
  file = {/home/skynet3/Zotero/storage/BJGH9FXD/Abadie - 2018 - On Statistical Non-Significance.pdf;/home/skynet3/Zotero/storage/492R39QE/1803.html}
}

@article{abadieStatisticalNonsignificanceEmpirical2020,
  title = {Statistical {{Nonsignificance}} in {{Empirical Economics}}},
  author = {Abadie, Alberto},
  date = {2020-06},
  journaltitle = {American Economic Review: Insights},
  volume = {2},
  number = {2},
  pages = {193--208},
  doi = {10.1257/aeri.20190252},
  url = {https://www.aeaweb.org/articles?id=10.1257/aeri.20190252&from=f},
  urldate = {2022-11-11},
  abstract = {Statistical significance is often interpreted as providing greater information than nonsignificance. In this article we show, however, that rejection of a point null often carries very little information, while failure to reject may be highly informative. This is particularly true in empirical contexts that are common in economics, where datasets are large and there are rarely reasons to put substantial prior probability on a point null. Our results challenge the usual practice of conferring point null rejections a higher level of scientific significance than non-rejections. Therefore, we advocate visible reporting and discussion of nonsignificant results.},
  langid = {english},
  keywords = {Design of Experiments: General,Hypothesis Testing: General},
  file = {/home/skynet3/Zotero/storage/Y3UVXDV4/Abadie - 2020 - Statistical Nonsignificance in Empirical Economics.pdf}
}

@article{abadieUsingSyntheticControls2021,
  title = {Using {{Synthetic Controls}}: {{Feasibility}}, {{Data Requirements}}, and {{Methodological Aspects}}},
  shorttitle = {Using {{Synthetic Controls}}},
  author = {Abadie, Alberto},
  date = {2021-06},
  journaltitle = {Journal of Economic Literature},
  volume = {59},
  number = {2},
  pages = {391--425},
  issn = {0022-0515},
  doi = {10.1257/jel.20191450},
  url = {https://www.aeaweb.org/articles?id=10.1257/jel.20191450&from=f},
  urldate = {2022-11-11},
  abstract = {Probably because of their interpretability and transparent nature, synthetic controls have become widely applied in empirical research in economics and the social sciences. This article aims to provide practical guidance to researchers employing synthetic control methods. The article starts with an overview and an introduction to synthetic control estimation. The main sections discuss the advantages of the synthetic control framework as a research design, and describe the settings where synthetic controls provide reliable estimates and those where they may fail. The article closes with a discussion of recent extensions, related methods, and avenues for future research.},
  langid = {english},
  keywords = {Aggregate Productivity,Cross-Country Output Convergence,Diffusion Processes,Dynamic Quantile Regressions,Dynamic Treatment Effect Models,Economic Methodology; Multiple or Simultaneous Equation Models: Time-Series Models,State Space Models; Quantitative Policy Modeling; Macroeconomics: Production; Economic Integration; Empirical Studies of Economic Growth},
  file = {/home/skynet3/Zotero/storage/2GRHQH6J/Abadie - 2021 - Using Synthetic Controls Feasibility, Data Requir.pdf;/home/skynet3/Zotero/storage/IINH5PQ6/articles.html}
}

@article{abadieWhenShouldYou2022,
  title = {When {{Should You Adjust Standard Errors}} for {{Clustering}}?*},
  shorttitle = {When {{Should You Adjust Standard Errors}} for {{Clustering}}?},
  author = {Abadie, Alberto and Athey, Susan and Imbens, Guido W and Wooldridge, Jeffrey M},
  date = {2022-10-06},
  journaltitle = {The Quarterly Journal of Economics},
  shortjournal = {The Quarterly Journal of Economics},
  pages = {qjac038},
  issn = {0033-5533},
  doi = {10.1093/qje/qjac038},
  url = {https://doi.org/10.1093/qje/qjac038},
  urldate = {2022-11-11},
  abstract = {Clustered standard errors, with clusters defined by factors such as geography, are widespread in empirical research in economics and many other disciplines. Formally, clustered standard errors adjust for the correlations induced by sampling the outcome variable from a data-generating process with unobserved cluster-level components. However, the standard econometric framework for clustering leaves important questions unanswered: (i) Why do we adjust standard errors for clustering in some ways but not others, e.g., by state but not by gender, and in observational studies, but not in completely randomized experiments? (ii) Why is conventional clustering an “all-or-nothing” adjustment, while within-cluster correlations can be strong or extremely weak? (iii) In what settings does the choice of whether and how to cluster make a difference? We address these and other questions using a novel framework for clustered inference on average treatment effects. In addition to the common sampling component, the new framework incorporates a design component that accounts for the variability induced on the estimator by the treatment assignment mechanism. We show that, when the number of clusters in the sample is a nonnegligible fraction of the number of clusters in the population, conventional cluster standard errors can be severely inflated, and propose new variance estimators that correct for this bias.},
  file = {/home/skynet3/Zotero/storage/ZXKBRNY7/Abadie et al. - 2022 - When Should You Adjust Standard Errors for Cluster.pdf;/home/skynet3/Zotero/storage/XWH3Q342/6750017.html}
}

@online{AbsoluteMinimumEvery2003,
  title = {The {{Absolute Minimum Every Software Developer Absolutely}}, {{Positively Must Know About Unicode}} and {{Character Sets}} ({{No Excuses}}!)},
  date = {2003-10-08T00:15:12+00:00},
  url = {https://www.joelonsoftware.com/2003/10/08/the-absolute-minimum-every-software-developer-absolutely-positively-must-know-about-unicode-and-character-sets-no-excuses/},
  urldate = {2022-11-11},
  abstract = {Ever wonder about that mysterious Content-Type tag? You know, the one you’re supposed to put in HTML and you never quite know what it should be? Did you ever get an email from your friends in…},
  langid = {american},
  organization = {{Joel on Software}}
}

@online{AcceleratingDeepLearning2019,
  title = {Accelerating {{Deep Learning}} by {{Focusing}} on the {{Biggest Losers}}},
  date = {2019-10-02T03:34:29+00:00},
  url = {https://deepai.org/publication/accelerating-deep-learning-by-focusing-on-the-biggest-losers},
  urldate = {2022-11-11},
  abstract = {10/02/19 - This paper introduces Selective-Backprop, a technique that accelerates the training of deep neural networks (DNNs) by prioritizing...},
  organization = {{DeepAI}},
  file = {/home/skynet3/Zotero/storage/YCJ5S96Z/accelerating-deep-learning-by-focusing-on-the-biggest-losers.html}
}

@online{AcceleratingGgplot2,
  title = {Accelerating Ggplot2},
  url = {https://ggforce.data-imaginist.com/index.html},
  urldate = {2022-11-11},
  abstract = {The aim of ggplot2 is to aid in visual data investigations. This     focus has led to a lack of facilities for composing specialised plots.     ggforce aims to be a collection of mainly new stats and geoms that fills     this gap. All additional functionality is aimed to come through the official     extension system so using ggforce should be a stable experience.},
  langid = {english},
  file = {/home/skynet3/Zotero/storage/AUQZ5FRE/index.html}
}

@online{AccuracyVsExplainability2015,
  title = {Accuracy vs {{Explainability}} of {{Machine Learning Models}} [{{NIPS}} Workshop Poster Review]},
  date = {2015-12-22T14:45:09},
  url = {https://www.inference.vc/accuracy-vs-explainability-in-machine-learning-models-nips-workshop-poster-review/},
  urldate = {2022-11-11},
  abstract = {My ex-labmate Ryan Turner presented an awesome poster at the NIPS workshop on Black Box Learning and Inference that was an eye-opener to me. Here I'm going to cover what I think was the main take-home message for me, but I encourage everyone to take a look at the paper...},
  langid = {english},
  organization = {{inFERENCe}}
}

@article{achenLetPutGarbageCan2005,
  title = {Let's {{Put Garbage-Can Regressions}} and {{Garbage-Can Probits Where They Belong}}},
  author = {Achen, Christopher H.},
  date = {2005-09-01},
  journaltitle = {Conflict Management and Peace Science},
  volume = {22},
  number = {4},
  pages = {327--339},
  publisher = {{SAGE Publications Ltd}},
  issn = {0738-8942},
  doi = {10.1080/07388940500339167},
  url = {https://doi.org/10.1080/07388940500339167},
  urldate = {2022-11-11},
  abstract = {Many social scientists believe that dumping long lists of explanatory variables into linear regression, probit, logit, and other statistical equations will successfully ?control? for the effects of auxiliary factors. Encouraged by convenient software and ever more powerful computing, researchers also believe that this conventional approach gives the true explanatory variables the best chance to emerge. The present paper argues that these beliefs are false, and that without intensive data analysis, linear regression models are likely to be inaccurate. Instead, a quite different and less mechanical research methodology is needed, one that integrates contemporary powerful statistical methods with deep substantive knowledge and classic data?analytic techniques of creative engagement with the data.},
  langid = {english}
}

@misc{achilleInformationComplexityLearning2020,
  title = {The {{Information Complexity}} of {{Learning Tasks}}, Their {{Structure}} and Their {{Distance}}},
  author = {Achille, Alessandro and Paolini, Giovanni and Mbeng, Glen and Soatto, Stefano},
  date = {2020-07-14},
  number = {arXiv:1904.03292},
  eprint = {1904.03292},
  eprinttype = {arxiv},
  primaryclass = {cs, math, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1904.03292},
  url = {http://arxiv.org/abs/1904.03292},
  urldate = {2022-11-11},
  abstract = {We introduce an asymmetric distance in the space of learning tasks, and a framework to compute their complexity. These concepts are foundational for the practice of transfer learning, whereby a parametric model is pre-trained for a task, and then fine-tuned for another. The framework we develop is non-asymptotic, captures the finite nature of the training dataset, and allows distinguishing learning from memorization. It encompasses, as special cases, classical notions from Kolmogorov complexity, Shannon, and Fisher Information. However, unlike some of those frameworks, it can be applied to large-scale models and real-world datasets. Our framework is the first to measure complexity in a way that accounts for the effect of the optimization scheme, which is critical in Deep Learning.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Information Theory,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/skynet3/Zotero/storage/ZNLTYZWX/Achille et al. - 2020 - The Information Complexity of Learning Tasks, thei.pdf;/home/skynet3/Zotero/storage/PTBFVLHR/1904.html}
}

@online{AcoppockGreenLabSOPStandard,
  title = {Acoppock/{{Green-Lab-SOP}}: {{Standard Operating Procedures}} for {{Don Green}}'s {{Lab}} at {{Columbia}}},
  shorttitle = {Acoppock/{{Green-Lab-SOP}}},
  url = {https://github.com/acoppock/Green-Lab-SOP},
  urldate = {2022-11-11},
  abstract = {Standard Operating Procedures for Don Green's Lab at Columbia - acoppock/Green-Lab-SOP: Standard Operating Procedures for Don Green's Lab at Columbia},
  langid = {english},
  organization = {{GitHub}},
  file = {/home/skynet3/Zotero/storage/JLEQPR7G/Green-Lab-SOP.html}
}

@misc{adamsBayesianOnlineChangepoint2007,
  ids = {adamsBayesianOnlineChangepoint2007a},
  title = {Bayesian {{Online Changepoint Detection}}},
  author = {Adams, Ryan Prescott and MacKay, David J. C.},
  date = {2007-10-19},
  number = {arXiv:0710.3742},
  eprint = {0710.3742},
  eprinttype = {arxiv},
  primaryclass = {stat},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/0710.3742},
  urldate = {2022-11-11},
  abstract = {Changepoints are abrupt variations in the generative parameters of a data sequence. Online detection of changepoints is useful in modelling and prediction of time series in application areas such as finance, biometrics, and robotics. While frequentist methods have yielded online filtering and prediction techniques, most Bayesian papers have focused on the retrospective segmentation problem. Here we examine the case where the model parameters before and after the changepoint are independent and we derive an online algorithm for exact inference of the most recent changepoint. We compute the probability distribution of the length of the current ``run,'' or time since the last changepoint, using a simple message-passing algorithm. Our implementation is highly modular so that the algorithm may be applied to a variety of types of data. We illustrate this modularity by demonstrating the algorithm on three different real-world data sets.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Machine Learning},
  file = {/home/skynet3/Zotero/storage/5R8PGZXY/Adams and MacKay - 2007 - Bayesian Online Changepoint Detection.pdf;/home/skynet3/Zotero/storage/ZVESPQAI/0710.html}
}

@online{Addition,
  title = {Addition},
  url = {https://www.wikidata.org/wiki/Q32043},
  urldate = {2022-11-11},
  abstract = {arithmetic operation},
  langid = {english}
}

@inreference{Addition2022,
  title = {Addition},
  booktitle = {Wikipedia},
  date = {2022-10-25T07:24:32Z},
  url = {https://en.wikipedia.org/w/index.php?title=Addition&oldid=1118111089},
  urldate = {2022-11-11},
  abstract = {Addition (usually signified by the plus symbol +) is one of the four basic operations of arithmetic, the other three being subtraction, multiplication and division. The addition of two whole numbers results in the total amount or sum of those values combined. The example in the adjacent image shows a combination of three apples and two apples, making a total of five apples. This observation is equivalent to the mathematical expression "3 + 2 = 5" (that is, "3 plus 2 is equal to 5"). Besides counting items, addition can also be defined and executed without referring to concrete objects, using abstractions called numbers instead, such as integers, real numbers and complex numbers. Addition belongs to arithmetic, a branch of mathematics. In algebra, another area of mathematics, addition can also be performed on abstract objects such as vectors, matrices, subspaces and subgroups. Addition has several important properties. It is commutative, meaning that the order of the operands does not matter, and it is associative, meaning that when one adds more than two numbers, the order in which addition is performed does not matter (see Summation). Repeated addition of 1 is the same as counting (see Successor function). Addition of 0 does not change a number. Addition also obeys predictable rules concerning related operations such as subtraction and multiplication. Performing addition is one of the simplest numerical tasks to do. Addition of very small numbers is accessible to toddlers; the most basic task, 1 + 1, can be performed by infants as young as five months, and even some members of other animal species. In primary education, students are taught to add numbers in the decimal system, starting with single digits and progressively tackling more difficult problems. Mechanical aids range from the ancient abacus to the modern computer, where research on the most efficient implementations of addition continues to this day.},
  langid = {english},
  annotation = {Page Version ID: 1118111089}
}

@inreference{Addition2022a,
  title = {Addition},
  booktitle = {Wikipedia},
  date = {2022-10-25T07:24:32Z},
  url = {https://en.wikipedia.org/w/index.php?title=Addition&oldid=1118111089},
  urldate = {2022-11-11},
  abstract = {Addition (usually signified by the plus symbol +) is one of the four basic operations of arithmetic, the other three being subtraction, multiplication and division. The addition of two whole numbers results in the total amount or sum of those values combined. The example in the adjacent image shows a combination of three apples and two apples, making a total of five apples. This observation is equivalent to the mathematical expression "3 + 2 = 5" (that is, "3 plus 2 is equal to 5"). Besides counting items, addition can also be defined and executed without referring to concrete objects, using abstractions called numbers instead, such as integers, real numbers and complex numbers. Addition belongs to arithmetic, a branch of mathematics. In algebra, another area of mathematics, addition can also be performed on abstract objects such as vectors, matrices, subspaces and subgroups. Addition has several important properties. It is commutative, meaning that the order of the operands does not matter, and it is associative, meaning that when one adds more than two numbers, the order in which addition is performed does not matter (see Summation). Repeated addition of 1 is the same as counting (see Successor function). Addition of 0 does not change a number. Addition also obeys predictable rules concerning related operations such as subtraction and multiplication. Performing addition is one of the simplest numerical tasks to do. Addition of very small numbers is accessible to toddlers; the most basic task, 1 + 1, can be performed by infants as young as five months, and even some members of other animal species. In primary education, students are taught to add numbers in the decimal system, starting with single digits and progressively tackling more difficult problems. Mechanical aids range from the ancient abacus to the modern computer, where research on the most efficient implementations of addition continues to this day.},
  langid = {english},
  annotation = {Page Version ID: 1118111089},
  file = {/home/skynet3/Zotero/storage/8S5GELT4/Addition.html}
}

@online{AdrianAnticoRemixAutoMLPackage,
  title = {{{AdrianAntico}}/{{RemixAutoML}}: {{R}} Package for Automation of Machine Learning, Forecasting, Feature Engineering, Model Evaluation, Model Interpretation, Recommenders, and {{EDA}}.},
  shorttitle = {{{AdrianAntico}}/{{RemixAutoML}}},
  url = {https://github.com/AdrianAntico/RemixAutoML},
  urldate = {2022-11-11},
  abstract = {R package for automation of machine learning, forecasting, feature engineering, model evaluation, model interpretation, recommenders, and EDA.  - AdrianAntico/RemixAutoML: R package for automation ...},
  langid = {english},
  organization = {{GitHub}},
  file = {/home/skynet3/Zotero/storage/Q9KQWEKD/RemixAutoML.html}
}

@online{AdvancedMatrixFactorization,
  title = {The {{Advanced Matrix Factorization Jungle}} - Igorcarron2},
  url = {https://sites.google.com/site/igorcarron2/matrixfactorizations},
  urldate = {2022-11-11},
  file = {/home/skynet3/Zotero/storage/8SZCK964/matrixfactorizations.html}
}

@misc{agrawalKernelInteractionTrick2019,
  title = {The {{Kernel Interaction Trick}}: {{Fast Bayesian Discovery}} of {{Pairwise Interactions}} in {{High Dimensions}}},
  shorttitle = {The {{Kernel Interaction Trick}}},
  author = {Agrawal, Raj and Huggins, Jonathan H. and Trippe, Brian and Broderick, Tamara},
  date = {2019-12-02},
  number = {arXiv:1905.06501},
  eprint = {1905.06501},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1905.06501},
  url = {http://arxiv.org/abs/1905.06501},
  urldate = {2022-11-11},
  abstract = {Discovering interaction effects on a response of interest is a fundamental problem faced in biology, medicine, economics, and many other scientific disciplines. In theory, Bayesian methods for discovering pairwise interactions enjoy many benefits such as coherent uncertainty quantification, the ability to incorporate background knowledge, and desirable shrinkage properties. In practice, however, Bayesian methods are often computationally intractable for even moderate-dimensional problems. Our key insight is that many hierarchical models of practical interest admit a particular Gaussian process (GP) representation; the GP allows us to capture the posterior with a vector of O(p) kernel hyper-parameters rather than O(p\^2) interactions and main effects. With the implicit representation, we can run Markov chain Monte Carlo (MCMC) over model hyper-parameters in time and memory linear in p per iteration. We focus on sparsity-inducing models and show on datasets with a variety of covariate behaviors that our method: (1) reduces runtime by orders of magnitude over naive applications of MCMC, (2) provides lower Type I and Type II error relative to state-of-the-art LASSO-based approaches, and (3) offers improved computational scaling in high dimensions relative to existing Bayesian and LASSO-based approaches.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Computation,Statistics - Machine Learning,Statistics - Methodology},
  file = {/home/skynet3/Zotero/storage/HAYZISXR/Agrawal et al. - 2019 - The Kernel Interaction Trick Fast Bayesian Discov.pdf;/home/skynet3/Zotero/storage/CQIPG4RL/1905.html}
}

@article{ahlskogQuantifyingBiasMeasurable2022,
  title = {Quantifying {{Bias}} from {{Measurable}} and {{Unmeasurable Confounders Across Three Domains}} of {{Individual Determinants}} of {{Political Preferences}}},
  author = {Ahlskog, Rafael and Oskarsson, Sven},
  date = {2022-02-22},
  journaltitle = {Political Analysis},
  pages = {1--14},
  publisher = {{Cambridge University Press}},
  issn = {1047-1987, 1476-4989},
  doi = {10.1017/pan.2022.2},
  url = {https://www.cambridge.org/core/journals/political-analysis/article/quantifying-bias-from-measurable-and-unmeasurable-confounders-across-three-domains-of-individual-determinants-of-political-preferences/D1D2DEE9E7180BDCFC592885BE66E9AF},
  urldate = {2022-11-11},
  abstract = {A core part of political research is to identify how political preferences are shaped. The nature of these questions is such that robust causal identification is often difficult to achieve, and we are not seldom stuck with observational methods that we know have limited causal validity. The purpose of this paper is to measure the magnitude of bias stemming from both measurable and unmeasurable confounders across three broad domains of individual determinants of political preferences: socio-economic factors, moral values, and psychological constructs. We leverage a unique combination of rich Swedish registry data for a large sample of identical twins, with a comprehensive battery of 34 political preference measures, and build a meta-analytical model comparing our most conservative observational (naive) estimates with discordant twin estimates. This allows us to infer the amount of bias from unobserved genetic and shared environmental factors that remains in the naive models for our predictors, while avoiding precision issues common in family-based designs. The results are sobering: in most cases, substantial bias remains in naive models. A rough heuristic is that about half of the effect size even in conservative observational estimates is composed of confounding.},
  langid = {english},
  keywords = {causal inference,family fixed effects,genetic confounding,policy preferences,twin},
  file = {/home/skynet3/Zotero/storage/2HVU4P8W/Ahlskog and Oskarsson - 2022 - Quantifying Bias from Measurable and Unmeasurable .pdf;/home/skynet3/Zotero/storage/BXS6DIQ7/D1D2DEE9E7180BDCFC592885BE66E9AF.html}
}

@article{al-sibahiEinsteinVIGeneral2021,
  title = {Einstein {{VI}}: {{General}} and {{Integrated Stein Variational Inference}} in {{NumPyro}}},
  shorttitle = {Einstein {{VI}}},
  author = {Al-Sibahi, Ahmad Salim and Rønning, Ola and Ley, Christophe and Hamelryck, Thomas Wim},
  date = {2021-03-05},
  url = {https://openreview.net/forum?id=nXSDybDWV3},
  urldate = {2022-11-11},
  abstract = {Stein Variational Inference is a technique for approximate Bayesian inferencethat is recently gaining popularity since it combines the scalability of traditionalVariational Inference (VI) with the flexibility of non-parametric particle basedinference methods. While there has been considerable progress in developmentof algorithms, integration in existing probabilistic programming languages (PPLs)with an easy-to-use interface is currently lacking. EinStein VI is a lightweightcomposable library that integrates the latest Stein Variational Inference methodswith the NumPyro PPL. Inference with EinStein VI relies on ELBO-within-Stein tosupport use of custom inference programs (guides), non-linear scaling of repulsionforce, second-order gradient updates using matrix-valued kernels and parametertransforms. We demonstrate the achieved synergy of the different Stein techniquesand the versatility of EinStein VI library by applying it on examples. Comparedto traditional Stochastic VI, EinStein VI is better at capturing uncertainty andrepresenting richer posteriors. We use several applications to show how one canuse Neural Transforms (NeuTra) and second-order optimization to provide betterinference using EinStein VI. We show how EinStein VI can be used to infer theparameters of a Latent Dirichlet Allocation model with a neural guide. The resultsindicate that Einstein VI can be combined with NumPyro’s support for automaticmarginalization to do inference over models with discrete latent variables. Finally,we introduce an example with a novel extension to Deep Markov Models, calledthe Stein Mixture Deep Markov Model (SM-DMM), which shows that EinStein VIcan be scaled to reasonably large models with over 500.000 parameters},
  langid = {english},
  file = {/home/skynet3/Zotero/storage/X78JWNQQ/Al-Sibahi et al. - 2021 - Einstein VI General and Integrated Stein Variatio.pdf}
}

@online{alexanderBillionscaleSemanticSimilarity2020,
  title = {Billion-Scale Semantic Similarity Search with {{FAISS}}+{{SBERT}}},
  author = {Alexander, Mathew},
  date = {2020-10-18T21:08:50},
  url = {https://towardsdatascience.com/billion-scale-semantic-similarity-search-with-faiss-sbert-c845614962e2},
  urldate = {2022-11-11},
  abstract = {Building the prototype for an intelligent search engine},
  langid = {english},
  organization = {{Medium}},
  file = {/home/skynet3/Zotero/storage/PQCHWQSL/billion-scale-semantic-similarity-search-with-faiss-sbert-c845614962e2.html}
}

@online{alexanderStopConfoundingYourself2014,
  title = {Stop {{Confounding Yourself}}! {{Stop Confounding Yourself}}!},
  author = {Alexander, Scott},
  date = {2014-04-27T01:18:17+00:00},
  url = {https://slatestarcodex.com/2014/04/26/stop-confounding-yourself-stop-confounding-yourself/},
  urldate = {2022-11-11},
  abstract = {As a perk of my job, I get a free subscription to the American Journal of Psychiatry. I am still not used to this. No enraging struggles with paywalls. No “one year embargo on full text\&\#8221…},
  langid = {american},
  organization = {{Slate Star Codex}},
  file = {/home/skynet3/Zotero/storage/BGQFW8LS/stop-confounding-yourself-stop-confounding-yourself.html}
}

@online{AliciaSchepGglabellerShiny,
  title = {{{AliciaSchep}}/Gglabeller: {{Shiny}} Gadget for Labeling Points on Ggplot},
  shorttitle = {{{AliciaSchep}}/Gglabeller},
  url = {https://github.com/AliciaSchep/gglabeller},
  urldate = {2022-11-11},
  abstract = {Shiny gadget for labeling points on ggplot. Contribute to AliciaSchep/gglabeller development by creating an account on GitHub.},
  langid = {english},
  organization = {{GitHub}},
  file = {/home/skynet3/Zotero/storage/IGI2HVHW/gglabeller.html}
}

@article{allenResearchNoteExamining2021,
  title = {Research Note: {{Examining}} Potential Bias in Large-Scale Censored Data},
  shorttitle = {Research Note},
  author = {Allen, Jennifer and Mobius, Markus and Rothschild, David M. and Watts, Duncan J.},
  date = {2021-07-26},
  journaltitle = {Harvard Kennedy School Misinformation Review},
  doi = {10.37016/mr-2020-74},
  url = {https://misinforeview.hks.harvard.edu/article/research-note-examining-potential-bias-in-large-scale-censored-data/},
  urldate = {2022-11-11},
  abstract = {We examine potential bias in Facebook’s 10-trillion cell URLs dataset, consisting of URLs shared on its platform and their engagement metrics. Despite the unprecedented size of the dataset, it was altered to protect user privacy in two ways: 1) by adding differentially private noise to engagement counts, and 2) by censoring the data with a},
  langid = {american},
  file = {/home/skynet3/Zotero/storage/G8YYK5LT/Allen et al. - 2021 - Research note Examining potential bias in large-s.pdf;/home/skynet3/Zotero/storage/44HVGRJV/research-note-examining-potential-bias-in-large-scale-censored-data.html}
}

@misc{alonBoostingSimpleLearners2022,
  title = {Boosting {{Simple Learners}}},
  author = {Alon, Noga and Gonen, Alon and Hazan, Elad and Moran, Shay},
  date = {2022-09-22},
  number = {arXiv:2001.11704},
  eprint = {2001.11704},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2001.11704},
  url = {http://arxiv.org/abs/2001.11704},
  urldate = {2022-11-11},
  abstract = {Boosting is a celebrated machine learning approach which is based on the idea of combining weak and moderately inaccurate hypotheses to a strong and accurate one. We study boosting under the assumption that the weak hypotheses belong to a class of bounded capacity. This assumption is inspired by the common convention that weak hypotheses are "rules-of-thumbs" from an "easy-to-learn class". (Schapire and Freund\textasciitilde '12, Shalev-Shwartz and Ben-David '14.) Formally, we assume the class of weak hypotheses has a bounded VC dimension. We focus on two main questions: (i) Oracle Complexity: How many weak hypotheses are needed to produce an accurate hypothesis? We design a novel boosting algorithm and demonstrate that it circumvents a classical lower bound by Freund and Schapire ('95, '12). Whereas the lower bound shows that \$\textbackslash Omega(\{1\}/\{\textbackslash gamma\^2\})\$ weak hypotheses with \$\textbackslash gamma\$-margin are sometimes necessary, our new method requires only \$\textbackslash tilde\{O\}(\{1\}/\{\textbackslash gamma\})\$ weak hypothesis, provided that they belong to a class of bounded VC dimension. Unlike previous boosting algorithms which aggregate the weak hypotheses by majority votes, the new boosting algorithm uses more complex ("deeper") aggregation rules. We complement this result by showing that complex aggregation rules are in fact necessary to circumvent the aforementioned lower bound. (ii) Expressivity: Which tasks can be learned by boosting weak hypotheses from a bounded VC class? Can complex concepts that are "far away" from the class be learned? Towards answering the first question we \{introduce combinatorial-geometric parameters which capture expressivity in boosting.\} As a corollary we provide an affirmative answer to the second question for well-studied classes, including half-spaces and decision stumps. Along the way, we establish and exploit connections with Discrepancy Theory.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/skynet3/Zotero/storage/VK8F86Y9/Alon et al. - 2022 - Boosting Simple Learners.pdf;/home/skynet3/Zotero/storage/BA3TNJHF/2001.html}
}

@article{alquierMatrixFactorizationMultivariate2019,
  title = {Matrix Factorization for Multivariate Time Series Analysis},
  author = {Alquier, Pierre and Marie, Nicolas},
  date = {2019-01-01},
  journaltitle = {Electronic Journal of Statistics},
  shortjournal = {Electron. J. Statist.},
  volume = {13},
  number = {2},
  eprint = {1903.05589},
  eprinttype = {arxiv},
  primaryclass = {math, stat},
  issn = {1935-7524},
  doi = {10.1214/19-EJS1630},
  url = {http://arxiv.org/abs/1903.05589},
  urldate = {2022-11-11},
  abstract = {Matrix factorization is a powerful data analysis tool. It has been used in multivariate time series analysis, leading to the decomposition of the series in a small set of latent factors. However, little is known on the statistical performances of matrix factorization for time series. In this paper, we extend the results known for matrix estimation in the i.i.d setting to time series. Moreover, we prove that when the series exhibit some additional structure like periodicity or smoothness, it is possible to improve on the classical rates of convergence.},
  archiveprefix = {arXiv},
  keywords = {Mathematics - Statistics Theory},
  file = {/home/skynet3/Zotero/storage/WUVBW3Y3/Alquier and Marie - 2019 - Matrix factorization for multivariate time series .pdf;/home/skynet3/Zotero/storage/UZJ79Q4Y/1903.html}
}

@misc{alquierUserfriendlyIntroductionPACBayes2021,
  title = {User-Friendly Introduction to {{PAC-Bayes}} Bounds},
  author = {Alquier, Pierre},
  date = {2021-11-08},
  number = {arXiv:2110.11216},
  eprint = {2110.11216},
  eprinttype = {arxiv},
  primaryclass = {cs, math, stat},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2110.11216},
  urldate = {2022-11-11},
  abstract = {Aggregated predictors are obtained by making a set of basic predictors vote according to some weights, that is, to some probability distribution. Randomized predictors are obtained by sampling in a set of basic predictors, according to some prescribed probability distribution. Thus, aggregated and randomized predictors have in common that they are not defined by a minimization problem, but by a probability distribution on the set of predictors. In statistical learning theory, there is a set of tools designed to understand the generalization ability of such procedures: PAC-Bayesian or PAC-Bayes bounds. Since the original PAC-Bayes bounds of D. McAllester, these tools have been considerably improved in many directions (we will for example describe a simplified version of the localization technique of O. Catoni that was missed by the community, and later rediscovered as "mutual information bounds"). Very recently, PAC-Bayes bounds received a considerable attention: for example there was workshop on PAC-Bayes at NIPS 2017, "(Almost) 50 Shades of Bayesian Learning: PAC-Bayesian trends and insights", organized by B. Guedj, F. Bach and P. Germain. One of the reason of this recent success is the successful application of these bounds to neural networks by G. Dziugaite and D. Roy. An elementary introduction to PAC-Bayes theory is still missing. This is an attempt to provide such an introduction.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Mathematics - Statistics Theory,Statistics - Machine Learning},
  file = {/home/skynet3/Zotero/storage/Z8WDLFC7/Alquier - 2021 - User-friendly introduction to PAC-Bayes bounds.pdf;/home/skynet3/Zotero/storage/Q9R9LPI9/2110.html}
}

@misc{alvarez-melisRobustnessInterpretabilityMethods2018,
  title = {On the {{Robustness}} of {{Interpretability Methods}}},
  author = {Alvarez-Melis, David and Jaakkola, Tommi S.},
  date = {2018-06-20},
  number = {arXiv:1806.08049},
  eprint = {1806.08049},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1806.08049},
  url = {http://arxiv.org/abs/1806.08049},
  urldate = {2022-11-11},
  abstract = {We argue that robustness of explanations---i.e., that similar inputs should give rise to similar explanations---is a key desideratum for interpretability. We introduce metrics to quantify robustness and demonstrate that current methods do not perform well according to these metrics. Finally, we propose ways that robustness can be enforced on existing interpretability approaches.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/skynet3/Zotero/storage/JP84WYHN/Alvarez-Melis and Jaakkola - 2018 - On the Robustness of Interpretability Methods.pdf;/home/skynet3/Zotero/storage/Y8TFFMBM/1806.html}
}

@misc{alvarezBayesianInferenceCovariance2016,
  title = {Bayesian Inference for a Covariance Matrix},
  author = {Alvarez, Ignacio and Niemi, Jarad and Simpson, Matt},
  date = {2016-07-08},
  number = {arXiv:1408.4050},
  eprint = {1408.4050},
  eprinttype = {arxiv},
  primaryclass = {stat},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/1408.4050},
  urldate = {2022-11-11},
  abstract = {Covariance matrix estimation arises in multivariate problems including multivariate normal sampling models and regression models where random effects are jointly modeled, e.g. random-intercept, random-slope models. A Bayesian analysis of these problems requires a prior on the covariance matrix. Here we assess, through a simulation study and a real data set, the impact this prior choice has on posterior inference of the covariance matrix. Inverse Wishart distribution is the natural choice for a covariance matrix prior because its conjugacy on normal model and simplicity, is usually available in Bayesian statistical software. However inverse Wishart distribution presents some undesirable properties from a modeling point of view. It can be too restrictive because assume the same amount of prior information about every variance parameters and, more important, it shows a prior relationship between the variances and correlations. Some alternatives distributions has been proposed. The scaled inverse Wishart distribution, which give more flexibility on the variance priors conserving the conjugacy property but does not eliminate the prior relationship between variances and correlations. Secondly, it is possible to fit separate priors for individual correlations and standard deviations. This strategy eliminates any prior relationship within the covariance matrix parameters, but it is not conjugate and therefore computationally slow.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Methodology},
  file = {/home/skynet3/Zotero/storage/NN5FJCCU/Alvarez et al. - 2016 - Bayesian inference for a covariance matrix.pdf;/home/skynet3/Zotero/storage/TQH2VC7W/1408.html}
}

@article{alvarezHowNotReproduce2022,
  title = {How ({{Not}}) to {{Reproduce}}: {{Practical Considerations}} to {{Improve Research Transparency}} in {{Political Science}}},
  shorttitle = {How ({{Not}}) to {{Reproduce}}},
  author = {Alvarez, R. Michael and Heuberger, Simon},
  date = {2022-01},
  journaltitle = {PS: Political Science \& Politics},
  volume = {55},
  number = {1},
  pages = {149--154},
  publisher = {{Cambridge University Press}},
  issn = {1049-0965, 1537-5935},
  doi = {10.1017/S1049096521001062},
  url = {https://www.cambridge.org/core/journals/ps-political-science-and-politics/article/abs/how-not-to-reproduce-practical-considerations-to-improve-research-transparency-in-political-science/32E7CF5D975C081BA666D3BD475D7913},
  urldate = {2022-11-11},
  abstract = {In recent years, scholars, journals, and professional organizations in political science have been working to improve research transparency. Although better transparency is a laudable goal, the implementation of standards for reproducibility still leaves much to be desired. This article identifies two practices that political science should adopt to improve research transparency: (1) journals must provide detailed replication guidance and run provided material; and (2) authors must begin their work with replication in mind. We focus on problems that occur when scholars provide research materials to journals for replication, and we outline best practices regarding documentation and code structure for researchers to use.},
  langid = {english},
  file = {/home/skynet3/Zotero/storage/M7J72LMN/32E7CF5D975C081BA666D3BD475D7913.html}
}

@article{alvarezResearchReplicationPractical2018,
  title = {Research {{Replication}}: {{Practical Considerations}}},
  shorttitle = {Research {{Replication}}},
  author = {Alvarez, R. Michael and Key, Ellen M. and Núñez, Lucas},
  date = {2018-04},
  journaltitle = {PS: Political Science \& Politics},
  volume = {51},
  number = {2},
  pages = {422--426},
  publisher = {{Cambridge University Press}},
  issn = {1049-0965, 1537-5935},
  doi = {10.1017/S1049096517002566},
  url = {https://www.cambridge.org/core/journals/ps-political-science-and-politics/article/research-replication-practical-considerations/B744967268CDAA3F44103AA5C8539EA2},
  urldate = {2022-11-11},
  abstract = {With the discipline’s push toward data access and research transparency (DA-RT), journal replication archives are becoming increasingly common. As researchers work to ensure that replication materials are provided, they also should pay attention to the content—rather than simply the provision—of journal archives. Based on our experience in analyzing and handling journal replication materials, we present a series of recommendations that can make them easier to understand and use. The provision of clear, functional, and well-documented replication materials is key for achieving the goals of transparent and replicable research. Furthermore, good replication materials enhance the development of extensions and related research by making state-of-the-art methodologies and analyses more accessible.},
  langid = {english},
  file = {/home/skynet3/Zotero/storage/S2PQD646/Alvarez et al. - 2018 - Research Replication Practical Considerations.pdf;/home/skynet3/Zotero/storage/WUYNQI98/B744967268CDAA3F44103AA5C8539EA2.html}
}

@online{AmericanUniversityWashington,
  title = {American {{University Washington D}}.{{C}}.},
  url = {https://www.american.edu/spa/data-science/upload/authors_how_to.pdf},
  urldate = {2022-11-11},
  file = {/home/skynet3/Zotero/storage/AXD6MZJM/authors_how_to.html}
}

@article{aminikhanghahiSurveyMethodsTime2017,
  title = {A {{Survey}} of {{Methods}} for {{Time Series Change Point Detection}}},
  author = {Aminikhanghahi, Samaneh and Cook, Diane J.},
  date = {2017-05},
  journaltitle = {Knowledge and information systems},
  shortjournal = {Knowl Inf Syst},
  volume = {51},
  number = {2},
  eprint = {28603327},
  eprinttype = {pmid},
  pages = {339--367},
  issn = {0219-1377},
  doi = {10.1007/s10115-016-0987-z},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5464762/},
  urldate = {2022-11-11},
  abstract = {Change points are abrupt variations in time series data. Such abrupt changes may represent transitions that occur between states. Detection of change points is useful in modelling and prediction of time series and is found in application areas such as medical condition monitoring, climate change detection, speech and image analysis, and human activity analysis. This survey article enumerates, categorizes, and compares many of the methods that have been proposed to detect change points in time series. The methods examined include both supervised and unsupervised algorithms that have been introduced and evaluated. We introduce several criteria to compare the algorithms. Finally, we present some grand challenges for the community to consider.},
  pmcid = {PMC5464762},
  file = {/home/skynet3/Zotero/storage/RYPIID46/Aminikhanghahi and Cook - 2017 - A Survey of Methods for Time Series Change Point D.pdf}
}

@online{AmortizedCausalDiscovery2020,
  title = {Amortized {{Causal Discovery}}: {{Learning}} to {{Infer Causal Graphs}} from {{Time-Series Data}}},
  shorttitle = {Amortized {{Causal Discovery}}},
  date = {2020-06-18T19:59:12+00:00},
  url = {https://deepai.org/publication/amortized-causal-discovery-learning-to-infer-causal-graphs-from-time-series-data},
  urldate = {2022-11-11},
  abstract = {06/18/20 - Standard causal discovery methods must fit a new model whenever they encounter samples from a new underlying causal graph. However...},
  organization = {{DeepAI}},
  file = {/home/skynet3/Zotero/storage/YBYKBHNE/amortized-causal-discovery-learning-to-infer-causal-graphs-from-time-series-data.html}
}

@report{amrheinEarthFlat052017,
  ids = {amrheinEarthFlat052017a},
  title = {The Earth Is Flat (P{$>$}0.05): {{Significance}} Thresholds and the Crisis of Unreplicable Research},
  shorttitle = {The Earth Is Flat (P{$>$}0.05)},
  author = {Amrhein, Valentin and Korner-Nievergelt, Fränzi and Roth, Tobias},
  date = {2017-06-14},
  number = {e2921v2},
  institution = {{PeerJ Inc.}},
  issn = {2167-9843},
  doi = {10.7287/peerj.preprints.2921v2},
  url = {https://peerj.com/preprints/2921},
  urldate = {2022-11-11},
  abstract = {The widespread use of 'statistical significance' as a license for making a claim of a scientific finding leads to considerable distortion of the scientific process (according to the American Statistical Association). We review why degrading p-values into 'significant' and 'nonsignificant' contributes to making studies irreproducible, or to making them seem irreproducible. A major problem is that we tend to take small p-values at face value, but mistrust results with larger p-values. In either case, p-values tell little about reliability of research, because they are hardly replicable even if an alternative hypothesis is true. Also significance (p≤0.05) is hardly replicable: at a good statistical power of 80\%, two studies will be 'conflicting', meaning that one is significant and the other is not, in one third of the cases if there is a true effect. A replication can therefore not be interpreted as having failed only because it is nonsignificant. Many apparent replication failures may thus reflect faulty judgment based on significance thresholds rather than a crisis of unreplicable research. Reliable conclusions on replicability and practical importance of a finding can only be drawn using cumulative evidence from multiple independent studies. However, applying significance thresholds makes cumulative knowledge unreliable. One reason is that with anything but ideal statistical power, significant effect sizes will be biased upwards. Interpreting inflated significant results while ignoring nonsignificant results will thus lead to wrong conclusions. But current incentives to hunt for significance lead to selective reporting and to publication bias against nonsignificant findings. Data dredging, p-hacking, and publication bias should be addressed by removing fixed significance thresholds. Consistent with the recommendations of the late Ronald Fisher, p-values should be interpreted as graded measures of the strength of evidence against the null hypothesis. Also larger p-values offer some evidence against the null hypothesis, and they cannot be interpreted as supporting the null hypothesis, falsely concluding that 'there is no effect'. Information on possible true effect sizes that are compatible with the data must be obtained from the point estimate, e.g., from a sample average, and from the interval estimate, such as a confidence interval. We review how confusion about interpretation of larger p-values can be traced back to historical disputes among the founders of modern statistics. We further discuss potential arguments against removing significance thresholds, for example that decision rules should rather be more stringent, that sample sizes could decrease, or that p-values should better be completely abandoned. We conclude that whatever method of statistical inference we use, dichotomous threshold thinking must give way to non-automated informed judgment.},
  langid = {english},
  file = {/home/skynet3/Zotero/storage/2T8NBP2S/Amrhein et al. - 2017 - The earth is flat (p0.05) Significance threshold.pdf;/home/skynet3/Zotero/storage/D6FSUNAU/2921.html}
}

@online{analyticsBringingMoreCausality2022,
  title = {Bringing More Causality to Analytics},
  author = {Analytics, Motif},
  date = {2022-10-11T17:06:04},
  url = {https://motifanalytics.medium.com/bringing-more-causality-to-analytics-d378108bb15},
  urldate = {2022-11-11},
  abstract = {I’ve spent a lot of my career trying to convince folks that taking causality seriously is the most reliable way to drive business impact with data. Models, estimates, and analyses that we can’t…},
  langid = {english},
  organization = {{Medium}},
  file = {/home/skynet3/Zotero/storage/JCULB65G/bringing-more-causality-to-analytics-d378108bb15.html}
}

@online{AnalyzingMinardVisualization2014,
  title = {Analyzing {{Minard}}'s {{Visualization Of Napoleon}}'s 1812 {{March}}},
  date = {2014-06-08},
  url = {https://thoughtbot.com/blog/analyzing-minards-visualization-of-napoleons-1812-march},
  urldate = {2022-11-11},
  abstract = {In The Visual Display of Quantitative Information, Edward Tufte calls...},
  langid = {english},
  organization = {{thoughtbot}},
  file = {/home/skynet3/Zotero/storage/VYICFEP9/analyzing-minards-visualization-of-napoleons-1812-march.html}
}

@misc{andersonSuperbloomBloomFilter2020,
  title = {Superbloom: {{Bloom}} Filter Meets {{Transformer}}},
  shorttitle = {Superbloom},
  author = {Anderson, John and Huang, Qingqing and Krichene, Walid and Rendle, Steffen and Zhang, Li},
  date = {2020-02-11},
  number = {arXiv:2002.04723},
  eprint = {2002.04723},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2002.04723},
  url = {http://arxiv.org/abs/2002.04723},
  urldate = {2022-11-11},
  abstract = {We extend the idea of word pieces in natural language models to machine learning tasks on opaque ids. This is achieved by applying hash functions to map each id to multiple hash tokens in a much smaller space, similarly to a Bloom filter. We show that by applying a multi-layer Transformer to these Bloom filter digests, we are able to obtain models with high accuracy. They outperform models of a similar size without hashing and, to a large degree, models of a much larger size trained using sampled softmax with the same computational budget. Our key observation is that it is important to use a multi-layer Transformer for Bloom filter digests to remove ambiguity in the hashed input. We believe this provides an alternative method to solving problems with large vocabulary size.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/skynet3/Zotero/storage/GMV4SMP4/Anderson et al. - 2020 - Superbloom Bloom filter meets Transformer.pdf;/home/skynet3/Zotero/storage/JI6TC7XE/2002.html}
}

@online{andreamanerobastinMarkovChainMonte2021,
  title = {Markov {{Chain Monte Carlo Methods}} for {{Bayesian Data Analysis}} in {{Astronomy}} - {{DataScienceCentral}}.Com},
  author = {AndreaManeroBastin},
  date = {2021-01-06T10:50:00+00:00},
  url = {https://www.datasciencecentral.com/markov-chain-monte-carlo-methods-for-bayesian-data-analysis-in/},
  urldate = {2022-11-11},
  abstract = {This article was written by Sanjib Sharma. Markov Chain Monte Carlo based Bayesian data analysis has now become the method of choice for analyzing and interpreting data in almost all disciplines of science. In astronomy, over the last decade, we have also seen a steady increase in the number of papers that employ Monte Carlo…~Read More »Markov Chain Monte Carlo Methods for Bayesian Data Analysis in Astronomy},
  langid = {american},
  organization = {{Data Science Central}},
  file = {/home/skynet3/Zotero/storage/SAPMJZ7J/markov-chain-monte-carlo-methods-for-bayesian-data-analysis-in.html}
}

@article{andrewsIdentificationCorrectionPublication2019,
  title = {Identification of and {{Correction}} for {{Publication Bias}}},
  author = {Andrews, Isaiah and Kasy, Maximilian},
  date = {2019-08},
  journaltitle = {American Economic Review},
  volume = {109},
  number = {8},
  pages = {2766--2794},
  issn = {0002-8282},
  doi = {10.1257/aer.20180310},
  url = {https://www.aeaweb.org/articles?id=10.1257/aer.20180310},
  urldate = {2022-11-11},
  abstract = {Some empirical results are more likely to be published than others. Selective publication leads to biased estimates and distorted inference. We propose two approaches for identifying the conditional probability of publication as a function of a study's results, the first based on systematic replication studies and the second on meta-studies. For known conditional publication probabilities, we propose bias-corrected estimators and confidence sets. We apply our methods to recent replication studies in experimental economics and psychology, and to a meta-study on the effect of the minimum wage. When replication and meta-study data are available, we find similar results from both.},
  langid = {english},
  keywords = {Estimation: General; Design of Experiments: General; Higher Education,Media,Research Institutions; Labor Demand; Wages; Compensation; and Labor Costs: Public Policy; Entertainment},
  file = {/home/skynet3/Zotero/storage/RTRQJCWG/Andrews and Kasy - 2019 - Identification of and Correction for Publication B.pdf}
}

@misc{angelopoulosGentleIntroductionConformal2022,
  ids = {angelopoulosGentleIntroductionConformal2022a},
  title = {A {{Gentle Introduction}} to {{Conformal Prediction}} and {{Distribution-Free Uncertainty Quantification}}},
  author = {Angelopoulos, Anastasios N. and Bates, Stephen},
  date = {2022-09-03},
  number = {arXiv:2107.07511},
  eprint = {2107.07511},
  eprinttype = {arxiv},
  primaryclass = {cs, math, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2107.07511},
  url = {http://arxiv.org/abs/2107.07511},
  urldate = {2022-11-11},
  abstract = {Black-box machine learning models are now routinely used in high-risk settings, like medical diagnostics, which demand uncertainty quantification to avoid consequential model failures. Conformal prediction is a user-friendly paradigm for creating statistically rigorous uncertainty sets/intervals for the predictions of such models. Critically, the sets are valid in a distribution-free sense: they possess explicit, non-asymptotic guarantees even without distributional assumptions or model assumptions. One can use conformal prediction with any pre-trained model, such as a neural network, to produce sets that are guaranteed to contain the ground truth with a user-specified probability, such as 90\%. It is easy-to-understand, easy-to-use, and general, applying naturally to problems arising in the fields of computer vision, natural language processing, deep reinforcement learning, and so on. This hands-on introduction is aimed to provide the reader a working understanding of conformal prediction and related distribution-free uncertainty quantification techniques with one self-contained document. We lead the reader through practical theory for and examples of conformal prediction and describe its extensions to complex machine learning tasks involving structured outputs, distribution shift, time-series, outliers, models that abstain, and more. Throughout, there are many explanatory illustrations, examples, and code samples in Python. With each code sample comes a Jupyter notebook implementing the method on a real-data example; the notebooks can be accessed and easily run using our codebase.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Mathematics - Statistics Theory,Statistics - Machine Learning,Statistics - Methodology},
  file = {/home/skynet3/Zotero/storage/R44TL77M/Angelopoulos and Bates - 2022 - A Gentle Introduction to Conformal Prediction and .pdf;/home/skynet3/Zotero/storage/4VNQFVZV/2107.html}
}

@article{angristIdentificationCausalEffects1996,
  title = {Identification of {{Causal Effects Using Instrumental Variables}}},
  author = {Angrist, Joshua D. and Imbens, Guido W. and Rubin, Donald B.},
  date = {1996-06},
  journaltitle = {Journal of the American Statistical Association},
  shortjournal = {Journal of the American Statistical Association},
  volume = {91},
  number = {434},
  pages = {444--455},
  issn = {0162-1459, 1537-274X},
  doi = {10.1080/01621459.1996.10476902},
  url = {http://www.tandfonline.com/doi/abs/10.1080/01621459.1996.10476902},
  urldate = {2022-11-11},
  langid = {english},
  file = {/home/skynet3/Zotero/storage/BJKLT3ZR/Angrist et al. - 1996 - Identification of Causal Effects Using Instrumenta.pdf}
}

@misc{anilFactoryFloorML2022,
  ids = {anilFactoryFloorML2022a},
  title = {On the {{Factory Floor}}: {{ML Engineering}} for {{Industrial-Scale Ads Recommendation Models}}},
  shorttitle = {On the {{Factory Floor}}},
  author = {Anil, Rohan and Gadanho, Sandra and Huang, Da and Jacob, Nijith and Li, Zhuoshu and Lin, Dong and Phillips, Todd and Pop, Cristina and Regan, Kevin and Shamir, Gil I. and Shivanna, Rakesh and Yan, Qiqi},
  date = {2022-09-12},
  number = {arXiv:2209.05310},
  eprint = {2209.05310},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2209.05310},
  url = {http://arxiv.org/abs/2209.05310},
  urldate = {2022-11-11},
  abstract = {For industrial-scale advertising systems, prediction of ad click-through rate (CTR) is a central problem. Ad clicks constitute a significant class of user engagements and are often used as the primary signal for the usefulness of ads to users. Additionally, in cost-per-click advertising systems where advertisers are charged per click, click rate expectations feed directly into value estimation. Accordingly, CTR model development is a significant investment for most Internet advertising companies. Engineering for such problems requires many machine learning (ML) techniques suited to online learning that go well beyond traditional accuracy improvements, especially concerning efficiency, reproducibility, calibration, credit attribution. We present a case study of practical techniques deployed in Google's search ads CTR model. This paper provides an industry case study highlighting important areas of current ML research and illustrating how impactful new ML methods are evaluated and made useful in a large-scale industrial setting.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Information Retrieval,Computer Science - Machine Learning},
  file = {/home/skynet3/Zotero/storage/CWDJWPTV/Anil et al. - 2022 - On the Factory Floor ML Engineering for Industria.pdf;/home/skynet3/Zotero/storage/72ZTP8MW/2209.html}
}

@online{applicationsCausalReasoningFundamentals,
  ids = {applicationsCausalReasoningFundamentalsa},
  title = {Causal {{Reasoning}}: {{Fundamentals}} and {{Machine Learning Applications}}},
  shorttitle = {Causal {{Reasoning}}},
  author = {Applications, Causal Reasoning: Fundamentals {and} Machine Learning},
  url = {https://causalinference.gitlab.io/book/},
  urldate = {2022-11-11},
  abstract = {Code, tutorials, and resources for causal inference},
  langid = {english},
  organization = {{Getting Started with Causal Inference}},
  file = {/home/skynet3/Zotero/storage/3EHXJVUV/book.html}
}

@online{AppliedEpidemiologyPublic,
  title = {R for Applied Epidemiology and Public Health | {{The Epidemiologist R Handbook}}},
  url = {https://epirhandbook.com/en/index.html},
  urldate = {2022-11-11}
}

@online{AppliedMachineLearning,
  title = {Applied {{Machine Learning}} at {{Facebook}}: {{A Datacenter Infrastructure Perspective}} - {{Meta Research}}},
  shorttitle = {Applied {{Machine Learning}} at {{Facebook}}},
  url = {https://research.facebook.com/publications/applied-machine-learning-at-facebook-a-datacenter-infrastructure-perspective/},
  urldate = {2022-11-11},
  abstract = {Facebook’s machine learning workloads are extremely diverse: services require many different types of models in practice. This paper describes the hardware and software infrastructure that supports machine learning at global scale.},
  langid = {english},
  organization = {{Meta Research}},
  file = {/home/skynet3/Zotero/storage/4PLQY6ZF/applied-machine-learning-at-facebook-a-datacenter-infrastructure-perspective.html}
}

@article{arahAnalyzingSelectionBias2019,
  title = {Analyzing {{Selection Bias}} for {{Credible Causal Inference}}: {{When}} in {{Doubt}}, {{DAG It Out}}},
  shorttitle = {Analyzing {{Selection Bias}} for {{Credible Causal Inference}}},
  author = {Arah, Onyebuchi A.},
  date = {2019-07},
  journaltitle = {Epidemiology},
  volume = {30},
  number = {4},
  pages = {517--520},
  issn = {1044-3983},
  doi = {10.1097/EDE.0000000000001033},
  url = {https://journals.lww.com/epidem/fulltext/2019/07000/analyzing_selection_bias_for_credible_causal.8.aspx},
  urldate = {2022-11-11},
  abstract = {An abstract is unavailable.},
  langid = {american},
  file = {/home/skynet3/Zotero/storage/SIK6BZIQ/Arah - 2019 - Analyzing Selection Bias for Credible Causal Infer.pdf;/home/skynet3/Zotero/storage/WWRC8ADE/analyzing_selection_bias_for_credible_causal.8.html}
}

@misc{arel-bundockQuantitativePoliticalScience2022,
  title = {Quantitative {{Political Science Research}} Is {{Greatly Underpowered}}},
  author = {Arel-Bundock, Vincent and Briggs, Ryan and Doucouliagos, Hristos and Aviña, Marco Mendoza and Stanley, T. D.},
  date = {2022-07-05T17:24:25},
  publisher = {{OSF Preprints}},
  doi = {10.31219/osf.io/7vy2f},
  url = {https://osf.io/7vy2f/},
  urldate = {2022-11-15},
  abstract = {We analyze the statistical power of political science research by collating over 16,000 hypothesis tests from about 2,000 articles. Even with generous assumptions, the median analysis has about 10\% power, and only about 1 in 10 tests have at least 80\% power to detect the consensus effects reported in the literature. There is also substantial heterogeneity in tests across research areas, with some being characterized by high-power but most having very low power. To contextualize our findings, we survey political methodologists to assess their expectations about power levels. Most methodologists greatly overestimate the statistical power of political science research.},
  langid = {american},
  keywords = {meta-analysis,Models and Methods,political science,Political Science,Social and Behavioral Sciences,statistical power},
  file = {/home/skynet3/Zotero/storage/BJ2XFIN4/Arel-Bundock et al. - 2022 - Quantitative Political Science Research is Greatly.pdf}
}

@article{arel-bundockVincentArelBundockDistributionFree2022,
  title = {Vincent {{Arel-Bundock}}: {{Distribution-Free Prediction Intervals}} with {{Conformal Inference}} Using `{{R}}`},
  shorttitle = {Vincent {{Arel-Bundock}}},
  author = {Arel-Bundock, Vincent},
  date = {2022-09-09},
  url = {http://arelbundock.com/posts/conformal/},
  urldate = {2022-11-11},
  file = {/home/skynet3/Zotero/storage/JGANNCBJ/conformal.html}
}

@misc{arjovskyInvariantRiskMinimization2020,
  title = {Invariant {{Risk Minimization}}},
  author = {Arjovsky, Martin and Bottou, Léon and Gulrajani, Ishaan and Lopez-Paz, David},
  date = {2020-03-27},
  number = {arXiv:1907.02893},
  eprint = {1907.02893},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1907.02893},
  url = {http://arxiv.org/abs/1907.02893},
  urldate = {2022-11-15},
  abstract = {We introduce Invariant Risk Minimization (IRM), a learning paradigm to estimate invariant correlations across multiple training distributions. To achieve this goal, IRM learns a data representation such that the optimal classifier, on top of that data representation, matches for all training distributions. Through theory and experiments, we show how the invariances learned by IRM relate to the causal structures governing the data and enable out-of-distribution generalization.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/skynet3/Zotero/storage/QVGAMGFM/Arjovsky et al. - 2020 - Invariant Risk Minimization.pdf;/home/skynet3/Zotero/storage/FM4ISGMK/1907.html}
}

@misc{arkhangelskyRolePropensityScore2022,
  title = {The {{Role}} of the {{Propensity Score}} in {{Fixed Effect Models}}},
  author = {Arkhangelsky, Dmitry and Imbens, Guido},
  date = {2022-04-12},
  number = {arXiv:1807.02099},
  eprint = {1807.02099},
  eprinttype = {arxiv},
  primaryclass = {econ, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1807.02099},
  url = {http://arxiv.org/abs/1807.02099},
  urldate = {2022-11-11},
  abstract = {We develop a new approach for estimating average treatment effects in observational studies with unobserved group-level heterogeneity. We consider a general model with group-level unconfoundedness and provide conditions under which aggregate statistics -- group-level averages of functions of treatments and covariates -- are sufficient to eliminate differences between groups. Building on these results, we generalize commonly used linear fixed-effect regression estimators in three ways. First, we allow researchers to explicitly select sufficient statistics themselves, whereas the standard specifications make this choice implicit. Second, we suggest using flexible adjustments for sufficient statistics. Finally, we propose robustifying the regression estimators using inverse propensity score weighting. In practice, we recommend researchers use all three modifications.},
  archiveprefix = {arXiv},
  keywords = {Economics - Econometrics,Statistics - Methodology},
  file = {/home/skynet3/Zotero/storage/GKLE5D9G/Arkhangelsky and Imbens - 2022 - The Role of the Propensity Score in Fixed Effect M.pdf;/home/skynet3/Zotero/storage/AYJ8X8S2/1807.html}
}

@misc{armstrongCausalityReduxEvolution2022,
  type = {SSRN Scholarly Paper},
  title = {Causality {{Redux}}: {{The Evolution}} of {{Empirical Methods}} in {{Accounting Research}} and the {{Growth}} of {{Quasi-Experiments}}},
  shorttitle = {Causality {{Redux}}},
  author = {Armstrong, Chris and Kepler, John D. and Samuels, Delphine and Taylor, Daniel J.},
  date = {2022-05-14},
  number = {3935088},
  location = {{Rochester, NY}},
  doi = {10.2139/ssrn.3935088},
  url = {https://papers.ssrn.com/abstract=3935088},
  urldate = {2022-11-11},
  abstract = {This paper reviews the empirical methods used in the accounting literature to draw causal inferences. Recent years have seen a burgeoning growth in the use of methods that seek to exploit as-if random variation in observational settings—i.e., “quasi-experiments.” We provide a synthesis of the major assumptions of these methods, discuss several practical considerations relevant to the application of these methods in the accounting literature, and provide a framework for thinking about whether and when quasi-experimental and non-experimental methods are well-suited for addressing causal questions of interest to accounting researchers. While there is growing interest in addressing causal questions within the literature, we caution against the idea that one should restrict attention to only those causal questions for which there are quasi-experiments. We offer a complementary approach for addressing causal questions that does not rely on the availability of a quasi-experiment, but rather relies on a combination of economic theory, developing and falsifying alternative explanations, triangulating results across multiple settings, measures, and research designs, and caveating results where appropriate.},
  langid = {english},
  keywords = {Causality Redux: The Evolution of Empirical Methods in Accounting Research and the Growth of Quasi-Experiments,Chris Armstrong,Daniel J. Taylor,Delphine Samuels,John D. Kepler,SSRN},
  file = {/home/skynet3/Zotero/storage/8NY576K2/Armstrong et al. - 2022 - Causality Redux The Evolution of Empirical Method.pdf;/home/skynet3/Zotero/storage/KXJ66QHS/papers.html}
}

@article{armstrongRobustEmpiricalBayes10000,
  title = {Robust {{Empirical Bayes Confidence Intervals}}},
  author = {Armstrong, Timothy B. and Kolesár, Michal and Plagborg-Møller, Mikkel},
  date = {10000},
  journaltitle = {Econometrica},
  file = {/home/skynet3/Zotero/storage/W7ZCEL44/ebci.html}
}

@article{aronowDoesRegressionProduce2016,
  title = {Does {{Regression Produce Representative Estimates}} of {{Causal Effects}}?},
  author = {Aronow, Peter M. and Samii, Cyrus},
  date = {2016},
  journaltitle = {American Journal of Political Science},
  volume = {60},
  number = {1},
  pages = {250--267},
  issn = {1540-5907},
  doi = {10.1111/ajps.12185},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/ajps.12185},
  urldate = {2022-11-11},
  abstract = {With an unrepresentative sample, the estimate of a causal effect may fail to characterize how effects operate in the population of interest. What is less well understood is that conventional estimation practices for observational studies may produce the same problem even with a representative sample. Causal effects estimated via multiple regression differentially weight each unit's contribution. The “effective sample” that regression uses to generate the estimate may bear little resemblance to the population of interest, and the results may be nonrepresentative in a manner similar to what quasi-experimental methods or experiments with convenience samples produce. There is no general external validity basis for preferring multiple regression on representative samples over quasi-experimental or experimental methods. We show how to estimate the “multiple regression weights” that allow one to study the effective sample. We discuss alternative approaches that, under certain conditions, recover representative average causal effects. The requisite conditions cannot always be met.},
  langid = {english},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/ajps.12185},
  file = {/home/skynet3/Zotero/storage/73IEM5U6/ajps.html}
}

@inreference{ArrayDataStructure2022,
  title = {Array (Data Structure)},
  booktitle = {Wikipedia},
  date = {2022-10-12T06:27:36Z},
  url = {https://en.wikipedia.org/w/index.php?title=Array_(data_structure)&oldid=1115591814},
  urldate = {2022-11-11},
  abstract = {In computer science, an array is a data structure consisting of a collection of elements (values or variables), each identified by at least one array index or key. An array is stored such that the position of each element can be computed from its index tuple by a mathematical formula. The simplest type of data structure is a linear array, also called one-dimensional array. For example, an array of ten 32-bit (4-byte) integer variables, with indices 0 through 9, may be stored as ten words at memory addresses 2000, 2004, 2008, ..., 2036, (in hexadecimal: 0x7D0, 0x7D4, 0x7D8, ..., 0x7F4) so that the element with index i has the address 2000 + (i × 4). The memory address of the first element of an array is called first address, foundation address, or base address. Because the mathematical concept of a matrix can be represented as a two-dimensional grid, two-dimensional arrays are also sometimes called "matrices". In some cases the term "vector" is used in computing to refer to an array, although tuples rather than vectors are the more mathematically correct equivalent. Tables are often implemented in the form of arrays, especially lookup tables; the word "table" is sometimes used as a synonym of array. Arrays are among the oldest and most important data structures, and are used by almost every program. They are also used to implement many other data structures, such as lists and strings. They effectively exploit the addressing logic of computers. In most modern computers and many external storage devices, the memory is a one-dimensional array of words, whose indices are their addresses. Processors, especially vector processors, are often optimized for array operations. Arrays are useful mostly because the element indices can be computed at run time. Among other things, this feature allows a single iterative statement to process arbitrarily many elements of an array. For that reason, the elements of an array data structure are required to have the same size and should use the same data representation. The set of valid index tuples and the addresses of the elements (and hence the element addressing formula) are usually, but not always, fixed while the array is in use. The term "array" may also refer to an array data type, a kind of data type provided by most high-level programming languages that consists of a collection of values or variables that can be selected by one or more indices computed at run-time. Array types are often implemented by array structures; however, in some languages they may be implemented by hash tables, linked lists, search trees, or other data structures. The term is also used, especially in the description of algorithms, to mean associative array or "abstract array", a theoretical computer science model (an abstract data type or ADT) intended to capture the essential properties of arrays.},
  langid = {english},
  annotation = {Page Version ID: 1115591814}
}

@online{ArrayFunctionRDocumentation,
  title = {Array Function - {{RDocumentation}}},
  url = {https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/array},
  urldate = {2022-11-11}
}

@online{Arxivsanity,
  title = {Arxiv-Sanity},
  url = {https://arxiv-sanity-lite.com/},
  urldate = {2022-11-11}
}

@article{ashworthDesignInferenceStrategic,
  ids = {ashworthDesignInferenceStrategica},
  title = {Design, {{Inference}}, and the {{Strategic Logic}} of {{Suicide Terrorism}}: {{A Rejoinder}}},
  author = {Ashworth, Scott and Clinton, Joshua D and Meirowitz, Adam and Ramsay, Kristopher W},
  pages = {7},
  langid = {english},
  file = {/home/skynet3/Zotero/storage/NYJYFD7R/Ashworth et al. - Design, Inference, and the Strategic Logic of Suic.pdf}
}

@online{AssistedModelBuilding,
  title = {Assisted {{Model Building}}, Using {{Surrogate Black-Box Models}} to {{Train Interpretable Spline Based Additive Models}}},
  url = {https://modeloriented.github.io/xspliner/index.html},
  urldate = {2022-11-11},
  abstract = {Builds generalized linear model with automatic data transformation.    The xspliner helps to build simple, interpretable models that inherits informations provided by more complicated ones.   The resulting model may be treated as explanation of provided black box, that was supplied prior to the algorithm.},
  langid = {english},
  file = {/home/skynet3/Zotero/storage/89MRDHYE/index.html}
}

@misc{atheyApproximateResidualBalancing2018,
  title = {Approximate {{Residual Balancing}}: {{De-Biased Inference}} of {{Average Treatment Effects}} in {{High Dimensions}}},
  shorttitle = {Approximate {{Residual Balancing}}},
  author = {Athey, Susan and Imbens, Guido W. and Wager, Stefan},
  date = {2018-01-31},
  number = {arXiv:1604.07125},
  eprint = {1604.07125},
  eprinttype = {arxiv},
  primaryclass = {econ, math, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1604.07125},
  url = {http://arxiv.org/abs/1604.07125},
  urldate = {2022-11-11},
  abstract = {There are many settings where researchers are interested in estimating average treatment effects and are willing to rely on the unconfoundedness assumption, which requires that the treatment assignment be as good as random conditional on pre-treatment variables. The unconfoundedness assumption is often more plausible if a large number of pre-treatment variables are included in the analysis, but this can worsen the performance of standard approaches to treatment effect estimation. In this paper, we develop a method for de-biasing penalized regression adjustments to allow sparse regression methods like the lasso to be used for sqrt\{n\}-consistent inference of average treatment effects in high-dimensional linear models. Given linearity, we do not need to assume that the treatment propensities are estimable, or that the average treatment effect is a sparse contrast of the outcome model parameters. Rather, in addition standard assumptions used to make lasso regression on the outcome model consistent under 1-norm error, we only require overlap, i.e., that the propensity score be uniformly bounded away from 0 and 1. Procedurally, our method combines balancing weights with a regularized regression adjustment.},
  archiveprefix = {arXiv},
  keywords = {Economics - Econometrics,Mathematics - Statistics Theory,Statistics - Methodology},
  file = {/home/skynet3/Zotero/storage/HEATL4K2/Athey et al. - 2018 - Approximate Residual Balancing De-Biased Inferenc.pdf;/home/skynet3/Zotero/storage/PKE5Y74L/1604.html}
}

@article{auRandomForestsDecision,
  ids = {auRandomForestsDecisiona},
  title = {Random {{Forests}}, {{Decision Trees}}, and {{Categorical Predictors}}: {{The}} “{{Absent Levels}}” {{Problem}}},
  author = {Au, Timothy C},
  pages = {30},
  abstract = {One advantage of decision tree based methods like random forests is their ability to natively handle categorical predictors without having to first transform them (e.g., by using feature engineering techniques). However, in this paper, we show how this capability can lead to an inherent “absent levels” problem for decision tree based methods that has never been thoroughly discussed, and whose consequences have never been carefully explored. This problem occurs whenever there is an indeterminacy over how to handle an observation that has reached a categorical split which was determined when the observation in question’s level was absent during training. Although these incidents may appear to be innocuous, by using Leo Breiman and Adele Cutler’s random forests FORTRAN code and the randomForest R package (Liaw and Wiener, 2002) as motivating case studies, we examine how overlooking the absent levels problem can systematically bias a model. Furthermore, by using three real data examples, we illustrate how absent levels can dramatically alter a model’s performance in practice, and we empirically demonstrate how some simple heuristics can be used to help mitigate the effects of the absent levels problem until a more robust theoretical solution is found.},
  langid = {english},
  file = {/home/skynet3/Zotero/storage/SZYAY966/Au - Random Forests, Decision Trees, and Categorical Pr.pdf}
}

@article{auspurgHasCredibilitySocial2021,
  ids = {auspurgHasCredibilitySocial2021a},
  title = {Has the {{Credibility}} of the {{Social Sciences Been Credibly Destroyed}}? {{Reanalyzing}} the “{{Many Analysts}}, {{One Data Set}}” {{Project}}},
  shorttitle = {Has the {{Credibility}} of the {{Social Sciences Been Credibly Destroyed}}?},
  author = {Auspurg, Katrin and Brüderl, Josef},
  date = {2021-01-01},
  journaltitle = {Socius},
  volume = {7},
  pages = {23780231211024421},
  publisher = {{SAGE Publications}},
  issn = {2378-0231},
  doi = {10.1177/23780231211024421},
  url = {https://doi.org/10.1177/23780231211024421},
  urldate = {2022-11-11},
  abstract = {In 2018, Silberzahn, Uhlmann, Nosek, and colleagues published an article in which 29 teams analyzed the same research question with the same data: Are soccer referees more likely to give red cards to players with dark skin tone than light skin tone? The results obtained by the teams differed extensively. Many concluded from this widely noted exercise that the social sciences are not rigorous enough to provide definitive answers. In this article, we investigate why results diverged so much. We argue that the main reason was an unclear research question: Teams differed in their interpretation of the research question and therefore used diverse research designs and model specifications. We show by reanalyzing the data that with a clear research question, a precise definition of the parameter of interest, and theory-guided causal reasoning, results vary only within a narrow range. The broad conclusion of our reanalysis is that social science research needs to be more precise in its ?estimands? to become credible.},
  langid = {english},
  file = {/home/skynet3/Zotero/storage/F5Q5VBGH/Auspurg and Brüderl - 2021 - Has the Credibility of the Social Sciences Been Cr.pdf}
}

@article{austinAdvancesPropensityScore2020,
  title = {Advances in Propensity Score Analysis},
  author = {Austin, Peter C},
  date = {2020-03-01},
  journaltitle = {Statistical Methods in Medical Research},
  shortjournal = {Stat Methods Med Res},
  volume = {29},
  number = {3},
  pages = {641--643},
  publisher = {{SAGE Publications Ltd STM}},
  issn = {0962-2802},
  doi = {10.1177/0962280219899248},
  url = {https://doi.org/10.1177/0962280219899248},
  urldate = {2022-11-11},
  langid = {english},
  file = {/home/skynet3/Zotero/storage/CRH3EDWC/Austin - 2020 - Advances in propensity score analysis.pdf}
}

@online{AutodidaxJAXCore,
  title = {Autodidax: {{JAX}} Core from Scratch — {{JAX}} Documentation},
  url = {https://jax.readthedocs.io/en/latest/autodidax.html},
  urldate = {2022-11-11},
  file = {/home/skynet3/Zotero/storage/5ALGPPMB/autodidax.html}
}

@online{AutomaticGenerationText,
  title = {Automatic {{Generation}} of {{Text Extraction Patterns}} from {{Examples}}},
  url = {http://regex.inginf.units.it/},
  urldate = {2022-11-11}
}

@online{AutoMLSurveyStateoftheArt2019,
  title = {{{AutoML}}: {{A Survey}} of the {{State-of-the-Art}}},
  shorttitle = {{{AutoML}}},
  date = {2019-08-02T05:56:13+00:00},
  url = {https://deepai.org/publication/automl-a-survey-of-the-state-of-the-art},
  urldate = {2022-11-11},
  abstract = {08/02/19 - Deep learning has penetrated all aspects of our lives and brought us great convenience. However, the process of building a high-qu...},
  organization = {{DeepAI}},
  file = {/home/skynet3/Zotero/storage/VBL9HPVN/automl-a-survey-of-the-state-of-the-art.html}
}

@article{avinIdentifiabilityPathSpecificEffects,
  title = {Identifiability of {{Path-Speciﬁc Effects}}},
  author = {Avin, Chen and Shpitser, Ilya and Pearl, Judea},
  pages = {7},
  abstract = {Counterfactual quantities representing pathspecific effects arise in cases where we are interested in computing the effect of one variable on another only along certain causal paths in the graph (in other words by excluding a set of edges from consideration). A recent paper [Pearl, 2001] details a method by which such an exclusion can be specified formally by fixing the value of the parent node of each excluded edge. In this paper we derive simple, graphical conditions for experimental identifiability of path-specific effects, namely, conditions under which path-specific effects can be estimated consistently from data obtained from controlled experiments.},
  langid = {english},
  file = {/home/skynet3/Zotero/storage/MLRWEDAJ/Avin et al. - Identiﬁability of Path-Speciﬁc Effects.pdf}
}

@software{AwesomePublicDatasets2022,
  title = {Awesome {{Public Datasets}}},
  date = {2022-11-11T23:11:42Z},
  origdate = {2014-11-20T06:20:50Z},
  url = {https://github.com/awesomedata/awesome-public-datasets/blob/42108e137d72c79061117009cdc8089c9b163159/README.rst},
  urldate = {2022-11-11},
  abstract = {A topic-centric list of HQ open datasets.},
  organization = {{AwesomeData}}
}

@online{AwesomepublicdatasetsREADMERst,
  title = {Awesome-Public-Datasets/{{README}}.Rst at Master · Awesomedata/Awesome-Public-Datasets},
  url = {https://github.com/awesomedata/awesome-public-datasets},
  urldate = {2022-11-11},
  abstract = {A topic-centric list of HQ open datasets. Contribute to awesomedata/awesome-public-datasets development by creating an account on GitHub.},
  langid = {english},
  organization = {{GitHub}},
  file = {/home/skynet3/Zotero/storage/FZRXC97Q/README.html}
}

@misc{baadeCrossValidationResiduals2018,
  title = {Cross Validation Residuals for Generalised Least Squares and Other Correlated Data Models},
  author = {Baade, Ingrid Annette},
  date = {2018-09-05},
  number = {arXiv:1809.01319},
  eprint = {1809.01319},
  eprinttype = {arxiv},
  primaryclass = {stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1809.01319},
  url = {http://arxiv.org/abs/1809.01319},
  urldate = {2022-11-11},
  abstract = {Cross validation residuals are well known for the ordinary least squares model. Here leave-M-out cross validation is extended to generalised least squares. The relationship between cross validation residuals and Cook's distance is demonstrated, in terms of an approximation to the difference in the generalised residual sum of squares for a model fit to all the data (training and test) and a model fit to a reduced dataset (training data only). For generalised least squares, as for ordinary least squares, there is no need to refit the model to reduced size datasets as all the values for K fold cross validation are available after fitting the model to all the data.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Machine Learning,Statistics - Methodology},
  file = {/home/skynet3/Zotero/storage/Q98DXFVP/Baade - 2018 - Cross validation residuals for generalised least s.pdf;/home/skynet3/Zotero/storage/KU53EI6B/1809.html}
}

@online{BackpropNotJust,
  ids = {BackpropNotJusta},
  title = {Backprop Is Not Just the Chain Rule — {{Graduate Descent}}},
  url = {http://timvieira.github.io/blog/post/2017/08/18/backprop-is-not-just-the-chain-rule/},
  urldate = {2022-11-11},
  file = {/home/skynet3/Zotero/storage/C7URFTW2/backprop-is-not-just-the-chain-rule.html}
}

@article{bahadoriEndtoEndBalancingCausal,
  ids = {bahadoriEndtoEndBalancingCausala},
  title = {End-to-{{End Balancing}} for {{Causal Continuous Treatment-Effect Estimation}}},
  author = {Bahadori, Mohammad Taha and Tchetgen, Eric J Tchetgen and Heckerman, David E},
  pages = {14},
  abstract = {We study the problem of observational causal inference with continuous treatments in the framework of inverse propensity-score weighting. To obtain stable weights, we design a new algorithm based on entropy balancing that learns weights to directly maximize causal inference accuracy using end-to-end optimization. In the process of optimization, these weights are automatically tuned to the specific dataset and causal inference algorithm being used. We provide a theoretical analysis demonstrating consistency of our approach. Using synthetic and real-world data, we show that our algorithm estimates causal effect more accurately than baseline entropy balancing.},
  langid = {english},
  file = {/home/skynet3/Zotero/storage/9Y6FXKTJ/Bahadori et al. - End-to-End Balancing for Causal Continuous Treatme.pdf}
}

@misc{baileyFinanceNotExcused2021,
  type = {SSRN Scholarly Paper},
  ids = {baileyFinanceNotExcused2021a},
  title = {Finance Is {{Not Excused}}: {{Why Finance Should Not Flout Basic Principles}} of {{Statistics}}},
  shorttitle = {Finance Is {{Not Excused}}},
  author = {Bailey, David H. and Lopez de Prado, Marcos},
  date = {2021-07-28},
  number = {3895330},
  location = {{Rochester, NY}},
  doi = {10.2139/ssrn.3895330},
  url = {https://papers.ssrn.com/abstract=3895330},
  urldate = {2022-11-11},
  abstract = {Several features of financial research make it particularly prone to the occurrence of false discoveries. First, the probability of finding a positive (profitable investment strategy) is very low, due to intense competition. Second, true findings are mostly short-lived, as a result of the non-stationary nature of financial systems. Third, unlike in the natural sciences, it is rarely possible to verify statistical findings through controlled experiments. Finance’s inability to conduct controlled experiments makes it virtually impossible to debunk a false claim. One would hope that, in such a field, researchers would be particularly careful when conducting statistical inference. Sadly, the opposite is true.Tenure-seeking researchers publish thousands of academic articles that promote dubious investment strategies, without controlling for multiple testing. Some of those articles are written for, funded, or promoted by investment firms with a commercial interest. As a consequence, today’s academic finance exhibits some resemblance with medicine’s predicament during the 1950-2000 period, when Big Tobacco paid for thousands of studies in support of their bottom line. Unlike finance, medical journals today impose strict controls for multiple testing. Academic finance’s denial of its replication crisis risks its branding as a pseudoscience.},
  langid = {english},
  keywords = {deflated Sharpe ratio,false discovery rate,Multiple testing,publication bias,selection bias,true positive rate},
  file = {/home/skynet3/Zotero/storage/KMBCVLUM/Bailey and Lopez de Prado - 2021 - Finance is Not Excused Why Finance Should Not Flo.pdf;/home/skynet3/Zotero/storage/DXLF8LAC/papers.html}
}

@incollection{baiSpikeandSlabMeetsLASSO2021,
  title = {Spike-and-{{Slab Meets LASSO}}: {{A Review}} of the {{Spike-and-Slab LASSO}}},
  shorttitle = {Spike-and-{{Slab Meets LASSO}}},
  author = {Bai, Ray and Rockova, Veronika and George, Edward I.},
  date = {2021-12-06},
  eprint = {2010.06451},
  eprinttype = {arxiv},
  primaryclass = {stat},
  pages = {81--108},
  doi = {10.1201/9781003089018-4},
  url = {http://arxiv.org/abs/2010.06451},
  urldate = {2022-11-11},
  abstract = {High-dimensional data sets have become ubiquitous in the past few decades, often with many more covariates than observations. In the frequentist setting, penalized likelihood methods are the most popular approach for variable selection and estimation in high-dimensional data. In the Bayesian framework, spike-and-slab methods are commonly used as probabilistic constructs for high-dimensional modeling. Within the context of linear regression, Rockova and George (2018) introduced the spike-and-slab LASSO (SSL), an approach based on a prior which provides a continuum between the penalized likelihood LASSO and the Bayesian point-mass spike-and-slab formulations. Since its inception, the spike-and-slab LASSO has been extended to a variety of contexts, including generalized linear models, factor analysis, graphical models, and nonparametric regression. The goal of this paper is to survey the landscape surrounding spike-and-slab LASSO methodology. First we elucidate the attractive properties and the computational tractability of SSL priors in high dimensions. We then review methodological developments of the SSL and outline several theoretical developments. We illustrate the methodology on both simulated and real datasets.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Methodology},
  file = {/home/skynet3/Zotero/storage/EYHKYNUF/Bai et al. - 2021 - Spike-and-Slab Meets LASSO A Review of the Spike-.pdf;/home/skynet3/Zotero/storage/PVB7EBRK/2010.html}
}

@article{baker500ScientistsLift2016,
  ids = {baker500ScientistsLift2016a},
  title = {1,500 Scientists Lift the Lid on Reproducibility},
  author = {Baker, Monya},
  date = {2016-05-01},
  journaltitle = {Nature},
  volume = {533},
  number = {7604},
  pages = {452--454},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  doi = {10.1038/533452a},
  url = {https://www.nature.com/articles/533452a},
  urldate = {2022-11-11},
  abstract = {Survey sheds light on the ‘crisis’ rocking research.},
  issue = {7604},
  langid = {english},
  keywords = {Peer review,Publishing,Research management},
  file = {/home/skynet3/Zotero/storage/X4MZSKXY/Baker - 2016 - 1,500 scientists lift the lid on reproducibility.pdf}
}

@misc{bakerHowMuchShould2022,
  type = {SSRN Scholarly Paper},
  ids = {bakerHowMuchShould2022a},
  title = {How {{Much Should We Trust Staggered Difference-In-Differences Estimates}}?},
  author = {Baker, Andrew and Larcker, David F. and Wang, Charles C. Y.},
  date = {2022-01-16},
  number = {3794018},
  location = {{Rochester, NY}},
  doi = {10.2139/ssrn.3794018},
  url = {https://papers.ssrn.com/abstract=3794018},
  urldate = {2022-11-11},
  abstract = {We explain when and how staggered difference-in-differences regression estimators, commonly applied to assess the impact of policy changes, are biased. These biases are likely to be relevant for a large portion of research settings in finance, accounting, and law that rely on staggered treatment timing, and can result in Type-I and Type-II errors. We summarize three alternative estimators developed in the econometrics and applied literature for addressing these biases, including their differences and tradeoffs. We apply these estimators to re-examine prior published results and show, in many cases, the alternative causal estimates or inferences differ substantially from prior papers.},
  langid = {english},
  keywords = {Difference in differences,dynamic treatment effects,generalized difference-in-differences,staggered difference-in-differences designs},
  file = {/home/skynet3/Zotero/storage/ALHKLNYL/Baker et al. - 2022 - How Much Should We Trust Staggered Difference-In-D.pdf;/home/skynet3/Zotero/storage/3527ZJ8J/papers.html}
}

@misc{baldanComplexityMeasuresFeatures2021,
  ids = {baldanComplexityMeasuresFeatures2021a},
  title = {Complexity {{Measures}} and {{Features}} for {{Times Series}} Classification},
  author = {Baldán, Francisco J. and Benítez, José M.},
  date = {2021-10-15},
  number = {arXiv:2002.12036},
  eprint = {2002.12036},
  eprinttype = {arxiv},
  primaryclass = {cs, math, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2002.12036},
  url = {http://arxiv.org/abs/2002.12036},
  urldate = {2022-11-11},
  abstract = {Classification of time series is a growing problem in different disciplines due to the progressive digitalization of the world. Currently, the state-of-the-art in time series classification is dominated by The Hierarchical Vote Collective of Transformation-based Ensembles. This algorithm is composed of several classifiers of different domains distributed in five large modules. The combination of the results obtained by each module weighed based on an internal evaluation process allows this algorithm to obtain the best results in state-of-the-art. One Nearest Neighbour with Dynamic Time Warping remains the base classifier in any time series classification problem for its simplicity and good results. Despite their performance, they share a weakness, which is that they are not interpretable. In the field of time series classification, there is a tradeoff between accuracy and interpretability. In this work, we propose a set of characteristics capable of extracting information on the structure of the time series to face time series classification problems. The use of these characteristics allows the use of traditional classification algorithms in time series problems. The experimental results of our proposal show no statistically significant differences from the second and third best models of the state-of-the-art. Apart from competitive results in accuracy, our proposal is able to offer interpretable results based on the set of characteristics proposed},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Information Theory,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/skynet3/Zotero/storage/CAF6HHFC/Baldán and Benítez - 2021 - Complexity Measures and Features for Times Series .pdf;/home/skynet3/Zotero/storage/PNGDQ424/2002.html}
}

@misc{balestrieroBatchNormalizationExplained2022,
  ids = {balestrieroBatchNormalizationExplained2022a},
  title = {Batch {{Normalization Explained}}},
  author = {Balestriero, Randall and Baraniuk, Richard G.},
  date = {2022-09-29},
  number = {arXiv:2209.14778},
  eprint = {2209.14778},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2209.14778},
  url = {http://arxiv.org/abs/2209.14778},
  urldate = {2022-11-11},
  abstract = {A critically important, ubiquitous, and yet poorly understood ingredient in modern deep networks (DNs) is batch normalization (BN), which centers and normalizes the feature maps. To date, only limited progress has been made understanding why BN boosts DN learning and inference performance; work has focused exclusively on showing that BN smooths a DN's loss landscape. In this paper, we study BN theoretically from the perspective of function approximation; we exploit the fact that most of today's state-of-the-art DNs are continuous piecewise affine (CPA) splines that fit a predictor to the training data via affine mappings defined over a partition of the input space (the so-called "linear regions"). \{\textbackslash em We demonstrate that BN is an unsupervised learning technique that -- independent of the DN's weights or gradient-based learning -- adapts the geometry of a DN's spline partition to match the data.\} BN provides a "smart initialization" that boosts the performance of DN learning, because it adapts even a DN initialized with random weights to align its spline partition with the data. We also show that the variation of BN statistics between mini-batches introduces a dropout-like random perturbation to the partition boundaries and hence the decision boundary for classification problems. This per mini-batch perturbation reduces overfitting and improves generalization by increasing the margin between the training samples and the decision boundary.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computational Geometry,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/skynet3/Zotero/storage/HLNHN3LL/Balestriero and Baraniuk - 2022 - Batch Normalization Explained.pdf;/home/skynet3/Zotero/storage/7IAL4FXX/2209.html}
}

@misc{balestrieroLearningHighDimension2021,
  ids = {balestrieroLearningHighDimension2021a},
  title = {Learning in {{High Dimension Always Amounts}} to {{Extrapolation}}},
  author = {Balestriero, Randall and Pesenti, Jerome and LeCun, Yann},
  date = {2021-10-29},
  number = {arXiv:2110.09485},
  eprint = {2110.09485},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2110.09485},
  url = {http://arxiv.org/abs/2110.09485},
  urldate = {2022-11-11},
  abstract = {The notion of interpolation and extrapolation is fundamental in various fields from deep learning to function approximation. Interpolation occurs for a sample \$x\$ whenever this sample falls inside or on the boundary of the given dataset's convex hull. Extrapolation occurs when \$x\$ falls outside of that convex hull. One fundamental (mis)conception is that state-of-the-art algorithms work so well because of their ability to correctly interpolate training data. A second (mis)conception is that interpolation happens throughout tasks and datasets, in fact, many intuitions and theories rely on that assumption. We empirically and theoretically argue against those two points and demonstrate that on any high-dimensional (\${$>\$$}100) dataset, interpolation almost surely never happens. Those results challenge the validity of our current interpolation/extrapolation definition as an indicator of generalization performances.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/home/skynet3/Zotero/storage/Y4SS83WX/Balestriero et al. - 2021 - Learning in High Dimension Always Amounts to Extra.pdf;/home/skynet3/Zotero/storage/Y8M8437I/2110.html}
}

@book{bansalAdvancedNaturalLanguage2021,
  title = {Advanced {{Natural Language Processing}} with {{TensorFlow}} 2: {{Build}} Effective Real-World {{NLP}} Applications Using {{NER}}, {{RNNs}}, Seq2seq Models, {{Transformers}}, and More},
  shorttitle = {Advanced {{Natural Language Processing}} with {{TensorFlow}} 2},
  author = {Bansal, Ashish},
  date = {2021-02-04},
  publisher = {{Packt Publishing}},
  location = {{Birmingham Mumbai}},
  isbn = {978-1-80020-093-7},
  langid = {english},
  pagetotal = {380}
}

@misc{bansalFastBayesianEstimation2020,
  title = {Fast {{Bayesian Estimation}} of {{Spatial Count Data Models}}},
  author = {Bansal, Prateek and Krueger, Rico and Graham, Daniel J.},
  date = {2020-10-16},
  number = {arXiv:2007.03681},
  eprint = {2007.03681},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2007.03681},
  url = {http://arxiv.org/abs/2007.03681},
  urldate = {2022-11-11},
  abstract = {Spatial count data models are used to explain and predict the frequency of phenomena such as traffic accidents in geographically distinct entities such as census tracts or road segments. These models are typically estimated using Bayesian Markov chain Monte Carlo (MCMC) simulation methods, which, however, are computationally expensive and do not scale well to large datasets. Variational Bayes (VB), a method from machine learning, addresses the shortcomings of MCMC by casting Bayesian estimation as an optimisation problem instead of a simulation problem. Considering all these advantages of VB, a VB method is derived for posterior inference in negative binomial models with unobserved parameter heterogeneity and spatial dependence. P\textbackslash 'olya-Gamma augmentation is used to deal with the non-conjugacy of the negative binomial likelihood and an integrated non-factorised specification of the variational distribution is adopted to capture posterior dependencies. The benefits of the proposed approach are demonstrated in a Monte Carlo study and an empirical application on estimating youth pedestrian injury counts in census tracts of New York City. The VB approach is around 45 to 50 times faster than MCMC on a regular eight-core processor in a simulation and an empirical study, while offering similar estimation and predictive accuracy. Conditional on the availability of computational resources, the embarrassingly parallel architecture of the proposed VB method can be exploited to further accelerate its estimation by up to 20 times.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning,Statistics - Methodology},
  file = {/home/skynet3/Zotero/storage/ZJI3LKNV/Bansal et al. - 2020 - Fast Bayesian Estimation of Spatial Count Data Mod.pdf;/home/skynet3/Zotero/storage/TY93Q5KD/2007.html}
}

@misc{banzhafValueStatisticalLife2021,
  type = {Working Paper},
  title = {The {{Value}} of {{Statistical Life}}: {{A Meta-analysis}} of {{Meta-analyses}}},
  shorttitle = {The {{Value}} of {{Statistical Life}}},
  author = {Banzhaf, H. Spencer},
  date = {2021-08},
  series = {Working {{Paper Series}}},
  number = {29185},
  publisher = {{National Bureau of Economic Research}},
  doi = {10.3386/w29185},
  url = {https://www.nber.org/papers/w29185},
  urldate = {2022-11-11},
  abstract = {The Value of Statistical Life (VSL) is arguably the most important number in benefit-cost analyses of environmental, health, and transportation policies. However, agencies have used a wide range of VSL values. One reason may be the embarrassment of riches when it comes to VSL studies. While meta-analysis is a standard way to synthesize information across studies, we now have multiple competing meta-analyses and reviews. Thus, to analysts, picking one such meta-analysis may feel as hard as picking a single "best study." This paper responds by taking the meta-analysis another step, estimating a meta-analysis (or mixture distribution) of six meta-analyses. The baseline model yields a central VSL of \$7.0m, with a 90\% confidence interval of \$2.4m to \$11.2m. The provided code allows users to easily change subjective weights on the studies, add new studies, or change adjustments for income, inflation, and latency.},
  file = {/home/skynet3/Zotero/storage/UG2TSX7F/Banzhaf - 2021 - The Value of Statistical Life A Meta-analysis of .pdf}
}

@article{barberaAutomatedTextClassification2021,
  title = {Automated {{Text Classification}} of {{News Articles}}: {{A Practical Guide}}},
  shorttitle = {Automated {{Text Classification}} of {{News Articles}}},
  author = {Barberá, Pablo and Boydstun, Amber E. and Linn, Suzanna and McMahon, Ryan and Nagler, Jonathan},
  date = {2021-01},
  journaltitle = {Political Analysis},
  volume = {29},
  number = {1},
  pages = {19--42},
  publisher = {{Cambridge University Press}},
  issn = {1047-1987, 1476-4989},
  doi = {10.1017/pan.2020.8},
  url = {https://www.cambridge.org/core/journals/political-analysis/article/abs/automated-text-classification-of-news-articles-a-practical-guide/10462DB284B1CD80C0FAE796AD786BC6},
  urldate = {2022-11-11},
  abstract = {Automated text analysis methods have made possible the classification of large corpora of text by measures such as topic and tone. Here, we provide a guide to help researchers navigate the consequential decisions they need to make before any measure can be produced from the text. We consider, both theoretically and empirically, the effects of such choices using as a running example efforts to measure the tone of New York Times coverage of the economy. We show that two reasonable approaches to corpus selection yield radically different corpora and we advocate for the use of keyword searches rather than predefined subject categories provided by news archives. We demonstrate the benefits of coding using article segments instead of sentences as units of analysis. We show that, given a fixed number of codings, it is better to increase the number of unique documents coded rather than the number of coders for each document. Finally, we find that supervised machine learning algorithms outperform dictionaries on a number of criteria. Overall, we intend this guide to serve as a reminder to analysts that thoughtfulness and human validation are key to text-as-data methods, particularly in an age when it is all too easy to computationally classify texts without attending to the methodological choices therein.},
  langid = {english},
  keywords = {automated content analysis,content analysis,statistical analysis of texts},
  file = {/home/skynet3/Zotero/storage/DGY4UWWD/Barberá et al. - 2021 - Automated Text Classification of News Articles A .pdf}
}

@online{barDoesItMake2017,
  type = {Forum post},
  title = {Does It Make Sense to Log-Transform the Dependent When Using {{Gradient Boosted Trees}}?},
  author = {Bar},
  date = {2017-02-15},
  url = {https://stats.stackexchange.com/q/262114},
  urldate = {2022-11-11},
  organization = {{Cross Validated}},
  file = {/home/skynet3/Zotero/storage/J4B9EDAU/263753.html}
}

@article{barnardModelingCovarianceMatrices2000,
  title = {Modeling {{Covariance Matrices}} in {{Terms}} of {{Standard Deviations}} and {{Correlations}}, with {{Application}} to {{Shrinkage}}},
  author = {Barnard, John and McCulloch, Robert and Meng, Xiao-Li},
  date = {2000},
  journaltitle = {Statistica Sinica},
  volume = {10},
  number = {4},
  eprint = {24306780},
  eprinttype = {jstor},
  pages = {1281--1311},
  publisher = {{Institute of Statistical Science, Academia Sinica}},
  issn = {1017-0405},
  abstract = {The covariance matrix plays an important role in statistical inference, yet modeling a covariance matrix is often a difficult task in practice due to its dimensionality and the non-negative definite constraint. In order to model a covariance matrix effectively, it is typically broken down into components based on modeling considerations or mathematical convenience. Decompositions that have received recent research attention include variance components, spectral decomposition, Cholesky decomposition, and matrix logarithm. In this paper we study a statistically motivated decomposition which appears to be relatively unexplored for the purpose of modeling. We model a covariance matrix in terms of its corresponding standard deviations and correlation matrix. We discuss two general modeling situations where this approach is useful: shrinkage estimation of regression coefficients, and a general location-scale model for both categorical and continuous variables. We present some simple choices for priors in terms of standard deviations and the correlation matrix, and describe a straightforward computational strategy for obtaining the posterior of the covariance matrix. We apply our method to real and simulated data sets in the context of shrinkage estimation.},
  file = {/home/skynet3/Zotero/storage/9ZV5SUS6/Barnard et al. - 2000 - Modeling Covariance Matrices in Terms of Standard .pdf}
}

@article{bartelsDerivationFrontDoor2022,
  title = {Derivation of Front Door Adjustment without Intervention on the Mediator},
  author = {Bartels, Christian},
  date = {2022-07-09},
  publisher = {{figshare}},
  doi = {10.6084/m9.figshare.20278347.v1},
  url = {https://figshare.com/articles/journal_contribution/Derivation_of_front_door_adjustment_without_intervention_on_the_mediator/20278347/1},
  urldate = {2022-11-11},
  abstract = {A derivation of what is known as front door adjustment in causal inference is presented.The derivation does not use an intervention on the mediator, which in many situations would not be possible in reality, nor does it condition on the unobserved confounder.},
  langid = {english},
  file = {/home/skynet3/Zotero/storage/FCC55W8Q/Bartels - 2022 - Derivation of front door adjustment without interv.pdf;/home/skynet3/Zotero/storage/X89GX9AR/1.html}
}

@online{BashScriptingCheatsheet,
  title = {Bash Scripting Cheatsheet},
  url = {https://devhints.io/bash},
  urldate = {2022-11-11},
  abstract = {Variables · Functions · Interpolation · Brace expansions · Loops · Conditional execution · Command substitution · One-page guide to Bash scripting},
  langid = {english},
  organization = {{Devhints.io cheatsheets}}
}

@article{basuBiasOLSEstimators2020,
  title = {Bias of {{OLS Estimators}} Due to {{Exclusion}} of {{Relevant Variables}} and {{Inclusion}} of {{Irrelevant Variables}}},
  author = {Basu, Deepankar},
  date = {2020-02},
  journaltitle = {Oxford Bulletin of Economics and Statistics},
  shortjournal = {Oxf Bull Econ Stat},
  volume = {82},
  number = {1},
  pages = {209--234},
  issn = {0305-9049, 1468-0084},
  doi = {10.1111/obes.12322},
  url = {https://onlinelibrary.wiley.com/doi/10.1111/obes.12322},
  urldate = {2022-11-11},
  abstract = {In this paper I discuss three issues related to bias of OLS estimators in a general multivariate setting. First, I discuss the bias that arises from omitting relevant variables. I offer a geometric interpretation of such bias and derive sufficient conditions in terms of sign restrictions that allows us to determine the direction of bias. Second, I show that inclusion of some omitted variables will not necessarily reduce the magnitude of OVB as long as some others remain omitted. Third, I show that inclusion of irrelevant variables in a model with omitted variables can also have an impact on the bias of OLS estimators. I use the running example of a simple wage regression to illustrate my arguments.},
  langid = {english},
  file = {/home/skynet3/Zotero/storage/HGEP489J/Basu - 2020 - Bias of OLS Estimators due to Exclusion of Relevan.pdf}
}

@misc{batesCrossvalidationWhatDoes2022,
  ids = {batesCrossvalidationWhatDoes2022a},
  title = {Cross-Validation: What Does It Estimate and How Well Does It Do It?},
  shorttitle = {Cross-Validation},
  author = {Bates, Stephen and Hastie, Trevor and Tibshirani, Robert},
  date = {2022-07-18},
  number = {arXiv:2104.00673},
  eprint = {2104.00673},
  eprinttype = {arxiv},
  primaryclass = {math, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2104.00673},
  url = {http://arxiv.org/abs/2104.00673},
  urldate = {2022-11-11},
  abstract = {Cross-validation is a widely-used technique to estimate prediction error, but its behavior is complex and not fully understood. Ideally, one would like to think that cross-validation estimates the prediction error for the model at hand, fit to the training data. We prove that this is not the case for the linear model fit by ordinary least squares; rather it estimates the average prediction error of models fit on other unseen training sets drawn from the same population. We further show that this phenomenon occurs for most popular estimates of prediction error, including data splitting, bootstrapping, and Mallow's Cp. Next, the standard confidence intervals for prediction error derived from cross-validation may have coverage far below the desired level. Because each data point is used for both training and testing, there are correlations among the measured accuracies for each fold, and so the usual estimate of variance is too small. We introduce a nested cross-validation scheme to estimate this variance more accurately, and we show empirically that this modification leads to intervals with approximately correct coverage in many examples where traditional cross-validation intervals fail.},
  archiveprefix = {arXiv},
  keywords = {Mathematics - Statistics Theory,Statistics - Computation,Statistics - Machine Learning,Statistics - Methodology},
  file = {/home/skynet3/Zotero/storage/GUZDUW88/Bates et al. - 2022 - Cross-validation what does it estimate and how we.pdf;/home/skynet3/Zotero/storage/GG9BI3DV/2104.html}
}

@article{bayerSurveyDataAugmentation2022,
  title = {A {{Survey}} on {{Data Augmentation}} for {{Text Classification}}},
  author = {Bayer, Markus and Kaufhold, Marc-André and Reuter, Christian},
  date = {2022-06-17},
  journaltitle = {ACM Computing Surveys},
  shortjournal = {ACM Comput. Surv.},
  eprint = {2107.03158},
  eprinttype = {arxiv},
  primaryclass = {cs},
  pages = {3544558},
  issn = {0360-0300, 1557-7341},
  doi = {10.1145/3544558},
  url = {http://arxiv.org/abs/2107.03158},
  urldate = {2022-11-11},
  abstract = {Data augmentation, the artificial creation of training data for machine learning by transformations, is a widely studied research field across machine learning disciplines. While it is useful for increasing a model's generalization capabilities, it can also address many other challenges and problems, from overcoming a limited amount of training data, to regularizing the objective, to limiting the amount data used to protect privacy. Based on a precise description of the goals and applications of data augmentation and a taxonomy for existing works, this survey is concerned with data augmentation methods for textual classification and aims to provide a concise and comprehensive overview for researchers and practitioners. Derived from the taxonomy, we divide more than 100 methods into 12 different groupings and give state-of-the-art references expounding which methods are highly promising by relating them to each other. Finally, research perspectives that may constitute a building block for future work are provided.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/home/skynet3/Zotero/storage/6SK8S356/Bayer et al. - 2022 - A Survey on Data Augmentation for Text Classificat.pdf;/home/skynet3/Zotero/storage/SPLF399T/2107.html}
}

@online{Bayesf22NotebookBayes,
  ids = {Bayesf22NotebookBayesa},
  title = {Bayesf22 {{Notebook}} - {{Bayes Rules}}!},
  url = {https://bayesf22-notebook.classes.andrewheiss.com/bayes-rules/},
  urldate = {2022-11-11}
}

@online{Bayesf22NotebookStatistical,
  title = {Bayesf22 {{Notebook}} - {{Statistical Rethinking}}},
  url = {https://bayesf22-notebook.classes.andrewheiss.com/rethinking/},
  urldate = {2022-11-11}
}

@online{BayesianAverageRatings,
  title = {Bayesian {{Average Ratings}} – {{Evan Miller}}},
  url = {https://www.evanmiller.org/bayesian-average-ratings.html},
  urldate = {2022-11-11},
  file = {/home/skynet3/Zotero/storage/LP7S7MV2/bayesian-average-ratings.html}
}

@online{BayesianAverageWikipedia,
  ids = {BayesianAverageWikipediaa},
  title = {Bayesian Average) - {{Wikipedia}}},
  url = {https://en.wikipedia.org/wiki/Bayesian_average)},
  urldate = {2022-11-11},
  langid = {english},
  file = {/home/skynet3/Zotero/storage/ZGHJ2R9X/Bayesian_average)\;.html}
}

@online{BayesianInferenceInteractive,
  ids = {BayesianInferenceInteractivea},
  title = {Bayesian Inference - an Interactive Visualization},
  url = {http://rpsychologist.com/d3/bayes/},
  urldate = {2022-11-11},
  abstract = {Interactive visualization of Bayesian estimation and hypothesis testing.},
  organization = {{R Psychologist}}
}

@online{BayesianMethodsHackers,
  title = {Bayesian {{Methods}} for {{Hackers}}},
  url = {https://camdavidsonpilon.github.io/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/},
  urldate = {2022-11-11}
}

@online{BayesianSpatialAnalysis,
  title = {Bayesian {{Spatial Analysis}}},
  url = {https://connordonegan.github.io/geostan/},
  urldate = {2022-11-11},
  abstract = {For Bayesian inference with spatial data, provides exploratory spatial analysis tools, multiple spatial model specifications, spatial model diagnostics, and special methods for inference with small area survey data (e.g., the America Community Survey (ACS)) and censored population health surveillance data. Models are pre-specified using the Stan programming language, a platform for Bayesian inference using Markov chain Monte Carlo (MCMC). References: Carpenter et al. (2017) {$<$}doi:10.18637/jss.v076.i01{$>$}; Donegan (2021) {$<$}doi:10.31219/osf.io/3ey65{$>$}; Donegan, Chun and Hughes (2020) {$<$}doi:10.1016/j.spasta.2020.100450{$>$}; Donegan, Chun and Griffith (2021) {$<$}doi:10.3390/ijerph18136856{$>$}; Morris et al. (2019) {$<$}doi:10.1016/j.sste.2019.100301{$>$}.},
  langid = {english},
  file = {/home/skynet3/Zotero/storage/7FYXZPZA/geostan.html}
}

@online{BayesianStatisticsReadings,
  title = {Bayesian {{Statistics Readings}}},
  url = {https://bayesf22.classes.andrewheiss.com/},
  urldate = {2022-11-11},
  abstract = {Independent readings course on Bayesian statistics at the Andrew Young School of Policy Studies at Georgia State University},
  langid = {english}
}

@misc{bayleCrossvalidationConfidenceIntervals2020,
  title = {Cross-Validation {{Confidence Intervals}} for {{Test Error}}},
  author = {Bayle, Pierre and Bayle, Alexandre and Janson, Lucas and Mackey, Lester},
  date = {2020-10-31},
  number = {arXiv:2007.12671},
  eprint = {2007.12671},
  eprinttype = {arxiv},
  primaryclass = {cs, math, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2007.12671},
  url = {http://arxiv.org/abs/2007.12671},
  urldate = {2022-11-11},
  abstract = {This work develops central limit theorems for cross-validation and consistent estimators of its asymptotic variance under weak stability conditions on the learning algorithm. Together, these results provide practical, asymptotically-exact confidence intervals for \$k\$-fold test error and valid, powerful hypothesis tests of whether one learning algorithm has smaller \$k\$-fold test error than another. These results are also the first of their kind for the popular choice of leave-one-out cross-validation. In our real-data experiments with diverse learning algorithms, the resulting intervals and tests outperform the most popular alternative methods from the literature.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Mathematics - Statistics Theory,Statistics - Machine Learning},
  file = {/home/skynet3/Zotero/storage/4CVVBMB6/Bayle et al. - 2020 - Cross-validation Confidence Intervals for Test Err.pdf;/home/skynet3/Zotero/storage/2AHLVA3H/2007.html}
}

@book{belzileTimeseRies,
  title = {{{timeseRies}}},
  author = {Belzile, Léo},
  url = {https://lbelzile.github.io/timeseRies/},
  urldate = {2022-11-11},
  abstract = {Web complement of MATH 342 (Time series) at EPFL.},
  file = {/home/skynet3/Zotero/storage/FJ2BC69W/timeseRies.html}
}

@misc{belzQuantifiedReproducibilityAssessment2022,
  title = {Quantified {{Reproducibility Assessment}} of {{NLP Results}}},
  author = {Belz, Anya and Popović, Maja and Mille, Simon},
  date = {2022-04-12},
  number = {arXiv:2204.05961},
  eprint = {2204.05961},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2204.05961},
  url = {http://arxiv.org/abs/2204.05961},
  urldate = {2022-11-11},
  abstract = {This paper describes and tests a method for carrying out quantified reproducibility assessment (QRA) that is based on concepts and definitions from metrology. QRA produces a single score estimating the degree of reproducibility of a given system and evaluation measure, on the basis of the scores from, and differences between, different reproductions. We test QRA on 18 system and evaluation measure combinations (involving diverse NLP tasks and types of evaluation), for each of which we have the original results and one to seven reproduction results. The proposed QRA method produces degree-of-reproducibility scores that are comparable across multiple reproductions not only of the same, but of different original studies. We find that the proposed method facilitates insights into causes of variation between reproductions, and allows conclusions to be drawn about what changes to system and/or evaluation design might lead to improved reproducibility.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/home/skynet3/Zotero/storage/LE35KCY2/Belz et al. - 2022 - Quantified Reproducibility Assessment of NLP Resul.pdf;/home/skynet3/Zotero/storage/VMP7UCSC/2204.html}
}

@software{bengtssonFutureUnifiedParallel2022,
  title = {Future: {{Unified Parallel}} and {{Distributed Processing}} in {{R}} for {{Everyone}}},
  shorttitle = {Future},
  author = {Bengtsson, Henrik},
  date = {2022-11-11T09:09:16Z},
  origdate = {2015-06-08T02:37:06Z},
  url = {https://github.com/HenrikBengtsson/future},
  urldate = {2022-11-11},
  abstract = {:rocket: R package: future: Unified Parallel and Distributed Processing in R for Everyone},
  keywords = {asynchronous,cran,distributed-computing,futures,hpc,hpc-clusters,parallel-computing,parallel-processing,parallelization,programming,promises,r}
}

@software{bengtssonMatrixStatsFunctionsThat2022,
  title = {{{matrixStats}}: {{Functions}} That {{Apply}} to {{Rows}} and {{Columns}} of {{Matrices}} (and to {{Vectors}})},
  shorttitle = {{{matrixStats}}},
  author = {Bengtsson, Henrik},
  date = {2022-09-19T14:29:31Z},
  origdate = {2014-06-14T23:59:10Z},
  url = {https://github.com/HenrikBengtsson/matrixStats},
  urldate = {2022-11-11},
  abstract = {R package: Methods that Apply to Rows and Columns of Matrices (and to Vectors)},
  keywords = {cran,matrix,package,performance,r,vector}
}

@online{BenignOverfittingLinear2019,
  title = {Benign {{Overfitting}} in {{Linear Regression}}},
  date = {2019-06-26T19:09:56+00:00},
  url = {https://deepai.org/publication/benign-overfitting-in-linear-regression},
  urldate = {2022-11-11},
  abstract = {06/26/19 - The phenomenon of benign overfitting is one of the key mysteries uncovered by deep learning methodology: deep neural networks seem...},
  organization = {{DeepAI}},
  file = {/home/skynet3/Zotero/storage/QVVB6CIQ/benign-overfitting-in-linear-regression.html}
}

@online{BenmarwickRrtoolsRrtools,
  title = {Benmarwick/Rrtools: Rrtools: {{Tools}} for {{Writing Reproducible Research}} in {{R}}},
  shorttitle = {Benmarwick/Rrtools},
  url = {https://github.com/benmarwick/rrtools},
  urldate = {2022-11-11},
  abstract = {rrtools: Tools for Writing Reproducible Research in R - benmarwick/rrtools: rrtools: Tools for Writing Reproducible Research in R},
  langid = {english},
  organization = {{GitHub}},
  file = {/home/skynet3/Zotero/storage/GQ7WDPDD/rrtools.html}
}

@software{bergeFixestFastUserfriendly2022,
  title = {Fixest: {{Fast}} and User-Friendly Fixed-Effects Estimation},
  shorttitle = {Fixest},
  author = {Bergé, Laurent},
  date = {2022-11-10T20:40:22Z},
  origdate = {2019-08-02T09:19:18Z},
  url = {https://github.com/lrberge/fixest},
  urldate = {2022-11-11},
  abstract = {Fixed-effects estimations}
}

@online{berkYourCrossValidation2021,
  title = {Your {{Cross Validation Error Confidence Intervals}} Are {{Wrong}} — Here’s How to {{Fix Them}}},
  author = {Berk, Michael},
  date = {2021-06-11T22:29:02},
  url = {https://towardsdatascience.com/your-cross-validation-error-confidence-intervals-are-wrong-heres-how-to-fix-them-abbfe28d390},
  urldate = {2022-11-11},
  abstract = {Researchers at Stanford (2021) developed a method that uses Nested Cross Validation (NCV) to account for dependence between data splits, thereby allowing us to calculate accurate confidence intervals…},
  langid = {english},
  organization = {{Medium}},
  file = {/home/skynet3/Zotero/storage/9AITGCXN/your-cross-validation-error-confidence-intervals-are-wrong-heres-how-to-fix-them-abbfe28d390.html}
}

@online{bernardiDonBeTricked2018,
  title = {Don’t Be Tricked by the {{Hashing Trick}}},
  author = {Bernardi, Lucas},
  date = {2018-06-14T13:59:38},
  url = {https://booking.ai/dont-be-tricked-by-the-hashing-trick-192a6aae3087},
  urldate = {2022-11-11},
  abstract = {In Machine Learning, the Hashing Trick is a technique to encode categorical features. It’s been gaining popularity lately after being…},
  langid = {english},
  organization = {{Medium}},
  file = {/home/skynet3/Zotero/storage/QHCV6GUL/dont-be-tricked-by-the-hashing-trick-192a6aae3087.html}
}

@misc{bernerModernMathematicsDeep2021,
  ids = {bernerModernMathematicsDeep2021a},
  title = {The {{Modern Mathematics}} of {{Deep Learning}}},
  author = {Berner, Julius and Grohs, Philipp and Kutyniok, Gitta and Petersen, Philipp},
  date = {2021-05-09},
  number = {arXiv:2105.04026},
  eprint = {2105.04026},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2105.04026},
  url = {http://arxiv.org/abs/2105.04026},
  urldate = {2022-11-11},
  abstract = {We describe the new field of mathematical analysis of deep learning. This field emerged around a list of research questions that were not answered within the classical framework of learning theory. These questions concern: the outstanding generalization power of overparametrized neural networks, the role of depth in deep architectures, the apparent absence of the curse of dimensionality, the surprisingly successful optimization performance despite the non-convexity of the problem, understanding what features are learned, why deep architectures perform exceptionally well in physical problems, and which fine aspects of an architecture affect the behavior of a learning task in which way. We present an overview of modern approaches that yield partial answers to these questions. For selected approaches, we describe the main ideas in more detail.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/skynet3/Zotero/storage/WHCXETPU/Berner et al. - 2021 - The Modern Mathematics of Deep Learning.pdf;/home/skynet3/Zotero/storage/U8LEBVGR/2105.html}
}

@misc{bernerWhyHowWe2021,
  title = {Why and {{How We Should Join}} the {{Shift From Significance Testing}} to {{Estimation}}},
  author = {Berner, Daniel and Amrhein, Valentin},
  date = {2021-12-14},
  number = {2021120235},
  publisher = {{Preprints}},
  doi = {10.20944/preprints202112.0235.v1},
  url = {https://www.preprints.org/manuscript/202112.0235/v1},
  urldate = {2022-11-11},
  abstract = {A paradigm shift away from null hypothesis significance testing seems in progress. Based on simulations, we illustrate some of the underlying motivations. First, P-values vary strongly from study to study, hence dichotomous inference using significance thresholds is usually unjustified. Second, statistically significant results have overestimated effect sizes, a bias declining with increasing statistical power. Third, statistically non-significant results have underestimated effect sizes, and this bias gets stronger with higher statistical power. Fourth, the tested statistical hypotheses generally lack biological justification and are often uninformative. Despite these problems, a screen of 48 papers from the 2020 volume of the Journal of Evolutionary Biology exemplifies that significance testing is still used almost universally in evolutionary biology. All screened studies tested the default null hypothesis of zero effect with the default significance threshold of p = 0.05, none presented a pre-planned alternative hypothesis, and none calculated statistical power and the probability of \&lsquo;false negatives\&rsquo; (beta error). The papers reported 49 significance tests on average. Of 41 papers that contained verbal descriptions of a \&lsquo;statistically non-significant\&rsquo; result, 26 (63\%) falsely claimed the absence of an effect. We conclude that our studies in ecology and evolutionary biology are mostly exploratory and descriptive. We should thus shift from claiming to \&ldquo;test\&rdquo; specific hypotheses statistically to describing and discussing many hypotheses (effect sizes) that are most compatible with our data, given our statistical model. We already have the means for doing so, because we routinely present compatibility (\&ldquo;confidence\&rdquo;) intervals covering these hypotheses.},
  langid = {english},
  keywords = {Compatibility interval,effect size,null hypothesis,p-value,statistical inference},
  file = {/home/skynet3/Zotero/storage/XKFAE42D/Berner and Amrhein - 2021 - Why and How We Should Join the Shift From Signific.pdf}
}

@online{bernhardssonWhySoftwareProjects,
  title = {Why Software Projects Take Longer than You Think: A Statistical Model},
  shorttitle = {Why Software Projects Take Longer than You Think},
  author = {Bernhardsson, Erik},
  url = {https://erikbern.com/2019/04/15/why-software-projects-take-longer-than-you-think-a-statistical-model.html},
  urldate = {2022-11-11},
  abstract = {Anyone who built software for a while knows that estimating how long something is going to take is hard. It's hard to come up with an unbiased estimate of how long something will take, when fundamentally the work in itself is about solving something.},
  langid = {english},
  organization = {{Erik Bernhardsson}},
  file = {/home/skynet3/Zotero/storage/3R5KCREQ/why-software-projects-take-longer-than-you-think-a-statistical-model.html}
}

@article{bertrandHowMuchShould2004,
  ids = {bertrandHowMuchShould2004a},
  title = {How {{Much Should We Trust Differences-In-Differences Estimates}}?*},
  shorttitle = {How {{Much Should We Trust Differences-In-Differences Estimates}}?},
  author = {Bertrand, Marianne and Duflo, Esther and Mullainathan, Sendhil},
  date = {2004-02-01},
  journaltitle = {The Quarterly Journal of Economics},
  shortjournal = {The Quarterly Journal of Economics},
  volume = {119},
  number = {1},
  pages = {249--275},
  issn = {0033-5533},
  doi = {10.1162/003355304772839588},
  url = {https://doi.org/10.1162/003355304772839588},
  urldate = {2022-11-11},
  abstract = {Most papers that employ Differences-in-Differences estimation (DD) use many years of data and focus on serially correlated outcomes but ignore that the resulting standard errors are inconsistent. To illustrate the severity of this issue, we randomly generate placebo laws in state-level data on female wages from the Current Population Survey. For each law, we use OLS to compute the DD estimate of its “effect” as well as the standard error of this estimate. These conventional DD standard errors severely understate the standard deviation of the estimators: we find an “effect” significant at the 5 percent level for up to 45 percent of the placebo interventions. We use Monte Carlo simulations to investigate how well existing methods help solve this problem. Econometric corrections that place a specific parametric form on the time-series process do not perform well. Bootstrap (taking into account the autocorrelation of the data) works well when the number of states is large enough. Two corrections based on asymptotic approximation of the variance-covariance matrix work well for moderate numbers of states and one correction that collapses the time series information into a “pre”- and “post”-period and explicitly takes into account the effective sample size works well even for small numbers of states.},
  file = {/home/skynet3/Zotero/storage/VFJNAG7P/Bertrand et al. - 2004 - How Much Should We Trust Differences-In-Difference.pdf;/home/skynet3/Zotero/storage/DMIKBKDJ/1876068.html}
}

@online{betancourtWriting,
  title = {Writing},
  author = {Betancourt, Michael},
  url = {https://betanalpha.github.io/writing/},
  urldate = {2022-11-11},
  langid = {english},
  organization = {{betanalpha.github.io}},
  file = {/home/skynet3/Zotero/storage/LWXV3AD4/writing.html}
}

@misc{betzSpatialInterdependenceInstrumental2017,
  title = {Spatial {{Interdependence}} and {{Instrumental Variable Models}}},
  author = {Betz, Timm and Cook, Scott J. and Hollenbach, Florian M.},
  date = {2017-10-17T21:28:15},
  publisher = {{SocArXiv}},
  doi = {10.31235/osf.io/pgrcu},
  url = {https://osf.io/preprints/socarxiv/pgrcu/},
  urldate = {2022-11-11},
  abstract = {Instrumental variable (IV) methods are widely used to address endogeneity concerns in re- search using observational data. Yet, a specific kind of endogeneity – spatial interdependence – is regularly ignored in this research, threatening claims of causal identification. We show that ignoring spatial interdependence results in asymptotically biased estimates, even when in- struments are randomly assigned. The extent of this bias increases when the instrument is also spatially distributed, which is the case for most widely-used instruments (such as rainfall, nat- ural disasters, economic shocks, regionally- or globally-weighted averages, etc.). We demon- strate the extent of these biases both analytically and via Monte Carlo simulation. Finally, we discuss a simple estimation strategy that can be employed to recover consistent estimates of the desired effects.},
  langid = {american},
  keywords = {Comparative Politics,Instrumental Variables,International Relations,Models and Methods,Political Science,Social and Behavioral Sciences,Spatial Analysis,Spatial Modeling,Two-Stage Least Squares},
  file = {/home/skynet3/Zotero/storage/WRP7AV2U/Betz et al. - 2017 - Spatial Interdependence and Instrumental Variable .pdf}
}

@online{BewareDefaultRandom,
  title = {Beware {{Default Random Forest Importances}}},
  url = {http://explained.ai/decision-tree-viz/index.html},
  urldate = {2022-11-11},
  abstract = {Training a model that accurately predicts outcomes is great, but most of the time you don't just need predictions, you want to be able to interpret your model. The problem is that the scikit-learn Random Forest feature importance and R's default Random Forest feature importance strategies are biased. To get reliable results in Python, use permutation importance, provided here and in our rfpimp package (via pip). For R, use importance=T in the Random Forest constructor then type=1 in R's importance() function.},
  file = {/home/skynet3/Zotero/storage/VTBBK3AT/index.html}
}

@online{BGGM,
  title = {{{BGGM}}},
  url = {https://donaldrwilliams.github.io/BGGM/index.html},
  urldate = {2022-11-11},
  abstract = {Fit Bayesian Gaussian graphical models. The methods are separated into      two Bayesian approaches for inference: hypothesis testing and estimation. There are      extensions for confirmatory hypothesis testing, comparing Gaussian graphical models,      and node wise predictability. These methods were recently introduced in the Gaussian      graphical model literature, including      Williams (2019) {$<$}doi:10.31234/osf.io/x8dpr{$>$},      Williams and Mulder (2019) {$<$}doi:10.31234/osf.io/ypxd8{$>$},     Williams, Rast, Pericchi, and Mulder (2019) {$<$}doi:10.31234/osf.io/yt386{$>$}.},
  langid = {english},
  file = {/home/skynet3/Zotero/storage/EEWXNKIB/index.html}
}

@misc{bhattacharjeeWhenAreNonParametric2020,
  ids = {bhattacharjeeWhenAreNonParametric2020a},
  title = {When Are {{Non-Parametric Methods Robust}}?},
  author = {Bhattacharjee, Robi and Chaudhuri, Kamalika},
  date = {2020-12-28},
  number = {arXiv:2003.06121},
  eprint = {2003.06121},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2003.06121},
  url = {http://arxiv.org/abs/2003.06121},
  urldate = {2022-11-11},
  abstract = {A growing body of research has shown that many classifiers are susceptible to \{\textbackslash em\{adversarial examples\}\} -- small strategic modifications to test inputs that lead to misclassification. In this work, we study general non-parametric methods, with a view towards understanding when they are robust to these modifications. We establish general conditions under which non-parametric methods are r-consistent -- in the sense that they converge to optimally robust and accurate classifiers in the large sample limit. Concretely, our results show that when data is well-separated, nearest neighbors and kernel classifiers are r-consistent, while histograms are not. For general data distributions, we prove that preprocessing by Adversarial Pruning (Yang et. al., 2019) -- that makes data well-separated -- followed by nearest neighbors or kernel classifiers also leads to r-consistency.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/skynet3/Zotero/storage/S3U6WUGV/Bhattacharjee and Chaudhuri - 2020 - When are Non-Parametric Methods Robust.pdf;/home/skynet3/Zotero/storage/9FVK532U/2003.html}
}

@misc{bhattUncertaintyFormTransparency2021,
  title = {Uncertainty as a {{Form}} of {{Transparency}}: {{Measuring}}, {{Communicating}}, and {{Using Uncertainty}}},
  shorttitle = {Uncertainty as a {{Form}} of {{Transparency}}},
  author = {Bhatt, Umang and Antorán, Javier and Zhang, Yunfeng and Liao, Q. Vera and Sattigeri, Prasanna and Fogliato, Riccardo and Melançon, Gabrielle Gauthier and Krishnan, Ranganath and Stanley, Jason and Tickoo, Omesh and Nachman, Lama and Chunara, Rumi and Srikumar, Madhulika and Weller, Adrian and Xiang, Alice},
  date = {2021-05-04},
  number = {arXiv:2011.07586},
  eprint = {2011.07586},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2011.07586},
  url = {http://arxiv.org/abs/2011.07586},
  urldate = {2022-11-11},
  abstract = {Algorithmic transparency entails exposing system properties to various stakeholders for purposes that include understanding, improving, and contesting predictions. Until now, most research into algorithmic transparency has predominantly focused on explainability. Explainability attempts to provide reasons for a machine learning model's behavior to stakeholders. However, understanding a model's specific behavior alone might not be enough for stakeholders to gauge whether the model is wrong or lacks sufficient knowledge to solve the task at hand. In this paper, we argue for considering a complementary form of transparency by estimating and communicating the uncertainty associated with model predictions. First, we discuss methods for assessing uncertainty. Then, we characterize how uncertainty can be used to mitigate model unfairness, augment decision-making, and build trustworthy systems. Finally, we outline methods for displaying uncertainty to stakeholders and recommend how to collect information required for incorporating uncertainty into existing ML pipelines. This work constitutes an interdisciplinary review drawn from literature spanning machine learning, visualization/HCI, design, decision-making, and fairness. We aim to encourage researchers and practitioners to measure, communicate, and use uncertainty as a form of transparency.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computers and Society,Computer Science - Human-Computer Interaction,Computer Science - Machine Learning},
  file = {/home/skynet3/Zotero/storage/QWCCJDG2/Bhatt et al. - 2021 - Uncertainty as a Form of Transparency Measuring, .pdf;/home/skynet3/Zotero/storage/I7UNJRGF/2011.html}
}

@article{bianchiCanTransparencyUndermine2022,
  ids = {bianchiCanTransparencyUndermine2022a},
  title = {Can Transparency Undermine Peer Review? {{A}} Simulation Model of Scientist Behavior under Open Peer Review},
  shorttitle = {Can Transparency Undermine Peer Review?},
  author = {Bianchi, Federico and Squazzoni, Flaminio},
  date = {2022-10-01},
  journaltitle = {Science and Public Policy},
  shortjournal = {Science and Public Policy},
  volume = {49},
  number = {5},
  pages = {791--800},
  issn = {0302-3427},
  doi = {10.1093/scipol/scac027},
  url = {https://doi.org/10.1093/scipol/scac027},
  urldate = {2022-11-11},
  abstract = {Transparency and accountability are keywords in corporate business, politics, and science. As part of the open science movement, many journals have started to adopt forms of open peer review beyond the closed (single- or double-blind) standard model. However, there is contrasting evidence on the impact of these innovations on the quality of peer review. Furthermore, their long-term consequences on scientists’ cooperation and competition are difficult to assess empirically. This paper aims to fill this gap by presenting an agent-based model that simulates competition and status dynamics between scholars in an artificial academic system. The results would suggest that if referees are sensitive to competition and status, the transparency achieved by open peer review could backfire on the quality of the process. Although only abstract and hypothetical, our findings suggest the importance of multidimensional values of peer review and the anonymity and confidentiality of the process.},
  file = {/home/skynet3/Zotero/storage/S2WDH7NQ/Bianchi and Squazzoni - 2022 - Can transparency undermine peer review A simulati.pdf;/home/skynet3/Zotero/storage/3D39PQBW/6602348.html}
}

@software{BIGbench2022,
  ids = {BIGbench2022a},
  title = {{{BIG-bench}} 🪑},
  date = {2022-11-11T20:46:48Z},
  origdate = {2021-01-15T23:28:20Z},
  url = {https://github.com/google/BIG-bench/blob/08d8478a8ee8bde8de5b20c4dbf049b6fc6ecb74/docs/paper/BIG-bench.pdf},
  urldate = {2022-11-11},
  abstract = {Beyond the Imitation Game collaborative benchmark for measuring and extrapolating the capabilities of language models},
  organization = {{Google}}
}

@online{BillionTaxiRides,
  ids = {BillionTaxiRidesa},
  title = {1.1 {{Billion Taxi Rides}} with {{SQLite}}, {{Parquet}} \& {{HDFS}}},
  url = {https://tech.marksblogg.com/billion-nyc-taxi-rides-sqlite-parquet-hdfs.html},
  urldate = {2022-11-11}
}

@online{BlackjaxdevsBlackjaxBlackJAX,
  title = {Blackjax-Devs/Blackjax: {{BlackJAX}} Is a Sampling Library Designed for Ease of Use, Speed and Modularity.},
  shorttitle = {Blackjax-Devs/Blackjax},
  url = {https://github.com/blackjax-devs/blackjax},
  urldate = {2022-11-11},
  abstract = {BlackJAX is a sampling library designed for ease of use, speed and modularity. - blackjax-devs/blackjax: BlackJAX is a sampling library designed for ease of use, speed and modularity.},
  langid = {english},
  organization = {{GitHub}},
  file = {/home/skynet3/Zotero/storage/CRM6597R/blackjax.html}
}

@article{blagusClassPredictionHighdimensional2010,
  title = {Class Prediction for High-Dimensional Class-Imbalanced Data},
  author = {Blagus, Rok and Lusa, Lara},
  date = {2010-10-20},
  journaltitle = {BMC Bioinformatics},
  shortjournal = {BMC Bioinformatics},
  volume = {11},
  number = {1},
  pages = {523},
  issn = {1471-2105},
  doi = {10.1186/1471-2105-11-523},
  url = {https://doi.org/10.1186/1471-2105-11-523},
  urldate = {2022-11-11},
  abstract = {The goal of class prediction studies is to develop rules to accurately predict the class membership of new samples. The rules are derived using the values of the variables available for each subject: the main characteristic of high-dimensional data is that the number of variables greatly exceeds the number of samples. Frequently the classifiers are developed using class-imbalanced data, i.e., data sets where the number of samples in each class is not equal. Standard classification methods used on class-imbalanced data often produce classifiers that do not accurately predict the minority class; the prediction is biased towards the majority class. In this paper we investigate if the high-dimensionality poses additional challenges when dealing with class-imbalanced prediction. We evaluate the performance of six types of classifiers on class-imbalanced data, using simulated data and a publicly available data set from a breast cancer gene-expression microarray study. We also investigate the effectiveness of some strategies that are available to overcome the effect of class imbalance.},
  keywords = {Class Imbalance,Classification Rule,Minority Class,Predictive Accuracy,Variable Selection},
  file = {/home/skynet3/Zotero/storage/R2NH8U72/Blagus and Lusa - 2010 - Class prediction for high-dimensional class-imbala.pdf;/home/skynet3/Zotero/storage/NLNBS7IQ/1471-2105-11-523.html}
}

@misc{blazquez-garciaReviewOutlierAnomaly2020,
  title = {A Review on Outlier/Anomaly Detection in Time Series Data},
  author = {Blázquez-García, Ane and Conde, Angel and Mori, Usue and Lozano, Jose A.},
  date = {2020-02-11},
  number = {arXiv:2002.04236},
  eprint = {2002.04236},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2002.04236},
  url = {http://arxiv.org/abs/2002.04236},
  urldate = {2022-11-11},
  abstract = {Recent advances in technology have brought major breakthroughs in data collection, enabling a large amount of data to be gathered over time and thus generating time series. Mining this data has become an important task for researchers and practitioners in the past few years, including the detection of outliers or anomalies that may represent errors or events of interest. This review aims to provide a structured and comprehensive state-of-the-art on outlier detection techniques in the context of time series. To this end, a taxonomy is presented based on the main aspects that characterize an outlier detection technique.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/skynet3/Zotero/storage/NMC49N8S/Blázquez-García et al. - 2020 - A review on outlieranomaly detection in time seri.pdf;/home/skynet3/Zotero/storage/2P77BNX8/2002.html}
}

@online{Blind75LeetCode,
  title = {Blind 75 {{LeetCode Questions}} - {{LeetCode Discuss}}},
  url = {https://leetcode.com/discuss/general-discussion/460599/blind-75-leetcode-questions},
  urldate = {2022-11-11},
  abstract = {Level up your coding skills and quickly land a job. This is the best place to expand your knowledge and get prepared for your next interview.},
  file = {/home/skynet3/Zotero/storage/ZGYMTE4X/blind-75-leetcode-questions.html}
}

@article{blomConditionalIndependencesCausal2021,
  ids = {blomConditionalIndependencesCausal2021a},
  title = {Conditional Independences and Causal Relations Implied by Sets of Equations},
  author = {Blom, Tineke and van Diepen, Mirthe M. and Mooij, Joris M.},
  date = {2021},
  journaltitle = {Journal of Machine Learning Research},
  volume = {22},
  number = {178},
  pages = {1--62},
  issn = {1533-7928},
  url = {http://jmlr.org/papers/v22/20-863.html},
  urldate = {2022-11-11},
  abstract = {Real-world complex systems are often modelled by sets of equations with endogenous and exogenous variables. What can we say about the causal and probabilistic aspects of variables that appear in these equations without explicitly solving the equations? We make use of Simon's causal ordering algorithm (Simon, 1953) to construct a causal ordering graph and prove that it expresses the effects of soft and perfect interventions on the equations under certain unique solvability assumptions. We further construct a Markov ordering graph and prove that it encodes conditional independences in the distribution implied by the equations with independent random exogenous variables, under a similar unique solvability assumption. We discuss how this approach reveals and addresses some of the limitations of existing causal modelling frameworks, such as causal Bayesian networks and structural causal models.},
  file = {/home/skynet3/Zotero/storage/462EZSCI/Blom et al. - 2021 - Conditional independences and causal relations imp.pdf}
}

@book{bonatRegressionModelsCount,
  title = {Regression {{Models}} for {{Count Data}}: Beyond the {{Poisson}} Model},
  shorttitle = {Regression {{Models}} for {{Count Data}}},
  author = {Bonat, Wagner Hugo and Zeviani, Walmes Marques and Jr, Eduardo Elias Ribeiro},
  url = {http://cursos.leg.ufpr.br/rmcd/},
  urldate = {2022-11-11},
  abstract = {Regression Models for Count Data: beyond the Poisson model},
  file = {/home/skynet3/Zotero/storage/AJFM2YWI/rmcd.html}
}

@online{BookStatisticalProofs,
  title = {The {{Book}} of {{Statistical Proofs}}},
  url = {https://statproofbook.github.io/},
  urldate = {2022-11-11},
  abstract = {The Book of Statistical Proofs – a centralized, open and collaboratively edited archive of statistical theorems for the computational sciences},
  langid = {english},
  organization = {{The Book of Statistical Proofs}},
  file = {/home/skynet3/Zotero/storage/NZP8E482/statproofbook.github.io.html}
}

@misc{bornscheinSmallDataBig2020,
  title = {Small {{Data}}, {{Big Decisions}}: {{Model Selection}} in the {{Small-Data Regime}}},
  shorttitle = {Small {{Data}}, {{Big Decisions}}},
  author = {Bornschein, Jorg and Visin, Francesco and Osindero, Simon},
  date = {2020-09-26},
  number = {arXiv:2009.12583},
  eprint = {2009.12583},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2009.12583},
  url = {http://arxiv.org/abs/2009.12583},
  urldate = {2022-11-11},
  abstract = {Highly overparametrized neural networks can display curiously strong generalization performance - a phenomenon that has recently garnered a wealth of theoretical and empirical research in order to better understand it. In contrast to most previous work, which typically considers the performance as a function of the model size, in this paper we empirically study the generalization performance as the size of the training set varies over multiple orders of magnitude. These systematic experiments lead to some interesting and potentially very useful observations; perhaps most notably that training on smaller subsets of the data can lead to more reliable model selection decisions whilst simultaneously enjoying smaller computational costs. Our experiments furthermore allow us to estimate Minimum Description Lengths for common datasets given modern neural network architectures, thereby paving the way for principled model selection taking into account Occams-razor.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/skynet3/Zotero/storage/2Q8SL48I/Bornschein et al. - 2020 - Small Data, Big Decisions Model Selection in the .pdf;/home/skynet3/Zotero/storage/AZ3BJ524/2009.html}
}

@article{botchkarevPerformanceMetricsError2019,
  title = {Performance {{Metrics}} ({{Error Measures}}) in {{Machine Learning Regression}}, {{Forecasting}} and {{Prognostics}}: {{Properties}} and {{Typology}}},
  shorttitle = {Performance {{Metrics}} ({{Error Measures}}) in {{Machine Learning Regression}}, {{Forecasting}} and {{Prognostics}}},
  author = {Botchkarev, Alexei},
  date = {2019},
  journaltitle = {Interdisciplinary Journal of Information, Knowledge, and Management},
  shortjournal = {IJIKM},
  volume = {14},
  eprint = {1809.03006},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  pages = {045--076},
  issn = {1555-1229, 1555-1237},
  doi = {10.28945/4184},
  url = {http://arxiv.org/abs/1809.03006},
  urldate = {2022-11-11},
  abstract = {Performance metrics (error measures) are vital components of the evaluation frameworks in various fields. The intention of this study was to overview of a variety of performance metrics and approaches to their classification. The main goal of the study was to develop a typology that will help to improve our knowledge and understanding of metrics and facilitate their selection in machine learning regression, forecasting and prognostics. Based on the analysis of the structure of numerous performance metrics, we propose a framework of metrics which includes four (4) categories: primary metrics, extended metrics, composite metrics, and hybrid sets of metrics. The paper identified three (3) key components (dimensions) that determine the structure and properties of primary metrics: method of determining point distance, method of normalization, method of aggregation of point distances over a data set.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning,Statistics - Methodology},
  file = {/home/skynet3/Zotero/storage/Q29VJK3Z/Botchkarev - 2019 - Performance Metrics (Error Measures) in Machine Le.pdf;/home/skynet3/Zotero/storage/7KYX2TMY/1809.html}
}

@article{boursNontechnicalExplanationCounterfactual2020,
  ids = {boursNontechnicalExplanationCounterfactual2020a},
  title = {A Nontechnical Explanation of the Counterfactual Definition of Confounding},
  author = {Bours, Martijn J.L.},
  date = {2020-05},
  journaltitle = {Journal of Clinical Epidemiology},
  shortjournal = {Journal of Clinical Epidemiology},
  volume = {121},
  pages = {91--100},
  issn = {08954356},
  doi = {10.1016/j.jclinepi.2020.01.021},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0895435619301738},
  urldate = {2022-11-11},
  abstract = {In research addressing causal questions about relations between exposures and outcomes, confounding is an issue when effects of interrelated exposures on an outcome are confused. For making valid inferences about cause-and-effect relationships, the biasing influence of confounding must be controlled by design or eliminated during data analysis. Consequently, researchers require a sound understanding of the concept of confounding to adequately deal with this type of bias when setting up and conducting (clinical) epidemiological research. For explaining confounding on a conceptual level, the counterfactual framework for causal inference is invaluable but can be very complicated. In this article, therefore, a nontechnical explanation of the counterfactual definition of confounding is presented. When considering confounding in a counterfactual way, the principle of exchangeability plays a pivotal role. Causal effects of an exposure on an outcome can be evaluated only when different exposure groups have comparable background risks of the outcome. Then, exposure groups are exchangeable and thus unconfounded. By providing a simplified explanation of the counterfactual principles of exchangeability, and consequences of nonexchangeability, this article aims to increase understanding of confounding on a conceptual level as well as the rationale underlying design and analytic strategies for dealing with confounding in (clinical) epidemiological research. Ó 2020 Elsevier Inc. All rights reserved.},
  langid = {english},
  file = {/home/skynet3/Zotero/storage/YWAPIZQ5/Bours - 2020 - A nontechnical explanation of the counterfactual d.pdf}
}

@article{bowmakerArtPracticeEconomics,
  ids = {bowmakerArtPracticeEconomicsa},
  title = {The {{Art}} and {{Practice}} of {{Economics Research}}: {{Lessons}} from {{Leading Minds}} ({{Edward Elgar Publishing}}, 2012)},
  author = {Bowmaker, Simon W},
  pages = {16},
  langid = {english},
  file = {/home/skynet3/Zotero/storage/6M6SAEJ7/Bowmaker - The Art and Practice of Economics Research Lesson.pdf}
}

@misc{brabecModelEvaluationNonconstant2020,
  title = {On {{Model Evaluation}} under {{Non-constant Class Imbalance}}},
  author = {Brabec, Jan and Komárek, Tomáš and Franc, Vojtěch and Machlica, Lukáš},
  date = {2020-04-15},
  number = {arXiv:2001.05571},
  eprint = {2001.05571},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2001.05571},
  url = {http://arxiv.org/abs/2001.05571},
  urldate = {2022-11-11},
  abstract = {Many real-world classification problems are significantly class-imbalanced to detriment of the class of interest. The standard set of proper evaluation metrics is well-known but the usual assumption is that the test dataset imbalance equals the real-world imbalance. In practice, this assumption is often broken for various reasons. The reported results are then often too optimistic and may lead to wrong conclusions about industrial impact and suitability of proposed techniques. We introduce methods focusing on evaluation under non-constant class imbalance. We show that not only the absolute values of commonly used metrics, but even the order of classifiers in relation to the evaluation metric used is affected by the change of the imbalance rate. Finally, we demonstrate that using subsampling in order to get a test dataset with class imbalance equal to the one observed in the wild is not necessary, and eventually can lead to significant errors in classifier's performance estimate.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/skynet3/Zotero/storage/KTG43S9P/Brabec et al. - 2020 - On Model Evaluation under Non-constant Class Imbal.pdf;/home/skynet3/Zotero/storage/M8PNAT5I/2001.html}
}

@video{bradyneal-causalinferenceBackdoorAdjustment2020,
  title = {4.6 - {{The Backdoor Adjustment}}},
  editor = {{Brady Neal - Causal Inference}},
  date = {2020-09-21},
  url = {https://www.youtube.com/watch?v=U1S8Rq8IcrY},
  urldate = {2022-11-15},
  editortype = {director}
}

@video{bradyneal-causalinferenceStructuralCausalModels2020,
  title = {4.7 - {{Structural Causal Models SCMs}}},
  editor = {{Brady Neal - Causal Inference}},
  date = {2020-09-21},
  url = {https://www.youtube.com/watch?v=dQeRqb0N6gs},
  urldate = {2022-11-15},
  editortype = {director}
}

@video{bradyneal-causalinferenceWhatArePotential2020,
  title = {2.1 - {{What}} Are {{Potential Outcomes}}?},
  editor = {{Brady Neal - Causal Inference}},
  date = {2020-09-07},
  url = {https://www.youtube.com/watch?v=q8x9aetyok0},
  urldate = {2022-11-15},
  editortype = {director}
}

@misc{brancoSurveyPredictiveModelling2015,
  title = {A {{Survey}} of {{Predictive Modelling}} under {{Imbalanced Distributions}}},
  author = {Branco, Paula and Torgo, Luis and Ribeiro, Rita},
  date = {2015-05-13},
  number = {arXiv:1505.01658},
  eprint = {1505.01658},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/1505.01658},
  urldate = {2022-11-11},
  abstract = {Many real world data mining applications involve obtaining predictive models using data sets with strongly imbalanced distributions of the target variable. Frequently, the least common values of this target variable are associated with events that are highly relevant for end users (e.g. fraud detection, unusual returns on stock markets, anticipation of catastrophes, etc.). Moreover, the events may have different costs and benefits, which when associated with the rarity of some of them on the available training data creates serious problems to predictive modelling techniques. This paper presents a survey of existing techniques for handling these important applications of predictive analytics. Although most of the existing work addresses classification tasks (nominal target variables), we also describe methods designed to handle similar problems within regression tasks (numeric target variables). In this survey we discuss the main challenges raised by imbalanced distributions, describe the main approaches to these problems, propose a taxonomy of these methods and refer to some related problems within predictive modelling.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,I.2.6},
  file = {/home/skynet3/Zotero/storage/ZF9M57X7/Branco et al. - 2015 - A Survey of Predictive Modelling under Imbalanced .pdf;/home/skynet3/Zotero/storage/3QRUGPVL/1505.html}
}

@article{brandsenWhatEntropyNew2022,
  title = {What Is {{Entropy}}? {{A}} New Perspective from Games of Chance},
  shorttitle = {What Is {{Entropy}}?},
  author = {Brandsen, Sarah and Geng, Isabelle Jianing and Gour, Gilad},
  date = {2022-02-11},
  journaltitle = {Physical Review E},
  shortjournal = {Phys. Rev. E},
  volume = {105},
  number = {2},
  eprint = {2103.08681},
  eprinttype = {arxiv},
  primaryclass = {math-ph, physics:quant-ph},
  pages = {024117},
  issn = {2470-0045, 2470-0053},
  doi = {10.1103/PhysRevE.105.024117},
  url = {http://arxiv.org/abs/2103.08681},
  urldate = {2022-11-11},
  abstract = {Given entropy's central role in multiple areas of physics and science, one important task is to develop a systematic and unifying approach to defining entropy. Games of chance become a natural candidate for characterising the uncertainty of a physical system, as a system's performance in gambling games depends solely on the uncertainty of its output. In this work, we construct families of games which induce pre-orders corresponding to majorization, conditional majorization, and channel majorization. Finally, we provide operational interpretations for all pre-orders, show the relevance of these results to dynamical resource theories, and find the only asymptotically continuous classical dynamic entropy.},
  archiveprefix = {arXiv},
  keywords = {Mathematical Physics,Quantum Physics},
  file = {/home/skynet3/Zotero/storage/3P32E7I6/Brandsen et al. - 2022 - What is Entropy A new perspective from games of c.pdf;/home/skynet3/Zotero/storage/5BFK8AAX/2103.html}
}

@misc{bransonEvaluatingKeyInstrumental2019,
  title = {Evaluating {{A Key Instrumental Variable Assumption Using Randomization Tests}}},
  author = {Branson, Zach and Keele, Luke},
  date = {2019-07-03},
  number = {arXiv:1907.01943},
  eprint = {1907.01943},
  eprinttype = {arxiv},
  primaryclass = {stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1907.01943},
  url = {http://arxiv.org/abs/1907.01943},
  urldate = {2022-11-11},
  abstract = {Instrumental variable (IV) analyses are becoming common in health services research and epidemiology. Most IV analyses use naturally occurring instruments, such as distance to a hospital. In these analyses, investigators must assume the instrument is as-if randomly assigned. This assumption cannot be tested directly, but it can be falsified. Most falsification tests in the literature compare relative prevalence or bias in observed covariates between the instrument and the exposure. These tests require investigators to make a covariate-by-covariate judgment about the validity of the IV design. Often, only some of the covariates are well-balanced, making it unclear if as-if randomization can be assumed for the instrument across all covariates. We propose an alternative falsification test that compares IV balance or bias to the balance or bias that would have been produced under randomization. A key advantage of our test is that it allows for global balance measures as well as easily interpretable graphical comparisons. Furthermore, our test does not rely on any parametric assumptions and can be used to validly assess if the instrument is significantly closer to being as-if randomized than the exposure. We demonstrate our approach on a recent IV application that uses bed availability in the intensive care unit (ICU) as an instrument for admission to the ICU.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Methodology},
  file = {/home/skynet3/Zotero/storage/B2DYRL9X/Branson and Keele - 2019 - Evaluating A Key Instrumental Variable Assumption .pdf;/home/skynet3/Zotero/storage/2SC5KX9E/1907.html}
}

@online{BreimanTwoCultures2020,
  title = {Breiman's "{{Two Cultures}}" {{Revisited}} and {{Reconciled}}},
  date = {2020-05-27T19:02:56+00:00},
  url = {https://deepai.org/publication/breiman-s-two-cultures-revisited-and-reconciled},
  urldate = {2022-11-11},
  abstract = {05/27/20 - In a landmark paper published in 2001, Leo Breiman described the tense standoff between two cultures of data modeling: parametric ...},
  organization = {{DeepAI}},
  file = {/home/skynet3/Zotero/storage/LDDFPENY/breiman-s-two-cultures-revisited-and-reconciled.html}
}

@online{bremen79MythExpertReviewer2021,
  title = {The {{Myth}} of the {{Expert Reviewer}}},
  author = {{bremen79}},
  date = {2021-07-06T15:50:54+00:00},
  url = {https://parameterfree.com/2021/07/06/the-myth-of-the-expert-reviewer/},
  urldate = {2022-11-11},
  abstract = {1. Prologue First of all, two months ago I got the official announcement that I got tenure. This is the end of a journey that started in 2003 with a PhD in computer vision for humanoid robotics at …},
  langid = {english},
  organization = {{Parameter-free Learning and Optimization Algorithms}},
  file = {/home/skynet3/Zotero/storage/CH34J8NK/the-myth-of-the-expert-reviewer.html}
}

@software{bresslerCloudForest2022,
  title = {{{CloudForest}}},
  author = {Bressler, Ryan},
  date = {2022-10-27T11:20:00Z},
  origdate = {2012-10-22T17:38:16Z},
  url = {https://github.com/ryanbressler/CloudForest},
  urldate = {2022-11-11},
  abstract = {Ensembles of decision trees in go/golang.}
}

@article{breznauObservingManyResearchers2022,
  title = {Observing Many Researchers Using the Same Data and Hypothesis Reveals a Hidden Universe of Uncertainty},
  author = {Breznau, Nate and Rinke, Eike Mark and Wuttke, Alexander and Nguyen, Hung H. V. and Adem, Muna and Adriaans, Jule and Alvarez-Benjumea, Amalia and Andersen, Henrik K. and Auer, Daniel and Azevedo, Flavio and Bahnsen, Oke and Balzer, Dave and Bauer, Gerrit and Bauer, Paul C. and Baumann, Markus and Baute, Sharon and Benoit, Verena and Bernauer, Julian and Berning, Carl and Berthold, Anna and Bethke, Felix S. and Biegert, Thomas and Blinzler, Katharina and Blumenberg, Johannes N. and Bobzien, Licia and Bohman, Andrea and Bol, Thijs and Bostic, Amie and Brzozowska, Zuzanna and Burgdorf, Katharina and Burger, Kaspar and Busch, Kathrin B. and Carlos-Castillo, Juan and Chan, Nathan and Christmann, Pablo and Connelly, Roxanne and Czymara, Christian S. and Damian, Elena and Ecker, Alejandro and Edelmann, Achim and Eger, Maureen A. and Ellerbrock, Simon and Forke, Anna and Forster, Andrea and Gaasendam, Chris and Gavras, Konstantin and Gayle, Vernon and Gessler, Theresa and Gnambs, Timo and Godefroidt, Amélie and Grömping, Max and Groß, Martin and Gruber, Stefan and Gummer, Tobias and Hadjar, Andreas and Heisig, Jan Paul and Hellmeier, Sebastian and Heyne, Stefanie and Hirsch, Magdalena and Hjerm, Mikael and Hochman, Oshrat and Hövermann, Andreas and Hunger, Sophia and Hunkler, Christian and Huth, Nora and Ignácz, Zsófia S. and Jacobs, Laura and Jacobsen, Jannes and Jaeger, Bastian and Jungkunz, Sebastian and Jungmann, Nils and Kauff, Mathias and Kleinert, Manuel and Klinger, Julia and Kolb, Jan-Philipp and Kołczyńska, Marta and Kuk, John and Kunißen, Katharina and Kurti Sinatra, Dafina and Langenkamp, Alexander and Lersch, Philipp M. and Löbel, Lea-Maria and Lutscher, Philipp and Mader, Matthias and Madia, Joan E. and Malancu, Natalia and Maldonado, Luis and Marahrens, Helge and Martin, Nicole and Martinez, Paul and Mayerl, Jochen and Mayorga, Oscar J. and McManus, Patricia and McWagner, Kyle and Meeusen, Cecil and Meierrieks, Daniel and Mellon, Jonathan and Merhout, Friedolin and Merk, Samuel and Meyer, Daniel and Micheli, Leticia and Mijs, Jonathan and Moya, Cristóbal and Neunhoeffer, Marcel and Nüst, Daniel and Nygård, Olav and Ochsenfeld, Fabian and Otte, Gunnar and Pechenkina, Anna O. and Prosser, Christopher and Raes, Louis and Ralston, Kevin and Ramos, Miguel R. and Roets, Arne and Rogers, Jonathan and Ropers, Guido and Samuel, Robin and Sand, Gregor and Schachter, Ariela and Schaeffer, Merlin and Schieferdecker, David and Schlueter, Elmar and Schmidt, Regine and Schmidt, Katja M. and Schmidt-Catran, Alexander and Schmiedeberg, Claudia and Schneider, Jürgen and Schoonvelde, Martijn and Schulte-Cloos, Julia and Schumann, Sandy and Schunck, Reinhard and Schupp, Jürgen and Seuring, Julian and Silber, Henning and Sleegers, Willem and Sonntag, Nico and Staudt, Alexander and Steiber, Nadia and Steiner, Nils and Sternberg, Sebastian and Stiers, Dieter and Stojmenovska, Dragana and Storz, Nora and Striessnig, Erich and Stroppe, Anne-Kathrin and Teltemann, Janna and Tibajev, Andrey and Tung, Brian and Vagni, Giacomo and Van Assche, Jasper and van der Linden, Meta and van der Noll, Jolanda and Van Hootegem, Arno and Vogtenhuber, Stefan and Voicu, Bogdan and Wagemans, Fieke and Wehl, Nadja and Werner, Hannah and Wiernik, Brenton M. and Winter, Fabian and Wolf, Christof and Yamada, Yuki and Zhang, Nan and Ziller, Conrad and Zins, Stefan and Żółtak, Tomasz},
  options = {useprefix=true},
  date = {2022-11},
  journaltitle = {Proceedings of the National Academy of Sciences},
  volume = {119},
  number = {44},
  pages = {e2203150119},
  publisher = {{Proceedings of the National Academy of Sciences}},
  doi = {10.1073/pnas.2203150119},
  url = {https://www.pnas.org/doi/full/10.1073/pnas.2203150119},
  urldate = {2022-11-11},
  abstract = {This study explores how researchers’ analytical choices affect the reliability of scientific findings. Most discussions of reliability problems in science focus on systematic biases. We broaden the lens to emphasize the idiosyncrasy of conscious and unconscious decisions that researchers make during data analysis. We coordinated 161 researchers in 73 research teams and observed their research decisions as they used the same data to independently test the same prominent social science hypothesis: that greater immigration reduces support for social policies among the public. In this typical case of social science research, research teams reported both widely diverging numerical findings and substantive conclusions despite identical start conditions. Researchers’ expertise, prior beliefs, and expectations barely predict the wide variation in research outcomes. More than 95\% of the total variance in numerical results remains unexplained even after qualitative coding of all identifiable decisions in each team’s workflow. This reveals a universe of uncertainty that remains hidden when considering a single study in isolation. The idiosyncratic nature of how researchers’ results and conclusions varied is a previously underappreciated explanation for why many scientific hypotheses remain contested. These results call for greater epistemic humility and clarity in reporting scientific findings.},
  file = {/home/skynet3/Zotero/storage/7LGDDSR6/Breznau et al. - 2022 - Observing many researchers using the same data and.pdf}
}

@misc{britoGeneralizedInstrumentalVariables2012,
  ids = {britoGeneralizedInstrumentalVariables2012a},
  title = {Generalized {{Instrumental Variables}}},
  author = {Brito, Carlos and Pearl, Judea},
  date = {2012-12-12},
  number = {arXiv:1301.0560},
  eprint = {1301.0560},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/1301.0560},
  urldate = {2022-11-11},
  abstract = {This paper concerns the assessment of direct causal effects from a combination of: (i) non-experimental data, and (ii) qualitative domain knowledge. Domain knowledge is encoded in the form of a directed acyclic graph (DAG), in which all interactions are assumed linear, and some variables are presumed to be unobserved. We provide a generalization of the well-known method of Instrumental Variables, which allows its application to models with few conditional independeces.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence},
  file = {/home/skynet3/Zotero/storage/C6B9WBXR/Brito and Pearl - 2012 - Generalized Instrumental Variables.pdf;/home/skynet3/Zotero/storage/M9FZVM8Y/1301.html}
}

@article{broCrossvalidationComponentModels2008,
  title = {Cross-Validation of Component Models: {{A}} Critical Look at Current Methods},
  shorttitle = {Cross-Validation of Component Models},
  author = {Bro, R. and Kjeldahl, K. and Smilde, A. K. and Kiers, H. A. L.},
  date = {2008-03},
  journaltitle = {Analytical and Bioanalytical Chemistry},
  shortjournal = {Anal Bioanal Chem},
  volume = {390},
  number = {5},
  pages = {1241--1251},
  issn = {1618-2642, 1618-2650},
  doi = {10.1007/s00216-007-1790-1},
  url = {http://link.springer.com/10.1007/s00216-007-1790-1},
  urldate = {2022-11-11},
  langid = {english}
}

@article{brodeurMethodsMatterPHacking2018,
  title = {Methods {{Matter}}: {{P-Hacking}} and {{Causal Inference}} in {{Economics}}},
  shorttitle = {Methods {{Matter}}},
  author = {Brodeur, Abel and Cook, Nikolai and Heyes, Anthony},
  date = {2018},
  journaltitle = {SSRN Electronic Journal},
  shortjournal = {SSRN Journal},
  issn = {1556-5068},
  doi = {10.2139/ssrn.3249910},
  url = {https://www.ssrn.com/abstract=3249910},
  urldate = {2022-11-11},
  langid = {english},
  file = {/home/skynet3/Zotero/storage/5QHJZTKM/Brodeur et al. - 2018 - Methods Matter P-Hacking and Causal Inference in .pdf}
}

@article{brodeurMethodsMatterPHacking2020,
  ids = {brodeurMethodsMatterPHacking2020a},
  title = {Methods {{Matter}}: P-{{Hacking}} and {{Publication Bias}} in {{Causal Analysis}} in {{Economics}}},
  shorttitle = {Methods {{Matter}}},
  author = {Brodeur, Abel and Cook, Nikolai and Heyes, Anthony},
  date = {2020-11-01},
  journaltitle = {American Economic Review},
  shortjournal = {American Economic Review},
  volume = {110},
  number = {11},
  pages = {3634--3660},
  issn = {0002-8282},
  doi = {10.1257/aer.20190687},
  url = {https://pubs.aeaweb.org/doi/10.1257/aer.20190687},
  urldate = {2022-11-11},
  abstract = {The credibility revolution in economics has promoted causal identification using randomized control trials (RCT), difference-in-differences (DID), instrumental variables (IV) and regression discontinuity design (RDD). Applying multiple approaches to over 21,000 hypothesis tests published in 25 leading economics journals, we find that the extent of p-hacking and publication bias varies greatly by method. IV (and to a lesser extent DID) are particularly problematic. We find no evidence that (i) papers published in the Top 5 journals are different to others; (ii) the journal “revise and resubmit” process mitigates the problem; (iii) things are improving through time. (JEL A14, C12, C52)},
  langid = {english},
  file = {/home/skynet3/Zotero/storage/2BM46KJP/Brodeur et al. - 2020 - Methods Matter p-Hacking and Publication Bias in .pdf}
}

@article{brodeurPHackingDataType2022,
  title = {P-{{Hacking}}, {{Data Type}} and {{Data-Sharing Policy}}},
  author = {Brodeur, Abel and Cook, Nikolai and Neisser, Carina},
  date = {2022},
  journaltitle = {SSRN Electronic Journal},
  shortjournal = {SSRN Journal},
  issn = {1556-5068},
  doi = {10.2139/ssrn.4232716},
  url = {https://www.ssrn.com/abstract=4232716},
  urldate = {2022-11-11},
  langid = {english},
  file = {/home/skynet3/Zotero/storage/KLR4D7VV/Brodeur et al. - 2022 - P-Hacking, Data Type and Data-Sharing Policy.pdf}
}

@misc{brodeurPreRegistrationPreanalysisPlans2022,
  title = {Do {{Pre-Registration}} and {{Pre-analysis Plans Reduce}} p-{{Hacking}} and {{Publication Bias}}?},
  author = {Brodeur, Abel and Cook, Nikolai and Hartley, Jonathan and Heyes, Anthony},
  date = {2022-08-11T18:29:46},
  publisher = {{MetaArXiv}},
  doi = {10.31222/osf.io/uxf39},
  url = {https://osf.io/preprints/metaarxiv/uxf39/},
  urldate = {2022-11-11},
  abstract = {Randomized controlled trials (RCTs) are increasingly prominent in economics, with pre-registration and pre-analysis plans (PAPs) promoted as important in ensuring the credibility of findings. We investigate whether these tools reduce the extent of p-hacking and publication bias by collecting and studying the universe of test statistics, 15,992 in total, from RCTs published in 15 leading economics journals from 2018 through 2021. In our primary analysis, we find no meaningful difference in the distribution of test statistics from pre-registered studies, compared to their non-pre-registered counterparts. However, pre-registered studies that have a complete PAP are significantly less p-hacked. These results point to the importance of PAPs, rather than pre-registration in itself, in ensuring credibility.},
  langid = {american},
  keywords = {Econometrics,Economics,p-Hacking,Pre-analysis plan,Pre-registration,Publication bias,Social and Behavioral Sciences},
  file = {/home/skynet3/Zotero/storage/E5UTZQ6N/Brodeur et al. - 2022 - Do Pre-Registration and Pre-analysis Plans Reduce .pdf}
}

@article{brodeurStarWarsEmpirics2016,
  title = {Star {{Wars}}: {{The Empirics Strike Back}}},
  shorttitle = {Star {{Wars}}},
  author = {Brodeur, Abel and Lé, Mathias and Sangnier, Marc and Zylberberg, Yanos},
  date = {2016-01},
  journaltitle = {American Economic Journal: Applied Economics},
  volume = {8},
  number = {1},
  pages = {1--32},
  issn = {1945-7782},
  doi = {10.1257/app.20150044},
  url = {https://www.aeaweb.org/articles?id=10.1257/app.20150044},
  urldate = {2022-11-11},
  abstract = {Using 50,000 tests published in the AER, JPE, and QJE, we identify a residual in the distribution of tests that cannot be explained solely by journals favoring rejection of the null hypothesis. We observe a two-humped camel shape with missing p-values between 0.25 and 0.10 that can be retrieved just after the 0.05 threshold and represent 10-20 percent of marginally rejected tests. Our interpretation is that researchers inflate the value of just-rejected tests by choosing "significant" specifications. We propose a method to measure this residual and describe how it varies by article and author characteristics. (JEL A11, C13)},
  langid = {english},
  keywords = {Market for Economists; Estimation: General,Role of Economics,Role of Economists},
  file = {/home/skynet3/Zotero/storage/GD46MTDH/Brodeur et al. - 2016 - Star Wars The Empirics Strike Back.pdf;/home/skynet3/Zotero/storage/HNVZ2B94/articles.html}
}

@online{BrodieGDiffobjCompare,
  title = {{{brodieG}}/Diffobj: {{Compare R Objects}} with a {{Diff}}},
  shorttitle = {{{brodieG}}/Diffobj},
  url = {https://github.com/brodieG/diffobj},
  urldate = {2022-11-11},
  abstract = {Compare R Objects with a Diff. Contribute to brodieG/diffobj development by creating an account on GitHub.},
  langid = {english},
  organization = {{GitHub}},
  file = {/home/skynet3/Zotero/storage/WT6ZMNCG/diffobj.html}
}

@article{brownGRIMTestSimple2017,
  ids = {brownGRIMTestSimple2017a},
  title = {The {{GRIM Test}}: {{A Simple Technique Detects Numerous Anomalies}} in the {{Reporting}} of {{Results}} in {{Psychology}}},
  shorttitle = {The {{GRIM Test}}},
  author = {Brown, Nicholas J. L. and Heathers, James A. J.},
  date = {2017-05-01},
  journaltitle = {Social Psychological and Personality Science},
  volume = {8},
  number = {4},
  pages = {363--369},
  publisher = {{SAGE Publications Inc}},
  issn = {1948-5506},
  doi = {10.1177/1948550616673876},
  url = {https://doi.org/10.1177/1948550616673876},
  urldate = {2022-11-11},
  abstract = {We present a simple mathematical technique that we call granularity-related inconsistency of means (GRIM) for verifying the summary statistics of research reports in psychology. This technique evaluates whether the reported means of integer data such as Likert-type scales are consistent with the given sample size and number of items. We tested this technique with a sample of 260 recent empirical articles in leading journals. Of the articles that we could test with the GRIM technique (N = 71), around half (N = 36) appeared to contain at least one inconsistent mean, and more than 20\% (N = 16) contained multiple such inconsistencies. We requested the data sets corresponding to 21 of these articles, receiving positive responses in 9 cases. We confirmed the presence of at least one reporting error in all cases, with three articles requiring extensive corrections. The implications for the reliability and replicability of empirical psychology are discussed.},
  langid = {english}
}

@book{bryanSTAT545,
  title = {{{STAT}} 545},
  author = {Bryan, Jenny and TAs, The STAT 545},
  url = {https://stat545.com/},
  urldate = {2022-11-11},
  abstract = {STAT 545: Data wrangling, exploration, and analysis with R.},
  file = {/home/skynet3/Zotero/storage/ZH89QYAA/index.html}
}

@article{buckBewarePerformativeReproducibility2021,
  title = {Beware Performative Reproducibility},
  author = {Buck, Stuart},
  date = {2021-07-06},
  journaltitle = {Nature},
  volume = {595},
  number = {7866},
  pages = {151--151},
  publisher = {{Nature Publishing Group}},
  doi = {10.1038/d41586-021-01824-z},
  url = {https://www.nature.com/articles/d41586-021-01824-z},
  urldate = {2022-11-11},
  abstract = {Well-meant changes to improve science could become empty gestures unless underlying values change.},
  issue = {7866},
  langid = {english},
  keywords = {Institutions,Research management},
  annotation = {Bandiera\_abtest: a Cg\_type: World View Subject\_term: Research management, Institutions},
  file = {/home/skynet3/Zotero/storage/82YG2JN3/Buck - 2021 - Beware performative reproducibility.pdf;/home/skynet3/Zotero/storage/ZDKPM6L4/d41586-021-01824-z.html}
}

@online{BuildingUsefulModels,
  title = {Building Useful Models for Industry—Some Tips},
  url = {https://khakieconomics.github.io/2017/01/01/Building-useful-models-for-industry.html},
  urldate = {2022-11-11}
}

@misc{bulsoComplexityLogisticRegression2019,
  title = {On the Complexity of Logistic Regression Models},
  author = {Bulso, Nicola and Marsili, Matteo and Roudi, Yasser},
  date = {2019-03-01},
  number = {arXiv:1903.00386},
  eprint = {1903.00386},
  eprinttype = {arxiv},
  primaryclass = {cond-mat, physics:physics, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1903.00386},
  url = {http://arxiv.org/abs/1903.00386},
  urldate = {2022-11-11},
  abstract = {We investigate the complexity of logistic regression models which is defined by counting the number of indistinguishable distributions that the model can represent (Balasubramanian, 1997). We find that the complexity of logistic models with binary inputs does not only depend on the number of parameters but also on the distribution of inputs in a non-trivial way which standard treatments of complexity do not address. In particular, we observe that correlations among inputs induce effective dependencies among parameters thus constraining the model and, consequently, reducing its complexity. We derive simple relations for the upper and lower bounds of the complexity. Furthermore, we show analytically that, defining the model parameters on a finite support rather than the entire axis, decreases the complexity in a manner that critically depends on the size of the domain. Based on our findings, we propose a novel model selection criterion which takes into account the entropy of the input distribution. We test our proposal on the problem of selecting the input variables of a logistic regression model in a Bayesian Model Selection framework. In our numerical tests, we find that, while the reconstruction errors of standard model selection approaches (AIC, BIC, \$\textbackslash ell\_1\$ regularization) strongly depend on the sparsity of the ground truth, the reconstruction error of our method is always close to the minimum in all conditions of sparsity, data size and strength of input correlations. Finally, we observe that, when considering categorical instead of binary inputs, in a simple and mathematically tractable case, the contribution of the alphabet size to the complexity is very small compared to that of parameter space dimension. We further explore the issue by analysing the dataset of the "13 keys to the White House" which is a method for forecasting the outcomes of US presidential elections.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Condensed Matter - Disordered Systems and Neural Networks,Physics - Data Analysis; Statistics and Probability,Statistics - Machine Learning},
  file = {/home/skynet3/Zotero/storage/76LGMVXD/Bulso et al. - 2019 - On the complexity of logistic regression models.pdf;/home/skynet3/Zotero/storage/MGESFX8J/1903.html}
}

@article{buntgenInfluenceDecisionmakingTree2021,
  title = {The Influence of Decision-Making in Tree Ring-Based Climate Reconstructions},
  author = {Büntgen, Ulf and Allen, Kathy and Anchukaitis, Kevin J. and Arseneault, Dominique and Boucher, Étienne and Bräuning, Achim and Chatterjee, Snigdhansu and Cherubini, Paolo and Churakova (Sidorova), Olga V. and Corona, Christophe and Gennaretti, Fabio and Grießinger, Jussi and Guillet, Sebastian and Guiot, Joel and Gunnarson, Björn and Helama, Samuli and Hochreuther, Philipp and Hughes, Malcolm K. and Huybers, Peter and Kirdyanov, Alexander V. and Krusic, Paul J. and Ludescher, Josef and Meier, Wolfgang J.-H. and Myglan, Vladimir S. and Nicolussi, Kurt and Oppenheimer, Clive and Reinig, Frederick and Salzer, Matthew W. and Seftigen, Kristina and Stine, Alexander R. and Stoffel, Markus and St. George, Scott and Tejedor, Ernesto and Trevino, Aleyda and Trouet, Valerie and Wang, Jianglin and Wilson, Rob and Yang, Bao and Xu, Guobao and Esper, Jan},
  date = {2021-06-07},
  journaltitle = {Nature Communications},
  shortjournal = {Nat Commun},
  volume = {12},
  number = {1},
  pages = {3411},
  publisher = {{Nature Publishing Group}},
  issn = {2041-1723},
  doi = {10.1038/s41467-021-23627-6},
  url = {https://www.nature.com/articles/s41467-021-23627-6},
  urldate = {2022-11-11},
  abstract = {Tree-ring chronologies underpin the majority of annually-resolved reconstructions of Common Era climate. However, they are derived using different datasets and techniques, the ramifications of which have hitherto been little explored. Here, we report the results of a double-blind experiment that yielded 15 Northern Hemisphere summer temperature reconstructions from a common network of regional tree-ring width datasets. Taken together as an ensemble, the Common Era reconstruction mean correlates with instrumental temperatures from 1794–2016 CE at 0.79 (p\,{$<$}\,0.001), reveals summer cooling in the years following large volcanic eruptions, and exhibits strong warming since the 1980s. Differing in their mean, variance, amplitude, sensitivity, and persistence, the ensemble members demonstrate the influence of subjectivity in the reconstruction process. We therefore recommend the routine use of ensemble reconstruction approaches to provide a more~consensual picture of past climate variability.},
  issue = {1},
  langid = {english},
  keywords = {Climate change,Palaeoclimate,Research data},
  file = {/home/skynet3/Zotero/storage/MVKZB746/Büntgen et al. - 2021 - The influence of decision-making in tree ring-base.pdf;/home/skynet3/Zotero/storage/YMM9K9LP/s41467-021-23627-6.html}
}

@misc{burgEvaluationChangePoint2022,
  ids = {burgEvaluationChangePoint2022a},
  title = {An {{Evaluation}} of {{Change Point Detection Algorithms}}},
  author = {van den Burg, Gerrit J. J. and Williams, Christopher K. I.},
  date = {2022-02-12},
  number = {arXiv:2003.06222},
  eprint = {2003.06222},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2003.06222},
  url = {http://arxiv.org/abs/2003.06222},
  urldate = {2022-11-11},
  abstract = {Change point detection is an important part of time series analysis, as the presence of a change point indicates an abrupt and significant change in the data generating process. While many algorithms for change point detection have been proposed, comparatively little attention has been paid to evaluating their performance on real-world time series. Algorithms are typically evaluated on simulated data and a small number of commonly-used series with unreliable ground truth. Clearly this does not provide sufficient insight into the comparative performance of these algorithms. Therefore, instead of developing yet another change point detection method, we consider it vastly more important to properly evaluate existing algorithms on real-world data. To achieve this, we present a data set specifically designed for the evaluation of change point detection algorithms that consists of 37 time series from various application domains. Each series was annotated by five human annotators to provide ground truth on the presence and location of change points. We analyze the consistency of the human annotators, and describe evaluation metrics that can be used to measure algorithm performance in the presence of multiple ground truth annotations. Next, we present a benchmark study where 14 algorithms are evaluated on each of the time series in the data set. Our aim is that this data set will serve as a proving ground in the development of novel change point detection algorithms.},
  archiveprefix = {arXiv},
  keywords = {62M10,Computer Science - Machine Learning,G.3,Statistics - Machine Learning,Statistics - Methodology},
  file = {/home/skynet3/Zotero/storage/I5NSPHHT/Burg and Williams - 2022 - An Evaluation of Change Point Detection Algorithms.pdf;/home/skynet3/Zotero/storage/UNZCKYRB/2003.html}
}

@article{burknerApproximateLeavefutureoutCrossvalidation2020,
  ids = {burknerApproximateLeavefutureoutCrossvalidation2020a},
  title = {Approximate Leave-Future-out Cross-Validation for {{Bayesian}} Time Series Models},
  author = {Bürkner, Paul-Christian and Gabry, Jonah and Vehtari, Aki},
  date = {2020-09-21},
  journaltitle = {Journal of Statistical Computation and Simulation},
  shortjournal = {Journal of Statistical Computation and Simulation},
  volume = {90},
  number = {14},
  eprint = {1902.06281},
  eprinttype = {arxiv},
  primaryclass = {stat},
  pages = {2499--2523},
  issn = {0094-9655, 1563-5163},
  doi = {10.1080/00949655.2020.1783262},
  url = {http://arxiv.org/abs/1902.06281},
  urldate = {2022-11-11},
  abstract = {One of the common goals of time series analysis is to use the observed series to inform predictions for future observations. In the absence of any actual new data to predict, cross-validation can be used to estimate a model's future predictive accuracy, for instance, for the purpose of model comparison or selection. Exact cross-validation for Bayesian models is often computationally expensive, but approximate cross-validation methods have been developed, most notably methods for leave-one-out cross-validation (LOO-CV). If the actual prediction task is to predict the future given the past, LOO-CV provides an overly optimistic estimate because the information from future observations is available to influence predictions of the past. To properly account for the time series structure, we can use leave-future-out cross-validation (LFO-CV). Like exact LOO-CV, exact LFO-CV requires refitting the model many times to different subsets of the data. Using Pareto smoothed importance sampling, we propose a method for approximating exact LFO-CV that drastically reduces the computational costs while also providing informative diagnostics about the quality of the approximation.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Methodology},
  file = {/home/skynet3/Zotero/storage/UF2Z6U3J/Bürkner et al. - 2020 - Approximate leave-future-out cross-validation for .pdf;/home/skynet3/Zotero/storage/R3P5LZML/1902.html}
}

@misc{burknerBayesianItemResponse2020,
  title = {Bayesian {{Item Response Modeling}} in {{R}} with Brms and {{Stan}}},
  author = {Bürkner, Paul-Christian},
  date = {2020-02-01},
  number = {arXiv:1905.09501},
  eprint = {1905.09501},
  eprinttype = {arxiv},
  primaryclass = {stat},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/1905.09501},
  urldate = {2022-11-11},
  abstract = {Item Response Theory (IRT) is widely applied in the human sciences to model persons' responses on a set of items measuring one or more latent constructs. While several R packages have been developed that implement IRT models, they tend to be restricted to respective prespecified classes of models. Further, most implementations are frequentist while the availability of Bayesian methods remains comparably limited. We demonstrate how to use the R package brms together with the probabilistic programming language Stan to specify and fit a wide range of Bayesian IRT models using flexible and intuitive multilevel formula syntax. Further, item and person parameters can be related in both a linear or non-linear manner. Various distributions for categorical, ordinal, and continuous responses are supported. Users may even define their own custom response distribution for use in the presented framework. Common IRT model classes that can be specified natively in the presented framework include 1PL and 2PL logistic models optionally also containing guessing parameters, graded response and partial credit ordinal models, as well as drift diffusion models of response times coupled with binary decisions. Posterior distributions of item and person parameters can be conveniently extracted and post-processed. Model fit can be evaluated and compared using Bayes factors and efficient cross-validation procedures.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Computation},
  file = {/home/skynet3/Zotero/storage/2XSKXFT2/Bürkner - 2020 - Bayesian Item Response Modeling in R with brms and.pdf;/home/skynet3/Zotero/storage/SD73QPZ8/1905.html}
}

@software{burknerBrms2022,
  title = {Brms},
  author = {Bürkner, Paul-Christian},
  date = {2022-11-11T09:08:53Z},
  origdate = {2015-06-17T16:29:31Z},
  url = {https://github.com/paul-buerkner/brms},
  urldate = {2022-11-11},
  abstract = {brms R package for Bayesian generalized multivariate non-linear multilevel models using Stan},
  keywords = {bayesian-inference,brms,multilevel-models,r-package,stan,statistical-models}
}

@online{burrsettlesThreeFacesBayes2016,
  title = {The {{Three Faces}} of {{Bayes}}},
  author = {{BURRSETTLES}},
  date = {2016-08-28T22:29:02+00:00},
  url = {https://slackprop.wordpress.com/2016/08/28/the-three-faces-of-bayes/},
  urldate = {2022-11-11},
  abstract = {Last summer, I was at a conference having lunch with Hal Daumé III~when~we got to talking about how “Bayesian” can be~a funny and ambiguous term. It seems like the definition should be …},
  langid = {english},
  organization = {{Slackpropagation}}
}

@software{buttsDidimputation2022,
  title = {Didimputation},
  author = {Butts, Kyle F.},
  date = {2022-11-05T05:48:42Z},
  origdate = {2021-07-26T22:07:07Z},
  url = {https://github.com/kylebutts/didimputation},
  urldate = {2022-11-11},
  abstract = {Difference-in-differences Imputation-based Estimator proposed by Borusyak, Jaravel, and Spiess (2021)}
}

@article{buttsGeographicDifferenceinDiscontinuities2021,
  ids = {buttsGeographicDifferenceinDiscontinuities2021a},
  title = {Geographic {{Difference-in-Discontinuities}}},
  author = {Butts, Kyle},
  date = {2021-11-23},
  journaltitle = {Applied Economics Letters},
  shortjournal = {Applied Economics Letters},
  eprint = {2109.07406},
  eprinttype = {arxiv},
  primaryclass = {econ},
  pages = {1--5},
  issn = {1350-4851, 1466-4291},
  doi = {10.1080/13504851.2021.2005236},
  url = {http://arxiv.org/abs/2109.07406},
  urldate = {2022-11-11},
  abstract = {A recent econometric literature has critiqued the use of regression discontinuities where administrative borders serves as the 'cutoff'. Identification in this context is difficult since multiple treatments can change at the cutoff and individuals can easily sort on either side of the border. This note extends the difference-in-discontinuities framework discussed in Grembi et. al. (2016) to a geographic setting. The paper formalizes the identifying assumptions in this context which will allow for the removal of time-invariant sorting and compound-treatments similar to the difference-in-differences methodology.},
  archiveprefix = {arXiv},
  keywords = {Economics - Econometrics},
  file = {/home/skynet3/Zotero/storage/5646W57L/Butts - 2021 - Geographic Difference-in-Discontinuities.pdf;/home/skynet3/Zotero/storage/BRNYU636/2109.html}
}

@software{byronXgboostSurv2021,
  title = {Xgboost.Surv},
  author = {Byron},
  date = {2021-08-31T06:13:58Z},
  origdate = {2019-10-24T00:37:28Z},
  url = {https://github.com/bcjaeger/xgboost.surv},
  urldate = {2022-11-11},
  abstract = {Extreme Gradient Boosting for Survival Analysis}
}

@article{bzdokInferencePredictionDiverge2020,
  ids = {bzdokInferencePredictionDiverge2020a},
  title = {Inference and {{Prediction Diverge}} in {{Biomedicine}}},
  author = {Bzdok, Danilo and Engemann, Denis and Thirion, Bertrand},
  date = {2020-11-13},
  journaltitle = {Patterns},
  shortjournal = {PATTER},
  volume = {1},
  number = {8},
  publisher = {{Elsevier}},
  issn = {2666-3899},
  doi = {10.1016/j.patter.2020.100119},
  url = {https://www.cell.com/patterns/abstract/S2666-3899(20)30160-4},
  urldate = {2022-11-11},
  langid = {english},
  keywords = {data science,DSML 3: Development/Pre-production: Data science output has been rolled out/validated across multiple domains/problems,explainable AI,reproducibility,scientific discovery,variable importance},
  file = {/home/skynet3/Zotero/storage/WWJUUVTR/Bzdok et al. - 2020 - Inference and Prediction Diverge in Biomedicine.pdf;/home/skynet3/Zotero/storage/E33MAXAG/S2666-3899(20)30160-4.html}
}

@misc{cachayEndtoEndWeakSupervision2021,
  title = {End-to-{{End Weak Supervision}}},
  author = {Cachay, Salva Rühling and Boecking, Benedikt and Dubrawski, Artur},
  date = {2021-11-30},
  number = {arXiv:2107.02233},
  eprint = {2107.02233},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2107.02233},
  url = {http://arxiv.org/abs/2107.02233},
  urldate = {2022-11-11},
  abstract = {Aggregating multiple sources of weak supervision (WS) can ease the data-labeling bottleneck prevalent in many machine learning applications, by replacing the tedious manual collection of ground truth labels. Current state of the art approaches that do not use any labeled training data, however, require two separate modeling steps: Learning a probabilistic latent variable model based on the WS sources -- making assumptions that rarely hold in practice -- followed by downstream model training. Importantly, the first step of modeling does not consider the performance of the downstream model. To address these caveats we propose an end-to-end approach for directly learning the downstream model by maximizing its agreement with probabilistic labels generated by reparameterizing previous probabilistic posteriors with a neural network. Our results show improved performance over prior work in terms of end model performance on downstream test sets, as well as in terms of improved robustness to dependencies among weak supervision sources.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/skynet3/Zotero/storage/MXBC5ILC/Cachay et al. - 2021 - End-to-End Weak Supervision.pdf;/home/skynet3/Zotero/storage/8MU53P7K/2107.html}
}

@misc{caiComprehensiveSurveyGraph2018,
  title = {A {{Comprehensive Survey}} of {{Graph Embedding}}: {{Problems}}, {{Techniques}} and {{Applications}}},
  shorttitle = {A {{Comprehensive Survey}} of {{Graph Embedding}}},
  author = {Cai, Hongyun and Zheng, Vincent W. and Chang, Kevin Chen-Chuan},
  date = {2018-02-02},
  number = {arXiv:1709.07604},
  eprint = {1709.07604},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1709.07604},
  url = {http://arxiv.org/abs/1709.07604},
  urldate = {2022-11-11},
  abstract = {Graph is an important data representation which appears in a wide diversity of real-world scenarios. Effective graph analytics provides users a deeper understanding of what is behind the data, and thus can benefit a lot of useful applications such as node classification, node recommendation, link prediction, etc. However, most graph analytics methods suffer the high computation and space cost. Graph embedding is an effective yet efficient way to solve the graph analytics problem. It converts the graph data into a low dimensional space in which the graph structural information and graph properties are maximally preserved. In this survey, we conduct a comprehensive review of the literature in graph embedding. We first introduce the formal definition of graph embedding as well as the related concepts. After that, we propose two taxonomies of graph embedding which correspond to what challenges exist in different graph embedding problem settings and how the existing work address these challenges in their solutions. Finally, we summarize the applications that graph embedding enables and suggest four promising future research directions in terms of computation efficiency, problem settings, techniques and application scenarios.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence},
  file = {/home/skynet3/Zotero/storage/UQZTSGXY/Cai et al. - 2018 - A Comprehensive Survey of Graph Embedding Problem.pdf;/home/skynet3/Zotero/storage/9Y2WAJ5A/1709.html}
}

@online{cairoDownloadDatasaurusNever,
  title = {Download the {{Datasaurus}}: {{Never}} Trust Summary Statistics Alone; Always Visualize Your Data},
  shorttitle = {Download the {{Datasaurus}}},
  author = {Cairo, Alberto},
  url = {http://www.thefunctionalart.com/2016/08/download-datasaurus-never-trust-summary.html},
  urldate = {2022-11-11},
  abstract = {This tweet  is quickly becoming the most popular I've ever written. I drew that dinosaur with this fantastic tool ~created by Robert Grant ,...},
  file = {/home/skynet3/Zotero/storage/KDGTGA7L/download-datasaurus-never-trust-summary.html}
}

@article{callawayDifferenceinDifferencesContinuousTreatment,
  ids = {callawayDifferenceinDifferencesContinuousTreatmenta},
  title = {Difference-in-{{Diﬀerences}} with a {{Continuous Treatment}}},
  author = {Callaway, Brantly and Goodman-Bacon, Andrew and Sant’Anna, Pedro H C},
  pages = {75},
  abstract = {This paper analyzes difference-in-differences setups with a continuous treatment. We show that treatment effect on the treated-type parameters can be identified under a generalized parallel trends assumption that is similar to the binary treatment setup. However, interpreting differences in these parameters across different values of the treatment can be particularly challenging due to treatment effect heterogeneity. We discuss alternative, typically stronger, assumptions that alleviate these challenges. We also provide a variety of treatment effect decomposition results, highlighting that parameters associated with popular two-way fixed-effect specifications can be hard to interpret, even when there are only two time periods. We introduce alternative estimation strategies that do not suffer from these drawbacks. Our results also cover cases where (i) there is no available untreated comparison group and (ii) there are multiple periods and variation in treatment timing, which are both common in empirical work.},
  langid = {english},
  file = {/home/skynet3/Zotero/storage/EQE7XVZG/Callaway et al. - Diﬀerence-in-Diﬀerences with a Continuous Treatmen.pdf}
}

@article{calsterMethodologyMetricsCurrent2021,
  title = {Methodology over Metrics: Current Scientific Standards Are a Disservice to Patients and Society},
  shorttitle = {Methodology over Metrics},
  author = {Calster, Ben Van and Wynants, Laure and Riley, Richard D. and van Smeden, Maarten and Collins, Gary S.},
  date = {2021-10-01},
  journaltitle = {Journal of Clinical Epidemiology},
  shortjournal = {Journal of Clinical Epidemiology},
  volume = {138},
  eprint = {34077797},
  eprinttype = {pmid},
  pages = {219--226},
  publisher = {{Elsevier}},
  issn = {0895-4356, 1878-5921},
  doi = {10.1016/j.jclinepi.2021.05.018},
  url = {https://www.jclinepi.com/article/S0895-4356(21)00170-0/fulltext},
  urldate = {2022-11-11},
  langid = {english},
  keywords = {Methodology,Reporting,Research quality},
  file = {/home/skynet3/Zotero/storage/3UCSMNZD/Calster et al. - 2021 - Methodology over metrics current scientific stand.pdf}
}

@article{camererEvaluatingReplicabilitySocial2018,
  title = {Evaluating the Replicability of Social Science Experiments in {{Nature}} and {{Science}} between 2010 and 2015},
  author = {Camerer, Colin F. and Dreber, Anna and Holzmeister, Felix and Ho, Teck-Hua and Huber, Jürgen and Johannesson, Magnus and Kirchler, Michael and Nave, Gideon and Nosek, Brian A. and Pfeiffer, Thomas and Altmejd, Adam and Buttrick, Nick and Chan, Taizan and Chen, Yiling and Forsell, Eskil and Gampa, Anup and Heikensten, Emma and Hummer, Lily and Imai, Taisuke and Isaksson, Siri and Manfredi, Dylan and Rose, Julia and Wagenmakers, Eric-Jan and Wu, Hang},
  date = {2018-09},
  journaltitle = {Nature Human Behaviour},
  shortjournal = {Nat Hum Behav},
  volume = {2},
  number = {9},
  pages = {637--644},
  publisher = {{Nature Publishing Group}},
  issn = {2397-3374},
  doi = {10.1038/s41562-018-0399-z},
  url = {https://www.nature.com/articles/s41562-018-0399-z},
  urldate = {2022-11-11},
  abstract = {Being able to replicate scientific findings is crucial for scientific progress1–15. We replicate 21 systematically selected experimental studies in the social sciences published in Nature and Science between 2010 and 201516–36. The replications follow analysis plans reviewed by the original authors and pre-registered prior to the replications. The replications are high powered, with sample sizes on average about five times higher than in the original studies. We find a significant effect in the same direction as the original study for 13 (62\%) studies, and the effect size of the replications is on average about 50\% of the original effect size. Replicability varies between 12 (57\%) and 14 (67\%) studies for complementary replicability indicators. Consistent with these results, the estimated true-positive rate is 67\% in a Bayesian analysis. The relative effect size of true positives is estimated to be 71\%, suggesting that both false positives and inflated effect sizes of true positives contribute to imperfect reproducibility. Furthermore, we find that peer beliefs of replicability are strongly related to replicability, suggesting that the research community could predict which results would replicate and that failures to replicate were not the result of chance alone.},
  issue = {9},
  langid = {english},
  keywords = {Economics,Psychology},
  file = {/home/skynet3/Zotero/storage/WATBYGZV/Camerer et al. - 2018 - Evaluating the replicability of social science exp.pdf;/home/skynet3/Zotero/storage/ML2B3TCM/s41562-018-0399-z.html}
}

@article{candal-pedreiraDoesRetractionMisconduct2020,
  ids = {candal-pedreiraDoesRetractionMisconduct2020a},
  title = {Does Retraction after Misconduct Have an Impact on Citations? {{A}} Pre-Post Study},
  shorttitle = {Does Retraction after Misconduct Have an Impact on Citations?},
  author = {Candal-Pedreira, Cristina and Ruano-Ravina, Alberto and Fernández, Esteve and Ramos, Jorge and Campos-Varela, Isabel and Pérez-Ríos, Mónica},
  date = {2020-11},
  journaltitle = {BMJ global health},
  shortjournal = {BMJ Glob Health},
  volume = {5},
  number = {11},
  eprint = {33187964},
  eprinttype = {pmid},
  pages = {e003719},
  issn = {2059-7908},
  doi = {10.1136/bmjgh-2020-003719},
  abstract = {BACKGROUND: Retracted articles continue to be cited after retraction, and this could have consequences for the scientific community and general population alike. This study was conducted to analyse the association of retraction on citations received by retracted papers due to misconduct using two-time frames: during a postretraction period equivalent to the time the article had been in print before retraction; and during the total postretraction period. METHODS: Quasiexperimental, pre-post evaluation study. A total of 304 retracted original articles and literature reviews indexed in MEDLINE fulfilled the inclusion criteria. Articles were required to have been published in a journal indexed in MEDLINE from January 2013 through December 2015 and been retracted between January 2014 and December 2016. The main outcome was the number of citations received before and after retraction. Results were broken down by journal quartile according to impact factor and the most cited papers during the preretraction period were specifically analysed. RESULTS: There was an increase in postretraction citations when compared with citations received preretraction. There were some exceptions however: first, citations received by articles published in first-quartile journals decreased immediately after retraction (p{$<$}0.05), only to increase again after some time had elapsed; and second, postretraction citations decreased significantly in the case of articles that had received many citations before their retraction (p{$<$}0.05). CONCLUSIONS: The results indicate that retraction of articles has no association on citations in the long term, since the retracted articles continue to be cited, thus circumventing their retraction.},
  langid = {english},
  pmcid = {PMC7668300},
  keywords = {health services research},
  file = {/home/skynet3/Zotero/storage/C742ISZF/Candal-Pedreira et al. - 2020 - Does retraction after misconduct have an impact on.pdf}
}

@misc{caoMCCF1CurvePerformance2020,
  ids = {caoMCCF1CurvePerformance2020a},
  title = {The {{MCC-F1}} Curve: A Performance Evaluation Technique for Binary Classification},
  shorttitle = {The {{MCC-F1}} Curve},
  author = {Cao, Chang and Chicco, Davide and Hoffman, Michael M.},
  date = {2020-06-17},
  number = {arXiv:2006.11278},
  eprint = {2006.11278},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2006.11278},
  url = {http://arxiv.org/abs/2006.11278},
  urldate = {2022-11-11},
  abstract = {Many fields use the ROC curve and the PR curve as standard evaluations of binary classification methods. Analysis of ROC and PR, however, often gives misleading and inflated performance evaluations, especially with an imbalanced ground truth. Here, we demonstrate the problems with ROC and PR analysis through simulations, and propose the MCC-F1 curve to address these drawbacks. The MCC-F1 curve combines two informative single-threshold metrics, MCC and the F1 score. The MCC-F1 curve more clearly differentiates good and bad classifiers, even with imbalanced ground truths. We also introduce the MCC-F1 metric, which provides a single value that integrates many aspects of classifier performance across the whole range of classification thresholds. Finally, we provide an R package that plots MCC-F1 curves and calculates related metrics.},
  archiveprefix = {arXiv},
  keywords = {68T05,Computer Science - Machine Learning,I.2.0,Statistics - Machine Learning},
  file = {/home/skynet3/Zotero/storage/6RCLJ7PY/Cao et al. - 2020 - The MCC-F1 curve a performance evaluation techniq.pdf;/home/skynet3/Zotero/storage/XYA59B3U/2006.html}
}

@misc{caoSurveyLearningSmall2022,
  ids = {caoSurveyLearningSmall2022a},
  title = {A {{Survey}} of {{Learning}} on {{Small Data}}},
  author = {Cao, Xiaofeng and Bu, Weixin and Huang, Shengjun and Tang, Yingpeng and Guo, Yaming and Chang, Yi and Tsang, Ivor W.},
  date = {2022-07-28},
  number = {arXiv:2207.14443},
  eprint = {2207.14443},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2207.14443},
  url = {http://arxiv.org/abs/2207.14443},
  urldate = {2022-11-11},
  abstract = {Learning on big data brings success for artificial intelligence (AI), but the annotation and training costs are expensive. In future, learning on small data is one of the ultimate purposes of AI, which requires machines to recognize objectives and scenarios relying on small data as humans. A series of machine learning models is going on this way such as active learning, few-shot learning, deep clustering. However, there are few theoretical guarantees for their generalization performance. Moreover, most of their settings are passive, that is, the label distribution is explicitly controlled by one specified sampling scenario. This survey follows the agnostic active sampling under a PAC (Probably Approximately Correct) framework to analyze the generalization error and label complexity of learning on small data using a supervised and unsupervised fashion. With these theoretical analyses, we categorize the small data learning models from two geometric perspectives: the Euclidean and non-Euclidean (hyperbolic) mean representation, where their optimization solutions are also presented and discussed. Later, some potential learning scenarios that may benefit from small data learning are then summarized, and their potential learning scenarios are also analyzed. Finally, some challenging applications such as computer vision, natural language processing that may benefit from learning on small data are also surveyed.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/home/skynet3/Zotero/storage/4ZGZZUP9/Cao et al. - 2022 - A Survey of Learning on Small Data.pdf;/home/skynet3/Zotero/storage/QDK5PWGZ/2207.html}
}

@online{CarloscinelliSensemakrSuite,
  title = {Carloscinelli/Sensemakr: {{Suite}} of Sensitivity Analysis Tools for {{OLS}}},
  shorttitle = {Carloscinelli/Sensemakr},
  url = {https://github.com/carloscinelli/sensemakr},
  urldate = {2022-11-11},
  abstract = {Suite of sensitivity analysis tools for OLS. Contribute to carloscinelli/sensemakr development by creating an account on GitHub.},
  langid = {english},
  organization = {{GitHub}},
  file = {/home/skynet3/Zotero/storage/43KMSX38/sensemakr.html}
}

@article{carmichaelDataScienceVs2018,
  title = {Data {{Science}} vs. {{Statistics}}: {{Two Cultures}}?},
  shorttitle = {Data {{Science}} vs. {{Statistics}}},
  author = {Carmichael, Iain and Marron, J. S.},
  date = {2018-06},
  journaltitle = {Japanese Journal of Statistics and Data Science},
  shortjournal = {Jpn J Stat Data Sci},
  volume = {1},
  number = {1},
  eprint = {1801.00371},
  eprinttype = {arxiv},
  primaryclass = {stat},
  pages = {117--138},
  issn = {2520-8756, 2520-8764},
  doi = {10.1007/s42081-018-0009-3},
  url = {http://arxiv.org/abs/1801.00371},
  urldate = {2022-11-11},
  abstract = {Data science is the business of learning from data, which is traditionally the business of statistics. Data science, however, is often understood as a broader, task-driven and computationally-oriented version of statistics. Both the term data science and the broader idea it conveys have origins in statistics and are a reaction to a narrower view of data analysis. Expanding upon the views of a number of statisticians, this paper encourages a big-tent view of data analysis. We examine how evolving approaches to modern data analysis relate to the existing discipline of statistics (e.g. exploratory analysis, machine learning, reproducibility, computation, communication and the role of theory). Finally, we discuss what these trends mean for the future of statistics by highlighting promising directions for communication, education and research.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Other Statistics},
  file = {/home/skynet3/Zotero/storage/FISH6XTA/Carmichael and Marron - 2018 - Data Science vs. Statistics Two Cultures.pdf;/home/skynet3/Zotero/storage/264YUIQH/1801.html}
}

@online{carpenterAllBayesianModels2013,
  title = {All {{Bayesian Models}} Are {{Generative}} (in {{Theory}})},
  author = {Carpenter, Bob},
  date = {2013-05-23T18:41:20+00:00},
  url = {https://lingpipe-blog.com/2013/05/23/all-bayesian-models-are-generative-in-theory/},
  urldate = {2022-11-11},
  abstract = {[This post is a followup to my previous post, Generative vs. discriminative; Bayesian vs. frequentist.] I had a brief chat with Andrew Gelman about the topic of generative vs. discriminative models…},
  langid = {english},
  organization = {{LingPipe Blog}},
  file = {/home/skynet3/Zotero/storage/6TYKX6GF/all-bayesian-models-are-generative-in-theory.html}
}

@online{carpenterGenerativeVsDiscriminative2013,
  title = {Generative vs. {{Discriminative}}; {{Bayesian}} vs. {{Frequentist}}},
  author = {Carpenter, Bob},
  date = {2013-04-12T20:19:49+00:00},
  url = {https://lingpipe-blog.com/2013/04/12/generative-vs-discriminative-bayesian-vs-frequentist/},
  urldate = {2022-11-11},
  abstract = {[There’s now a followup post, All Bayesian models are generative (in theory).] I was helping Boyi Xie get ready for his Ph.D. qualifying exams in computer science at Columbia and at one point…},
  langid = {english},
  organization = {{LingPipe Blog}},
  file = {/home/skynet3/Zotero/storage/VWFYHLH3/generative-vs-discriminative-bayesian-vs-frequentist.html}
}

@article{cashinOverviewSystematicReviews2019,
  title = {An Overview of Systematic Reviews Found Suboptimal Reporting and Methodological Limitations of Mediation Studies Investigating Causal Mechanisms},
  author = {Cashin, Aidan G. and Lee, Hopin and Lamb, Sarah E. and Hopewell, Sally and Mansell, Gemma and Williams, Christopher M. and Kamper, Steven J. and Henschke, Nicholas and McAuley, James H.},
  date = {2019-07},
  journaltitle = {Journal of Clinical Epidemiology},
  shortjournal = {J Clin Epidemiol},
  volume = {111},
  eprint = {30904567},
  eprinttype = {pmid},
  pages = {60-68.e1},
  issn = {1878-5921},
  doi = {10.1016/j.jclinepi.2019.03.005},
  abstract = {OBJECTIVES: The objective of this study was to investigate whether systematic reviews of mediation studies identify limitations in reporting quality and methodological conduct. STUDY DESIGN AND SETTING: We conducted an overview of systematic reviews. We searched four databases (MEDLINE, PsycINFO, Cochrane Database of Systematic Reviews, and PubMed) to identify systematic reviews of studies that used mediation analysis to investigate mechanisms of health care interventions or exposures in clinical populations between 2007 and 2017. Two reviewers independently screened titles and abstracts. Summary data on the characteristics, reporting quality, and methodological conduct of the studies included in the systematic reviews were extracted independently by two reviewers. The protocol was prospectively registered on PROSPERO (CRD42017059834). RESULTS: Fifty-four systematic reviews were included, representing 11 health care fields, 26 health conditions, and 2008 mediation studies. Eighteen of fifty-four systematic reviews (33\%) explicitly stated that the reporting of primary studies was suboptimal. Of these, 14/18 (78\%) reviews noted incomplete reporting of effect sizes and precision estimates from mediation analyses. Twenty-nine of fifty-four systematic reviews (54\%) identified limitations in the methodological conduct of primary studies. CONCLUSION: The reporting and methodological conduct of studies investigating mechanisms in health care seems to be suboptimal. Guidance is needed to improve the quality, completeness, and transparency of mediation studies.},
  langid = {english},
  keywords = {Causal inference,Causality,Health care,Mechanism,Mediation analysis,Overview,Quality of reporting,Research Design,Systematic Reviews as Topic},
  file = {/home/skynet3/Zotero/storage/BSPEPAJX/Cashin et al. - 2019 - An overview of systematic reviews found suboptimal.pdf}
}

@misc{casiniStructuralBreaksTime2018,
  title = {Structural {{Breaks}} in {{Time Series}}},
  author = {Casini, Alessandro and Perron, Pierre},
  date = {2018-05-10},
  number = {arXiv:1805.03807},
  eprint = {1805.03807},
  eprinttype = {arxiv},
  primaryclass = {econ, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1805.03807},
  url = {http://arxiv.org/abs/1805.03807},
  urldate = {2022-11-11},
  abstract = {This chapter covers methodological issues related to estimation, testing and computation for models involving structural changes. Our aim is to review developments as they relate to econometric applications based on linear models. Substantial advances have been made to cover models at a level of generality that allow a host of interesting practical applications. These include models with general stationary regressors and errors that can exhibit temporal dependence and heteroskedasticity, models with trending variables and possible unit roots and cointegrated models, among others. Advances have been made pertaining to computational aspects of constructing estimates, their limit distributions, tests for structural changes, and methods to determine the number of changes present. A variety of topics are covered. The first part summarizes and updates developments described in an earlier review, Perron (2006), with the exposition following heavily that of Perron (2008). Additions are included for recent developments: testing for common breaks, models with endogenous regressors (emphasizing that simply using least-squares is preferable over instrumental variables methods), quantile regressions, methods based on Lasso, panel data models, testing for changes in forecast accuracy, factors models and methods of inference based on a continuous records asymptotic framework. Our focus is on the so-called off-line methods whereby one wants to retrospectively test for breaks in a given sample of data and form confidence intervals about the break dates. The aim is to provide the readers with an overview of methods that are of direct usefulness in practice as opposed to issues that are mostly of theoretical interest.},
  archiveprefix = {arXiv},
  keywords = {Economics - Econometrics,Statistics - Methodology},
  file = {/home/skynet3/Zotero/storage/U68FD3J7/Casini and Perron - 2018 - Structural Breaks in Time Series.pdf;/home/skynet3/Zotero/storage/B57LP7TC/1805.html}
}

@article{cassidyFailingGrade892019,
  title = {Failing {{Grade}}: 89\% of {{Introduction-to-Psychology Textbooks That Define}} or {{Explain Statistical Significance Do So Incorrectly}}},
  shorttitle = {Failing {{Grade}}},
  author = {Cassidy, Scott A. and Dimova, Ralitza and Giguère, Benjamin and Spence, Jeffrey R. and Stanley, David J.},
  date = {2019-09-01},
  journaltitle = {Advances in Methods and Practices in Psychological Science},
  volume = {2},
  number = {3},
  pages = {233--239},
  publisher = {{SAGE Publications Inc}},
  issn = {2515-2459},
  doi = {10.1177/2515245919858072},
  url = {https://doi.org/10.1177/2515245919858072},
  urldate = {2022-11-11},
  abstract = {Null-hypothesis significance testing (NHST) is commonly used in psychology; however, it is widely acknowledged that NHST is not well understood by either psychology professors or psychology students. In the current study, we investigated whether introduction-to-psychology textbooks accurately define and explain statistical significance. We examined 30 introductory-psychology textbooks, including the best-selling books from the United States and Canada, and found that 89\% incorrectly defined or explained statistical significance. Incorrect definitions and explanations were most often consistent with the odds-against-chance fallacy. These results suggest that it is common for introduction-to-psychology students to be taught incorrect interpretations of statistical significance. We hope that our results will create awareness among authors of introductory-psychology books and provide the impetus for corrective action. To help with classroom instruction, we provide slides that correctly describe NHST and may be useful for introductory-psychology instructors.},
  langid = {english},
  file = {/home/skynet3/Zotero/storage/V9FPXRFT/Cassidy et al. - 2019 - Failing Grade 89% of Introduction-to-Psychology T.pdf}
}

@online{CatboostCatboostFast,
  title = {Catboost/Catboost: {{A}} Fast, Scalable, High Performance {{Gradient Boosting}} on {{Decision Trees}} Library, Used for Ranking, Classification, Regression and Other Machine Learning Tasks for {{Python}}, {{R}}, {{Java}}, {{C}}++. {{Supports}} Computation on {{CPU}} and {{GPU}}.},
  shorttitle = {Catboost/Catboost},
  url = {https://github.com/catboost/catboost},
  urldate = {2022-11-11},
  abstract = {A fast, scalable, high performance Gradient Boosting on Decision Trees library, used for ranking, classification, regression and other machine learning tasks for Python, R, Java, C++. Supports comp...},
  langid = {english},
  organization = {{GitHub}},
  file = {/home/skynet3/Zotero/storage/MBNU5I55/catboost.html}
}

@online{CatBoostLSSExtensionCatBoost2020,
  title = {{{CatBoostLSS}} – {{An}} Extension of {{CatBoost}} to Probabilistic Forecasting},
  date = {2020-01-04T15:42:44+00:00},
  url = {https://deepai.org/publication/catboostlss-an-extension-of-catboost-to-probabilistic-forecasting},
  urldate = {2022-11-11},
  abstract = {01/04/20 - We propose a new framework of CatBoost that predicts the entire conditional distribution of a univariate response variable. In par...},
  organization = {{DeepAI}},
  file = {/home/skynet3/Zotero/storage/9CJRZ5IF/catboostlss-an-extension-of-catboost-to-probabilistic-forecasting.html}
}

@book{cattaneoPracticalIntroductionRegression2019,
  ids = {cattaneoPracticalIntroductionRegression2019a},
  title = {A {{Practical Introduction}} to {{Regression Discontinuity Designs}}: {{Foundations}}},
  shorttitle = {A {{Practical Introduction}} to {{Regression Discontinuity Designs}}},
  author = {Cattaneo, Matias D. and Idrobo, Nicolas and Titiunik, Rocio},
  date = {2019-11-30},
  eprint = {1911.09511},
  eprinttype = {arxiv},
  primaryclass = {econ, stat},
  doi = {10.1017/9781108684606},
  url = {http://arxiv.org/abs/1911.09511},
  urldate = {2022-11-11},
  abstract = {In this Element and its accompanying Element, Matias D. Cattaneo, Nicolas Idrobo, and Rocio Titiunik provide an accessible and practical guide for the analysis and interpretation of Regression Discontinuity (RD) designs that encourages the use of a common set of practices and facilitates the accumulation of RD-based empirical evidence. In this Element, the authors discuss the foundations of the canonical Sharp RD design, which has the following features: (i) the score is continuously distributed and has only one dimension, (ii) there is only one cutoff, and (iii) compliance with the treatment assignment is perfect. In the accompanying Element, the authors discuss practical and conceptual extensions to the basic RD setup.},
  archiveprefix = {arXiv},
  keywords = {Economics - Econometrics,Statistics - Applications,Statistics - Computation,Statistics - Methodology},
  file = {/home/skynet3/Zotero/storage/PIDVC5WN/Cattaneo et al. - 2019 - A Practical Introduction to Regression Discontinui.pdf;/home/skynet3/Zotero/storage/6G6RVCNJ/1911.html}
}

@online{CausalDiscoveryIncomplete2020,
  title = {Causal {{Discovery}} from {{Incomplete Data}}: {{A Deep Learning Approach}}},
  shorttitle = {Causal {{Discovery}} from {{Incomplete Data}}},
  date = {2020-01-15T14:28:21+00:00},
  url = {https://deepai.org/publication/causal-discovery-from-incomplete-data-a-deep-learning-approach},
  urldate = {2022-11-11},
  abstract = {01/15/20 - As systems are getting more autonomous with the development of artificial intelligence, it is important to discover the causal kno...},
  organization = {{DeepAI}},
  file = {/home/skynet3/Zotero/storage/QYE7V66P/causal-discovery-from-incomplete-data-a-deep-learning-approach.html}
}

@online{CausalInferenceAnimated,
  title = {Causal {{Inference Animated Plots}}},
  url = {https://nickchk.com/causalgraphs.html},
  urldate = {2022-11-11}
}

@online{CausalInferenceBrave,
  title = {Causal {{Inference}} for {{The Brave}} and {{True}} — {{Causal Inference}} for the {{Brave}} and {{True}}},
  url = {https://matheusfacure.github.io/python-causality-handbook/landing-page.html},
  urldate = {2022-11-11},
  file = {/home/skynet3/Zotero/storage/KUYFNTDX/landing-page.html}
}

@online{CausalInferenceMixtape,
  title = {Causal {{Inference The Mixtape}}},
  url = {https://mixtape.scunning.com/index.html},
  urldate = {2022-11-11}
}

@online{CausalInferenceStructural2022,
  title = {Causal {{Inference Through}} the {{Structural Causal Marginal Problem}}},
  date = {2022-02-02T21:45:10+00:00},
  url = {https://deepai.org/publication/causal-inference-through-the-structural-causal-marginal-problem},
  urldate = {2022-11-11},
  abstract = {02/02/22 - We introduce an approach to counterfactual inference based on merging information from multiple datasets. We consider a causal ref...},
  organization = {{DeepAI}},
  file = {/home/skynet3/Zotero/storage/DPKNEMT6/causal-inference-through-the-structural-causal-marginal-problem.html}
}

@online{CausalitybasedFeatureSelection2019,
  title = {Causality-Based {{Feature Selection}}: {{Methods}} and {{Evaluations}}},
  shorttitle = {Causality-Based {{Feature Selection}}},
  date = {2019-11-17T03:49:39+00:00},
  url = {https://deepai.org/publication/causality-based-feature-selection-methods-and-evaluations},
  urldate = {2022-11-11},
  abstract = {11/17/19 - Feature selection is a crucial preprocessing step in data analytics and machine learning. Classical feature selection algorithms s...},
  organization = {{DeepAI}},
  file = {/home/skynet3/Zotero/storage/RCQ2W59C/causality-based-feature-selection-methods-and-evaluations.html}
}

@online{CausallyDisentangledRepresentations2021,
  title = {On {{Causally Disentangled Representations}}},
  date = {2021-12-10T18:56:27+00:00},
  url = {https://deepai.org/publication/on-causally-disentangled-representations},
  urldate = {2022-11-11},
  abstract = {12/10/21 - Representation learners that disentangle factors of variation have already proven to be important in addressing various real world...},
  organization = {{DeepAI}},
  file = {/home/skynet3/Zotero/storage/FR5QS832/on-causally-disentangled-representations.html}
}

@online{CausalMachineLearning,
  title = {Causal {{Machine Learning}}: {{A Survey}} and {{Open Problems}}},
  url = {https://ai.papers.bar/paper/460ac86ef8e611ecb9b9d35608ee6155},
  urldate = {2022-11-11},
  file = {/home/skynet3/Zotero/storage/ZB2PZIQM/460ac86ef8e611ecb9b9d35608ee6155.html}
}

@online{CausalModelTheory,
  title = {Causal Model and Theory},
  url = {https://stats.andrewheiss.com/donors-crackdowns-aid/00_causal-model-theory.html},
  urldate = {2022-11-11},
  file = {/home/skynet3/Zotero/storage/LXBNU6IL/00_causal-model-theory.html}
}

@online{CausaltextCausaltextpapersCurated,
  title = {Causaltext/Causal-Text-Papers: {{Curated}} Research at the Intersection of Causal Inference and Natural Language Processing.},
  shorttitle = {Causaltext/Causal-Text-Papers},
  url = {https://github.com/causaltext/causal-text-papers},
  urldate = {2022-11-11},
  abstract = {Curated research at the intersection of causal inference and natural language processing. - causaltext/causal-text-papers: Curated research at the intersection of causal inference and natural langu...},
  langid = {english},
  organization = {{GitHub}},
  file = {/home/skynet3/Zotero/storage/DZ7QY9MF/causal-text-papers.html}
}

@online{CenterForPeaceAndSecurityStudiesIntroductiontoMachineLearningCourse,
  title = {{{CenterForPeaceAndSecurityStudies}}/{{IntroductiontoMachineLearning}}: {{Course Materials}} for {{Introduction}} to {{Machine Learning}}, {{Rex W}}. {{Douglass}} 2018},
  shorttitle = {{{CenterForPeaceAndSecurityStudies}}/{{IntroductiontoMachineLearning}}},
  url = {https://github.com/CenterForPeaceAndSecurityStudies/IntroductiontoMachineLearning},
  urldate = {2022-11-11},
  abstract = {Course Materials for Introduction to Machine Learning, Rex W. Douglass 2018 - CenterForPeaceAndSecurityStudies/IntroductiontoMachineLearning: Course Materials for Introduction to Machine Learning, ...},
  langid = {english},
  organization = {{GitHub}},
  file = {/home/skynet3/Zotero/storage/UZD8QHL9/IntroductiontoMachineLearning.html}
}

@online{CentralLimitTheorem,
  title = {Central {{Limit Theorem}}},
  url = {http://mfviz.com/central-limit/?utm_content=buffere918f&utm_medium=social&utm_source=twitter.com&utm_campaign=buffer},
  urldate = {2022-11-11},
  file = {/home/skynet3/Zotero/storage/X2KPFGSQ/central-limit.html}
}

@misc{chandrasekherImputationHighDimensionalLinear2020,
  title = {Imputation for {{High-Dimensional Linear Regression}}},
  author = {Chandrasekher, Kabir Aladin and Alaoui, Ahmed El and Montanari, Andrea},
  date = {2020-01-24},
  number = {arXiv:2001.09180},
  eprint = {2001.09180},
  eprinttype = {arxiv},
  primaryclass = {math, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2001.09180},
  url = {http://arxiv.org/abs/2001.09180},
  urldate = {2022-11-11},
  abstract = {We study high-dimensional regression with missing entries in the covariates. A common strategy in practice is to \textbackslash emph\{impute\} the missing entries with an appropriate substitute and then implement a standard statistical procedure acting as if the covariates were fully observed. Recent literature on this subject proposes instead to design a specific, often complicated or non-convex, algorithm tailored to the case of missing covariates. We investigate a simpler approach where we fill-in the missing entries with their conditional mean given the observed covariates. We show that this imputation scheme coupled with standard off-the-shelf procedures such as the LASSO and square-root LASSO retains the minimax estimation rate in the random-design setting where the covariates are i.i.d.\textbackslash{} sub-Gaussian. We further show that the square-root LASSO remains \textbackslash emph\{pivotal\} in this setting. It is often the case that the conditional expectation cannot be computed exactly and must be approximated from data. We study two cases where the covariates either follow an autoregressive (AR) process, or are jointly Gaussian with sparse precision matrix. We propose tractable estimators for the conditional expectation and then perform linear regression via LASSO, and show similar estimation rates in both cases. We complement our theoretical results with simulations on synthetic and semi-synthetic examples, illustrating not only the sharpness of our bounds, but also the broader utility of this strategy beyond our theoretical assumptions.},
  archiveprefix = {arXiv},
  keywords = {Mathematics - Statistics Theory,Statistics - Machine Learning,Statistics - Methodology},
  file = {/home/skynet3/Zotero/storage/SKJEZD4Q/Chandrasekher et al. - 2020 - Imputation for High-Dimensional Linear Regression.pdf;/home/skynet3/Zotero/storage/KGZDVL8R/2001.html}
}

@book{chanIntroductionProbabilityData2021,
  title = {Introduction to {{Probability}} for {{Data Science}}},
  author = {Chan, Stanley H.},
  date = {2021-11-05},
  eprint = {GR2jzgEACAAJ},
  eprinttype = {googlebooks},
  publisher = {{Michigan Publishing}},
  abstract = {"Probability is one of the most interesting subjects in electrical engineering and computer science. It bridges our favorite engineering principles to the practical reality, a world that is full of uncertainty. However, because probability is such a mature subject, the undergraduate textbooks alone might fill several rows of shelves in a library. When the literature is so rich, the challenge becomes how one can pierce through to the insight while diving into the details. For example, many of you have used a normal random variable before, but have you ever wondered where the 'bell shape' comes from? Every probability class will teach you about flipping a coin, but how can 'flipping a coin' ever be useful in machine learning today? Data scientists use the Poisson random variables to model the internet traffic, but where does the gorgeous Poisson equation come from? This book is designed to fill these gaps with knowledge that is essential to all data science students." -- Preface.},
  isbn = {978-1-60785-746-4},
  langid = {english},
  pagetotal = {704}
}

@misc{chatardWordCautionMany2020,
  ids = {chatardWordCautionMany2020a},
  title = {A {{Word}} of {{Caution}} about {{Many Labs}} 4: {{If You Fail}} to {{Follow Your Preregistered Plan}}, {{You May Fail}} to {{Find}} a {{Real Effect}}},
  shorttitle = {A {{Word}} of {{Caution}} about {{Many Labs}} 4},
  author = {Chatard, Armand and Hirschberger, Gilad and Pyszczynski, Tom},
  date = {2020-02-07T00:59:30},
  publisher = {{PsyArXiv}},
  doi = {10.31234/osf.io/ejubn},
  url = {https://psyarxiv.com/ejubn/},
  urldate = {2022-11-11},
  abstract = {A team of 37 researchers (Many Labs 4; Klein et al., 2019) has recently reported that it has failed to replicate the effect of mortality salience on worldview defense – a classic finding from terror management theory (TMT). This collaborative project (21 labs, N = 2220) has the potential to provide useful information regarding the robustness of an often-replicated and influential empirical finding.  However, Klein et al. (2019) deviated from their preregistered plan by including smaller samples than specified in their pre-registration. This undisclosed deviation from their plan is problematic because it undermines the potential benefits of pre-registration; indeed, the negative results of their meta-analyses appear to be predominantly driven by small studies. We reanalyzed the Many Labs 4 data after excluding studies that did not meet the preregistered minimum sample size (40 participants per cell). Results showed that the data actually do replicate the original study. This successful replication emerged only in the expert advice variation of these studies, which addresses one of the purposes of this study by showing that replication is more likely to occur when researchers follow the advice of researchers with considerable experience in this domain. We discuss the importance of following preregistered plans to avoid misleading conclusions and potential issues involved with literal replications of effects that may depend on societal zeitgeist at the time data are collected.},
  langid = {american},
  keywords = {failure to replicate,Many Labs 4,Meta-science,preregistered,reanalysis,replication,small sample studies,Social and Behavioral Sciences,Social and Personality Psychology,Terror Management Theory,Theory and Philosophy of Science},
  file = {/home/skynet3/Zotero/storage/D8TLZ5FB/Chatard et al. - 2020 - A Word of Caution about Many Labs 4 If You Fail t.pdf}
}

@misc{chattopadhyayImpliedWeightsLinear2022,
  title = {On the Implied Weights of Linear Regression for Causal Inference},
  author = {Chattopadhyay, Ambarish and Zubizarreta, Jose R.},
  date = {2022-07-07},
  number = {arXiv:2104.06581},
  eprint = {2104.06581},
  eprinttype = {arxiv},
  primaryclass = {stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2104.06581},
  url = {http://arxiv.org/abs/2104.06581},
  urldate = {2022-11-11},
  abstract = {A basic principle in the design of observational studies is to approximate the randomized experiment that would have been conducted under controlled circumstances. Now, linear regression models are commonly used to analyze observational data and estimate causal effects. How do linear regression adjustments in observational studies emulate key features of randomized experiments, such as covariate balance, self-weighted sampling, and study representativeness? In this paper, we provide answers to this and related questions by analyzing the implied (individual-level data) weights of linear regression methods. We derive new closed-form expressions of the weights and examine their properties in both finite and asymptotic regimes. We show that the implied weights of general regression problems can be equivalently obtained by solving a convex optimization problem. Among others, we study doubly and multiply robust properties of regression estimators from the perspective of their implied weights. This equivalence allows us to bridge ideas from the regression modeling and causal inference literatures. As a result, we propose novel regression diagnostics for causal inference that are part of the design stage of an observational study. As special cases, we analyze the implied weights in common settings such as multi-valued treatments and regression adjustment after matching. We implement the weights and diagnostics in the new lmw package for R.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Methodology},
  file = {/home/skynet3/Zotero/storage/UQ5K632G/Chattopadhyay and Zubizarreta - 2022 - On the implied weights of linear regression for ca.pdf;/home/skynet3/Zotero/storage/XPL89BBV/2104.html}
}

@article{chaudoinWeReallyKnow2018,
  title = {Do {{We Really Know}} the {{WTO Cures Cancer}}?},
  author = {Chaudoin, Stephen and Hays, Jude and Hicks, Raymond},
  date = {2018-10},
  journaltitle = {British Journal of Political Science},
  volume = {48},
  number = {4},
  pages = {903--928},
  publisher = {{Cambridge University Press}},
  issn = {0007-1234, 1469-2112},
  doi = {10.1017/S000712341600034X},
  url = {https://www.cambridge.org/core/journals/british-journal-of-political-science/article/abs/do-we-really-know-the-wto-cures-cancer/B84A6FCF516FAE3ED7E0C20FE3DA42CF},
  urldate = {2022-11-11},
  abstract = {This article uses a replication experiment of ninety-four specifications from sixteen different studies to show the severity of the problem of selection on unobservables. Using a variety of approaches, it shows that membership in the General Agreement on Tariffs and Trade/World Trade Organization has a significant effect on a surprisingly high number of dependent variables (34 per cent) that have little or no theoretical relationship to the WTO. To make the exercise even more conservative, the study demonstrates that membership in a low-impact environmental treaty, the Convention on Trade in Endangered Species, yields similarly high false positive rates. The authors advocate theoretically informed sensitivity analysis, showing how prior theoretical knowledge conditions the crucial choice of covariates for sensitivity tests. While the current study focuses on international institutions, the arguments also apply to other subfields and applications.},
  langid = {english},
  keywords = {false positives,international political economy,methodology},
  file = {/home/skynet3/Zotero/storage/Q2J8UAMJ/B84A6FCF516FAE3ED7E0C20FE3DA42CF.html}
}

@article{chavalariasEvolutionReportingValues2016,
  title = {Evolution of {{Reporting P Values}} in the {{Biomedical Literature}}, 1990-2015},
  author = {Chavalarias, David and Wallach, Joshua David and Li, Alvin Ho Ting and Ioannidis, John P. A.},
  date = {2016-03-15},
  journaltitle = {JAMA},
  shortjournal = {JAMA},
  volume = {315},
  number = {11},
  pages = {1141--1148},
  issn = {0098-7484},
  doi = {10.1001/jama.2016.1952},
  url = {https://doi.org/10.1001/jama.2016.1952},
  urldate = {2022-11-11},
  abstract = {The use and misuse of P values has generated extensive debates.To evaluate in large scale the P values reported in the abstracts and full text of biomedical research articles over the past 25 years and determine how frequently statistical information is presented in ways other than P values.Automated text-mining analysis was performed to extract data on P values reported in 12\,821\,790 MEDLINE abstracts and in 843\,884 abstracts and full-text articles in PubMed Central (PMC) from 1990 to 2015. Reporting of P values in 151 English-language core clinical journals and specific article types as classified by PubMed also was evaluated. A random sample of 1000 MEDLINE abstracts was manually assessed for reporting of P values and other types of statistical information; of those abstracts reporting empirical data, 100 articles were also assessed in full text.P values reported.Text mining identified 4\,572\,043 P values in 1\,608\,736 MEDLINE abstracts and 3\,438\,299 P values in 385\,393 PMC full-text articles. Reporting of P values in abstracts increased from 7.3\% in 1990 to 15.6\% in 2014. In 2014, P values were reported in 33.0\% of abstracts from the 151 core clinical journals (n\,=\,29\,725 abstracts), 35.7\% of meta-analyses (n\,=\,5620), 38.9\% of clinical trials (n\,=\,4624), 54.8\% of randomized controlled trials (n\,=\,13\,544), and 2.4\% of reviews (n\,=\,71\,529). The distribution of reported P values in abstracts and in full text showed strong clustering at P values of .05 and of .001 or smaller. Over time, the “best” (most statistically significant) reported P values were modestly smaller and the “worst” (least statistically significant) reported P values became modestly less significant. Among the MEDLINE abstracts and PMC full-text articles with P values, 96\% reported at least 1 P value of .05 or lower, with the proportion remaining steady over time in PMC full-text articles. In 1000 abstracts that were manually reviewed, 796 were from articles reporting empirical data; P values were reported in 15.7\% (125/796 [95\% CI, 13.2\%-18.4\%]) of abstracts, confidence intervals in 2.3\% (18/796 [95\% CI, 1.3\%-3.6\%]), Bayes factors in 0\% (0/796 [95\% CI, 0\%-0.5\%]), effect sizes in 13.9\% (111/796 [95\% CI, 11.6\%-16.5\%]), other information that could lead to estimation of P values in 12.4\% (99/796 [95\% CI, 10.2\%-14.9\%]), and qualitative statements about significance in 18.1\% (181/1000 [95\% CI, 15.8\%-20.6\%]); only 1.8\% (14/796 [95\% CI, 1.0\%-2.9\%]) of abstracts reported at least 1 effect size and at least 1 confidence interval. Among 99 manually extracted full-text articles with data, 55 reported P values, 4 presented confidence intervals for all reported effect sizes, none used Bayesian methods, 1 used false-discovery rates, 3 used sample size/power calculations, and 5 specified the primary outcome.In this analysis of P values reported in MEDLINE abstracts and in PMC articles from 1990-2015, more MEDLINE abstracts and articles reported P values over time, almost all abstracts and articles with P values reported statistically significant results, and, in a subgroup analysis, few articles included confidence intervals, Bayes factors, or effect sizes. Rather than reporting isolated P values, articles should include effect sizes and uncertainty metrics.},
  file = {/home/skynet3/Zotero/storage/F4NHV8Q9/Chavalarias et al. - 2016 - Evolution of Reporting P Values in the Biomedical .pdf;/home/skynet3/Zotero/storage/DXKHLBHN/2503172.html}
}

@article{chaventClustGeoPackageHierarchical2018,
  title = {{{ClustGeo}}: An {{R}} Package for Hierarchical Clustering with Spatial Constraints},
  shorttitle = {{{ClustGeo}}},
  author = {Chavent, Marie and Kuentz-Simonet, Vanessa and Labenne, Amaury and Saracco, Jérôme},
  date = {2018-12},
  journaltitle = {Computational Statistics},
  shortjournal = {Comput Stat},
  volume = {33},
  number = {4},
  eprint = {1707.03897},
  eprinttype = {arxiv},
  primaryclass = {stat},
  pages = {1799--1822},
  issn = {0943-4062, 1613-9658},
  doi = {10.1007/s00180-018-0791-1},
  url = {http://arxiv.org/abs/1707.03897},
  urldate = {2022-11-11},
  abstract = {In this paper, we propose a Ward-like hierarchical clustering algorithm including spatial/geographical constraints. Two dissimilarity matrices \$D\_0\$ and \$D\_1\$ are inputted, along with a mixing parameter \$\textbackslash alpha \textbackslash in [0,1]\$. The dissimilarities can be non-Euclidean and the weights of the observations can be non-uniform. The first matrix gives the dissimilarities in the "feature space" and the second matrix gives the dissimilarities in the "constraint space". The criterion minimized at each stage is a convex combination of the homogeneity criterion calculated with \$D\_0\$ and the homogeneity criterion calculated with \$D\_1\$. The idea is then to determine a value of \$\textbackslash alpha\$ which increases the spatial contiguity without deteriorating too much the quality of the solution based on the variables of interest i.e. those of the feature space. This procedure is illustrated on a real dataset using the R package ClustGeo.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Computation,Statistics - Machine Learning},
  file = {/home/skynet3/Zotero/storage/4J9YTXC2/Chavent et al. - 2018 - ClustGeo an R package for hierarchical clustering.pdf;/home/skynet3/Zotero/storage/YAB7HJ8Q/1707.html}
}

@misc{chenCleanAnnotateHow2022,
  title = {Clean or {{Annotate}}: {{How}} to {{Spend}} a {{Limited Data Collection Budget}}},
  shorttitle = {Clean or {{Annotate}}},
  author = {Chen, Derek and Yu, Zhou and Bowman, Samuel R.},
  date = {2022-06-12},
  number = {arXiv:2110.08355},
  eprint = {2110.08355},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2110.08355},
  url = {http://arxiv.org/abs/2110.08355},
  urldate = {2022-11-11},
  abstract = {Crowdsourcing platforms are often used to collect datasets for training machine learning models, despite higher levels of inaccurate labeling compared to expert labeling. There are two common strategies to manage the impact of such noise. The first involves aggregating redundant annotations, but comes at the expense of labeling substantially fewer examples. Secondly, prior works have also considered using the entire annotation budget to label as many examples as possible and subsequently apply denoising algorithms to implicitly clean the dataset. We find a middle ground and propose an approach which reserves a fraction of annotations to explicitly clean up highly probable error samples to optimize the annotation process. In particular, we allocate a large portion of the labeling budget to form an initial dataset used to train a model. This model is then used to identify specific examples that appear most likely to be incorrect, which we spend the remaining budget to relabel. Experiments across three model variations and four natural language processing tasks show our approach outperforms or matches both label aggregation and advanced denoising methods designed to handle noisy labels when allocated the same finite annotation budget.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/home/skynet3/Zotero/storage/QF42N3ZV/Chen et al. - 2022 - Clean or Annotate How to Spend a Limited Data Col.pdf;/home/skynet3/Zotero/storage/JGDHIVHC/2110.html}
}

@misc{chenExplainingModelsPropagating2019,
  ids = {chenExplainingModelsPropagating2019a},
  title = {Explaining {{Models}} by {{Propagating Shapley Values}} of {{Local Components}}},
  author = {Chen, Hugh and Lundberg, Scott and Lee, Su-In},
  date = {2019-11-26},
  number = {arXiv:1911.11888},
  eprint = {1911.11888},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1911.11888},
  url = {http://arxiv.org/abs/1911.11888},
  urldate = {2022-11-11},
  abstract = {In healthcare, making the best possible predictions with complex models (e.g., neural networks, ensembles/stacks of different models) can impact patient welfare. In order to make these complex models explainable, we present DeepSHAP for mixed model types, a framework for layer wise propagation of Shapley values that builds upon DeepLIFT (an existing approach for explaining neural networks). We show that in addition to being able to explain neural networks, this new framework naturally enables attributions for stacks of mixed models (e.g., neural network feature extractor into a tree model) as well as attributions of the loss. Finally, we theoretically justify a method for obtaining attributions with respect to a background distribution (under a Shapley value framework).},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/skynet3/Zotero/storage/IXI3TITM/Chen et al. - 2019 - Explaining Models by Propagating Shapley Values of.pdf;/home/skynet3/Zotero/storage/Z38WQREH/1911.html}
}

@misc{chengHowManyLabelers2022,
  title = {How Many Labelers Do You Have? {{A}} Closer Look at Gold-Standard Labels},
  shorttitle = {How Many Labelers Do You Have?},
  author = {Cheng, Chen and Asi, Hilal and Duchi, John},
  date = {2022-06-23},
  number = {arXiv:2206.12041},
  eprint = {2206.12041},
  eprinttype = {arxiv},
  primaryclass = {cs, math, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2206.12041},
  url = {http://arxiv.org/abs/2206.12041},
  urldate = {2022-11-11},
  abstract = {The construction of most supervised learning datasets revolves around collecting multiple labels for each instance, then aggregating the labels to form a type of ``gold-standard.''. We question the wisdom of this pipeline by developing a (stylized) theoretical model of this process and analyzing its statistical consequences, showing how access to non-aggregated label information can make training well-calibrated models easier or -- in some cases -- even feasible, whereas it is impossible with only gold-standard labels. The entire story, however, is subtle, and the contrasts between aggregated and fuller label information depend on the particulars of the problem, where estimators that use aggregated information exhibit robust but slower rates of convergence, while estimators that can effectively leverage all labels converge more quickly if they have fidelity to (or can learn) the true labeling process. The theory we develop in the stylized model makes several predictions for real-world datasets, including when non-aggregate labels should improve learning performance, which we test to corroborate the validity of our predictions.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Human-Computer Interaction,Computer Science - Machine Learning,Mathematics - Statistics Theory},
  file = {/home/skynet3/Zotero/storage/7ZJLQKVP/Cheng et al. - 2022 - How many labelers do you have A closer look at go.pdf;/home/skynet3/Zotero/storage/KPCMI94X/2206.html}
}

@article{chenOneStandardError2021,
  title = {The {{One Standard Error Rule}} for {{Model Selection}}: {{Does It Work}}?},
  shorttitle = {The {{One Standard Error Rule}} for {{Model Selection}}},
  author = {Chen, Yuchen and Yang, Yuhong},
  date = {2021-12},
  journaltitle = {Stats},
  volume = {4},
  number = {4},
  pages = {868--892},
  publisher = {{Multidisciplinary Digital Publishing Institute}},
  issn = {2571-905X},
  doi = {10.3390/stats4040051},
  url = {https://www.mdpi.com/2571-905X/4/4/51},
  urldate = {2022-11-11},
  abstract = {Previous research provided a lot of discussion on the selection of regularization parameters when it comes to the application of regularization methods for high-dimensional regression. The popular “One Standard Error Rule” (1se rule) used with cross validation (CV) is to select the most parsimonious model whose prediction error is not much worse than the minimum CV error. This paper examines the validity of the 1se rule from a theoretical angle and also studies its estimation accuracy and performances in applications of regression estimation and variable selection, particularly for Lasso in a regression framework. Our theoretical result shows that when a regression procedure produces the regression estimator converging relatively fast to the true regression function, the standard error estimation formula in the 1se rule is justified asymptotically. The numerical results show the following: 1. the 1se rule in general does not necessarily provide a good estimation for the intended standard deviation of the cross validation error. The estimation bias can be 50–100\% upwards or downwards in various situations; 2. the results tend to support that 1se rule usually outperforms the regular CV in sparse variable selection and alleviates the over-selection tendency of Lasso; 3. in regression estimation or prediction, the 1se rule often performs worse. In addition, comparisons are made over two real data sets: Boston Housing Prices (large sample size n, small/moderate number of variables p) and Bardet–Biedl data (large p, small n). Data guided simulations are done to provide insight on the relative performances of the 1se rule and the regular CV.},
  issue = {4},
  langid = {english},
  keywords = {estimation accuracy,regression estimation,subsampling,tuning parameter selection,variable selection},
  file = {/home/skynet3/Zotero/storage/7CLPNKJ3/Chen and Yang - 2021 - The One Standard Error Rule for Model Selection D.pdf;/home/skynet3/Zotero/storage/NDAJ4Y6B/51.html}
}

@misc{chenTrainingDeepNets2016,
  title = {Training {{Deep Nets}} with {{Sublinear Memory Cost}}},
  author = {Chen, Tianqi and Xu, Bing and Zhang, Chiyuan and Guestrin, Carlos},
  date = {2016-04-22},
  number = {arXiv:1604.06174},
  eprint = {1604.06174},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1604.06174},
  url = {http://arxiv.org/abs/1604.06174},
  urldate = {2022-11-11},
  abstract = {We propose a systematic approach to reduce the memory consumption of deep neural network training. Specifically, we design an algorithm that costs O(sqrt(n)) memory to train a n layer network, with only the computational cost of an extra forward pass per mini-batch. As many of the state-of-the-art models hit the upper bound of the GPU memory, our algorithm allows deeper and more complex models to be explored, and helps advance the innovations in deep learning research. We focus on reducing the memory cost to store the intermediate feature maps and gradients during training. Computation graph analysis is used for automatic in-place operation and memory sharing optimizations. We show that it is possible to trade computation for memory - giving a more memory efficient training algorithm with a little extra computation cost. In the extreme case, our analysis also shows that the memory consumption can be reduced to O(log n) with as little as O(n log n) extra cost for forward computation. Our experiments show that we can reduce the memory cost of a 1,000-layer deep residual network from 48G to 7G with only 30 percent additional running time cost on ImageNet problems. Similarly, significant memory cost reduction is observed in training complex recurrent neural networks on very long sequences.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/home/skynet3/Zotero/storage/H3A7XGN2/Chen et al. - 2016 - Training Deep Nets with Sublinear Memory Cost.pdf;/home/skynet3/Zotero/storage/46XLEQDM/1604.html}
}

@misc{chenTrueModelTrue2020,
  title = {True to the {{Model}} or {{True}} to the {{Data}}?},
  author = {Chen, Hugh and Janizek, Joseph D. and Lundberg, Scott and Lee, Su-In},
  date = {2020-06-29},
  number = {arXiv:2006.16234},
  eprint = {2006.16234},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2006.16234},
  url = {http://arxiv.org/abs/2006.16234},
  urldate = {2022-11-11},
  abstract = {A variety of recent papers discuss the application of Shapley values, a concept for explaining coalitional games, for feature attribution in machine learning. However, the correct way to connect a machine learning model to a coalitional game has been a source of controversy. The two main approaches that have been proposed differ in the way that they condition on known features, using either (1) an interventional or (2) an observational conditional expectation. While previous work has argued that one of the two approaches is preferable in general, we argue that the choice is application dependent. Furthermore, we argue that the choice comes down to whether it is desirable to be true to the model or true to the data. We use linear models to investigate this choice. After deriving an efficient method for calculating observational conditional expectation Shapley values for linear models, we investigate how correlation in simulated data impacts the convergence of observational conditional expectation Shapley values. Finally, we present two real data examples that we consider to be representative of possible use cases for feature attribution -- (1) credit risk modeling and (2) biological discovery. We show how a different choice of value function performs better in each scenario, and how possible attributions are impacted by modeling choices.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/skynet3/Zotero/storage/GBZHQKCX/Chen et al. - 2020 - True to the Model or True to the Data.pdf;/home/skynet3/Zotero/storage/KG84L7DG/2006.html}
}

@article{chenUsingRandomForest,
  title = {Using {{Random Forest}} to {{Learn Imbalanced Data}}},
  author = {Chen, Chao},
  pages = {12},
  abstract = {In this paper we propose two ways to deal with the imbalanced data classification problem using random forest. One is based on cost sensitive learning, and the other is based on a sampling technique. Performance metrics such as precision and recall, false positive rate and false negative rate, F-measure and weighted accuracy are computed. Both methods are shown to improve the prediction accuracy of the minority class, and have favorable performance compared to the existing algorithms.},
  langid = {english},
  file = {/home/skynet3/Zotero/storage/ZUTMWU44/Chen - Using Random Forest to Learn Imbalanced Data.pdf}
}

@misc{cherapanamjeriAlgorithmsHeavyTailedStatistics2019,
  title = {Algorithms for {{Heavy-Tailed Statistics}}: {{Regression}}, {{Covariance Estimation}}, and {{Beyond}}},
  shorttitle = {Algorithms for {{Heavy-Tailed Statistics}}},
  author = {Cherapanamjeri, Yeshwanth and Hopkins, Samuel B. and Kathuria, Tarun and Raghavendra, Prasad and Tripuraneni, Nilesh},
  date = {2019-12-23},
  number = {arXiv:1912.11071},
  eprint = {1912.11071},
  eprinttype = {arxiv},
  primaryclass = {cs, math, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1912.11071},
  url = {http://arxiv.org/abs/1912.11071},
  urldate = {2022-11-11},
  abstract = {We study efficient algorithms for linear regression and covariance estimation in the absence of Gaussian assumptions on the underlying distributions of samples, making assumptions instead about only finitely-many moments. We focus on how many samples are needed to do estimation and regression with high accuracy and exponentially-good success probability. For covariance estimation, linear regression, and several other problems, estimators have recently been constructed with sample complexities and rates of error matching what is possible when the underlying distribution is Gaussian, but algorithms for these estimators require exponential time. We narrow the gap between the Gaussian and heavy-tailed settings for polynomial-time estimators with: 1. A polynomial-time estimator which takes \$n\$ samples from a random vector \$X \textbackslash in R\^d\$ with covariance \$\textbackslash Sigma\$ and produces \$\textbackslash hat\{\textbackslash Sigma\}\$ such that in spectral norm \$\textbackslash |\textbackslash hat\{\textbackslash Sigma\} - \textbackslash Sigma \textbackslash |\_2 \textbackslash leq \textbackslash tilde\{O\}(d\^\{3/4\}/\textbackslash sqrt\{n\})\$ w.p. \$1-2\^\{-d\}\$. The information-theoretically optimal error bound is \$\textbackslash tilde\{O\}(\textbackslash sqrt\{d/n\})\$; previous approaches to polynomial-time algorithms were stuck at \$\textbackslash tilde\{O\}(d/\textbackslash sqrt\{n\})\$. 2. A polynomial-time algorithm which takes \$n\$ samples \$(X\_i,Y\_i)\$ where \$Y\_i = \textbackslash langle u,X\_i \textbackslash rangle + \textbackslash varepsilon\_i\$ and produces \$\textbackslash hat\{u\}\$ such that the loss \$\textbackslash |u - \textbackslash hat\{u\}\textbackslash |\^2 \textbackslash leq O(d/n)\$ w.p. \$1-2\^\{-d\}\$ for any \$n \textbackslash geq d\^\{3/2\} \textbackslash log(d)\^\{O(1)\}\$. This (information-theoretically optimal) error is achieved by inefficient algorithms for any \$n \textbackslash gg d\$; previous polynomial-time algorithms suffer loss \$\textbackslash Omega(d\^2/n)\$ and require \$n \textbackslash gg d\^2\$. Our algorithms use degree-\$8\$ sum-of-squares semidefinite programs. We offer preliminary evidence that improving these rates of error in polynomial time is not possible in the median of means framework our algorithms employ.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Data Structures and Algorithms,Mathematics - Statistics Theory},
  file = {/home/skynet3/Zotero/storage/TR35WSYM/Cherapanamjeri et al. - 2019 - Algorithms for Heavy-Tailed Statistics Regression.pdf;/home/skynet3/Zotero/storage/NCKESFT2/1912.html}
}

@article{chernozhukovDoubleDebiasedMachine2018,
  title = {Double/Debiased Machine Learning for Treatment and Structural Parameters},
  author = {Chernozhukov, Victor and Chetverikov, Denis and Demirer, Mert and Duflo, Esther and Hansen, Christian and Newey, Whitney and Robins, James},
  date = {2018-02-01},
  journaltitle = {The Econometrics Journal},
  volume = {21},
  number = {1},
  pages = {C1-C68},
  issn = {1368-4221, 1368-423X},
  doi = {10.1111/ectj.12097},
  url = {https://academic.oup.com/ectj/article/21/1/C1/5056401},
  urldate = {2022-11-11},
  langid = {english},
  file = {/home/skynet3/Zotero/storage/GC56LJ95/Chernozhukov et al. - 2018 - Doubledebiased machine learning for treatment and.pdf}
}

@article{choiPackagesItemResponse2019,
  ids = {choiPackagesItemResponse2019a},
  title = {R {{Packages}} for {{Item Response Theory Analysis}}: {{Descriptions}} and {{Features}}},
  shorttitle = {R {{Packages}} for {{Item Response Theory Analysis}}},
  author = {Choi, Youn-Jeng and Asilkalkan, Abdullah},
  date = {2019-07-03},
  journaltitle = {Measurement: Interdisciplinary Research and Perspectives},
  volume = {17},
  number = {3},
  pages = {168--175},
  publisher = {{Routledge}},
  issn = {1536-6367},
  doi = {10.1080/15366367.2019.1586404},
  url = {https://doi.org/10.1080/15366367.2019.1586404},
  urldate = {2022-11-11},
  abstract = {About 45 R packages to analyze data using item response theory (IRT) have been developed over the last decade. This article introduces these 45 R packages with their descriptions and features. It also describes possible advanced IRT models using R packages, as well as dichotomous and polytomous IRT models, and R packages that contain applications such as differential item functioning and equating are also introduced. Thus, this article helps researchers who plan to use IRT-based analysis to decide on the type of IRT analysis and choose the appropriate R packages.},
  keywords = {differential item functioning,equating,item response theory,R package,simulation study},
  annotation = {\_eprint: https://doi.org/10.1080/15366367.2019.1586404}
}

@software{chourdakisClauCy2022,
  title = {{{ClauCy}}},
  author = {Chourdakis, Emmanouil Theofanis},
  date = {2022-11-11T12:55:16Z},
  origdate = {2018-09-25T16:50:22Z},
  url = {https://github.com/mmxgn/spacy-clausie},
  urldate = {2022-11-11},
  abstract = {Implementation of the ClausIE information extraction system for python+spacy},
  keywords = {clausie,information-extraction,nlp,problog,python-spacy,spacy}
}

@misc{christianOperationalizingReplicationStandard2018,
  title = {Operationalizing the {{Replication Standard}}: {{A Case Study}} of the {{Data Curation}} and {{Verification Workflow}} for {{Scholarly Journals}}},
  shorttitle = {Operationalizing the {{Replication Standard}}},
  author = {Christian, Thu-Mai Lewis and Lafferty-Hess, Sophia and Jacoby, William G. and Carsey, Thomas M.},
  date = {2018-04-27T14:43:03},
  publisher = {{SocArXiv}},
  doi = {10.31235/osf.io/cfdba},
  url = {https://osf.io/preprints/socarxiv/cfdba/},
  urldate = {2022-11-11},
  abstract = {In response to widespread concerns about the integrity of research published in scholarly journals, several initiatives have emerged that are promoting research transparency through access to data underlying published scientific findings. Journal editors, in particular, have made a commitment to research transparency by issuing data policies that require authors to submit their data, code, and documentation to data repositories to allow for public access to the data. In the case of the American Journal of Political Science (AJPS) Data Replication Policy, the data also must undergo an independent verification process in which materials are reviewed for quality as a condition of final manuscript publication and acceptance. Aware of the specialized expertise of the data archives, AJPS called upon the Odum Institute Data Archive to provide a data review service that performs data curation and verification of replication datasets. This article presents a case study of the collaboration between AJPS and the Odum Institute Data Archive to develop a workflow that bridges manuscript publication and data review processes. The case study describes the challenges and the successes of the workflow integration, and offers lessons learned that may be applied by other data archives that are considering expanding their services to include data curation and verification services to support reproducible research.},
  langid = {american},
  keywords = {data curation,data policy,data verification,Library and Information Science,replication,reproducibility,Scholarly Publishing,Social and Behavioral Sciences},
  file = {/home/skynet3/Zotero/storage/N6A6N5R6/Christian et al. - 2018 - Operationalizing the Replication Standard A Case .pdf}
}

@misc{christiansenCausalFrameworkDistribution2021,
  title = {A Causal Framework for Distribution Generalization},
  author = {Christiansen, Rune and Pfister, Niklas and Jakobsen, Martin Emil and Gnecco, Nicola and Peters, Jonas},
  date = {2021-08-17},
  number = {arXiv:2006.07433},
  eprint = {2006.07433},
  eprinttype = {arxiv},
  primaryclass = {stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2006.07433},
  url = {http://arxiv.org/abs/2006.07433},
  urldate = {2022-11-15},
  abstract = {We consider the problem of predicting a response \$Y\$ from a set of covariates \$X\$ when test and training distributions differ. Since such differences may have causal explanations, we consider test distributions that emerge from interventions in a structural causal model, and focus on minimizing the worst-case risk. Causal regression models, which regress the response on its direct causes, remain unchanged under arbitrary interventions on the covariates, but they are not always optimal in the above sense. For example, for linear models and bounded interventions, alternative solutions have been shown to be minimax prediction optimal. We introduce the formal framework of distribution generalization that allows us to analyze the above problem in partially observed nonlinear models for both direct interventions on \$X\$ and interventions that occur indirectly via exogenous variables \$A\$. It takes into account that, in practice, minimax solutions need to be identified from data. Our framework allows us to characterize under which class of interventions the causal function is minimax optimal. We prove sufficient conditions for distribution generalization and present corresponding impossibility results. We propose a practical method, NILE, that achieves distribution generalization in a nonlinear IV setting with linear extrapolation. We prove consistency and present empirical results.},
  archiveprefix = {arXiv},
  keywords = {Primary 62Gxx; secondary 62G35; 62G08; 62D20,Statistics - Methodology},
  file = {/home/skynet3/Zotero/storage/D6H9MY6N/Christiansen et al. - 2021 - A causal framework for distribution generalization.pdf;/home/skynet3/Zotero/storage/I7YLESWR/2006.html}
}

@article{christogiannisSelffulfillingProphecyPosthoc2022,
  title = {The Self-Fulfilling Prophecy of Post-Hoc Power Calculations},
  author = {Christogiannis, Christos and Nikolakopoulos, Stavros and Pandis, Nikolaos and Mavridis, Dimitris},
  date = {2022-02-01},
  journaltitle = {American Journal of Orthodontics and Dentofacial Orthopedics},
  shortjournal = {American Journal of Orthodontics and Dentofacial Orthopedics},
  volume = {161},
  number = {2},
  eprint = {35094754},
  eprinttype = {pmid},
  pages = {315--317},
  publisher = {{Elsevier}},
  issn = {0889-5406, 1097-6752},
  doi = {10.1016/j.ajodo.2021.10.008},
  url = {https://www.ajodo.org/article/S0889-5406(21)00697-1/fulltext},
  urldate = {2022-11-11},
  langid = {english},
  file = {/home/skynet3/Zotero/storage/L6X7ES73/Christogiannis et al. - 2022 - The self-fulfilling prophecy of post-hoc power cal.pdf;/home/skynet3/Zotero/storage/FGMFKN33/fulltext.html}
}

@article{cinelliCrashCourseGood2020,
  ids = {cinelliCrashCourseGood2020a},
  title = {A {{Crash Course}} in {{Good}} and {{Bad Controls}}},
  author = {Cinelli, Carlos and Forney, Andrew and Pearl, Judea},
  date = {2020},
  journaltitle = {SSRN Electronic Journal},
  shortjournal = {SSRN Journal},
  issn = {1556-5068},
  doi = {10.2139/ssrn.3689437},
  url = {https://www.ssrn.com/abstract=3689437},
  urldate = {2022-11-11},
  abstract = {Many students of statistics and econometrics express frustration with the way a problem known as “bad control” is treated in the traditional literature. The issue arises when the addition of a variable to a regression equation produces an unintended discrepancy between the regression coefficient and the effect that the coefficient is intended to represent. Avoiding such discrepancies presents a challenge to all analysts in the data intensive sciences. This note describes graphical tools for understanding, visualizing, and resolving the problem through a series of illustrative examples. By making this “crash course” accessible to instructors and practitioners, we hope to avail these tools to a broader community of scientists concerned with the causal interpretation of regression models.},
  langid = {english},
  file = {/home/skynet3/Zotero/storage/C92H8DWU/Cinelli et al. - 2020 - A Crash Course in Good and Bad Controls.pdf}
}

@book{cinelliMakingSenseSensitivity2018,
  ids = {cinelliMakingSenseSensitivity2018a},
  title = {Making {{Sense}} of {{Sensitivity}}: {{Extending Omitted Variable Bias}}},
  shorttitle = {Making {{Sense}} of {{Sensitivity}}},
  author = {Cinelli, Carlos and Hazlett, Chad},
  date = {2018-01-15},
  journaltitle = {Working paper},
  abstract = {We extend the omitted variable bias framework with a suite of tools for sensitivity analysis in regression models that: (i) does not require assumptions about the treatment assignment nor the nature of confounders; (ii) naturally handles multiple confounders, possibly acting non-linearly; (iii) exploits expert knowledge to bound sensitivity parameters; and, (iv) can be easily computed using only standard regression results. In particular, we introduce two novel sensitivity measures suited for routine reporting. The robustness value describes the minimum strength of association unobserved confounding would need to have, both with the treatment and the outcome, to change the research conclusions. The partial R2 of the treatment with the outcome shows how strongly confounders explaining all the residual outcome variation would have to be associated with the treatment to eliminate the estimated effect. Next, we offer graphical tools for elaborating on problematic confounders, examining the sensitivity of point estimates, t-values, as well as “extreme scenarios”. Finally, we describe problems with a common “benchmarking” practice and introduce a novel procedure to formally bound the strength of confounders based on comparison to observed covariates. We apply these methods to a running example that estimates the effect of exposure to violence on attitudes toward peace.},
  file = {/home/skynet3/Zotero/storage/5XARLL4R/Cinelli and Hazlett - 2018 - Making Sense of Sensitivity Extending Omitted Var.pdf}
}

@online{Ck37VarimpactVariable,
  ids = {Ck37VarimpactVariablea},
  title = {Ck37/Varimpact: {{Variable}} Importance through Targeted Causal Inference, with {{Alan Hubbard}}},
  shorttitle = {Ck37/Varimpact},
  url = {https://github.com/ck37/varimpact},
  urldate = {2022-11-11},
  abstract = {Variable importance through targeted causal inference, with Alan Hubbard - ck37/varimpact: Variable importance through targeted causal inference, with Alan Hubbard},
  langid = {english},
  organization = {{GitHub}},
  file = {/home/skynet3/Zotero/storage/XA9HVZXI/varImpact.html}
}

@article{clarkePhantomMenaceOmitted2005,
  ids = {clarkePhantomMenaceOmitted2005a},
  title = {The {{Phantom Menace}}: {{Omitted Variable Bias}} in {{Econometric Research}}},
  shorttitle = {The {{Phantom Menace}}},
  author = {Clarke, Kevin A.},
  date = {2005-09-01},
  journaltitle = {Conflict Management and Peace Science},
  volume = {22},
  number = {4},
  pages = {341--352},
  publisher = {{SAGE Publications Ltd}},
  issn = {0738-8942},
  doi = {10.1080/07388940500339183},
  url = {https://doi.org/10.1080/07388940500339183},
  urldate = {2022-11-11},
  abstract = {Quantitative political science is awash in control variables. The justification for these bloated specifications is usually the fear of omitted variable bias. A key underlying assumption is that the danger posed by omitted variable bias can be ameliorated by the inclusion of relevant control variables. Unfortunately, as this article demonstrates, there is nothing in the mathematics of regression analysis that supports this conclusion. The inclusion of additional control variables may increase or decrease the bias, and we cannot know for sure which is the case in any particular situation. A brief discussion of alternative strategies for achieving experimental control follows the main result.},
  langid = {english}
}

@article{clarkMichaelClarkMixed2019,
  title = {Michael {{Clark}}: {{Mixed Models}} for {{Big Data}}},
  shorttitle = {Michael {{Clark}}},
  author = {Clark, Michael},
  date = {2019-10-20},
  url = {https://m-clark.github.io/posts/2019-10-20-big-mixed-models/},
  urldate = {2022-11-11},
  abstract = {Explorations of a fast penalized regression approach with mgcv},
  file = {/home/skynet3/Zotero/storage/ACUU2WZT/2019-10-20-big-mixed-models.html}
}

@misc{clarkUnsupervisedDiscoveryTemporal2019,
  title = {Unsupervised {{Discovery}} of {{Temporal Structure}} in {{Noisy Data}} with {{Dynamical Components Analysis}}},
  author = {Clark, David G. and Livezey, Jesse A. and Bouchard, Kristofer E.},
  date = {2019-10-27},
  number = {arXiv:1905.09944},
  eprint = {1905.09944},
  eprinttype = {arxiv},
  primaryclass = {cs, math},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1905.09944},
  url = {http://arxiv.org/abs/1905.09944},
  urldate = {2022-11-11},
  abstract = {Linear dimensionality reduction methods are commonly used to extract low-dimensional structure from high-dimensional data. However, popular methods disregard temporal structure, rendering them prone to extracting noise rather than meaningful dynamics when applied to time series data. At the same time, many successful unsupervised learning methods for temporal, sequential and spatial data extract features which are predictive of their surrounding context. Combining these approaches, we introduce Dynamical Components Analysis (DCA), a linear dimensionality reduction method which discovers a subspace of high-dimensional time series data with maximal predictive information, defined as the mutual information between the past and future. We test DCA on synthetic examples and demonstrate its superior ability to extract dynamical structure compared to commonly used linear methods. We also apply DCA to several real-world datasets, showing that the dimensions extracted by DCA are more useful than those extracted by other methods for predicting future states and decoding auxiliary variables. Overall, DCA robustly extracts dynamical structure in noisy, high-dimensional data while retaining the computational efficiency and geometric interpretability of linear dimensionality reduction methods.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Information Theory,Computer Science - Machine Learning},
  file = {/home/skynet3/Zotero/storage/E3S9IP4M/Clark et al. - 2019 - Unsupervised Discovery of Temporal Structure in No.pdf;/home/skynet3/Zotero/storage/KGAIEQDS/1905.html}
}

@online{ClassAlgorithmsGeneral2020,
  title = {A {{Class}} of {{Algorithms}} for {{General Instrumental Variable Models}}},
  date = {2020-06-11T12:32:24+00:00},
  url = {https://deepai.org/publication/a-class-of-algorithms-for-general-instrumental-variable-models},
  urldate = {2022-11-11},
  abstract = {06/11/20 - Causal treatment effect estimation is a key problem that arises in a variety of real-world settings, from personalized medicine to...},
  organization = {{DeepAI}},
  file = {/home/skynet3/Zotero/storage/AK62U82V/a-class-of-algorithms-for-general-instrumental-variable-models.html}
}

@online{ClusteredStandardErrors,
  title = {Clustered Standard Errors vs. Multilevel Modeling | {{Statistical Modeling}}, {{Causal Inference}}, and {{Social Science}}},
  url = {https://statmodeling.stat.columbia.edu/2007/11/28/clustered_stand/},
  urldate = {2022-11-11},
  file = {/home/skynet3/Zotero/storage/EJAI98YA/clustered_stand.html}
}

@online{CMUAdvancedNLP,
  title = {{{CMU Advanced NLP}} 2022 - {{YouTube}}},
  url = {https://www.youtube.com/playlist?list=PL8PYTP1V4I8D0UkqW2fEhgLrnlDW9QK7z},
  urldate = {2022-11-11},
  file = {/home/skynet3/Zotero/storage/Q6T7RV3C/playlist.html}
}

@article{cobbMereRenovationToo2015,
  ids = {cobbMereRenovationToo2015a},
  title = {Mere {{Renovation}} Is {{Too Little Too Late}}: {{We Need}} to {{Rethink}} Our {{Undergraduate Curriculum}} from the {{Ground Up}}},
  shorttitle = {Mere {{Renovation}} Is {{Too Little Too Late}}},
  author = {Cobb, George},
  date = {2015-10-02},
  journaltitle = {The American Statistician},
  shortjournal = {The American Statistician},
  volume = {69},
  number = {4},
  pages = {266--282},
  issn = {0003-1305, 1537-2731},
  doi = {10.1080/00031305.2015.1093029},
  url = {http://www.tandfonline.com/doi/full/10.1080/00031305.2015.1093029},
  urldate = {2022-11-11},
  langid = {english},
  file = {/home/skynet3/Zotero/storage/HMY36Q6S/Cobb - 2015 - Mere Renovation is Too Little Too Late We Need to.pdf}
}

@online{CodementorInstant1on1,
  title = {Codementor - {{Instant}} 1-on-1 {{Mentor}} for {{Programming}} \& {{Design}}},
  url = {https://www.codementor.io/@arpitbhayani/solving-an-age-old-problem-using-bayesian-average-15fy4ww08p);},
  urldate = {2022-11-11},
  abstract = {Codementor connects novice/intermediate coders with experienced mentors for instant help via code/screen sharing, video, and text.  Codementor is your instant 1:1 mentor for programming \& design.},
  file = {/home/skynet3/Zotero/storage/CSMBQRD4/solving-an-age-old-problem-using-bayesian-average-15fy4ww08p)\;.html}
}

@article{cohenHowShouldNovelty2017,
  ids = {cohenHowShouldNovelty2017a},
  title = {How Should Novelty Be Valued in Science?},
  author = {Cohen, Barak A},
  editor = {Rodgers, Peter},
  date = {2017-07-25},
  journaltitle = {eLife},
  volume = {6},
  pages = {e28699},
  publisher = {{eLife Sciences Publications, Ltd}},
  issn = {2050-084X},
  doi = {10.7554/eLife.28699},
  url = {https://doi.org/10.7554/eLife.28699},
  urldate = {2022-11-11},
  abstract = {Scientists are under increasing pressure to do "novel" research. Here I explore whether there are risks to overemphasizing novelty when deciding what constitutes good science. I review studies from the philosophy of science to help understand how important an explicit emphasis on novelty might be for scientific progress. I also review studies from the sociology of science to anticipate how emphasizing novelty might impact the structure and function of the scientific community. I conclude that placing too much value on novelty could have counterproductive effects on both the rate of progress in science and the organization of the scientific community. I finish by recommending that our current emphasis on novelty be replaced by a renewed emphasis on predictive power as a characteristic of good science.},
  keywords = {novelty,peer review,philosophy of science,science policy,scientific publishing,sociology of science},
  file = {/home/skynet3/Zotero/storage/CK2NUFVD/Cohen - 2017 - How should novelty be valued in science.pdf}
}

@article{cokerTheoryStatisticalInference2021,
  title = {A {{Theory}} of {{Statistical Inference}} for {{Ensuring}} the {{Robustness}} of {{Scientific Results}}},
  author = {Coker, Beau and Rudin, Cynthia and King, Gary},
  date = {2021-10},
  journaltitle = {Management Science},
  shortjournal = {Management Science},
  volume = {67},
  number = {10},
  eprint = {1804.08646},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  pages = {6174--6197},
  issn = {0025-1909, 1526-5501},
  doi = {10.1287/mnsc.2020.3818},
  url = {http://arxiv.org/abs/1804.08646},
  urldate = {2022-11-11},
  abstract = {Inference is the process of using facts we know to learn about facts we do not know. A theory of inference gives assumptions necessary to get from the former to the latter, along with a definition for and summary of the resulting uncertainty. Any one theory of inference is neither right nor wrong, but merely an axiom that may or may not be useful. Each of the many diverse theories of inference can be valuable for certain applications. However, no existing theory of inference addresses the tendency to choose, from the range of plausible data analysis specifications consistent with prior evidence, those that inadvertently favor one's own hypotheses. Since the biases from these choices are a growing concern across scientific fields, and in a sense the reason the scientific community was invented in the first place, we introduce a new theory of inference designed to address this critical problem. We introduce hacking intervals, which are the range of a summary statistic one may obtain given a class of possible endogenous manipulations of the data. Hacking intervals require no appeal to hypothetical data sets drawn from imaginary superpopulations. A scientific result with a small hacking interval is more robust to researcher manipulation than one with a larger interval, and is often easier to interpret than a classical confidence interval. Some versions of hacking intervals turn out to be equivalent to classical confidence intervals, which means they may also provide a more intuitive and potentially more useful interpretation of classical confidence intervals.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/skynet3/Zotero/storage/VF2PLG4M/Coker et al. - 2021 - A Theory of Statistical Inference for Ensuring the.pdf;/home/skynet3/Zotero/storage/8YAB4ADS/1804.html}
}

@article{colaresiPreplicationReplicationProposal2016,
  ids = {colaresiPreplicationReplicationProposal2016a},
  title = {Preplication, {{Replication}}: {{A Proposal}} to {{Efficiently Upgrade Journal Replication Standards}}},
  shorttitle = {Preplication, {{Replication}}},
  author = {Colaresi, Michael},
  date = {2016-11-01},
  journaltitle = {International Studies Perspectives},
  shortjournal = {International Studies Perspectives},
  volume = {17},
  number = {4},
  pages = {367--378},
  issn = {1528-3577},
  doi = {10.1093/isp/ekv016},
  url = {https://doi.org/10.1093/isp/ekv016},
  urldate = {2022-11-11},
  abstract = {Despite 20 years of progress in promoting replication standards in International Relations (IR), significant problems remain in both the provision of data and the incentives to replicate published research. While replicable research is a public good, there appear to be private incentives for researchers to follow socially suboptimal research strategies. The current situation has led to a growing concern in IR, as well as across the social sciences, that published research findings may not represent accurate appraisals of the evidence on particular research questions. In this article, I discuss the role of private information in the publication process and review the incentives for producing replicable and nonreplicable research. A small, but potentially important, change in a journal’s workflow could both deter the publication of nonreplicable work and lower the costs for researchers to build and expand upon existing published research. The suggestion, termed Preplication, is for journals to run the replication data and code for conditionally accepted articles before publication, just as journals routinely check for compliance with style guides. This change could be implemented alongside other revisions to journal policies around the discipline. In fact, Preplication is already in use at several journals, and I provide an update as to how the process has worked at International Interactions.}
}

@misc{colemanSelectionProxyEfficient2020,
  ids = {colemanSelectionProxyEfficient2020a},
  title = {Selection via {{Proxy}}: {{Efficient Data Selection}} for {{Deep Learning}}},
  shorttitle = {Selection via {{Proxy}}},
  author = {Coleman, Cody and Yeh, Christopher and Mussmann, Stephen and Mirzasoleiman, Baharan and Bailis, Peter and Liang, Percy and Leskovec, Jure and Zaharia, Matei},
  date = {2020-10-26},
  number = {arXiv:1906.11829},
  eprint = {1906.11829},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1906.11829},
  url = {http://arxiv.org/abs/1906.11829},
  urldate = {2022-11-11},
  abstract = {Data selection methods, such as active learning and core-set selection, are useful tools for machine learning on large datasets. However, they can be prohibitively expensive to apply in deep learning because they depend on feature representations that need to be learned. In this work, we show that we can greatly improve the computational efficiency by using a small proxy model to perform data selection (e.g., selecting data points to label for active learning). By removing hidden layers from the target model, using smaller architectures, and training for fewer epochs, we create proxies that are an order of magnitude faster to train. Although these small proxy models have higher error rates, we find that they empirically provide useful signals for data selection. We evaluate this "selection via proxy" (SVP) approach on several data selection tasks across five datasets: CIFAR10, CIFAR100, ImageNet, Amazon Review Polarity, and Amazon Review Full. For active learning, applying SVP can give an order of magnitude improvement in data selection runtime (i.e., the time it takes to repeatedly train and select points) without significantly increasing the final error (often within 0.1\%). For core-set selection on CIFAR10, proxies that are over 10x faster to train than their larger, more accurate targets can remove up to 50\% of the data without harming the final accuracy of the target, leading to a 1.6x end-to-end training time improvement.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/skynet3/Zotero/storage/L2YLHUWD/Coleman et al. - 2020 - Selection via Proxy Efficient Data Selection for .pdf;/home/skynet3/Zotero/storage/LSKPI6RE/1906.html}
}

@article{colganAmericanBiasGlobal2019,
  title = {American {{Bias}} in {{Global Security Studies Data}}},
  author = {Colgan, Jeff D},
  date = {2019-07-01},
  journaltitle = {Journal of Global Security Studies},
  shortjournal = {Journal of Global Security Studies},
  volume = {4},
  number = {3},
  pages = {358--371},
  issn = {2057-3170},
  doi = {10.1093/jogss/ogz030},
  url = {https://doi.org/10.1093/jogss/ogz030},
  urldate = {2022-11-11},
  abstract = {Three major datasets contain problematic interpretative judgments, arguably biased toward the United States: the Polity dataset; Reiter and Stam's data on war outcomes; and Singh and Way's data on nuclear proliferation. These examples raise the possibility that important datasets in global security studies, and in political science more generally, are systematically affected by an American bias. Bias means that, non-Americans might code the same observations differently, on average. The issue arises because Americans, on average, seem to have certain predispositions that non-Americans, on average, do not have. Other nationalities have their own predispositions. I also demonstrate that each of the three empirical examples has significant implications for causal inferences, altering certain statistical findings based upon them. For instance, I reexamine Haber and Menaldo's study of the resource curse, showing that alternative data coding casts substantial doubt on their inferences.}
}

@online{ColinFayTidystringdistString,
  title = {{{ColinFay}}/Tidystringdist: {{String}} Distance Calculation the Tidy Way.},
  shorttitle = {{{ColinFay}}/Tidystringdist},
  url = {https://github.com/ColinFay/tidystringdist},
  urldate = {2022-11-11},
  abstract = {String distance calculation the tidy way. . Contribute to ColinFay/tidystringdist development by creating an account on GitHub.},
  langid = {english},
  organization = {{GitHub}},
  file = {/home/skynet3/Zotero/storage/VDKZP79B/tidystringdist.html}
}

@misc{collierRealityLimitsLanguage2022,
  title = {On {{Reality}} and the {{Limits}} of {{Language Data}}},
  author = {Collier, Nigel H. and Liu, Fangyu and Shareghi, Ehsan},
  date = {2022-08-25},
  number = {arXiv:2208.11981},
  eprint = {2208.11981},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2208.11981},
  url = {http://arxiv.org/abs/2208.11981},
  urldate = {2022-11-11},
  abstract = {Recent advances in neural network language models have shown that it is possible to derive expressive meaning representations by leveraging linguistic associations in large-scale natural language data. These potentially Gestalt representations have enabled state-of-the-art performance for many practical applications. It would appear that we are on a pathway to empirically deriving a robust and expressive computable semantics. A key question that arises is how far can language data alone enable computers to understand the necessary truth about the physical world? Attention to this question is warranted because our future interactions with intelligent machines depends on how well our techniques correctly represent and process the concepts (objects, properties, and processes) that humans commonly observe to be true. After reviewing existing protocols, the objective of this work is to explore this question using a novel and tightly controlled reasoning test and to highlight what models might learn directly from pure linguistic data.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/skynet3/Zotero/storage/RI6AJXQE/Collier et al. - 2022 - On Reality and the Limits of Language Data.pdf;/home/skynet3/Zotero/storage/LRD4F5QI/2208.html}
}

@misc{collinsElicitingLearningSoft2022,
  title = {Eliciting and {{Learning}} with {{Soft Labels}} from {{Every Annotator}}},
  author = {Collins, Katherine M. and Bhatt, Umang and Weller, Adrian},
  date = {2022-08-29},
  number = {arXiv:2207.00810},
  eprint = {2207.00810},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2207.00810},
  url = {http://arxiv.org/abs/2207.00810},
  urldate = {2022-11-11},
  abstract = {The labels used to train machine learning (ML) models are of paramount importance. Typically for ML classification tasks, datasets contain hard labels, yet learning using soft labels has been shown to yield benefits for model generalization, robustness, and calibration. Earlier work found success in forming soft labels from multiple annotators' hard labels; however, this approach may not converge to the best labels and necessitates many annotators, which can be expensive and inefficient. We focus on efficiently eliciting soft labels from individual annotators. We collect and release a dataset of soft labels (which we call CIFAR-10S) over the CIFAR-10 test set via a crowdsourcing study (N=248). We demonstrate that learning with our labels achieves comparable model performance to prior approaches while requiring far fewer annotators -- albeit with significant temporal costs per elicitation. Our elicitation methodology therefore shows nuanced promise in enabling practitioners to enjoy the benefits of improved model performance and reliability with fewer annotators, and serves as a guide for future dataset curators on the benefits of leveraging richer information, such as categorical uncertainty, from individual annotators.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computers and Society,Computer Science - Human-Computer Interaction,Computer Science - Machine Learning},
  file = {/home/skynet3/Zotero/storage/GTADSPI7/Collins et al. - 2022 - Eliciting and Learning with Soft Labels from Every.pdf;/home/skynet3/Zotero/storage/FP56QIN2/2207.html}
}

@misc{collischonMethodsEstimateCausal2021,
  title = {Methods to {{Estimate Causal Effects}} - {{An Overview}} on {{IV}}, {{DiD}} and {{RDD}} and a {{Guide}} on {{How}} to {{Apply}} Them in {{Practice}}},
  author = {Collischon, Matthias},
  date = {2021-03-18T10:20:27},
  publisher = {{SocArXiv}},
  doi = {10.31235/osf.io/usvta},
  url = {https://osf.io/preprints/socarxiv/usvta/},
  urldate = {2022-11-11},
  abstract = {The identification of causal effects has gained increasing attention in social sciences over the last years and this trend also has found its way into sociology, albeit on a relatively small scale. This article provides an overview of three methods to identify causal effects that are rarely used in sociology: instrumental variable (IV) regression, difference-in-differences (DiD), and regression discontinuity design (RDD). I provide intuitive introductions to these methods, discuss identifying assumptions, limitations of the methods, promising extension, and present an exemplary study for each estimation method that can serve as a benchmark when applying these estimation techniques. Furthermore, the online appendix to this article contains Stata syntax that simulates data and shows how to apply these techniques in practice.},
  langid = {american},
  keywords = {causal effects,difference-in-differences,instrumental variables,Methodology,regression discontinuity design,Social and Behavioral Sciences,Sociology},
  file = {/home/skynet3/Zotero/storage/VXMP4XJB/Collischon - 2021 - Methods to Estimate Causal Effects - An Overview o.pdf}
}

@online{ColorizedMathEquations,
  title = {Colorized {{Math Equations}} – {{BetterExplained}}},
  url = {https://betterexplained.com/articles/colorized-math-equations/},
  urldate = {2022-11-11}
}

@online{ComparisonDataTable,
  title = {Comparison with {{R}}’s Data.Table — Datatable Documentation},
  url = {https://datatable.readthedocs.io/en/latest/manual/comparison_with_rdatatable.html},
  urldate = {2022-11-11}
}

@online{ComparisonLibrariesPandas,
  title = {Comparison with {{R}} / {{R}} Libraries — Pandas 1.5.1 Documentation},
  url = {https://pandas.pydata.org/docs/getting_started/comparison/comparison_with_r.html},
  urldate = {2022-11-11},
  file = {/home/skynet3/Zotero/storage/8RYC9VZB/comparison_with_r.html}
}

@online{CompendiumCleanGraphs,
  title = {A {{Compendium}} of {{Clean Graphs}} in {{R}}},
  url = {http://shinyapps.org/apps/RGraphCompendium/index.php?utm_content=bufferd23cb&utm_medium=social&utm_source=twitter.com&utm_campaign=buffer},
  urldate = {2022-11-11}
}

@online{ComprehensiveSurveyImage2022,
  title = {A {{Comprehensive Survey}} of {{Image Augmentation Techniques}} for {{Deep Learning}}},
  date = {2022-05-03T13:45:04+00:00},
  url = {https://deepai.org/publication/a-comprehensive-survey-of-image-augmentation-techniques-for-deep-learning},
  urldate = {2022-11-11},
  abstract = {05/03/22 - Deep learning has been achieving decent performance in computer vision requiring a large volume of images, however, collecting ima...},
  organization = {{DeepAI}},
  file = {/home/skynet3/Zotero/storage/2WYCQYA5/a-comprehensive-survey-of-image-augmentation-techniques-for-deep-learning.html}
}

@online{ComputationalLinearAlgebra,
  title = {Computational {{Linear Algebra}} - {{YouTube}}},
  url = {https://www.youtube.com/playlist?list=PLtmWHNX-gukIc92m1K0P6bIOnZb-mg0hY},
  urldate = {2022-11-15},
  file = {/home/skynet3/Zotero/storage/BHLSQQFC/playlist.html}
}

@online{ComputerInformationScience,
  title = {Computer and {{Information Science}} | {{A Department}} of the {{School}} of {{Engineering}} and {{Applied Science}}},
  url = {https://www.cis.upenn.edu/},
  urldate = {2022-11-11},
  abstract = {Computer and Information Science | A Department of the School of Engineering and Applied Science},
  langid = {american},
  file = {/home/skynet3/Zotero/storage/H3ZRLQNQ/www.cis.upenn.edu.html}
}

@online{CondaMythsMisconceptions,
  title = {Conda: {{Myths}} and {{Misconceptions}} | {{Pythonic Perambulations}}},
  url = {https://jakevdp.github.io/blog/2016/08/25/conda-myths-and-misconceptions/},
  urldate = {2022-11-11},
  file = {/home/skynet3/Zotero/storage/8YQ3SS5R/conda-myths-and-misconceptions.html}
}

@book{congdonAppliedBayesianModelling2014,
  title = {Applied {{Bayesian Modelling}}},
  author = {Congdon, Peter},
  date = {2014-05-23},
  eprint = {ImejAwAAQBAJ},
  eprinttype = {googlebooks},
  publisher = {{John Wiley \& Sons}},
  abstract = {This book provides an accessible approach to Bayesian computing and data analysis, with an emphasis on the interpretation of real data sets. Following in the tradition of the successful first edition, this book aims to make a wide range of statistical modeling applications accessible using tested code that can be readily adapted to the reader's own applications. The second edition has been thoroughly reworked and updated to take account of advances in the field. A new set of worked examples is included. The novel aspect of the first edition was the coverage of statistical modeling using WinBUGS and OPENBUGS. This feature continues in the new edition along with examples using R to broaden appeal and for completeness of coverage.},
  isbn = {978-1-118-89505-4},
  langid = {english},
  pagetotal = {465},
  keywords = {Mathematics / General,Mathematics / Probability & Statistics / Bayesian Analysis,Mathematics / Probability & Statistics / Stochastic Processes}
}

@article{conleyPlausiblyExogenous2012,
  ids = {conleyPlausiblyExogenous2012a},
  title = {Plausibly {{Exogenous}}},
  author = {Conley, Timothy G. and Hansen, Christian B. and Rossi, Peter E.},
  date = {2012-02-01},
  journaltitle = {The Review of Economics and Statistics},
  shortjournal = {The Review of Economics and Statistics},
  volume = {94},
  number = {1},
  pages = {260--272},
  issn = {0034-6535},
  doi = {10.1162/REST_a_00139},
  url = {https://doi.org/10.1162/REST_a_00139},
  urldate = {2022-11-11},
  abstract = {Instrumental variable (IV) methods are widely used to identify causal effects in models with endogenous explanatory variables. Often the instrument exclusion restriction that underlies the validity of the usual IV inference is suspect; that is, instruments are only plausibly exogenous. We present practical methods for performing inference while relaxing the exclusion restriction. We illustrate the approaches with empirical examples that examine the effect of 401(k) participation on asset accumulation, price elasticity of demand for margarine, and returns to schooling. We find that inference is informative even with a substantial relaxation of the exclusion restriction in two of the three cases.},
  file = {/home/skynet3/Zotero/storage/WGD366LD/Plausibly-Exogenous.html}
}

@article{cookLostAggregationImproving2019,
  title = {Lost in {{Aggregation}}: {{Improving Event Analysis}} with {{Report-Level Data}}},
  shorttitle = {Lost in {{Aggregation}}},
  author = {Cook, Scott J. and Weidmann, Nils B.},
  date = {2019},
  journaltitle = {American Journal of Political Science},
  volume = {63},
  number = {1},
  pages = {250--264},
  issn = {1540-5907},
  doi = {10.1111/ajps.12398},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/ajps.12398},
  urldate = {2022-11-11},
  abstract = {Most measures of social conflict processes are derived from primary and secondary source reports. In many cases, reports are used to create event-level data sets by aggregating information from multiple, and often conflicting, reports to single event observations. We argue that this pre-aggregation is less innocuous than it seems, costing applied researchers opportunities for improved inference. First, researchers cannot evaluate the consequences of different methods of report aggregation. Second, aggregation discards report-level information (i.e., variation across reports) that is useful in addressing measurement error inherent in event data. Therefore, we advocate that data should be supplied and analyzed at the report level. We demonstrate the consequences of using aggregated event data as a predictor or outcome variable, and how analysis can be improved using report-level information directly. These gains are demonstrated with simulated-data experiments and in the analysis of real-world data, using the newly available Mass Mobilization in Autocracies Database (MMAD).},
  langid = {english},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/ajps.12398},
  file = {/home/skynet3/Zotero/storage/V7PPQWIA/Cook and Weidmann - 2019 - Lost in Aggregation Improving Event Analysis with.pdf;/home/skynet3/Zotero/storage/NGM7RX2R/ajps.html}
}

@article{cookNotesNegativeBinomial,
  ids = {cookNotesNegativeBinomiala},
  title = {Notes on the {{Negative Binomial Distribution}}},
  author = {Cook, John D},
  pages = {5},
  langid = {english},
  file = {/home/skynet3/Zotero/storage/Y29635J9/Cook - Notes on the Negative Binomial Distribution.pdf}
}

@article{cookRaceBottomSpatial2022,
  ids = {cookRaceBottomSpatial2022a},
  title = {Race to the Bottom: {{Spatial}} Aggregation and Event Data},
  shorttitle = {Race to the Bottom},
  author = {Cook, Scott J. and Weidmann, Nils B.},
  date = {2022-05-04},
  journaltitle = {International Interactions},
  volume = {48},
  number = {3},
  pages = {471--491},
  publisher = {{Routledge}},
  issn = {0305-0629},
  doi = {10.1080/03050629.2022.2025365},
  url = {https://doi.org/10.1080/03050629.2022.2025365},
  urldate = {2022-11-11},
  abstract = {Researchers now have greater access to granular georeferenced (i.e., spatial) data on social and political phenomena than ever before. Such data have seen wide use, as they offer the potential for researchers to analyze local phenomena, test mechanisms, and better understand micro-level behavior. With these political event data, it has become increasingly common for researchers to select the smallest spatial scale permitted by the data. We argue that this practice requires greater scrutiny, as smaller spatial or temporal scales do not necessarily improve the quality of inferences. While highly disaggregated data reduce some threats to inference (e.g., aggregation bias), they increase the risk of others (e.g., outcome misclassification). Therefore, we argue that researchers should adopt a more principled approach when selecting the spatial scale for their analysis. To help inform this choice, we characterize the aggregation problem for spatial data, discuss the consequences of too much (or too little) aggregation, and provide some guidance for applied researchers. We demonstrate these issues using both simulated experiments and an analysis of spatial patterns of violence in Afghanistan.Los investigadores tienen ahora un acceso como nunca antes a datos georreferenciados granulares (es decir, espaciales) sobre fenómenos sociales y políticos. Estos datos se han utilizado ampliamente, ya que ofrecen a los investigadores la posibilidad de analizar fenómenos locales, probar mecanismos y comprender mejor el comportamiento a nivel micro. Con estos datos sobre acontecimientos políticos, es cada vez más frecuente que los investigadores seleccionen la escala espacial más pequeña que permitan los datos. Sostenemos que esta práctica requiere un mayor escrutinio, ya que las escalas espaciales o temporales no necesariamente mejoran la calidad de las inferencias. Si bien los datos altamente desagregados reducen algunas amenazas para la inferencia (por ejemplo, el sesgo de agregación), aumentan el riesgo de otras (por ejemplo, la clasificación errónea de los resultados). Por lo tanto, sostenemos que los investigadores deberían adoptar un enfoque basándose más en principios a la hora de seleccionar la escala espacial para su análisis. Para contribuir a realizar esta elección, caracterizamos el problema de la agregación de los datos espaciales, analizamos las consecuencias de una agregación excesiva (o insuficiente) y ofrecemos algunas orientaciones para la investigación aplicada. Demostramos estas cuestiones utilizando tanto experimentos simulados como un análisis de los patrones de violencia en Afganistán.Les chercheurs ont maintenant un meilleur accès à des données granulaires géoréférencées (c-à-d, spatiales) sur les phénomènes politiques et sociaux que jamais auparavant. Ces données ont été largement utilisées, car elles offrent aux chercheurs le potentiel d’analyser des phénomènes locaux, de tester des mécanismes et de mieux comprendre les comportements au niveau micro. Avec ces données sur les événements politiques, il est devenu de plus en plus courant pour les chercheurs de sélectionner la plus petite échelle spatiale permise par les données. Nous soutenons que cette pratique exige un examen plus approfondi, car des échelles spatiales ou temporelles plus petites n’améliorent pas nécessairement la qualité des déductions. Bien que les données très désagrégées réduisent certains risques pour les déductions (p. ex. biais d’agrégation), elles accroissent le risque d’autres facteurs (p. ex. mauvaise classification des résultats). Par conséquent, nous soutenons que les chercheurs devraient adopter une approche plus raisonnée lorsqu’ils choisissent l’échelle spatiale pour leur analyse. Afin d’éclairer ce choix, nous caractérisons le problème de l’agrégation des données spatiales, nous discutons des conséquences d’une trop grande (ou trop faible) agrégations des données et nous fournissons quelques conseils aux chercheurs appliqués. Nous démontrons ces problèmes en utilisant à la fois des expérimentations simulées et une analyse des schémas spatiaux de la violence en Afghanistan.},
  keywords = {Ecological inference,event data,measurement error,spatial scale},
  annotation = {\_eprint: https://doi.org/10.1080/03050629.2022.2025365}
}

@article{cookTwoWrongsMake2017,
  title = {Two {{Wrongs Make}} a {{Right}}: {{Addressing Underreporting}} in {{Binary Data}} from {{Multiple Sources}}},
  shorttitle = {Two {{Wrongs Make}} a {{Right}}},
  author = {Cook, Scott J. and Blas, Betsabe and Carroll, Raymond J. and Sinha, Samiran},
  date = {2017-04},
  journaltitle = {Political analysis : an annual publication of the Methodology Section of the American Political Science Association},
  shortjournal = {Polit Anal},
  volume = {25},
  number = {2},
  eprint = {29104409},
  eprinttype = {pmid},
  pages = {223--240},
  issn = {1047-1987},
  doi = {10.1017/pan.2016.13},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5667662/},
  urldate = {2022-11-11},
  abstract = {Media-based event data—i.e., data comprised from reporting by media outlets—are widely used in political science research. However, events of interest (e.g., strikes, protests, conflict) are often underreported by these primary and secondary sources, producing incomplete data that risks inconsistency and bias in subsequent analysis. While general strategies exist to help ameliorate this bias, these methods do not make full use of the information often available to researchers. Specifically, much of the event data used in the social sciences is drawn from multiple, overlapping news sources (e.g., Agence France-Presse, Reuters). Therefore, we propose a novel maximum likelihood estimator that corrects for misclassification in data arising from multiple sources. In the most general formulation of our estimator, researchers can specify separate sets of predictors for the true-event model and each of the misclassification models characterizing whether a source fails to report on an event. As such, researchers are able to accurately test theories on both the causes of and reporting on an event of interest. Simulations evidence that our technique regularly out performs current strategies that either neglect misclassification, the unique features of the data-generating process, or both. We also illustrate the utility of this method with a model of repression using the Social Conflict in Africa Database.},
  pmcid = {PMC5667662},
  file = {/home/skynet3/Zotero/storage/I9EZAF2L/Cook et al. - 2017 - Two Wrongs Make a Right Addressing Underreporting.pdf}
}

@misc{covertExplainingRemovingUnified2022,
  title = {Explaining by {{Removing}}: {{A Unified Framework}} for {{Model Explanation}}},
  shorttitle = {Explaining by {{Removing}}},
  author = {Covert, Ian and Lundberg, Scott and Lee, Su-In},
  date = {2022-05-12},
  number = {arXiv:2011.14878},
  eprint = {2011.14878},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2011.14878},
  url = {http://arxiv.org/abs/2011.14878},
  urldate = {2022-11-11},
  abstract = {Researchers have proposed a wide variety of model explanation approaches, but it remains unclear how most methods are related or when one method is preferable to another. We describe a new unified class of methods, removal-based explanations, that are based on the principle of simulating feature removal to quantify each feature's influence. These methods vary in several respects, so we develop a framework that characterizes each method along three dimensions: 1) how the method removes features, 2) what model behavior the method explains, and 3) how the method summarizes each feature's influence. Our framework unifies 26 existing methods, including several of the most widely used approaches: SHAP, LIME, Meaningful Perturbations, and permutation tests. This newly understood class of explanation methods has rich connections that we examine using tools that have been largely overlooked by the explainability literature. To anchor removal-based explanations in cognitive psychology, we show that feature removal is a simple application of subtractive counterfactual reasoning. Ideas from cooperative game theory shed light on the relationships and trade-offs among different methods, and we derive conditions under which all removal-based explanations have information-theoretic interpretations. Through this analysis, we develop a unified framework that helps practitioners better understand model explanation tools, and that offers a strong theoretical foundation upon which future explainability research can build.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/skynet3/Zotero/storage/TS837T3X/Covert et al. - 2022 - Explaining by Removing A Unified Framework for Mo.pdf;/home/skynet3/Zotero/storage/UABDM622/2011.html}
}

@misc{covertUnderstandingGlobalFeature2020,
  title = {Understanding {{Global Feature Contributions With Additive Importance Measures}}},
  author = {Covert, Ian and Lundberg, Scott and Lee, Su-In},
  date = {2020-10-27},
  number = {arXiv:2004.00668},
  eprint = {2004.00668},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2004.00668},
  url = {http://arxiv.org/abs/2004.00668},
  urldate = {2022-11-11},
  abstract = {Understanding the inner workings of complex machine learning models is a long-standing problem and most recent research has focused on local interpretability. To assess the role of individual input features in a global sense, we explore the perspective of defining feature importance through the predictive power associated with each feature. We introduce two notions of predictive power (model-based and universal) and formalize this approach with a framework of additive importance measures, which unifies numerous methods in the literature. We then propose SAGE, a model-agnostic method that quantifies predictive power while accounting for feature interactions. Our experiments show that SAGE can be calculated efficiently and that it assigns more accurate importance values than other methods.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/skynet3/Zotero/storage/U6CTZL22/Covert et al. - 2020 - Understanding Global Feature Contributions With Ad.pdf;/home/skynet3/Zotero/storage/ESDR4P28/2004.html}
}

@online{COVIDMethodologyV2,
  ids = {COVIDMethodologyV2a},
  title = {{{COVID}}\_{{Methodology}}\_v2.Pdf},
  url = {https://drive.google.com/file/d/1ERjcGXD2jvfDFXdI0_NtF4X95UeQ5f4W/view?usp=embed_facebook},
  urldate = {2022-11-11},
  organization = {{Google Docs}},
  file = {/home/skynet3/Zotero/storage/NXK4NMJW/view.html}
}

@article{coxStatisticalSignificance2020,
  ids = {coxStatisticalSignificance2020a},
  title = {Statistical {{Significance}}},
  author = {Cox, D.R.},
  date = {2020},
  journaltitle = {Annual Review of Statistics and Its Application},
  volume = {7},
  number = {1},
  pages = {1--10},
  doi = {10.1146/annurev-statistics-031219-041051},
  url = {https://doi.org/10.1146/annurev-statistics-031219-041051},
  urldate = {2022-11-11},
  abstract = {A broad review is given of the role of statistical significance tests in the analysis of empirical data. Four main types of application are outlined. The first, conceptually quite different from the others, concerns decision making in such contexts as medical screening and industrial inspection. The others assess the security of conclusions. The article concludes with an outline discussion of some more specialized points.},
  keywords = {probability,statistical significance},
  annotation = {\_eprint: https://doi.org/10.1146/annurev-statistics-031219-041051}
}

@article{criminisiDecisionForestsUnified2012,
  title = {Decision {{Forests}}: {{A Unified Framework}} for {{Classification}}, {{Regression}}, {{Density Estimation}}, {{Manifold Learning}} and {{Semi-Supervised Learning}}},
  shorttitle = {Decision {{Forests}}},
  author = {Criminisi, Antonio and Shotton, Jamie and Konukoglu, Ender},
  date = {2012-01-01},
  journaltitle = {Foundations and Trends® in Computer Graphics and Vision},
  volume = {7},
  pages = {81--227},
  url = {https://www.microsoft.com/en-us/research/publication/decision-forests-a-unified-framework-for-classification-regression-density-estimation-manifold-learning-and-semi-supervised-learning/},
  urldate = {2022-11-11},
  abstract = {This review presents a unified, efficient model of random decision forests which can be applied to a number of machine learning, computer vision, and medical image analysis tasks. Our model extends existing forest-based techniques as it unifies classification, regression, density estimation, manifold learning, semi-supervised learning, and active learning under the same decision forest framework.This gives […]},
  langid = {american},
  file = {/home/skynet3/Zotero/storage/ZSLX95DB/Criminisi et al. - 2012 - Decision Forests A Unified Framework for Classifi.pdf;/home/skynet3/Zotero/storage/3TNTIPCG/decision-forests-a-unified-framework-for-classification-regression-density-estimation-manifold-.html}
}

@article{cristeaValuesDisplayItems2018,
  title = {P Values in Display Items Are Ubiquitous and Almost Invariably Significant: {{A}} Survey of Top Science Journals},
  shorttitle = {P Values in Display Items Are Ubiquitous and Almost Invariably Significant},
  author = {Cristea, Ioana Alina and Ioannidis, John P. A.},
  date = {2018-05-15},
  journaltitle = {PLOS ONE},
  shortjournal = {PLOS ONE},
  volume = {13},
  number = {5},
  pages = {e0197440},
  publisher = {{Public Library of Science}},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0197440},
  url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0197440},
  urldate = {2022-11-11},
  abstract = {P values represent a widely used, but pervasively misunderstood and fiercely contested method of scientific inference. Display items, such as figures and tables, often containing the main results, are an important source of P values. We conducted a survey comparing the overall use of P values and the occurrence of significant P values in display items of a sample of articles in the three top multidisciplinary journals (Nature, Science, PNAS) in 2017 and, respectively, in 1997. We also examined the reporting of multiplicity corrections and its potential influence on the proportion of statistically significant P values. Our findings demonstrated substantial and growing reliance on P values in display items, with increases of 2.5 to 14.5 times in 2017 compared to 1997. The overwhelming majority of P values (94\%, 95\% confidence interval [CI] 92\% to 96\%) were statistically significant. Methods to adjust for multiplicity were almost non-existent in 1997, but reported in many articles relying on P values in 2017 (Nature 68\%, Science 48\%, PNAS 38\%). In their absence, almost all reported P values were statistically significant (98\%, 95\% CI 96\% to 99\%). Conversely, when any multiplicity corrections were described, 88\% (95\% CI 82\% to 93\%) of reported P values were statistically significant. Use of Bayesian methods was scant (2.5\%) and rarely (0.7\%) articles relied exclusively on Bayesian statistics. Overall, wider appreciation of the need for multiplicity corrections is a welcome evolution, but the rapid growth of reliance on P values and implausibly high rates of reported statistical significance are worrisome.},
  langid = {english},
  keywords = {Analysis of variance,Bayesian method,Bayesian statistics,Metaanalysis,Scientific publishing,Software tools,Statistical data,Volcanoes},
  file = {/home/skynet3/Zotero/storage/E2NXT8UI/Cristea and Ioannidis - 2018 - P values in display items are ubiquitous and almos.pdf}
}

@online{CriticalReviewUse2022,
  title = {A {{Critical Review}} on the {{Use}} (and {{Misuse}}) of {{Differential Privacy}} in {{Machine Learning}}},
  date = {2022-06-09T17:13:10+00:00},
  url = {https://deepai.org/publication/a-critical-review-on-the-use-and-misuse-of-differential-privacy-in-machine-learning},
  urldate = {2022-11-11},
  abstract = {06/09/22 - We review the use of differential privacy (DP) for privacy protection in machine learning (ML). We show that, driven by the aim of...},
  organization = {{DeepAI}},
  file = {/home/skynet3/Zotero/storage/2RJBYFUX/a-critical-review-on-the-use-and-misuse-of-differential-privacy-in-machine-learning.html}
}

@online{CriticalValuesRobusta,
  title = {Critical {{Values Robust}} to {{P-hacking}}},
  url = {https://www.pascalmichaillat.org/12.html},
  urldate = {2022-11-11},
  abstract = {This paper develops a model of p-hacking by researchers. It then gives critical values that correct the inflated type 1 error rates caused by p-hacking.},
  langid = {english},
  organization = {{Pascal Michaillat}},
  file = {/home/skynet3/Zotero/storage/MJLPA255/12.html}
}

@online{CS231nConvolutionalNeural,
  title = {{{CS231n Convolutional Neural Networks}} for {{Visual Recognition}}},
  url = {https://cs231n.github.io/neural-networks-3/},
  urldate = {2022-11-11}
}

@book{cunninghamCausalInference2021,
  title = {Causal {{Inference}}},
  author = {Cunningham, Scott},
  date = {2021-01-26},
  eprint = {DZ4REAAAQBAJ},
  eprinttype = {googlebooks},
  publisher = {{Yale University Press}},
  abstract = {An accessible, contemporary introduction to the methods for determining cause and effect in the social sciences \&quot;Causation versus correlation has been the basis of arguments--economic and otherwise--since the beginning of time. Causal Inference: The Mixtape uses legit real-world examples that I found genuinely thought-provoking. It\&\#39;s rare that a book prompts readers to expand their outlook; this one did for me.\&quot;--Marvin Young (Young MC)  Causal inference encompasses the tools that allow social scientists to determine what causes what. In a messy world, causal inference is what helps establish the causes and effects of the actions being studied--for example, the impact (or lack thereof) of increases in the minimum wage on employment, the effects of early childhood education on incarceration later in life, or the influence on economic growth of introducing malaria nets in developing regions. Scott Cunningham introduces students and practitioners to the methods necessary to arrive at meaningful answers to the questions of causation, using a range of modeling techniques and coding instructions for both the R and the Stata programming languages.},
  isbn = {978-0-300-25168-5},
  langid = {english},
  pagetotal = {585},
  keywords = {Business & Economics / Econometrics}
}

@online{cunninghamMuchQuieterRevolution2021,
  type = {Substack newsletter},
  ids = {cunninghamMuchQuieterRevolution2021a},
  title = {The {{Much Quieter Revolution}} of {{Synthetic Control}}: {{Episode I}}},
  shorttitle = {The {{Much Quieter Revolution}} of {{Synthetic Control}}},
  author = {{cunningham}, scott},
  date = {2021-11-08},
  url = {https://causalinf.substack.com/p/the-much-quieter-revolution-of-synthetic?utm_medium=web},
  urldate = {2022-11-11},
  abstract = {An Abadie, Diamond and Hainmueller explainer},
  organization = {{Scott's Substack}},
  file = {/home/skynet3/Zotero/storage/XWPNX86Z/the-much-quieter-revolution-of-synthetic.html}
}

@online{cunninghamWaitingEventStudies2021,
  type = {Substack newsletter},
  title = {Waiting for {{Event Studies}}: {{A Play}} in {{Three Acts}}},
  shorttitle = {Waiting for {{Event Studies}}},
  author = {{cunningham}, scott},
  date = {2021-04-01},
  url = {https://causalinf.substack.com/p/waiting-for-event-studies-a-play},
  urldate = {2022-11-11},
  abstract = {Sun and Abraham (2020) Explainer},
  organization = {{Scott's Substack}},
  file = {/home/skynet3/Zotero/storage/KBW7S5K3/waiting-for-event-studies-a-play.html}
}

@article{cziborDozenThingsExperimental2019,
  title = {The {{Dozen Things Experimental Economists Should Do}} ({{More}} Of)},
  author = {Czibor, Eszter and Jimenez-Gomez, David and List, John},
  date = {2019},
  journaltitle = {Artefactual Field Experiments},
  series = {Artefactual {{Field Experiments}}},
  number = {00648},
  publisher = {{The Field Experiments Website}},
  url = {https://ideas.repec.org/p/feb/artefa/00648.html},
  urldate = {2022-11-11},
  abstract = {What was once broadly viewed as an impossibility - learning from experimental data in economics - has now become commonplace. Governmental bodies, think tanks, and corporations around the world employ teams of experimental researchers to answer their most pressing questions. For their part, in the past two decades academics have begun to more actively partner with organizations to generate data via field experimentation. While this revolution in evidence-based approaches has served to deepen the economic science, recently a credibility crisis has caused even the most ardent experimental proponents to pause. This study takes a step back from the burgeoning experimental literature and introduces 12 actions that might help to alleviate this credibility crisis and raise experimental economics to an even higher level. In this way, we view our "12 action wish list" as discussion points to enrich the field.},
  langid = {english},
  file = {/home/skynet3/Zotero/storage/3NAYD69U/Czibor et al. - 2019 - The Dozen Things Experimental Economists Should Do.pdf;/home/skynet3/Zotero/storage/SKNJJ5LX/00648.html}
}

@article{daiPoliticalEventCoding,
  ids = {daiPoliticalEventCodinga},
  title = {Political {{Event Coding}} as {{Text}} to {{Text Sequence Generation}}},
  author = {Dai, Yaoyao and Radford, Benjamin J and Halterman, Andrew},
  pages = {7},
  abstract = {We report on the current status of an effort to produce political event data from unstructured text via a Transformer language model. Compelled by the current lack of publicly available and up-to-date event coding software, we seek to train a model that can produce structured political event records at the sentence level. Our approach differs from previous efforts in that we conceptualize this task as one of text-to-text sequence generation. We motivate this choice by outlining desirable properties of text generation models for the needs of event coding. To overcome the lack of sufficient training data, we also describe a method for generating synthetic text and event record pairs that we use to fit our model.},
  langid = {english},
  file = {/home/skynet3/Zotero/storage/GCVPPA97/Dai et al. - Political Event Coding as Text to Text Sequence Ge.pdf}
}

@misc{damourMultiCauseCausalInference2019,
  title = {On {{Multi-Cause Causal Inference}} with {{Unobserved Confounding}}: {{Counterexamples}}, {{Impossibility}}, and {{Alternatives}}},
  shorttitle = {On {{Multi-Cause Causal Inference}} with {{Unobserved Confounding}}},
  author = {D'Amour, Alexander},
  date = {2019-03-19},
  number = {arXiv:1902.10286},
  eprint = {1902.10286},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1902.10286},
  url = {http://arxiv.org/abs/1902.10286},
  urldate = {2022-11-11},
  abstract = {Unobserved confounding is a central barrier to drawing causal inferences from observational data. Several authors have recently proposed that this barrier can be overcome in the case where one attempts to infer the effects of several variables simultaneously. In this paper, we present two simple, analytical counterexamples that challenge the general claims that are central to these approaches. In addition, we show that nonparametric identification is impossible in this setting. We discuss practical implications, and suggest alternatives to the methods that have been proposed so far in this line of work: using proxy variables and shifting focus to sensitivity analysis.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/skynet3/Zotero/storage/U5UXHJXQ/D'Amour - 2019 - On Multi-Cause Causal Inference with Unobserved Co.pdf;/home/skynet3/Zotero/storage/VWPJL2VP/1902.html}
}

@misc{damourUnderspecificationPresentsChallenges2020,
  title = {Underspecification {{Presents Challenges}} for {{Credibility}} in {{Modern Machine Learning}}},
  author = {D'Amour, Alexander and Heller, Katherine and Moldovan, Dan and Adlam, Ben and Alipanahi, Babak and Beutel, Alex and Chen, Christina and Deaton, Jonathan and Eisenstein, Jacob and Hoffman, Matthew D. and Hormozdiari, Farhad and Houlsby, Neil and Hou, Shaobo and Jerfel, Ghassen and Karthikesalingam, Alan and Lucic, Mario and Ma, Yian and McLean, Cory and Mincu, Diana and Mitani, Akinori and Montanari, Andrea and Nado, Zachary and Natarajan, Vivek and Nielson, Christopher and Osborne, Thomas F. and Raman, Rajiv and Ramasamy, Kim and Sayres, Rory and Schrouff, Jessica and Seneviratne, Martin and Sequeira, Shannon and Suresh, Harini and Veitch, Victor and Vladymyrov, Max and Wang, Xuezhi and Webster, Kellie and Yadlowsky, Steve and Yun, Taedong and Zhai, Xiaohua and Sculley, D.},
  date = {2020-11-24},
  number = {arXiv:2011.03395},
  eprint = {2011.03395},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2011.03395},
  url = {http://arxiv.org/abs/2011.03395},
  urldate = {2022-11-11},
  abstract = {ML models often exhibit unexpectedly poor behavior when they are deployed in real-world domains. We identify underspecification as a key reason for these failures. An ML pipeline is underspecified when it can return many predictors with equivalently strong held-out performance in the training domain. Underspecification is common in modern ML pipelines, such as those based on deep learning. Predictors returned by underspecified pipelines are often treated as equivalent based on their training domain performance, but we show here that such predictors can behave very differently in deployment domains. This ambiguity can lead to instability and poor model behavior in practice, and is a distinct failure mode from previously identified issues arising from structural mismatch between training and deployment domains. We show that this problem appears in a wide variety of practical ML pipelines, using examples from computer vision, medical imaging, natural language processing, clinical risk prediction based on electronic health records, and medical genomics. Our results show the need to explicitly account for underspecification in modeling pipelines that are intended for real-world deployment in any domain.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/skynet3/Zotero/storage/NIV7ZIBL/D'Amour et al. - 2020 - Underspecification Presents Challenges for Credibi.pdf;/home/skynet3/Zotero/storage/6BNFLPE2/2011.html}
}

@article{danielMakingApplesOranges2021,
  title = {Making Apples from Oranges: {{Comparing}} Noncollapsible Effect Estimators and Their Standard Errors after Adjustment for Different Covariate Sets},
  shorttitle = {Making Apples from Oranges},
  author = {Daniel, Rhian and Zhang, Jingjing and Farewell, Daniel},
  date = {2021},
  journaltitle = {Biometrical Journal},
  volume = {63},
  number = {3},
  pages = {528--557},
  issn = {1521-4036},
  doi = {10.1002/bimj.201900297},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/bimj.201900297},
  urldate = {2022-11-11},
  abstract = {We revisit the well-known but often misunderstood issue of (non)collapsibility of effect measures in regression models for binary and time-to-event outcomes. We describe an existing simple but largely ignored procedure for marginalizing estimates of conditional odds ratios and propose a similar procedure for marginalizing estimates of conditional hazard ratios (allowing for right censoring), demonstrating its performance in simulation studies and in a reanalysis of data from a small randomized trial in primary biliary cirrhosis patients. In addition, we aim to provide an educational summary of issues surrounding (non)collapsibility from a causal inference perspective and to promote the idea that the words conditional and adjusted (likewise marginal and unadjusted) should not be used interchangeably.},
  langid = {english},
  keywords = {covariate adjustment,Cox proportional hazards regression,logistic regression,noncollapsibility},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/bimj.201900297},
  file = {/home/skynet3/Zotero/storage/KBNDVMH8/Daniel et al. - 2021 - Making apples from oranges Comparing noncollapsib.pdf;/home/skynet3/Zotero/storage/3FVFBY5I/bimj.html}
}

@online{DanielMugge0000000194087597,
  title = {Daniel {{Mügge}} (0000-0001-9408-7597)},
  url = {https://orcid.org/0000-0001-9408-7597},
  urldate = {2022-11-11},
  abstract = {ORCID record for Daniel Mügge. ORCID provides an identifier for individuals to use with their name as they engage in research, scholarship, and innovation activities.},
  langid = {english},
  organization = {{ORCID}},
  file = {/home/skynet3/Zotero/storage/LNSSP534/0000-0001-9408-7597.html}
}

@software{DatacleaningValidate2022,
  title = {Data-Cleaning/Validate},
  date = {2022-10-30T01:12:07Z},
  origdate = {2014-02-21T13:50:42Z},
  url = {https://github.com/data-cleaning/validate},
  urldate = {2022-11-11},
  abstract = {Professional data validation for the R environment},
  organization = {{Data cleaning for statistical purpose}},
  keywords = {data-cleaning,r,validation}
}

@online{dataexpertDayLifeSilicon2022,
  title = {A {{Day}} in the {{Life}} of a {{Silicon Valley Data Engineer}}},
  author = {DataExpert},
  date = {2022-03-03T15:42:21},
  url = {https://towardsdatascience.com/a-day-in-the-life-of-a-google-data-engineer-722f1b2206cc},
  urldate = {2022-11-11},
  abstract = {The Data Engineer has been gaining popularity in the past 10 years, but what exactly do Data Engineers do? Data Engineers in my experience…},
  langid = {english},
  organization = {{Medium}},
  file = {/home/skynet3/Zotero/storage/RKYSCQ4S/a-day-in-the-life-of-a-google-data-engineer-722f1b2206cc.html}
}

@online{DataFrameFunction,
  title = {Data.Frame Function - {{RDocumentation}}},
  url = {https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/data.frame},
  urldate = {2022-11-11},
  file = {/home/skynet3/Zotero/storage/NG2J9VSU/data.html}
}

@online{DataScience2e,
  title = {R for {{Data Science}} (2e) - 15~ {{Regular}} Expressions},
  url = {https://r4ds.hadley.nz/regexps.html},
  urldate = {2022-11-11}
}

@online{DataStructuresPython,
  title = {5. {{Data Structures}} — {{Python}} 3.11.0 Documentation},
  url = {https://docs.python.org/3/tutorial/datastructures.html},
  urldate = {2022-11-11}
}

@online{DataValidationMachine,
  title = {Data Validation for Machine Learning | the Morning Paper},
  url = {https://blog.acolyer.org/2019/06/05/data-validation-for-machine-learning/},
  urldate = {2022-11-11},
  langid = {british},
  file = {/home/skynet3/Zotero/storage/VEI3HYDN/data-validation-for-machine-learning.html}
}

@book{DataVisualization,
  title = {Data {{Visualization}}},
  url = {https://socviz.co/},
  urldate = {2022-11-11},
  abstract = {A practical introduction.}
}

@online{DatavisualizationChSelected,
  title = {Datavisualization.Ch {{Selected Tools}}},
  url = {http://selection.datavisualization.ch/},
  urldate = {2022-11-11}
}

@online{DbarangerInteractionPoweRInteractionPoweR,
  title = {Dbaranger/{{InteractionPoweR}}: {{InteractionPoweR}}: {{Power}} Analysis for Interactions via Simulation in {{R}}},
  shorttitle = {Dbaranger/{{InteractionPoweR}}},
  url = {https://github.com/dbaranger/InteractionPoweR},
  urldate = {2022-11-11},
  abstract = {InteractionPoweR: Power analysis for interactions via simulation in R - dbaranger/InteractionPoweR: InteractionPoweR: Power analysis for interactions via simulation in R},
  langid = {english},
  organization = {{GitHub}},
  file = {/home/skynet3/Zotero/storage/WRK68MFA/InteractionPoweR.html}
}

@online{DealingDisagreementsLooking2021,
  title = {Dealing with {{Disagreements}}: {{Looking Beyond}} the {{Majority Vote}} in {{Subjective Annotations}}},
  shorttitle = {Dealing with {{Disagreements}}},
  date = {2021-10-12T03:12:34+00:00},
  url = {https://deepai.org/publication/dealing-with-disagreements-looking-beyond-the-majority-vote-in-subjective-annotations},
  urldate = {2022-11-11},
  abstract = {10/12/21 - Majority voting and averaging are common approaches employed to resolve annotator disagreements and derive single ground truth lab...},
  organization = {{DeepAI}},
  file = {/home/skynet3/Zotero/storage/4Q72DPVA/dealing-with-disagreements-looking-beyond-the-majority-vote-in-subjective-annotations.html}
}

@misc{deanerManyProxyControls2021,
  title = {Many {{Proxy Controls}}},
  author = {Deaner, Ben},
  date = {2021-10-08},
  number = {arXiv:2110.03973},
  eprint = {2110.03973},
  eprinttype = {arxiv},
  primaryclass = {econ, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2110.03973},
  url = {http://arxiv.org/abs/2110.03973},
  urldate = {2022-11-11},
  abstract = {A recent literature considers causal inference using noisy proxies for unobserved confounding factors. The proxies are divided into two sets that are independent conditional on the confounders. One set of proxies are `negative control treatments' and the other are `negative control outcomes'. Existing work applies to low-dimensional settings with a fixed number of proxies and confounders. In this work we consider linear models with many proxy controls and possibly many confounders. A key insight is that if each group of proxies is strictly larger than the number of confounding factors, then a matrix of nuisance parameters has a low-rank structure and a vector of nuisance parameters has a sparse structure. We can exploit the rank-restriction and sparsity to reduce the number of free parameters to be estimated. The number of unobserved confounders is not known a priori but we show that it is identified, and we apply penalization methods to adapt to this quantity. We provide an estimator with a closed-form as well as a doubly-robust estimator that must be evaluated using numerical methods. We provide conditions under which our doubly-robust estimator is uniformly root-\$n\$ consistent, asymptotically centered normal, and our suggested confidence intervals have asymptotically correct coverage. We provide simulation evidence that our methods achieve better performance than existing approaches in high dimensions, particularly when the number of proxies is substantially larger than the number of confounders.},
  archiveprefix = {arXiv},
  keywords = {Economics - Econometrics,Statistics - Machine Learning},
  file = {/home/skynet3/Zotero/storage/2AD8MD9J/Deaner - 2021 - Many Proxy Controls.pdf;/home/skynet3/Zotero/storage/82SC785Q/2110.html}
}

@misc{deanerProxyControlsPanel2021,
  ids = {deanerProxyControlsPanel2021a},
  title = {Proxy {{Controls}} and {{Panel Data}}},
  author = {Deaner, Ben},
  date = {2021-01-05},
  number = {arXiv:1810.00283},
  eprint = {1810.00283},
  eprinttype = {arxiv},
  primaryclass = {econ},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1810.00283},
  url = {http://arxiv.org/abs/1810.00283},
  urldate = {2022-11-11},
  abstract = {We present a flexible approach to estimation, and inference in nonparametric, non-separable models using `proxy controls': covariates that do not satisfy a standard `unconfoundedness' assumption but are informative proxies for variables that do. Our analysis applies to cross-sectional settings but is particularly well-suited to panel models. Our identification results motivate a simple and `well-posed' nonparametric estimator. We derive convergence rates for the estimator and construct uniform confidence bands with asymptotically correct size. In panel settings, our methods provide a novel approach to the difficult problem of identification with non-separable, general heterogeneity and fixed T. In panels, observations from different periods serve as proxies for unobserved heterogeneity and our key identifying assumptions follow from restrictions on the serial dependence structure. We apply our methodology to two empirical settings. We estimate causal effects of grade retention on cognitive performance using cross-sectional variation and we estimate consumer demand counterfactuals using panel data.},
  archiveprefix = {arXiv},
  keywords = {Economics - Econometrics},
  file = {/home/skynet3/Zotero/storage/H47PJCJE/Deaner - 2021 - Proxy Controls and Panel Data.pdf;/home/skynet3/Zotero/storage/HRUETIZN/1810.html}
}

@online{DebugRefactorMagrittr,
  title = {Debug and {{Refactor}} Magrittr {{Pipelines}}},
  url = {https://alistaire47.github.io/pipecleaner/},
  urldate = {2022-11-11},
  abstract = {Tools for refactoring and debugging magrittr pipelines.},
  langid = {english},
  file = {/home/skynet3/Zotero/storage/YF9ZIIRK/pipecleaner.html}
}

@online{DeclareDesign,
  title = {{{DeclareDesign}}},
  url = {https://declaredesign.org/},
  urldate = {2022-11-11},
  file = {/home/skynet3/Zotero/storage/JM47LYTH/declaredesign.org.html}
}

@online{DeepDiveHow,
  title = {A {{Deep Dive Into How R Fits}} a {{Linear Model}}},
  url = {http://madrury.github.io/jekyll/update/statistics/2016/07/20/lm-in-R.html},
  urldate = {2022-11-11},
  file = {/home/skynet3/Zotero/storage/WQC9BRNF/lm-in-R.html}
}

@online{DeepLearning,
  ids = {DeepLearninga},
  title = {Deep {{Learning}}},
  url = {https://www.deeplearningbook.org/},
  urldate = {2022-11-11},
  file = {/home/skynet3/Zotero/storage/HXH2I8X9/www.deeplearningbook.org.html}
}

@online{DeeptimePythonLibrary2021,
  title = {Deeptime: A {{Python}} Library for Machine Learning Dynamical Models from Time Series Data},
  shorttitle = {Deeptime},
  date = {2021-10-28T10:53:03+00:00},
  url = {https://deepai.org/publication/deeptime-a-python-library-for-machine-learning-dynamical-models-from-time-series-data},
  urldate = {2022-11-11},
  abstract = {10/28/21 - Generation and analysis of time-series data is relevant to many quantitative fields ranging from economics to fluid mechanics. In ...},
  organization = {{DeepAI}},
  file = {/home/skynet3/Zotero/storage/5LWWXZIX/deeptime-a-python-library-for-machine-learning-dynamical-models-from-time-series-data.html}
}

@online{deevybeeFauxPeerreviewedJournals2020,
  ids = {deevybeeFauxPeerreviewedJournals2020a},
  title = {Faux Peer-Reviewed Journals: A Threat to Research Integrity},
  shorttitle = {Faux Peer-Reviewed Journals},
  author = {Deevybee},
  date = {2020-12-06},
  url = {http://deevybee.blogspot.com/2020/12/},
  urldate = {2022-11-11},
  organization = {{BishopBlog}},
  file = {/home/skynet3/Zotero/storage/EY9WQLDW/12.html}
}

@article{degtiarReviewGeneralizabilityTransportability2023,
  title = {A {{Review}} of {{Generalizability}} and {{Transportability}}},
  author = {Degtiar, Irina and Rose, Sherri},
  date = {2023-03-07},
  journaltitle = {Annual Review of Statistics and Its Application},
  shortjournal = {Annu. Rev. Stat. Appl.},
  volume = {10},
  number = {1},
  eprint = {2102.11904},
  eprinttype = {arxiv},
  primaryclass = {stat},
  pages = {annurev-statistics-042522-103837},
  issn = {2326-8298, 2326-831X},
  doi = {10.1146/annurev-statistics-042522-103837},
  url = {http://arxiv.org/abs/2102.11904},
  urldate = {2022-11-12},
  abstract = {When assessing causal effects, determining the target population to which the results are intended to generalize is a critical decision. Randomized and observational studies each have strengths and limitations for estimating causal effects in a target population. Estimates from randomized data may have internal validity but are often not representative of the target population. Observational data may better reflect the target population, and hence be more likely to have external validity, but are subject to potential bias due to unmeasured confounding. While much of the causal inference literature has focused on addressing internal validity bias, both internal and external validity are necessary for unbiased estimates in a target population. This paper presents a framework for addressing external validity bias, including a synthesis of approaches for generalizability and transportability, the assumptions they require, as well as tests for the heterogeneity of treatment effects and differences between study and target populations.},
  archiveprefix = {arXiv},
  keywords = {62-02,Statistics - Applications,Statistics - Machine Learning,Statistics - Methodology},
  file = {/home/skynet3/Zotero/storage/DEWU84D9/Degtiar and Rose - 2023 - A Review of Generalizability and Transportability.pdf;/home/skynet3/Zotero/storage/666GAW5F/2102.html}
}

@misc{dehghaniBenchmarkLottery2021,
  title = {The {{Benchmark Lottery}}},
  author = {Dehghani, Mostafa and Tay, Yi and Gritsenko, Alexey A. and Zhao, Zhe and Houlsby, Neil and Diaz, Fernando and Metzler, Donald and Vinyals, Oriol},
  date = {2021-07-14},
  number = {arXiv:2107.07002},
  eprint = {2107.07002},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2107.07002},
  url = {http://arxiv.org/abs/2107.07002},
  urldate = {2022-11-11},
  abstract = {The world of empirical machine learning (ML) strongly relies on benchmarks in order to determine the relative effectiveness of different algorithms and methods. This paper proposes the notion of "a benchmark lottery" that describes the overall fragility of the ML benchmarking process. The benchmark lottery postulates that many factors, other than fundamental algorithmic superiority, may lead to a method being perceived as superior. On multiple benchmark setups that are prevalent in the ML community, we show that the relative performance of algorithms may be altered significantly simply by choosing different benchmark tasks, highlighting the fragility of the current paradigms and potential fallacious interpretation derived from benchmarking ML methods. Given that every benchmark makes a statement about what it perceives to be important, we argue that this might lead to biased progress in the community. We discuss the implications of the observed phenomena and provide recommendations on mitigating them using multiple machine learning domains and communities as use cases, including natural language processing, computer vision, information retrieval, recommender systems, and reinforcement learning.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Information Retrieval,Computer Science - Machine Learning},
  file = {/home/skynet3/Zotero/storage/XR83U42T/Dehghani et al. - 2021 - The Benchmark Lottery.pdf;/home/skynet3/Zotero/storage/L5J3WW8A/2107.html}
}

@online{DelayPriorsPdf,
  ids = {DelayPriorsPdfa},
  title = {Delay/Priors.Pdf at Master · Lsbastos/{{Delay}}},
  url = {https://github.com/lsbastos/Delay},
  urldate = {2022-11-11},
  abstract = {Scripts and data for Modelling reporting delay for disease surveillance data using Bayesian methods. - Delay/priors.pdf at master · lsbastos/Delay},
  langid = {english},
  organization = {{GitHub}},
  file = {/home/skynet3/Zotero/storage/MVYM9GDD/priors.html}
}

@online{DelvingDeepImbalanced2021,
  title = {Delving into {{Deep Imbalanced Regression}}},
  date = {2021-02-18T18:56:03+00:00},
  url = {https://deepai.org/publication/delving-into-deep-imbalanced-regression},
  urldate = {2022-11-11},
  abstract = {02/18/21 - Real-world data often exhibit imbalanced distributions, where certain target values have significantly fewer observations. Existin...},
  organization = {{DeepAI}},
  file = {/home/skynet3/Zotero/storage/KPLTJ4CL/delving-into-deep-imbalanced-regression.html}
}

@article{demsvarStatisticalComparisonsClassifiers,
  ids = {demsvarStatisticalComparisonsClassifiersa},
  title = {Statistical {{Comparisons}} of {{Classiﬁers}} over {{Multiple Data Sets}}},
  author = {Demsˇar, Janez and Demsar, Janez},
  pages = {30},
  abstract = {While methods for comparing two learning algorithms on a single data set have been scrutinized for quite some time already, the issue of statistical tests for comparisons of more algorithms on multiple data sets, which is even more essential to typical machine learning studies, has been all but ignored. This article reviews the current practice and then theoretically and empirically examines several suitable tests. Based on that, we recommend a set of simple, yet safe and robust non-parametric tests for statistical comparisons of classifiers: the Wilcoxon signed ranks test for comparison of two classifiers and the Friedman test with the corresponding post-hoc tests for comparison of more classifiers over multiple data sets. Results of the latter can also be neatly presented with the newly introduced CD (critical difference) diagrams.},
  langid = {english},
  file = {/home/skynet3/Zotero/storage/IP9ZT85X/Demsˇar and Demsar - Statistical Comparisons of Classiﬁers over Multipl.pdf}
}

@book{dengCausalInferenceIts,
  title = {Causal {{Inference}} and {{Its Applications}} in {{Online Industry}}},
  author = {Deng, Alex},
  url = {https://alexdeng.github.io/causal/},
  urldate = {2022-11-11},
  abstract = {this is a draft book.},
  file = {/home/skynet3/Zotero/storage/NMQE4X69/causal.html}
}

@misc{dengMultipleImputationXGBoost2022,
  title = {Multiple {{Imputation Through XGBoost}}},
  author = {Deng, Yongshi and Lumley, Thomas},
  date = {2022-08-09},
  number = {arXiv:2106.01574},
  eprint = {2106.01574},
  eprinttype = {arxiv},
  primaryclass = {stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2106.01574},
  url = {http://arxiv.org/abs/2106.01574},
  urldate = {2022-11-11},
  abstract = {Multiple imputation is increasingly used in tackling missing data. While some conventional multiple imputation approaches are well studied and have shown empirical validity, they entail limitations in processing large datasets with complex data structures. Their imputation performances usually rely on proper specifications of imputation models, which require expert knowledge of the inherent relations among variables. In addition, these standard approaches tend to be computationally inefficient for medium and large datasets. In this paper, we propose a scalable multiple imputation framework mixgb, which is based on XGBoost, bootstrapping and predictive mean matching. XGBoost, one of the fastest implementations of gradient boosted trees, is able to automatically retain interactions and non-linear relations in a dataset while achieving high computational efficiency. With the aid of bootstrapping, and predictive mean matching, we show that our approach obtains less biased estimates and better reflects appropriate imputation variability. The proposed framework is implemented in an R package mixgb. Supplementary materials for this article are available online.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Methodology},
  file = {/home/skynet3/Zotero/storage/9G7LYSP8/Deng and Lumley - 2022 - Multiple Imputation Through XGBoost.pdf;/home/skynet3/Zotero/storage/YRF3DAVR/2106.html}
}

@misc{dengSurveySocietalEvent2021,
  ids = {dengSurveySocietalEvent2021a},
  title = {A {{Survey}} on {{Societal Event Forecasting}} with {{Deep Learning}}},
  author = {Deng, Songgaojun and Ning, Yue},
  date = {2021-12-12},
  number = {arXiv:2112.06345},
  eprint = {2112.06345},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2112.06345},
  urldate = {2022-11-11},
  abstract = {Population-level societal events, such as civil unrest and crime, often have a significant impact on our daily life. Forecasting such events is of great importance for decision-making and resource allocation. Event prediction has traditionally been challenging due to the lack of knowledge regarding the true causes and underlying mechanisms of event occurrence. In recent years, research on event forecasting has made significant progress due to two main reasons: (1) the development of machine learning and deep learning algorithms and (2) the accessibility of public data such as social media, news sources, blogs, economic indicators, and other meta-data sources. The explosive growth of data and the remarkable advancement in software/hardware technologies have led to applications of deep learning techniques in societal event studies. This paper is dedicated to providing a systematic and comprehensive overview of deep learning technologies for societal event predictions. We focus on two domains of societal events: \textbackslash textit\{civil unrest\} and \textbackslash textit\{crime\}. We first introduce how event forecasting problems are formulated as a machine learning prediction task. Then, we summarize data resources, traditional methods, and recent development of deep learning models for these problems. Finally, we discuss the challenges in societal event forecasting and put forward some promising directions for future research.},
  archiveprefix = {arXiv},
  keywords = {68T07,Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/home/skynet3/Zotero/storage/A4IIRHCZ/Deng and Ning - 2021 - A Survey on Societal Event Forecasting with Deep L.pdf;/home/skynet3/Zotero/storage/CYIBXA98/2112.html}
}

@misc{dennyTextPreprocessingUnsupervised2017,
  type = {SSRN Scholarly Paper},
  ids = {dennyTextPreprocessingUnsupervised2017a},
  title = {Text {{Preprocessing}} for {{Unsupervised Learning}}: {{Why It Matters}}, {{When It Misleads}}, and {{What}} to {{Do}} about {{It}}},
  shorttitle = {Text {{Preprocessing}} for {{Unsupervised Learning}}},
  author = {Denny, Matthew and Spirling, Arthur},
  date = {2017-09-27},
  number = {2849145},
  location = {{Rochester, NY}},
  doi = {10.2139/ssrn.2849145},
  url = {https://papers.ssrn.com/abstract=2849145},
  urldate = {2022-11-11},
  abstract = {Despite the popularity of unsupervised techniques for political science text-as-data research, the importance and implications of preprocessing decisions in this domain have received scant systematic attention.  Yet, as we show, such decisions have profound effects on the results of real models for real data. We argue that substantive theory is typically too vague to be of use for feature selection, and that the supervised literature is not necessarily a helpful source of advice.  To aid researchers working in unsupervised settings, we introduce a statistical procedure and software that examines the sensitivity of findings under alternate preprocessing regimes.  This approach complements a researcher's substantive understanding of a problem by providing a characterization of the variability changes in preprocessing choices may induce when analyzing a particular dataset.  In making scholars aware of the degree to which their results are likely to be sensitive to their preprocessing decisions, it aids replication efforts.},
  langid = {english},
  keywords = {forking paths,preprocessing,text-as-data},
  file = {/home/skynet3/Zotero/storage/9V8XTB8R/Denny and Spirling - 2017 - Text Preprocessing for Unsupervised Learning Why .pdf;/home/skynet3/Zotero/storage/B35GBH4T/papers.html}
}

@online{DepartmentMathematicsUniversity,
  title = {Department of {{Mathematics}} | {{The University}} of {{Chicago}}},
  url = {https://mathematics.uchicago.edu/},
  urldate = {2022-11-11},
  file = {/home/skynet3/Zotero/storage/S6QGGH6N/mathematics.uchicago.edu.html}
}

@online{DepartmentStatistics,
  ids = {DepartmentStatisticsa,DepartmentStatisticsb},
  title = {Department of {{Statistics}}},
  url = {https://stat.columbia.edu},
  urldate = {2022-11-11},
  abstract = {Columbia University},
  langid = {english},
  organization = {{Department of Statistics}},
  file = {/home/skynet3/Zotero/storage/46WLLDNX/stat.columbia.edu.html;/home/skynet3/Zotero/storage/IPT8KB94/stat.columbia.edu.html}
}

@online{dernoncourtWhatTradeoffBatch2020,
  type = {Forum post},
  title = {What Is the Trade-off between Batch Size and Number of Iterations to Train a Neural Network?},
  author = {Dernoncourt, Franck},
  date = {2020-10-31},
  url = {https://stats.stackexchange.com/q/164876},
  urldate = {2022-11-11},
  organization = {{Cross Validated}},
  file = {/home/skynet3/Zotero/storage/4SPZNF2M/236393.html}
}

@misc{desaRepresentationTradeoffsHyperbolic2018,
  title = {Representation {{Tradeoffs}} for {{Hyperbolic Embeddings}}},
  author = {De Sa, Christopher and Gu, Albert and Ré, Christopher and Sala, Frederic},
  date = {2018-04-24},
  number = {arXiv:1804.03329},
  eprint = {1804.03329},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/1804.03329},
  urldate = {2022-11-11},
  abstract = {Hyperbolic embeddings offer excellent quality with few dimensions when embedding hierarchical data structures like synonym or type hierarchies. Given a tree, we give a combinatorial construction that embeds the tree in hyperbolic space with arbitrarily low distortion without using optimization. On WordNet, our combinatorial embedding obtains a mean-average-precision of 0.989 with only two dimensions, while Nickel et al.'s recent construction obtains 0.87 using 200 dimensions. We provide upper and lower bounds that allow us to characterize the precision-dimensionality tradeoff inherent in any hyperbolic embedding. To embed general metric spaces, we propose a hyperbolic generalization of multidimensional scaling (h-MDS). We show how to perform exact recovery of hyperbolic points from distances, provide a perturbation analysis, and give a recovery result that allows us to reduce dimensionality. The h-MDS approach offers consistently low distortion even with few dimensions across several datasets. Finally, we extract lessons from the algorithms and theory above to design a PyTorch-based implementation that can handle incomplete information and is scalable.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/skynet3/Zotero/storage/CEK9ZQJ4/De Sa et al. - 2018 - Representation Tradeoffs for Hyperbolic Embeddings.pdf;/home/skynet3/Zotero/storage/ZMUG49GE/1804.html}
}

@misc{dettmersLLMInt88bit2022,
  ids = {dettmersLLMInt88bit2022a},
  title = {{{LLM}}.Int8(): 8-Bit {{Matrix Multiplication}} for {{Transformers}} at {{Scale}}},
  shorttitle = {{{LLM}}.Int8()},
  author = {Dettmers, Tim and Lewis, Mike and Belkada, Younes and Zettlemoyer, Luke},
  date = {2022-11-10},
  number = {arXiv:2208.07339},
  eprint = {2208.07339},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2208.07339},
  url = {http://arxiv.org/abs/2208.07339},
  urldate = {2022-11-11},
  abstract = {Large language models have been widely adopted but require significant GPU memory for inference. We develop a procedure for Int8 matrix multiplication for feed-forward and attention projection layers in transformers, which cut the memory needed for inference by half while retaining full precision performance. With our method, a 175B parameter 16/32-bit checkpoint can be loaded, converted to Int8, and used immediately without performance degradation. This is made possible by understanding and working around properties of highly systematic emergent features in transformer language models that dominate attention and transformer predictive performance. To cope with these features, we develop a two-part quantization procedure, LLM.int8(). We first use vector-wise quantization with separate normalization constants for each inner product in the matrix multiplication, to quantize most of the features. However, for the emergent outliers, we also include a new mixed-precision decomposition scheme, which isolates the outlier feature dimensions into a 16-bit matrix multiplication while still more than 99.9\% of values are multiplied in 8-bit. Using LLM.int8(), we show empirically it is possible to perform inference in LLMs with up to 175B parameters without any performance degradation. This result makes such models much more accessible, for example making it possible to use OPT-175B/BLOOM on a single server with consumer GPUs. We open-source our software.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/home/skynet3/Zotero/storage/KM8U3Q6H/Dettmers et al. - 2022 - LLM.int8() 8-bit Matrix Multiplication for Transf.pdf;/home/skynet3/Zotero/storage/SHU6LLDW/2208.html}
}

@article{devezerCaseFormalMethodology,
  title = {The Case for Formal Methodology in Scientific Reform},
  author = {Devezer, Berna and Navarro, Danielle J. and Vandekerckhove, Joachim and Ozge Buzbas, Erkan},
  journaltitle = {Royal Society Open Science},
  volume = {8},
  number = {3},
  pages = {200805},
  publisher = {{Royal Society}},
  doi = {10.1098/rsos.200805},
  url = {https://royalsocietypublishing.org/doi/10.1098/rsos.200805},
  urldate = {2022-11-11},
  abstract = {Current attempts at methodological reform in sciences come in response to an overall lack of rigor in methodological and scientific practices in experimental sciences. However, most methodological reform attempts suffer from similar mistakes and over-generalizations to the ones they aim to address. We argue that this can be attributed in part to lack of formalism and first principles. Considering the costs of allowing false claims to become canonized, we argue for formal statistical rigor and scientific nuance in methodological reform. To attain this rigor and nuance, we propose a five-step formal approach for solving methodological problems. To illustrate the use and benefits of such formalism, we present a formal statistical analysis of three popular claims in the metascientific literature: (i) that reproducibility is the cornerstone of science; (ii) that data must not be used twice in any analysis; and (iii) that exploratory projects imply poor statistical practice. We show how our formal approach can inform and shape debates about such methodological claims.},
  keywords = {double-dipping,exploratory research,replication,reproducibility,scientific reform},
  file = {/home/skynet3/Zotero/storage/VIWXZ6XN/Devezer et al. - The case for formal methodology in scientific refo.pdf}
}

@online{DiagnosingBiasedInference,
  title = {Diagnosing {{Biased Inference}} with {{Divergences}}},
  url = {https://mc-stan.org/users/documentation/case-studies/divergences_and_bias.html},
  urldate = {2022-11-11},
  file = {/home/skynet3/Zotero/storage/8H6NH2RY/divergences_and_bias.html}
}

@misc{diazCausalInfluenceCausal2022,
  ids = {diazCausalInfluenceCausal2022a},
  title = {Causal Influence, Causal Effects, and Path Analysis in the Presence of Intermediate Confounding},
  author = {Díaz, Iván},
  date = {2022-05-16},
  number = {arXiv:2205.08000},
  eprint = {2205.08000},
  eprinttype = {arxiv},
  primaryclass = {stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2205.08000},
  url = {http://arxiv.org/abs/2205.08000},
  urldate = {2022-11-11},
  abstract = {Recent approaches to causal inference have focused on the identification and estimation of \textbackslash textit\{causal effects\}, defined as (properties of) the distribution of counterfactual outcomes under hypothetical actions that alter the nodes of a graphical model. In this article we explore an alternative approach using the concept of \textbackslash textit\{causal influence\}, defined through operations that alter the information propagated through the edges of a directed acyclic graph. Causal influence may be more useful than causal effects in settings in which interventions on the causal agents are infeasible or of no substantive interest, for example when considering gender, race, or genetics as a causal agent. Furthermore, the "information transfer" interventions proposed allow us to solve a long-standing problem in causal mediation analysis, namely the non-parametric identification of path-specific effects in the presence of treatment-induced mediator-outcome confounding. We propose efficient non-parametric estimators for a covariance version of the proposed causal influence measures, using data-adaptive regression coupled with semi-parametric efficiency theory to address model misspecification bias while retaining \$\textbackslash sqrt\{n\}\$-consistency and asymptotic normality. We illustrate the use of our methods in two examples using publicly available data.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Methodology},
  file = {/home/skynet3/Zotero/storage/KLQGZD74/Díaz - 2022 - Causal influence, causal effects, and path analysi.pdf;/home/skynet3/Zotero/storage/2JCNYA4V/2205.html}
}

@online{DiDReadingGroup,
  title = {{{DiD Reading Group}} · {{Taylor Wright}}},
  url = {https://taylorjwright.github.io/did-reading-group/},
  urldate = {2022-11-11},
  file = {/home/skynet3/Zotero/storage/98RUXX6J/did-reading-group.html}
}

@misc{dillonTensorFlowDistributions2017,
  title = {{{TensorFlow Distributions}}},
  author = {Dillon, Joshua V. and Langmore, Ian and Tran, Dustin and Brevdo, Eugene and Vasudevan, Srinivas and Moore, Dave and Patton, Brian and Alemi, Alex and Hoffman, Matt and Saurous, Rif A.},
  date = {2017-11-28},
  number = {arXiv:1711.10604},
  eprint = {1711.10604},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1711.10604},
  url = {http://arxiv.org/abs/1711.10604},
  urldate = {2022-11-11},
  abstract = {The TensorFlow Distributions library implements a vision of probability theory adapted to the modern deep-learning paradigm of end-to-end differentiable computation. Building on two basic abstractions, it offers flexible building blocks for probabilistic computation. Distributions provide fast, numerically stable methods for generating samples and computing statistics, e.g., log density. Bijectors provide composable volume-tracking transformations with automatic caching. Together these enable modular construction of high dimensional distributions and transformations not possible with previous libraries (e.g., pixelCNNs, autoregressive flows, and reversible residual networks). They are the workhorse behind deep probabilistic programming systems like Edward and empower fast black-box inference in probabilistic models built on deep-network components. TensorFlow Distributions has proven an important part of the TensorFlow toolkit within Google and in the broader deep learning community.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Programming Languages,Statistics - Machine Learning},
  file = {/home/skynet3/Zotero/storage/LHHEB2CQ/Dillon et al. - 2017 - TensorFlow Distributions.pdf;/home/skynet3/Zotero/storage/UM8N24AJ/1711.html}
}

@online{DiscoveringReliableCorrelations2019,
  title = {Discovering {{Reliable Correlations}} in {{Categorical Data}}},
  date = {2019-08-30T12:18:29+00:00},
  url = {https://deepai.org/publication/discovering-reliable-correlations-in-categorical-data},
  urldate = {2022-11-11},
  abstract = {08/30/19 - In many scientific tasks we are interested in discovering whether there exist any correlations in our data. This raises many quest...},
  organization = {{DeepAI}},
  file = {/home/skynet3/Zotero/storage/5YRMSKW5/discovering-reliable-correlations-in-categorical-data.html}
}

@online{DiveDeepLearning,
  title = {Dive into {{Deep Learning}} — {{Dive}} into {{Deep Learning}} 1.0.0-Alpha1.Post0 Documentation},
  url = {http://d2l.ai/},
  urldate = {2022-11-11}
}

@online{Division,
  title = {Division},
  url = {https://www.wikidata.org/wiki/Q1226939},
  urldate = {2022-11-11},
  abstract = {arithmetic operation; one of the four basic operations of arithmetic (others being addition, subtraction, multiplication).The division of two natural numbers is the process of calculating the number of times one number is contained within one another},
  langid = {english}
}

@inreference{DivisionMathematics2022,
  ids = {DivisionMathematics2022a},
  title = {Division (Mathematics)},
  booktitle = {Wikipedia},
  date = {2022-09-01T18:35:24Z},
  url = {https://en.wikipedia.org/w/index.php?title=Division_(mathematics)&oldid=1107952867},
  urldate = {2022-11-11},
  abstract = {Division is one of the four basic operations of arithmetic, the ways that numbers are combined to make new numbers. The other operations are addition, subtraction, and multiplication. At an elementary level the division of two natural numbers is, among other possible interpretations, the process of calculating the number of times one number is contained within another.:{$\mkern1mu$}7{$\mkern1mu$} This number of times need not be an integer. For example, if 20 apples are divided evenly between 4 people, everyone receives 5 apples (see picture). The division with remainder or Euclidean division of two natural numbers provides an integer quotient, which is the number of times the second number is completely contained in the first number, and a remainder, which is the part of the first number that remains, when in the course of computing the quotient, no further full chunk of the size of the second number can be allocated. For example, if 21 apples are divided between 4 people, everyone receives 5 apples again, and 1 apple remains. For division to always yield one number rather than a quotient plus a remainder, the natural numbers must be extended to rational numbers or real numbers. In these enlarged number systems, division is the inverse operation to multiplication, that is a = c / b means a × b = c, as long as b is not zero. If b = 0, then this is a division by zero, which is not defined.:{$\mkern1mu$}246{$\mkern1mu$} In the 21-apples example, everyone would receive 5 apple and a quarter of an apple, thus avoiding any leftover. Both forms of division appear in various algebraic structures, different ways of defining mathematical structure. Those in which a Euclidean division (with remainder) is defined are called Euclidean domains and include polynomial rings in one indeterminate (which define multiplication and addition over single-variabled formulas). Those in which a division (with a single result) by all nonzero elements is defined are called fields and division rings. In a ring the elements by which division is always possible are called the units (for example, 1 and −1 in the ring of integers). Another generalization of division to algebraic structures is the quotient group, in which the result of "division" is a group rather than a number.},
  langid = {english},
  annotation = {Page Version ID: 1107952867},
  file = {/home/skynet3/Zotero/storage/24MLFPWS/Division_(mathematics).html}
}

@online{DlabberkeleyRMachineLearningDLab,
  title = {Dlab-Berkeley/{{R-Machine-Learning}}: {{D-Lab}}'s 6 Hour Introduction to Machine Learning in {{R}}. {{Learn}} the Fundamentals of Machine Learning, Regression, and Classification, Using Tidymodels in {{R}}.},
  shorttitle = {Dlab-Berkeley/{{R-Machine-Learning}}},
  url = {https://github.com/dlab-berkeley/R-Machine-Learning},
  urldate = {2022-11-11},
  abstract = {D-Lab\&\#39;s 6 hour introduction to machine learning in R. Learn the fundamentals of machine learning, regression, and classification, using tidymodels in R.  - dlab-berkeley/R-Machine-Learning: D-L...},
  langid = {english},
  organization = {{GitHub}},
  file = {/home/skynet3/Zotero/storage/RFMW8PAE/R-Machine-Learning.html}
}

@online{Documentation,
  title = {Documentation},
  url = {//mc-stan.org/users/documentation/},
  urldate = {2022-11-11},
  langid = {english},
  organization = {{stan-dev.github.io}}
}

@online{Documentationa,
  title = {R: {{Documentation}}},
  url = {https://www.r-project.org/other-docs.html},
  urldate = {2022-11-11},
  file = {/home/skynet3/Zotero/storage/R7QI93ZR/other-docs.html}
}

@online{DocumentDeduplicationLocality,
  title = {Document {{Deduplication}} with {{Locality Sensitive Hashing}}},
  url = {https://mattilyra.github.io/2017/05/23/document-deduplication-with-lsh.html},
  urldate = {2022-11-11},
  file = {/home/skynet3/Zotero/storage/RDEII2X3/document-deduplication-with-lsh.html}
}

@online{DoesBatchSize,
  title = {Does Batch Size Matter?},
  url = {https://blog.janestreet.com/does-batch-size-matter/},
  urldate = {2022-11-11},
  abstract = {This post is aimed at readers who are already familiar withstochastic gradient descent(SGD) and terms like “batch size”.  For an introduction to theseideas, ...},
  organization = {{Jane Street Tech Blog}}
}

@book{dogucuBayesRulesIntroduction,
  title = {Bayes {{Rules}}! {{An Introduction}} to {{Applied Bayesian Modeling}}},
  author = {Dogucu, Miles Q. Ott, Mine, Alicia A. Johnson},
  url = {https://www.bayesrulesbook.com/},
  urldate = {2022-11-11},
  abstract = {An introduction to applied Bayesian modeling.}
}

@online{doiSimplyStatistics,
  title = {Simply {{Statistics}}},
  author = {published yet DOI, Authors Affiliations Published Not},
  url = {https://simplystatistics.org/},
  urldate = {2022-11-11},
  organization = {{Simply Statistics}}
}

@misc{dolatabadiCOLLIDERRobustTraining2022,
  ids = {dolatabadiCOLLIDERRobustTraining2022a},
  title = {{{COLLIDER}}: {{A Robust Training Framework}} for {{Backdoor Data}}},
  shorttitle = {{{COLLIDER}}},
  author = {Dolatabadi, Hadi M. and Erfani, Sarah and Leckie, Christopher},
  date = {2022-10-12},
  number = {arXiv:2210.06704},
  eprint = {2210.06704},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2210.06704},
  url = {http://arxiv.org/abs/2210.06704},
  urldate = {2022-11-11},
  abstract = {Deep neural network (DNN) classifiers are vulnerable to backdoor attacks. An adversary poisons some of the training data in such attacks by installing a trigger. The goal is to make the trained DNN output the attacker's desired class whenever the trigger is activated while performing as usual for clean data. Various approaches have recently been proposed to detect malicious backdoored DNNs. However, a robust, end-to-end training approach, like adversarial training, is yet to be discovered for backdoor poisoned data. In this paper, we take the first step toward such methods by developing a robust training framework, COLLIDER, that selects the most prominent samples by exploiting the underlying geometric structures of the data. Specifically, we effectively filter out candidate poisoned data at each training epoch by solving a geometrical coreset selection objective. We first argue how clean data samples exhibit (1) gradients similar to the clean majority of data and (2) low local intrinsic dimensionality (LID). Based on these criteria, we define a novel coreset selection objective to find such samples, which are used for training a DNN. We show the effectiveness of the proposed method for robust training of DNNs on various poisoned datasets, reducing the backdoor success rate significantly.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/skynet3/Zotero/storage/F9VZQQZR/Dolatabadi et al. - 2022 - COLLIDER A Robust Training Framework for Backdoor.pdf;/home/skynet3/Zotero/storage/SGXQSUKI/2210.html}
}

@article{donoho50YearsData2017,
  ids = {donoho50YearsData2017a},
  title = {50 {{Years}} of {{Data Science}}},
  author = {Donoho, David},
  date = {2017-10-02},
  journaltitle = {Journal of Computational and Graphical Statistics},
  volume = {26},
  number = {4},
  pages = {745--766},
  publisher = {{Taylor \& Francis}},
  issn = {1061-8600},
  doi = {10.1080/10618600.2017.1384734},
  url = {https://doi.org/10.1080/10618600.2017.1384734},
  urldate = {2022-11-11},
  abstract = {More than 50 years ago, John Tukey called for a reformation of academic statistics. In “The Future of Data Analysis,” he pointed to the existence of an as-yet unrecognized science, whose subject of interest was learning from data, or “data analysis.” Ten to 20 years ago, John Chambers, Jeff Wu, Bill Cleveland, and Leo Breiman independently once again urged academic statistics to expand its boundaries beyond the classical domain of theoretical statistics; Chambers called for more emphasis on data preparation and presentation rather than statistical modeling; and Breiman called for emphasis on prediction rather than inference. Cleveland and Wu even suggested the catchy name “data science” for this envisioned field. A recent and growing phenomenon has been the emergence of “data science” programs at major universities, including UC Berkeley, NYU, MIT, and most prominently, the University of Michigan, which in September 2015 announced a \$100M “Data Science Initiative” that aims to hire 35 new faculty. Teaching in these new programs has significant overlap in curricular subject matter with traditional statistics courses; yet many academic statisticians perceive the new programs as “cultural appropriation.” This article reviews some ingredients of the current “data science moment,” including recent commentary about data science in the popular media, and about how/whether data science is really different from statistics. The now-contemplated field of data science amounts to a superset of the fields of statistics and machine learning, which adds some technology for “scaling up” to “big data.” This chosen superset is motivated by commercial rather than intellectual developments. Choosing in this way is likely to miss out on the really important intellectual event of the next 50 years. Because all of science itself will soon become data that can be mined, the imminent revolution in data science is not about mere “scaling up,” but instead the emergence of scientific studies of data analysis science-wide. In the future, we will be able to predict how a proposal to change data analysis workflows would impact the validity of data analysis across all of science, even predicting the impacts field-by-field. Drawing on work by Tukey, Cleveland, Chambers, and Breiman, I present a vision of data science based on the activities of people who are “learning from data,” and I describe an academic field dedicated to improving that activity in an evidence-based manner. This new field is a better academic enlargement of statistics and machine learning than today’s data science initiatives, while being able to accommodate the same short-term goals. Based on a presentation at the Tukey Centennial Workshop, Princeton, NJ, September 18, 2015.},
  keywords = {Correction,Cross-study analysis,Data analysis,Data science,Meta analysis,Predictive modeling,Quantitative programming environments,Statistics},
  annotation = {\_eprint: https://doi.org/10.1080/10618600.2017.1384734},
  file = {/home/skynet3/Zotero/storage/NHRE4ZTV/Donoho - 2017 - 50 Years of Data Science.pdf}
}

@online{doshiTransformersExplainedVisually2021,
  title = {Transformers {{Explained Visually}} — {{Not}} Just How, but {{Why}} They Work so Well},
  author = {Doshi, Ketan},
  date = {2021-06-08T23:30:28},
  url = {https://towardsdatascience.com/transformers-explained-visually-not-just-how-but-why-they-work-so-well-d840bd61a9d3},
  urldate = {2022-11-11},
  abstract = {A Gentle Guide to how the Attention Score calculations capture relationships between words in a sequence, in Plain English.},
  langid = {english},
  organization = {{Medium}},
  file = {/home/skynet3/Zotero/storage/5EFPB6VV/transformers-explained-visually-not-just-how-but-why-they-work-so-well-d840bd61a9d3.html}
}

@inreference{DotProduct2022,
  ids = {DotProduct2022a},
  title = {Dot Product},
  booktitle = {Wikipedia},
  date = {2022-09-10T00:23:48Z},
  url = {https://en.wikipedia.org/w/index.php?title=Dot_product&oldid=1109459520},
  urldate = {2022-11-11},
  abstract = {In mathematics, the dot product or scalar product is an algebraic operation that takes two equal-length sequences of numbers (usually coordinate vectors), and returns a single number.  In Euclidean geometry, the dot product of the Cartesian coordinates of two vectors is widely used. It is often called the inner product (or rarely projection product) of Euclidean space, even though it is not the only inner product that can be defined on Euclidean space (see  Inner product space for more). Algebraically, the dot product is the sum of the products of the corresponding entries of the two sequences of numbers. Geometrically, it is the product of the Euclidean magnitudes of the two vectors and the cosine of the angle between them. These definitions are equivalent when using Cartesian coordinates. In modern geometry, Euclidean spaces are often defined by using vector spaces. In this case, the dot product is used for defining lengths (the length of a vector is the square root of the dot product of the vector by itself) and angles (the cosine of the angle between two vectors is the quotient of their dot product by the product of their lengths). The name "dot product" is derived from the centered dot " · " that is often used to designate this operation; the alternative name "scalar product" emphasizes that the result is a scalar, rather than a vector, as is the case for the vector product in three-dimensional space.},
  langid = {english},
  annotation = {Page Version ID: 1109459520},
  file = {/home/skynet3/Zotero/storage/4PT3PDQ4/Dot_product.html}
}

@online{DoublyRobustDifferenceinDifferences,
  title = {Doubly {{Robust Difference-in-Differences Estimators}}},
  url = {https://psantanna.com/DRDID/},
  urldate = {2022-11-11},
  abstract = {Implements the locally efficient doubly robust difference-in-differences (DiD)     estimators for the average treatment effect proposed by Sant'Anna and Zhao (2020)     {$<$}doi:10.1016/j.jeconom.2020.06.003{$>$}. The estimator combines inverse probability weighting and outcome     regression estimators (also implemented in the package) to form estimators with     more attractive statistical properties. Two different estimation methods can be used     to estimate the nuisance functions.},
  langid = {english},
  file = {/home/skynet3/Zotero/storage/IV8FEXXI/DRDID.html}
}

@article{douglassMeasuringLandscapeCivil2018,
  title = {Measuring the Landscape of Civil War},
  author = {Douglass, Rex W. and Harkness, Kristen A.},
  date = {2018-02-15},
  journaltitle = {Journal of Peace Research},
  publisher = {{SAGE PublicationsSage UK: London, England}},
  doi = {10.1177/0022343318754959},
  url = {https://journals.sagepub.com/eprint/dRCkdD4ZWSp99x8cinAV/full},
  urldate = {2022-11-11},
  abstract = {Subnational conflict research increasingly utilizes georeferenced event datasets to understand contentious politics and violence. Yet, how exactly locations are...},
  langid = {english},
  file = {/home/skynet3/Zotero/storage/VRW3FZG4/Douglass and Harkness - 2018 - Measuring the landscape of civil war.pdf;/home/skynet3/Zotero/storage/ZJNA6P9R/full.html}
}

@article{doumaAnalysingContinuousProportions2019,
  ids = {doumaAnalysingContinuousProportions2019a},
  title = {Analysing Continuous Proportions in Ecology and Evolution: {{A}} Practical Introduction to Beta and {{Dirichlet}} Regression},
  shorttitle = {Analysing Continuous Proportions in Ecology and Evolution},
  author = {Douma, Jacob C. and Weedon, James T.},
  date = {2019},
  journaltitle = {Methods in Ecology and Evolution},
  volume = {10},
  number = {9},
  pages = {1412--1430},
  issn = {2041-210X},
  doi = {10.1111/2041-210X.13234},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/2041-210X.13234},
  urldate = {2022-11-11},
  abstract = {Proportional data, in which response variables are expressed as percentages or fractions of a whole, are analysed in many subfields of ecology and evolution. The scale-independence of proportions makes them appropriate to analyse many biological phenomena, but statistical analyses are not straightforward, since proportions can only take values from zero to one and their variance is usually not constant across the range of the predictor. Transformations to overcome these problems are often applied, but can lead to biased estimates and difficulties in interpretation. In this paper, we provide an overview of the different types of proportional data and discuss the different analysis strategies available. In particular, we review and discuss the use of promising, but little used, techniques for analysing continuous (also called non-count-based or non-binomial) proportions (e.g. percent cover, fraction time spent on an activity): beta and Dirichlet regression, and some of their most important extensions. A major distinction can be made between proportions arising from counts and those arising from continuous measurements. For proportions consisting of two categories, count-based data are best analysed using well-developed techniques such as logistic regression, while continuous proportions can be analysed with beta regression models. In the case of {$>$}2 categories, multinomial logistic regression or Dirichlet regression can be applied. Both beta and Dirichlet regression techniques model proportions at their original scale, which makes statistical inference more straightforward and produce less biased estimates relative to transformation-based solutions. Extensions to beta regression, such as models for variable dispersion, zero-one augmented data and mixed effects designs have been developed and are reviewed and applied to case studies. Finally, we briefly discuss some issues regarding model fitting, inference, and reporting that are particularly relevant to beta and Dirichlet regression. Beta regression and Dirichlet regression overcome some problems inherent in applying classic statistical approaches to proportional data. To facilitate the adoption of these techniques by practitioners in ecology and evolution, we present detailed, annotated demonstration scripts covering all variations of beta and Dirichlet regression discussed in the article, implemented in the freely available language for statistical computing, r.},
  langid = {english},
  keywords = {beta regression,Dirichlet regression,fractions,non-count proportions,one augmented,proportions,transformations,zero augmented},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/2041-210X.13234},
  file = {/home/skynet3/Zotero/storage/JGH86KIT/Douma and Weedon - 2019 - Analysing continuous proportions in ecology and ev.pdf;/home/skynet3/Zotero/storage/QFTC3NCB/2041-210X.html}
}

@misc{duInfluenceDataPreprocessing2021,
  title = {The {{Influence}} of {{Data Pre-processing}} and {{Post-processing}} on {{Long Document Summarization}}},
  author = {Du, Xinwei and Dong, Kailun and Zhang, Yuchen and Li, Yongsheng and Tsay, Ruei-Yu},
  date = {2021-12-02},
  number = {arXiv:2112.01660},
  eprint = {2112.01660},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2112.01660},
  url = {http://arxiv.org/abs/2112.01660},
  urldate = {2022-11-11},
  abstract = {Long document summarization is an important and hard task in the field of natural language processing. A good performance of the long document summarization reveals the model has a decent understanding of the human language. Currently, most researches focus on how to modify the attention mechanism of the transformer to achieve a higher ROUGE score. The study of data pre-processing and post-processing are relatively few. In this paper, we use two pre-processing methods and a post-processing method and analyze the effect of these methods on various long document summarization models.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/home/skynet3/Zotero/storage/S3J3P6GA/Du et al. - 2021 - The Influence of Data Pre-processing and Post-proc.pdf;/home/skynet3/Zotero/storage/5J4I4K7J/2112.html}
}

@article{dunfordIntegratedPictureConflict,
  title = {An {{Integrated Picture}} of {{Conﬂict}}},
  author = {Dunford, Eric T and Cunningham, David E and Backer, David and Donnay, Karsten},
  pages = {43},
  abstract = {Growth in event datasets is fostering research about patterns, dynamics, causes, and consequences of conflict. Studies typically rely on a single dataset. Instead, we advocate integrating multiple datasets to improve measurement and analysis. We have generated an integrated dataset covering all violent events for Africa from 1997-2018 from three leading datasets (ACLED, UCDP-GED, and GTD). Our approach involves both pre-processing the data so that they are comparable and using an automated approach to produce an integrated dataset that is transparent and reproducible. Through examining these integrated data, we find substantial overlap across these three datasets. At the same time, each dataset includes events that conceptually should be captured in the other datasets, but are not. Thus, we view these integrated data as offering a better measure of violent conflict. A statistical analysis shows that geographic features frequently used in analyses of the location of conflict events — including the distance from the capital or a border, terrain, economic development, and population–have different effects on the incidence and frequency of conflict events when using integrated data as compared to individual datasets. These illustrations highlight the potential for integration to advance conflict research by yielding a more complete and accurate picture of activity, which has repercussions for both descriptive and theoretical findings. Integration is likely to be increasingly worthwhile as event datasets proliferate, expand in coverage, and exhibit wider applications.},
  langid = {english},
  file = {/home/skynet3/Zotero/storage/9Q7Q5IWM/Dunford et al. - An Integrated Picture of Conﬂict.pdf}
}

@book{durbinTimeSeriesAnalysis2001,
  title = {Time {{Series Analysis}} by {{State Space Methods}}},
  author = {Durbin, James and Koopman, Siem Jan},
  date = {2001-01-01},
  edition = {1st edition},
  publisher = {{Oxford Univ Pr}},
  location = {{Oxford ; New York}},
  isbn = {978-0-19-852354-3},
  langid = {english},
  pagetotal = {253}
}

@misc{durouxImpactSubsamplingPruning2016,
  ids = {durouxImpactSubsamplingPruning2016a},
  title = {Impact of Subsampling and Pruning on Random Forests},
  author = {Duroux, Roxane and Scornet, Erwan},
  date = {2016-03-14},
  number = {arXiv:1603.04261},
  eprint = {1603.04261},
  eprinttype = {arxiv},
  primaryclass = {math, stat},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/1603.04261},
  urldate = {2022-11-11},
  abstract = {Random forests are ensemble learning methods introduced by Breiman (2001) that operate by averaging several decision trees built on a randomly selected subspace of the data set. Despite their widespread use in practice, the respective roles of the different mechanisms at work in Breiman's forests are not yet fully understood, neither is the tuning of the corresponding parameters. In this paper, we study the influence of two parameters , namely the subsampling rate and the tree depth, on Breiman's forests performance. More precisely, we show that fully developed sub-sampled forests and pruned (without subsampling) forests have similar performances, as long as respective parameters are well chosen. Moreover , experiments show that a proper tuning of subsampling or pruning lead in most cases to an improvement of Breiman's original forests errors.},
  archiveprefix = {arXiv},
  keywords = {Mathematics - Statistics Theory},
  file = {/home/skynet3/Zotero/storage/P2ILGAD6/Duroux and Scornet - 2016 - Impact of subsampling and pruning on random forest.pdf;/home/skynet3/Zotero/storage/8QHS9RVR/1603.html}
}

@online{EarthObservation,
  title = {Earth {{Observation}}},
  url = {https://eod-grss-ieee.com/dataset-search},
  urldate = {2022-11-11},
  file = {/home/skynet3/Zotero/storage/XBEP2ZTE/dataset-search.html}
}

@online{EasyBayesianBootstrap,
  title = {Easy {{Bayesian Bootstrap}} in {{R}} - {{Publishable Stuff}}},
  url = {https://www.sumsar.net/blog/2015/07/easy-bayesian-bootstrap-in-r/?utm_content=buffer53c16&utm_medium=social&utm_source=twitter.com&utm_campaign=buffer},
  urldate = {2022-11-11},
  file = {/home/skynet3/Zotero/storage/DHVECXYI/easy-bayesian-bootstrap-in-r.html}
}

@online{EasySpatialModeling,
  title = {Easy {{Spatial Modeling}} with {{Random Forest}}},
  url = {https://blasbenito.github.io/spatialRF/},
  urldate = {2022-11-11},
  abstract = {Automatic generation and selection of spatial predictors for spatial regression with Random Forest. Spatial predictors are surrogates of variables driving the spatial structure of a response variable. The package offers two methods to generate spatial predictors from a distance matrix among training cases: 1) Morans Eigenvector Maps (MEMs; Dray, Legendre, and Peres-Neto 2006 {$<$}DOI:10.1016/j.ecolmodel.2006.02.015{$>$}): computed as the eigenvectors of a weighted matrix of distances; 2) RFsp (Hengl et al. {$<$}DOI:10.7717/peerj.5518{$>$}): columns of the distance matrix used as spatial predictors. Spatial predictors help minimize the spatial autocorrelation of the model residuals and facilitate an honest assessment of the importance scores of the non-spatial predictors. Additionally, functions to reduce multicollinearity, identify relevant variable interactions, tune random forest hyperparameters, assess model transferability via spatial cross-validation, and explore model results via partial dependence curves and interaction surfaces are included in the package. The modelling functions are built around the highly efficient ranger' package (Wright and Ziegler 2017 {$<$}DOI:10.18637/jss.v077.i01{$>$}).},
  langid = {english},
  file = {/home/skynet3/Zotero/storage/2Y3RJF2Y/spatialRF.html}
}

@online{EasystatsPerformanceModels,
  title = {Easystats/Performance: {{Models}}' Quality and Performance Metrics ({{R2}}, {{ICC}}, {{LOO}}, {{AIC}}, {{BF}}, ...)},
  shorttitle = {Easystats/Performance},
  url = {https://github.com/easystats/performance},
  urldate = {2022-11-11},
  abstract = {:muscle: Models' quality and performance metrics (R2, ICC, LOO, AIC, BF, ...) - easystats/performance: Models' quality and performance metrics (R2, ICC, LOO, AIC, BF, ...)},
  langid = {english},
  organization = {{GitHub}},
  file = {/home/skynet3/Zotero/storage/BUA7LA68/performance.html}
}

@software{eckertAlexeckertParallelDist2022,
  title = {Alexeckert/{{parallelDist}}},
  author = {Eckert, Alexander},
  date = {2022-10-13T20:41:27Z},
  origdate = {2017-06-06T20:05:38Z},
  url = {https://github.com/alexeckert/parallelDist},
  urldate = {2022-11-11},
  abstract = {R Package: Parallel Distance Matrix Computation using Multiple Threads},
  keywords = {cran,data-science,distance-computations,matrices,r}
}

@misc{ecklesNoiseInducedRandomizationRegression2022,
  ids = {ecklesNoiseInducedRandomizationRegression2022a},
  title = {Noise-{{Induced Randomization}} in {{Regression Discontinuity Designs}}},
  author = {Eckles, Dean and Ignatiadis, Nikolaos and Wager, Stefan and Wu, Han},
  date = {2022-04-12},
  number = {arXiv:2004.09458},
  eprint = {2004.09458},
  eprinttype = {arxiv},
  primaryclass = {econ, stat},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2004.09458},
  urldate = {2022-11-11},
  abstract = {Regression discontinuity designs are used to estimate causal effects in settings where treatment is determined by whether an observed running variable crosses a pre-specified threshold. While the resulting sampling design is sometimes described as akin to a locally randomized experiment in a neighborhood of the threshold, standard formal analyses do not make reference to probabilistic treatment assignment and instead identify treatment effects via continuity arguments. Here we propose a new approach to identification, estimation, and inference in regression discontinuity designs that exploits measurement error, or other noise, in the running variable. Under an assumption that the measurement error is exogenous, we show how to estimate causal effects using a class of linear estimators that weight treated and control units so as to balance a latent variable of which the running variable is a noisy measure. We find this approach to facilitate inference for familiar estimands from the literature, as well as policy-relevant estimands that correspond to the effects of realistic changes to the existing treatment assignment rule. We demonstrate the method with a study of retention of HIV patients, and evaluate its performance using both simulated data and a regression discontinuity design artificially constructed from test scores in early childhood.},
  archiveprefix = {arXiv},
  keywords = {Economics - Econometrics,Statistics - Methodology},
  file = {/home/skynet3/Zotero/storage/DJY9Q8GU/Eckles et al. - 2022 - Noise-Induced Randomization in Regression Disconti.pdf;/home/skynet3/Zotero/storage/5P5ADKEY/2004.html}
}

@online{EconometricsFreeSoftware,
  title = {Econometrics and {{Free Software}}},
  url = {https://www.brodrigues.co/},
  urldate = {2022-11-11}
}

@online{EconometricsLaboratoryUC,
  title = {Econometrics {{Laboratory}}, {{UC Berkeley}}},
  url = {https://eml.berkeley.edu/},
  urldate = {2022-11-11},
  file = {/home/skynet3/Zotero/storage/ZXG4TXMF/eml.berkeley.edu.html}
}

@online{Economics,
  title = {Economics and {{R}}},
  url = {http://skranz.github.io//r/2021/01/05/FindingEconomicArticles4.html},
  urldate = {2022-11-11}
}

@article{efronPredictionEstimationAttribution,
  ids = {efronPredictionEstimationAttribution2020,efronPredictionEstimationAttributiona},
  title = {Prediction, {{Estimation}}, and {{Attribution}}},
  author = {Efron, Bradley},
  pages = {48},
  publisher = {{Taylor \& Francis}},
  abstract = {The scientific needs and computational limitations of the Twentieth Century fashioned classical statistical methodology. Both the needs and limitations have changed in the Twenty-First, and so has the methodology. Large-scale prediction algorithms — neural nets, deep learning, boosting, support vector machines, random forests —have achieved star status in the popular press. They are recognizable as heirs to the regression tradition, but ones carried out at enormous scale and on titanic data sets. How do these algorithms compare with standard regression techniques such as ordinary least squares or logistic regression? Several key discrepancies will be examined, centering on the differences between prediction and estimation or prediction and attribution (significance testing.) Most of the discussion is carried out through small numerical examples.},
  langid = {english},
  keywords = {Black box,Ephemeral predictors,Random forests,Surface plus noise},
  file = {/home/skynet3/Zotero/storage/WFT3Y2PU/Efron - Prediction, Estimation, and Attribution.pdf}
}

@misc{ehrlingerGgRandomForestsExploringRandom2016,
  ids = {ehrlingerGgRandomForestsExploringRandom2016a},
  title = {{{ggRandomForests}}: {{Exploring Random Forest Survival}}},
  shorttitle = {{{ggRandomForests}}},
  author = {Ehrlinger, John},
  date = {2016-12-28},
  number = {arXiv:1612.08974},
  eprint = {1612.08974},
  eprinttype = {arxiv},
  primaryclass = {stat},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/1612.08974},
  urldate = {2022-11-11},
  abstract = {Random forest (Leo Breiman 2001a) (RF) is a non-parametric statistical method requiring no distributional assumptions on covariate relation to the response. RF is a robust, nonlinear technique that optimizes predictive accuracy by fitting an ensemble of trees to stabilize model estimates. Random survival forests (RSF) (Ishwaran and Kogalur 2007; Ishwaran et al. 2008) are an extension of Breimans RF techniques allowing efficient nonparametric analysis of time to event data. The randomForestSRC package (Ishwaran and Kogalur 2014) is a unified treatment of Breimans random forest for survival, regression and classification problems. Predictive accuracy makes RF an attractive alternative to parametric models, though complexity and interpretability of the forest hinder wider application of the method. We introduce the ggRandomForests package, tools for visually understand random forest models grown in R (R Core Team 2014) with the randomForestSRC package. The ggRandomForests package is structured to extract intermediate data objects from randomForestSRC objects and generate figures using the ggplot2 (Wickham 2009) graphics package. This document is structured as a tutorial for building random forest for survival with the randomForestSRC package and using the ggRandomForests package for investigating how the forest is constructed. We analyse the Primary Biliary Cirrhosis of the liver data from a clinical trial at the Mayo Clinic (Fleming and Harrington 1991). Our aim is to demonstrate the strength of using Random Forest methods for both prediction and information retrieval, specifically in time to event data settings.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Computation,Statistics - Machine Learning},
  file = {/home/skynet3/Zotero/storage/NIEH4S75/Ehrlinger - 2016 - ggRandomForests Exploring Random Forest Survival.pdf;/home/skynet3/Zotero/storage/RZJGRMA4/1612.html}
}

@online{EkeanyBorutaShapTree,
  title = {Ekeany/{{Boruta-Shap}}: {{A Tree}} Based Feature Selection Tool Which Combines Both the {{Boruta}} Feature Selection Algorithm with Shapley Values.},
  shorttitle = {Ekeany/{{Boruta-Shap}}},
  url = {https://github.com/Ekeany/Boruta-Shap},
  urldate = {2022-11-11},
  abstract = {A Tree based feature selection tool which combines both the Boruta feature selection algorithm with shapley values.   - Ekeany/Boruta-Shap: A Tree based feature selection tool which combines both t...},
  langid = {english},
  organization = {{GitHub}},
  file = {/home/skynet3/Zotero/storage/FA7MGRDV/Boruta-Shap.html}
}

@misc{elderLearningPredictionIntervals2020,
  title = {Learning {{Prediction Intervals}} for {{Model Performance}}},
  author = {Elder, Benjamin and Arnold, Matthew and Murthi, Anupama and Navratil, Jiri},
  date = {2020-12-15},
  number = {arXiv:2012.08625},
  eprint = {2012.08625},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2012.08625},
  url = {http://arxiv.org/abs/2012.08625},
  urldate = {2022-11-11},
  abstract = {Understanding model performance on unlabeled data is a fundamental challenge of developing, deploying, and maintaining AI systems. Model performance is typically evaluated using test sets or periodic manual quality assessments, both of which require laborious manual data labeling. Automated performance prediction techniques aim to mitigate this burden, but potential inaccuracy and a lack of trust in their predictions has prevented their widespread adoption. We address this core problem of performance prediction uncertainty with a method to compute prediction intervals for model performance. Our methodology uses transfer learning to train an uncertainty model to estimate the uncertainty of model performance predictions. We evaluate our approach across a wide range of drift conditions and show substantial improvement over competitive baselines. We believe this result makes prediction intervals, and performance prediction in general, significantly more practical for real-world use.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/home/skynet3/Zotero/storage/I5BKGU3M/Elder et al. - 2020 - Learning Prediction Intervals for Model Performanc.pdf;/home/skynet3/Zotero/storage/RSY6DUJ4/2012.html}
}

@online{ElementsInformationTheory,
  title = {Elements of {{Information Theory}}, 2nd {{Edition}} | {{Wiley}}},
  url = {https://www.wiley.com/en-us/Elements+of+Information+Theory%2C+2nd+Edition-p-9780471241959},
  urldate = {2022-11-11},
  abstract = {The latest edition of this classic is updated with new problem sets and material The Second Edition of this fundamental textbook maintains the books tradition of clear, thought-provoking instruction. Readers are provided once again with an instructive mix of mathematics, physics, statistics, and information theory. All the essential topics in information theory are covered in detail, including entropy, data compression, channel capacity, rate distortion, network information theory, and hypothesis testing. The authors provide readers with a solid understanding of the underlying theory and applications. Problem sets and a telegraphic summary at the end of each chapter further assist readers. The historical notes that follow each chapter recap the main points. The Second Edition features: * Chapters reorganized to improve teaching * 200 new problems * New material on source coding, portfolio theory, and feedback capacity * Updated references Now current and enhanced, the Second Edition of Elements of Information Theory remains the ideal textbook for upper-level undergraduate and graduate courses in electrical engineering, statistics, and telecommunications.},
  langid = {american},
  organization = {{Wiley.com}},
  file = {/home/skynet3/Zotero/storage/R9UBIU8K/Elements+of+Information+Theory,+2nd+Edition-p-9780471241959.html}
}

@article{elliottDetectingPHacking2022,
  title = {Detecting P-{{Hacking}}},
  author = {Elliott, Graham and Kudrin, Nikolay and Wüthrich, Kaspar},
  date = {2022},
  journaltitle = {Econometrica},
  volume = {90},
  number = {2},
  pages = {887--906},
  issn = {1468-0262},
  doi = {10.3982/ECTA18583},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.3982/ECTA18583},
  urldate = {2022-11-11},
  abstract = {We theoretically analyze the problem of testing for p-hacking based on distributions of p-values across multiple studies. We provide general results for when such distributions have testable restrictions (are non-increasing) under the null of no p-hacking. We find novel additional testable restrictions for p-values based on t-tests. Specifically, the shape of the power functions results in both complete monotonicity as well as bounds on the distribution of p-values. These testable restrictions result in more powerful tests for the null hypothesis of no p-hacking. When there is also publication bias, our tests are joint tests for p-hacking and publication bias. A reanalysis of two prominent data sets shows the usefulness of our new tests.},
  langid = {english},
  keywords = {complete monotonicity,p-curve,p-values,publication bias},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.3982/ECTA18583},
  file = {/home/skynet3/Zotero/storage/WCU4FNM2/ECTA18583.html}
}

@article{enamoradoScalingDataMultiple2021,
  title = {Scaling {{Data}} from {{Multiple Sources}}},
  author = {Enamorado, Ted and López-Moctezuma, Gabriel and Ratkovic, Marc},
  date = {2021-04},
  journaltitle = {Political Analysis},
  volume = {29},
  number = {2},
  pages = {212--235},
  publisher = {{Cambridge University Press}},
  issn = {1047-1987, 1476-4989},
  doi = {10.1017/pan.2020.24},
  url = {https://www.cambridge.org/core/journals/political-analysis/article/abs/scaling-data-from-multiple-sources/1F9D30D8DDCE44379E8B962C29DADBAB?utm_source=hootsuite&utm_medium=twitter&utm_campaign=PAN_Nov20},
  urldate = {2022-11-11},
  abstract = {We introduce a method for scaling two datasets from different sources. The proposed method estimates a latent factor common to both datasets as well as an idiosyncratic factor unique to each. In addition, it offers a flexible modeling strategy that permits the scaled locations to be a function of covariates, and efficient implementation allows for inference through resampling. A simulation study shows that our proposed method improves over existing alternatives in capturing the variation common to both datasets, as well as the latent factors specific to each. We apply our proposed method to vote and speech data from the 112th U.S. Senate. We recover a shared subspace that aligns with a standard ideological dimension running from liberals to conservatives, while recovering the words most associated with each senator’s location. In addition, we estimate a word-specific subspace that ranges from national security to budget concerns, and a vote-specific subspace with Tea Party senators on one extreme and senior committee leaders on the other.},
  langid = {english},
  keywords = {multidimensional scaling,principal component analysis,U.S. Senate},
  file = {/home/skynet3/Zotero/storage/QWJFPG75/Enamorado et al. - 2021 - Scaling Data from Multiple Sources.pdf;/home/skynet3/Zotero/storage/JIJD84VE/1F9D30D8DDCE44379E8B962C29DADBAB.html}
}

@misc{engelkeExtremalBehaviorAggregated2017,
  title = {Extremal {{Behavior}} of {{Aggregated Data}} with an {{Application}} to {{Downscaling}}},
  author = {Engelke, Sebastian and de Fondeville, Raphael and Oesting, Marco},
  options = {useprefix=true},
  date = {2017-12-28},
  number = {arXiv:1712.09816},
  eprint = {1712.09816},
  eprinttype = {arxiv},
  primaryclass = {stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1712.09816},
  url = {http://arxiv.org/abs/1712.09816},
  urldate = {2022-11-11},
  abstract = {The distribution of spatially aggregated data from a stochastic process \$X\$ may exhibit a different tail behavior than its marginal distributions. For a large class of aggregating functionals \$\textbackslash ell\$ we introduce the \$\textbackslash ell\$-extremal coefficient that quantifies this difference as a function of the extremal spatial dependence in \$X\$. We also obtain the joint extremal dependence for multiple aggregation functionals applied to the same process. Explicit formulas for the \$\textbackslash ell\$-extremal coefficients and multivariate dependence structures are derived in important special cases. The results provide a theoretical link between the extremal distribution of the aggregated data and the corresponding underlying process, which we exploit to develop a method for statistical downscaling. We apply our framework to downscale daily temperature maxima in the south of France from a gridded data set and use our model to generate high resolution maps of the warmest day during the 2003 heatwave.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Applications,Statistics - Methodology},
  file = {/home/skynet3/Zotero/storage/GF7PLFAD/Engelke et al. - 2017 - Extremal Behavior of Aggregated Data with an Appli.pdf;/home/skynet3/Zotero/storage/7MG334HY/1712.html}
}

@article{epskampGaussianGraphicalModel2018,
  ids = {epskampGaussianGraphicalModel2018a},
  title = {The {{Gaussian Graphical Model}} in {{Cross-Sectional}} and {{Time-Series Data}}},
  author = {Epskamp, Sacha and Waldorp, Lourens J. and Mõttus, René and Borsboom, Denny},
  date = {2018-07-04},
  journaltitle = {Multivariate Behavioral Research},
  volume = {53},
  number = {4},
  eprint = {29658809},
  eprinttype = {pmid},
  pages = {453--480},
  publisher = {{Routledge}},
  issn = {0027-3171},
  doi = {10.1080/00273171.2018.1454823},
  url = {https://doi.org/10.1080/00273171.2018.1454823},
  urldate = {2022-11-11},
  abstract = {We discuss the Gaussian graphical model (GGM; an undirected network of partial correlation coefficients) and detail its utility as an exploratory data analysis tool. The GGM shows which variables predict one-another, allows for sparse modeling of covariance structures, and may highlight potential causal relationships between observed variables. We describe the utility in three kinds of psychological data sets: data sets in which consecutive cases are assumed independent (e.g., cross-sectional data), temporally ordered data sets (e.g., n = 1 time series), and a mixture of the 2 (e.g., n {$>$} 1 time series). In time-series analysis, the GGM can be used to model the residual structure of a vector-autoregression analysis (VAR), also termed graphical VAR. Two network models can then be obtained: a temporal network and a contemporaneous network. When analyzing data from multiple subjects, a GGM can also be formed on the covariance structure of stationary means—the between-subjects network. We discuss the interpretation of these models and propose estimation methods to obtain these networks, which we implement in the R packages graphicalVAR and mlVAR. The methods are showcased in two empirical examples, and simulation studies on these methods are included in the supplementary materials.},
  keywords = {exploratory-data analysis,multilevel modeling,multivariate analysis,network modeling,Time-series analysis},
  annotation = {\_eprint: https://doi.org/10.1080/00273171.2018.1454823},
  file = {/home/skynet3/Zotero/storage/CESWH992/Epskamp et al. - 2018 - The Gaussian Graphical Model in Cross-Sectional an.pdf}
}

@online{Equinox,
  title = {Equinox},
  url = {https://docs.kidger.site/equinox/},
  urldate = {2022-11-11},
  file = {/home/skynet3/Zotero/storage/V4LIQRZY/equinox.html}
}

@misc{ermakovConsistencyInconsistencyNonparametric2019,
  title = {On Consistency and Inconsistency of Nonparametric Tests},
  author = {Ermakov, Mikhail},
  date = {2019-09-11},
  number = {arXiv:1807.09076},
  eprint = {1807.09076},
  eprinttype = {arxiv},
  primaryclass = {math, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1807.09076},
  url = {http://arxiv.org/abs/1807.09076},
  urldate = {2022-11-11},
  abstract = {For \$\textbackslash chi\^2-\$tests with increasing number of cells, Cramer-von Mises tests, tests generated \$\textbackslash mathbb\{L\}\_2\$- norms of kernel estimators and tests generated quadratic forms of estimators of Fourier coefficients, we find necessary and sufficient conditions of consistency and inconsistency for sequences of alternatives having a given rate of convergence to hypothesis in \$\textbackslash mathbb\{L\}\_2\$-norm. We provide transparent interpretations of these conditions allowing to understand the structure of such consistent sequences. For problem of signal detection in Gaussian white noise we show that, if set of alternatives is bounded closed center-symmetric convex set \$U\$ with deleted "small" \$\textbackslash mathbb\{L\}\_2\$ -- ball, then compactness of set \$U\$ is necessary condition for existence of consistent tests.},
  archiveprefix = {arXiv},
  keywords = {62F03; 62G10; 62G20,Mathematics - Statistics Theory},
  file = {/home/skynet3/Zotero/storage/WS4F78U8/Ermakov - 2019 - On consistency and inconsistency of nonparametric .pdf;/home/skynet3/Zotero/storage/4Q2FRUUL/1807.html}
}

@article{erringtonChallengesAssessingReplicability2021,
  title = {Challenges for Assessing Replicability in Preclinical Cancer Biology},
  author = {Errington, Timothy M and Denis, Alexandria and Perfito, Nicole and Iorns, Elizabeth and Nosek, Brian A},
  editor = {Rodgers, Peter and Franco, Eduardo},
  date = {2021-12-07},
  journaltitle = {eLife},
  volume = {10},
  pages = {e67995},
  publisher = {{eLife Sciences Publications, Ltd}},
  issn = {2050-084X},
  doi = {10.7554/eLife.67995},
  url = {https://doi.org/10.7554/eLife.67995},
  urldate = {2022-11-11},
  abstract = {We conducted the Reproducibility Project: Cancer Biology to investigate the replicability of preclinical research in cancer biology. The initial aim of the project was to repeat 193 experiments from 53 high-impact papers, using an approach in which the experimental protocols and plans for data analysis had to be peer reviewed and accepted for publication before experimental work could begin. However, the various barriers and challenges we encountered while designing and conducting the experiments meant that we were only able to repeat 50 experiments from 23 papers. Here we report these barriers and challenges. First, many original papers failed to report key descriptive and inferential statistics: the data needed to compute effect sizes and conduct power analyses was publicly accessible for just 4 of 193 experiments. Moreover, despite contacting the authors of the original papers, we were unable to obtain these data for 68\% of the experiments. Second, none of the 193 experiments were described in sufficient detail in the original paper to enable us to design protocols to repeat the experiments, so we had to seek clarifications from the original authors. While authors were extremely or very helpful for 41\% of experiments, they were minimally helpful for 9\% of experiments, and not at all helpful (or did not respond to us) for 32\% of experiments. Third, once experimental work started, 67\% of the peer-reviewed protocols required modifications to complete the research and just 41\% of those modifications could be implemented. Cumulatively, these three factors limited the number of experiments that could be repeated. This experience draws attention to a basic and fundamental concern about replication – it is hard to assess whether reported findings are credible.},
  keywords = {open data,open science,preregistration,replication,reproducibility,Reproducibility Project: Cancer Biology},
  file = {/home/skynet3/Zotero/storage/B9WP7YGX/Errington et al. - 2021 - Challenges for assessing replicability in preclini.pdf}
}

@article{Error,
  title = {Error},
  url = {https://onlinelibrary.wiley.com/doi/full/10.1111/ecog.02881](https://onlinelibrary.wiley.com/doi/full/10.1111/ecog.02881},
  urldate = {2022-11-11},
  langid = {english},
  file = {/home/skynet3/Zotero/storage/ZYDWP42A/ecog.html}
}

@misc{ethayarajhUnderstandingDatasetDifficulty2022,
  ids = {ethayarajhUnderstandingDatasetDifficulty2022a},
  title = {Understanding {{Dataset Difficulty}} with \$\textbackslash mathcal\{\vphantom\}{{V}}\vphantom\{\}\$-{{Usable Information}}},
  author = {Ethayarajh, Kawin and Choi, Yejin and Swayamdipta, Swabha},
  date = {2022-06-14},
  number = {arXiv:2110.08420},
  eprint = {2110.08420},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2110.08420},
  url = {http://arxiv.org/abs/2110.08420},
  urldate = {2022-11-11},
  abstract = {Estimating the difficulty of a dataset typically involves comparing state-of-the-art models to humans; the bigger the performance gap, the harder the dataset is said to be. However, this comparison provides little understanding of how difficult each instance in a given distribution is, or what attributes make the dataset difficult for a given model. To address these questions, we frame dataset difficulty -- w.r.t. a model \$\textbackslash mathcal\{V\}\$ -- as the lack of \$\textbackslash mathcal\{V\}\$-\$\textbackslash textit\{usable information\}\$ (Xu et al., 2019), where a lower value indicates a more difficult dataset for \$\textbackslash mathcal\{V\}\$. We further introduce \$\textbackslash textit\{pointwise \$\textbackslash mathcal\{V\}\$-information\}\$ (PVI) for measuring the difficulty of individual instances w.r.t. a given distribution. While standard evaluation metrics typically only compare different models for the same dataset, \$\textbackslash mathcal\{V\}\$-\$\textbackslash textit\{usable information\}\$ and PVI also permit the converse: for a given model \$\textbackslash mathcal\{V\}\$, we can compare different datasets, as well as different instances/slices of the same dataset. Furthermore, our framework allows for the interpretability of different input attributes via transformations of the input, which we use to discover annotation artefacts in widely-used NLP benchmarks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/skynet3/Zotero/storage/ES8G644D/Ethayarajh et al. - 2022 - Understanding Dataset Difficulty with $mathcal V .pdf;/home/skynet3/Zotero/storage/HNN2497T/2110.html}
}

@misc{etievantCausalInferenceOversimplified2020,
  title = {Causal Inference under Over-Simplified Longitudinal Causal Models},
  author = {Etievant, Lola and Viallon, Vivian},
  date = {2020-06-01},
  number = {arXiv:1810.01294},
  eprint = {1810.01294},
  eprinttype = {arxiv},
  primaryclass = {stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1810.01294},
  url = {http://arxiv.org/abs/1810.01294},
  urldate = {2022-11-11},
  abstract = {Many causal models of interest in epidemiology involve longitudinal exposures, confounders and mediators. However, in practice, repeated measurements are not always available. Then, practitioners tend to overlook the time-varying nature of exposures and work under over-simplified causal models. Our objective here was to assess whether - and how - the causal effect identified under such misspecified causal models relates to true causal effects of interest. We focus on situations regarding the type of available data for exposures: when they correspond to (i) ``instantaneous'' levels measured at inclusion in the study or (ii) summary measures of their levels up to inclusion in the study. In each of these two situations, we derive sufficient conditions ensuring that the quantities estimated in practice under over-simplified causal models can be expressed as true longitudinal causal effects of interest, or some weighted averages thereof. Unsurprisingly, these sufficient conditions are very restrictive, and our results state that inference based on either ``instantaneous'' levels or summary measures usually returns quantities that do not directly relate to any causal effect of interest and should be interpreted with caution. They raise the need for the availability of repeated measurements and/or the development of sensitivity analyses when such data is not available.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Methodology},
  file = {/home/skynet3/Zotero/storage/NVTFRAQL/Etievant and Viallon - 2020 - Causal inference under over-simplified longitudina.pdf;/home/skynet3/Zotero/storage/JDVD7PN4/1810.html}
}

@misc{etzIntroductionConceptLikelihood2017,
  title = {Introduction to the Concept of Likelihood and Its Applications},
  author = {Etz, Alexander},
  date = {2017-10-19T15:33:31},
  publisher = {{PsyArXiv}},
  doi = {10.31234/osf.io/85ywt},
  url = {https://psyarxiv.com/85ywt/},
  urldate = {2022-11-11},
  abstract = {We introduce the statistical concept known as likelihood and discuss how it underlies common Frequentist and Bayesian statistical methods. This article is suitable for researchers interested in understanding the basis of their statistical tools, and is also ideal for teachers to use in their classrooms to introduce the topic to students at a conceptual level.},
  langid = {american},
  keywords = {Bayes Factor,Bayesian,Estimation,Frequentist,Likelihood,Quantitative Methods,Social and Behavioral Sciences},
  file = {/home/skynet3/Zotero/storage/4R4N4254/Etz - 2017 - Introduction to the concept of likelihood and its .pdf}
}

@online{EvolutionComputationalSocial,
  title = {8. {{The Evolution}} of {{Computational Social Science}} from a {{Sociology Perspective}} with {{Chris Bail}} by {{Diaries}} of {{Social Data Research}}},
  url = {https://anchor.fm/diaries-soc-data-research/episodes/The-Evolution-of-Computational-Social-Science-from-a-Sociology-Perspective-with-Chris-Bail-e17vikf},
  urldate = {2022-11-11},
  abstract = {This unique episode centers on a "meta" discussion on interdisciplinary work involving large-scale social data. We interview Chris Bail, a Professor of Sociology and Public Policy at Duke University. Last year, Chris and co-authors Achim Edelman, Tom Wolff, and Danielle Montagne published an overview paper titled "Computational Social Science and Sociology" in the Annual Review of Sociology. We discuss the challenges of defining this large research area, the benefits of making "lateral connections" with potential colleagues as a graduate student, and taking risks in pursuing new research directions. We also highlight the process behind the creation and growth of the Summer Institute in Computational Social Science, which Chris co-founded with Matt Salganik.},
  langid = {english},
  organization = {{Anchor}},
  file = {/home/skynet3/Zotero/storage/WKZNW62P/The-Evolution-of-Computational-Social-Science-from-a-Sociology-Perspective-with-Chris-Bail-e17v.html}
}

@online{ExplainedVisually,
  ids = {ExplainedVisuallya},
  title = {Explained {{Visually}}},
  url = {https://setosa.io/ev/},
  urldate = {2022-11-11},
  organization = {{Explained Visually}}
}

@online{ExplainingMLModels,
  title = {Explaining {{ML}} Models with {{SHAP}} and {{SAGE}}},
  url = {https://iancovert.com/blog/understanding-shap-sage/},
  urldate = {2022-11-11},
  file = {/home/skynet3/Zotero/storage/NLCYFFXV/understanding-shap-sage.html}
}

@online{ExploringNeuralNetworks,
  title = {Exploring {{Neural Networks Visually}} in the {{Browser}}},
  url = {https://cprimozic.net/blog/neural-network-experiments-and-visualizations/},
  urldate = {2022-11-11},
  abstract = {Introduces a browser-based sandbox for building, training, visualizing, and experimenting with neural networks.  Includes background information on the tool, usage information, technical implementation details, and a collection of observations and findings from using it myself.},
  langid = {english},
  organization = {{Casey Primozic's Blog}},
  file = {/home/skynet3/Zotero/storage/C8H6JEL7/neural-network-experiments-and-visualizations.html}
}

@software{Fable2022,
  title = {Fable},
  date = {2022-11-11T09:05:24Z},
  origdate = {2018-02-21T02:19:35Z},
  url = {https://github.com/tidyverts/fable},
  urldate = {2022-11-11},
  abstract = {Tidy time series forecasting},
  organization = {{tidyverts}},
  keywords = {forecasting}
}

@misc{farissEnhancingValidityObservational2016,
  type = {SSRN Scholarly Paper},
  ids = {farissEnhancingValidityObservational2016a},
  title = {Enhancing {{Validity}} in {{Observational Settings When Replication Is Not Possible}}},
  author = {Fariss, Christopher J. and Jones, Zachary},
  date = {2016-04-12},
  number = {2543525},
  location = {{Rochester, NY}},
  doi = {10.2139/ssrn.2543525},
  url = {https://papers.ssrn.com/abstract=2543525},
  urldate = {2022-11-11},
  abstract = {We argue that political scientists can provide additional evidence for the predictive validity of observational and quasi-experimental research designs by minimizing the expected prediction error or generalization error of their empirical models. For observational and quasi-experimental data not generated by a stochastic mechanism under the researcher's control, the reproduction of statistical analyses is possible but replication of the data generating procedures is not. Estimating the generalization error of a model for this type of data and then adjusting the model to minimize this estimate --- regularization --- provides evidence for the predictive validity of the study by decreasing the risk of overfitting. Estimating generalization error also allows for model comparisons that highlight underfitting: when a model generalizes poorly due to missing systematic features of the data generating process. Thus, minimizing generalization error provides a principled method for modeling relationships between variables that are measured but whose relationships with the outcome(s) are left unspecified by a deductively valid theory. Overall, the minimization of generalization error is important because it quantifies the expected reliability of predictions in a way that is similar to external validity, consequently increasing the validity of the study's conclusions.},
  langid = {english},
  keywords = {Christopher J. Fariss,Enhancing Validity in Observational Settings When Replication Is Not Possible,SSRN,Zachary Jones},
  file = {/home/skynet3/Zotero/storage/VZ28EM69/Fariss and Jones - 2016 - Enhancing Validity in Observational Settings When .pdf;/home/skynet3/Zotero/storage/8P77YRNM/papers.html}
}

@software{FarrellDayMiceRanger2022,
  title = {{{FarrellDay}}/{{miceRanger}}},
  date = {2022-11-09T18:03:28Z},
  origdate = {2020-01-07T14:23:55Z},
  url = {https://github.com/FarrellDay/miceRanger},
  urldate = {2022-11-11},
  abstract = {miceRanger: Fast Imputation with Random Forests in R},
  organization = {{Farrell Day}},
  keywords = {imputation-methods,machine-learning,mice,missing-data,missing-values,r,random-forests}
}

@online{FarrellDayMiceRangerMiceRanger,
  title = {{{FarrellDay}}/{{miceRanger}}: {{miceRanger}}: {{Fast Imputation}} with {{Random Forests}} in {{R}}},
  shorttitle = {{{FarrellDay}}/{{miceRanger}}},
  url = {https://github.com/FarrellDay/miceRanger},
  urldate = {2022-11-11},
  abstract = {miceRanger: Fast Imputation with Random Forests in R - FarrellDay/miceRanger: miceRanger: Fast Imputation with Random Forests in R},
  langid = {english},
  organization = {{GitHub}},
  file = {/home/skynet3/Zotero/storage/5Z8EZA79/miceRanger.html}
}

@software{FastaiNumericallinearalgebra2022,
  title = {Fastai/Numerical-Linear-Algebra},
  date = {2022-11-15T04:21:50Z},
  origdate = {2017-05-16T17:28:12Z},
  url = {https://github.com/fastai/numerical-linear-algebra},
  urldate = {2022-11-15},
  abstract = {Free online textbook of Jupyter notebooks for fast.ai Computational Linear Algebra course},
  organization = {{fast.ai}},
  keywords = {algorithms,data-science,deep-learning,linear-algebra,machine-learning,numpy,python}
}

@online{FateewiBayesdfaBayesian,
  title = {Fate-Ewi/Bayesdfa: {{Bayesian DFA}} with {{Stan}}},
  shorttitle = {Fate-Ewi/Bayesdfa},
  url = {https://github.com/fate-ewi/bayesdfa},
  urldate = {2022-11-11},
  abstract = {Bayesian DFA with Stan. Contribute to fate-ewi/bayesdfa development by creating an account on GitHub.},
  langid = {english},
  organization = {{GitHub}},
  file = {/home/skynet3/Zotero/storage/VNWKREYC/bayesdfa.html}
}

@article{faulknerLocallyAdaptiveSmoothing2018,
  title = {Locally {{Adaptive Smoothing}} with {{Markov Random Fields}} and {{Shrinkage Priors}}},
  author = {Faulkner, James R. and Minin, Vladimir N.},
  date = {2018-03},
  journaltitle = {Bayesian Analysis},
  volume = {13},
  number = {1},
  pages = {225--252},
  publisher = {{International Society for Bayesian Analysis}},
  issn = {1936-0975, 1931-6690},
  doi = {10.1214/17-BA1050},
  url = {https://projecteuclid.org/journals/bayesian-analysis/volume-13/issue-1/Locally-Adaptive-Smoothing-with-Markov-Random-Fields-and-Shrinkage-Priors/10.1214/17-BA1050.full},
  urldate = {2022-11-11},
  abstract = {We present a locally adaptive nonparametric curve fitting method that operates within a fully Bayesian framework. This method uses shrinkage priors to induce sparsity in order-k differences in the latent trend function, providing a combination of local adaptation and global control. Using a scale mixture of normals representation of shrinkage priors, we make explicit connections between our method and kth order Gaussian Markov random field smoothing. We call the resulting processes shrinkage prior Markov random fields (SPMRFs). We use Hamiltonian Monte Carlo to approximate the posterior distribution of model parameters because this method provides superior performance in the presence of the high dimensionality and strong parameter correlations exhibited by our models. We compare the performance of three prior formulations using simulated data and find the horseshoe prior provides the best compromise between bias and precision. We apply SPMRF models to two benchmark data examples frequently used to test nonparametric methods. We find that this method is flexible enough to accommodate a variety of data generating models and offers the adaptive properties and computational tractability to make it a useful addition to the Bayesian nonparametric toolbox.},
  keywords = {Hamiltonian Monte Carlo,horseshoe prior,Lévy process,nonparametric},
  file = {/home/skynet3/Zotero/storage/YMV5NFZR/Faulkner and Minin - 2018 - Locally Adaptive Smoothing with Markov Random Fiel.pdf;/home/skynet3/Zotero/storage/NTUNP8MA/17-BA1050.html}
}

@online{FeathrLinkedInFeature,
  title = {Feathr: {{LinkedIn}}’s Feature Store Is Now Available on {{Azure}}},
  shorttitle = {Feathr},
  url = {https://azure.microsoft.com/en-us/blog/feathr-linkedin-s-feature-store-is-now-available-on-azure/},
  urldate = {2022-11-11},
  abstract = {With the advance of AI and machine learning, companies start to use complex machine learning pipelines in various applications, such as recommendation systems, fraud detection, and more. These complex systems usually require hundreds to thousands of features to support time-sensitive business ...},
  langid = {english},
  file = {/home/skynet3/Zotero/storage/6AQWV7TD/feathr-linkedin-s-feature-store-is-now-available-on-azure.html}
}

@inproceedings{fegerGamificationScienceStudy2019,
  title = {Gamification in {{Science}}: {{A Study}} of {{Requirements}} in the {{Context}} of {{Reproducible Research}}},
  shorttitle = {Gamification in {{Science}}},
  booktitle = {Proceedings of the 2019 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Feger, Sebastian S. and Dallmeier-Tiessen, Sünje and Woźniak, Paweł W. and Schmidt, Albrecht},
  date = {2019-05-02},
  eprint = {1903.02446},
  eprinttype = {arxiv},
  primaryclass = {cs},
  pages = {1--14},
  doi = {10.1145/3290605.3300690},
  url = {http://arxiv.org/abs/1903.02446},
  urldate = {2022-11-11},
  abstract = {The need for data preservation and reproducible research is widely recognized in the scientific community. Yet, researchers often struggle to find the motivation to contribute to data repositories and to use tools that foster reproducibility. In this paper, we explore possible uses of gamification to support reproducible practices in High Energy Physics. To understand how gamification can be effective in research tools, we participated in a workshop and performed interviews with data analysts. We then designed two interactive prototypes of a research preservation service that use contrasting gamification strategies. The evaluation of the prototypes showed that gamification needs to address core scientific challenges, in particular the fair reflection of quality and individual contribution. Through thematic analysis, we identified four themes which describe perceptions and requirements of gamification in research: Contribution, Metrics, Applications and Scientific practice. Based on these, we discuss design implications for gamification in science.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Human-Computer Interaction},
  file = {/home/skynet3/Zotero/storage/ZUG867DU/Feger et al. - 2019 - Gamification in Science A Study of Requirements i.pdf;/home/skynet3/Zotero/storage/23I9ZN6N/1903.html}
}

@article{fernandez-delgadoWeNeedHundreds,
  ids = {fernandez-delgadoWeNeedHundredsa},
  title = {Do We {{Need Hundreds}} of {{Classiﬁers}} to {{Solve Real World Classiﬁcation Problems}}?},
  author = {Fernandez-Delgado, Manuel and Cernadas, Eva and Barro, Senen and Amorim, Dinani},
  pages = {49},
  abstract = {We evaluate 179 classifiers arising from 17 families (discriminant analysis, Bayesian, neural networks, support vector machines, decision trees, rule-based classifiers, boosting, bagging, stacking, random forests and other ensembles, generalized linear models, nearestneighbors, partial least squares and principal component regression, logistic and multinomial regression, multiple adaptive regression splines and other methods), implemented in Weka, R (with and without the caret package), C and Matlab, including all the relevant classifiers available today. We use 121 data sets, which represent the whole UCI data base (excluding the large-scale problems) and other own real problems, in order to achieve significant conclusions about the classifier behavior, not dependent on the data set collection. The classifiers most likely to be the bests are the random forest (RF) versions, the best of which (implemented in R and accessed via caret) achieves 94.1\% of the maximum accuracy overcoming 90\% in the 84.3\% of the data sets. However, the difference is not statistically significant with the second best, the SVM with Gaussian kernel implemented in C using LibSVM, which achieves 92.3\% of the maximum accuracy. A few models are clearly better than the remaining ones: random forest, SVM with Gaussian and polynomial kernels, extreme learning machine with Gaussian kernel, C5.0 and avNNet (a committee of multi-layer perceptrons implemented in R with the caret package). The random forest is clearly the best family of classifiers (3 out of 5 bests classifiers are RF), followed by SVM (4 classifiers in the top-10), neural networks and boosting ensembles (5 and 3 members in the top-20, respectively).},
  langid = {english},
  file = {/home/skynet3/Zotero/storage/7V6J6J6L/Fernandez-Delgado et al. - Do we Need Hundreds of Classiﬁers to Solve Real Wo.pdf}
}

@online{Filter,
  title = {Filter},
  url = {https://www.wikidata.org/wiki/Q2672873},
  urldate = {2022-11-11},
  abstract = {function in programming},
  langid = {english}
}

@inreference{FilterHigherorderFunction2022,
  ids = {FilterHigherorderFunction2022a},
  title = {Filter (Higher-Order Function)},
  booktitle = {Wikipedia},
  date = {2022-03-31T18:29:43Z},
  url = {https://en.wikipedia.org/w/index.php?title=Filter_(higher-order_function)&oldid=1080346575},
  urldate = {2022-11-11},
  abstract = {In functional programming, filter is a higher-order function that processes a data structure (usually a list) in some order to produce a new data structure containing exactly those elements of the original data structure for which a given predicate returns the boolean value true.},
  langid = {english},
  annotation = {Page Version ID: 1080346575},
  file = {/home/skynet3/Zotero/storage/R5H6ALYV/Filter_(higher-order_function).html}
}

@online{fixDunningKrugerEffectAutocorrelation2022,
  title = {The {{Dunning-Kruger Effect}} Is {{Autocorrelation}}},
  author = {Fix, Blair},
  date = {2022-04-08T15:35:54+00:00},
  url = {https://economicsfromthetopdown.com/2022/04/08/the-dunning-kruger-effect-is-autocorrelation/},
  urldate = {2022-11-11},
  abstract = {Do unskilled people actually underestimate their incompetence?},
  langid = {american},
  organization = {{Economics from the Top Down}},
  file = {/home/skynet3/Zotero/storage/CCWKR49A/the-dunning-kruger-effect-is-autocorrelation.html}
}

@online{FK83ScoringRulesScoring,
  title = {{{FK83}}/{{scoringRules}}: Scoring Rules to Evaluate Probabilistic Forecasts},
  shorttitle = {{{FK83}}/{{scoringRules}}},
  url = {https://github.com/FK83/scoringRules},
  urldate = {2022-11-11},
  abstract = {scoring rules to evaluate probabilistic forecasts. Contribute to FK83/scoringRules development by creating an account on GitHub.},
  langid = {english},
  organization = {{GitHub}},
  file = {/home/skynet3/Zotero/storage/PC8AG6FT/scoringRules.html}
}

@book{ForecastingPrinciplesPractice,
  title = {Forecasting: {{Principles}} and {{Practice}} (3rd Ed)},
  shorttitle = {Forecasting},
  url = {https://otexts.com/fpp3/},
  urldate = {2022-11-11},
  abstract = {3rd edition},
  file = {/home/skynet3/Zotero/storage/XZYIDUQU/fpp3.html}
}

@online{ForecastingScaleHow,
  title = {Forecasting at {{Scale}}: {{How}} and {{Why We Developed Prophet}} for {{Forecasting}} at {{Facebook}} - {{YouTube}}},
  url = {https://www.youtube.com/watch?v=pOYAXv15r3A},
  urldate = {2022-11-11}
}

@article{franco-villoriaUnifiedViewBayesian2019,
  title = {A Unified View on {{Bayesian}} Varying Coefficient Models},
  author = {Franco-Villoria, Maria and Ventrucci, Massimo and Rue, Håvard},
  date = {2019-01},
  journaltitle = {Electronic Journal of Statistics},
  volume = {13},
  number = {2},
  pages = {5334--5359},
  publisher = {{Institute of Mathematical Statistics and Bernoulli Society}},
  issn = {1935-7524, 1935-7524},
  doi = {10.1214/19-EJS1653},
  url = {https://projecteuclid.org/journals/electronic-journal-of-statistics/volume-13/issue-2/A-unified-view-on-Bayesian-varying-coefficient-models/10.1214/19-EJS1653.full},
  urldate = {2022-11-11},
  abstract = {Varying coefficient models are useful in applications where the effect of the covariate might depend on some other covariate such as time or location. Various applications of these models often give rise to case-specific prior distributions for the parameter(s) describing how much the coefficients vary. In this work, we introduce a unified view of varying coefficients models, arguing for a way of specifying these prior distributions that are coherent across various applications, avoid overfitting and have a coherent interpretation. We do this by considering varying coefficients models as a flexible extension of the natural simpler model and capitalising on the recently proposed framework of penalized complexity (PC) priors. We illustrate our approach in two spatial examples where varying coefficient models are relevant.},
  keywords = {INLA,overfitting,penalized complexity prior,varying coefficient models},
  file = {/home/skynet3/Zotero/storage/FHCJKD6L/Franco-Villoria et al. - 2019 - A unified view on Bayesian varying coefficient mod.pdf;/home/skynet3/Zotero/storage/L2Y79TLK/19-EJS1653.html}
}

@article{francois-lavetIntroductionDeepReinforcement2018,
  ids = {francois-lavetIntroductionDeepReinforcement2018a},
  title = {An {{Introduction}} to {{Deep Reinforcement Learning}}},
  author = {Francois-Lavet, Vincent and Henderson, Peter and Islam, Riashat and Bellemare, Marc G. and Pineau, Joelle},
  date = {2018},
  journaltitle = {Foundations and Trends® in Machine Learning},
  shortjournal = {FNT in Machine Learning},
  volume = {11},
  number = {3-4},
  eprint = {1811.12560},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  pages = {219--354},
  issn = {1935-8237, 1935-8245},
  doi = {10.1561/2200000071},
  url = {http://arxiv.org/abs/1811.12560},
  urldate = {2022-11-11},
  abstract = {Deep reinforcement learning is the combination of reinforcement learning (RL) and deep learning. This field of research has been able to solve a wide range of complex decision-making tasks that were previously out of reach for a machine. Thus, deep RL opens up many new applications in domains such as healthcare, robotics, smart grids, finance, and many more. This manuscript provides an introduction to deep reinforcement learning models, algorithms and techniques. Particular focus is on the aspects related to generalization and how deep RL can be used for practical applications. We assume the reader is familiar with basic machine learning concepts.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/skynet3/Zotero/storage/XDKEXJUE/Francois-Lavet et al. - 2018 - An Introduction to Deep Reinforcement Learning.pdf;/home/skynet3/Zotero/storage/TBZQGCEF/1811.html}
}

@misc{francoisArbitrarinessPeerReview2015,
  title = {Arbitrariness of Peer Review: {{A Bayesian}} Analysis of the {{NIPS}} Experiment},
  shorttitle = {Arbitrariness of Peer Review},
  author = {Francois, Olivier},
  date = {2015-07-23},
  number = {arXiv:1507.06411},
  eprint = {1507.06411},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1507.06411},
  url = {http://arxiv.org/abs/1507.06411},
  urldate = {2022-11-11},
  abstract = {The principle of peer review is central to the evaluation of research, by ensuring that only high-quality items are funded or published. But peer review has also received criticism, as the selection of reviewers may introduce biases in the system. In 2014, the organizers of the ``Neural Information Processing Systems\textbackslash rq\textbackslash rq\{\} conference conducted an experiment in which \$10\textbackslash\%\$ of submitted manuscripts (166 items) went through the review process twice. Arbitrariness was measured as the conditional probability for an accepted submission to get rejected if examined by the second committee. This number was equal to \$60\textbackslash\%\$, for a total acceptance rate equal to \$22.5\textbackslash\%\$. Here we present a Bayesian analysis of those two numbers, by introducing a hidden parameter which measures the probability that a submission meets basic quality criteria. The standard quality criteria usually include novelty, clarity, reproducibility, correctness and no form of misconduct, and are met by a large proportions of submitted items. The Bayesian estimate for the hidden parameter was equal to \$56\textbackslash\%\$ (\$95\textbackslash\%\$CI: \$ I = (0.34, 0.83)\$), and had a clear interpretation. The result suggested the total acceptance rate should be increased in order to decrease arbitrariness estimates in future review processes.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Digital Libraries,Statistics - Machine Learning,Statistics - Other Statistics},
  file = {/home/skynet3/Zotero/storage/3WSR5FUE/Francois - 2015 - Arbitrariness of peer review A Bayesian analysis .pdf;/home/skynet3/Zotero/storage/CV5LIBJA/1507.html}
}

@misc{frankLargescaleLinearRegression2015,
  title = {Large-Scale Linear Regression: {{Development}} of High-Performance Routines},
  shorttitle = {Large-Scale Linear Regression},
  author = {Frank, Alvaro and Fabregat-Traver, Diego and Bientinesi, Paolo},
  date = {2015-04-29},
  number = {arXiv:1504.07890},
  eprint = {1504.07890},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1504.07890},
  url = {http://arxiv.org/abs/1504.07890},
  urldate = {2022-11-11},
  abstract = {In statistics, series of ordinary least squares problems (OLS) are used to study the linear correlation among sets of variables of interest; in many studies, the number of such variables is at least in the millions, and the corresponding datasets occupy terabytes of disk space. As the availability of large-scale datasets increases regularly, so does the challenge in dealing with them. Indeed, traditional solvers---which rely on the use of black-box" routines optimized for one single OLS---are highly inefficient and fail to provide a viable solution for big-data analyses. As a case study, in this paper we consider a linear regression consisting of two-dimensional grids of related OLS problems that arise in the context of genome-wide association analyses, and give a careful walkthrough for the development of \{\textbackslash sc ols-grid\}, a high-performance routine for shared-memory architectures; analogous steps are relevant for tailoring OLS solvers to other applications. In particular, we first illustrate the design of efficient algorithms that exploit the structure of the OLS problems and eliminate redundant computations; then, we show how to effectively deal with datasets that do not fit in main memory; finally, we discuss how to cast the computation in terms of efficient kernels and how to achieve scalability. Importantly, each design decision along the way is justified by simple performance models. \{\textbackslash sc ols-grid\} enables the solution of \$10\^\{11\}\$ correlated OLS problems operating on terabytes of data in a matter of hours.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computational Engineering; Finance; and Science,Computer Science - Mathematical Software},
  file = {/home/skynet3/Zotero/storage/BQNJYJ2B/Frank et al. - 2015 - Large-scale linear regression Development of high.pdf;/home/skynet3/Zotero/storage/5EMR7NI5/1504.html}
}

@book{frantzExploringEnterpriseDatabases,
  title = {Exploring {{Enterprise Databases}} with {{R}}: {{A Tidyverse Approach}}},
  shorttitle = {Exploring {{Enterprise Databases}} with {{R}}},
  author = {Frantz, Sophie Yang, M. Edward (Ed) Borasky, Jim Tyhurst, Scott Came, Mary Anne Thygesen, {and} Ian, John David Smith},
  url = {https://smithjd.github.io/sql-pet/},
  urldate = {2022-11-11},
  abstract = {An introduction to Docker and PostgreSQL for R users to simulate use cases behind corporate walls.},
  file = {/home/skynet3/Zotero/storage/56QBP5A6/sql-pet.html}
}

@misc{franzRatiosShortGuide2007,
  title = {Ratios: {{A}} Short Guide to Confidence Limits and Proper Use},
  shorttitle = {Ratios},
  author = {Franz, Volker H.},
  date = {2007-10-10},
  number = {arXiv:0710.2024},
  eprint = {0710.2024},
  eprinttype = {arxiv},
  primaryclass = {stat},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/0710.2024},
  urldate = {2022-11-11},
  abstract = {Researchers often calculate ratios of measured quantities. Specifying confidence limits for ratios is difficult and the appropriate methods are often unknown. Appropriate methods are described (Fieller, Taylor, special bootstrap methods). For the Fieller method a simple geometrical interpretation is given. Monte Carlo simulations show when these methods are appropriate and that the most frequently used methods (index method and zero-variance method) can lead to large liberal deviations from the desired confidence level. It is discussed when we can use standard regression or measurement error models and when we have to resort to specific models for heteroscedastic data. Finally, an old warning is repeated that we should be aware of the problems of spurious correlations if we use ratios.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Applications,Statistics - Methodology},
  file = {/home/skynet3/Zotero/storage/X58CYM7K/Franz - 2007 - Ratios A short guide to confidence limits and pro.pdf;/home/skynet3/Zotero/storage/KAVAEPAK/0710.html}
}

@article{frasslTenSimpleRules2018,
  title = {Ten Simple Rules for Collaboratively Writing a Multi-Authored Paper},
  author = {Frassl, Marieke A. and Hamilton, David P. and Denfeld, Blaize A. and de Eyto, Elvira and Hampton, Stephanie E. and Keller, Philipp S. and Sharma, Sapna and Lewis, Abigail S. L. and Weyhenmeyer, Gesa A. and O’Reilly, Catherine M. and Lofton, Mary E. and Catalán, Núria},
  date = {2018-11-15},
  journaltitle = {PLOS Computational Biology},
  shortjournal = {PLOS Computational Biology},
  volume = {14},
  number = {11},
  pages = {e1006508},
  publisher = {{Public Library of Science}},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1006508},
  url = {https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1006508},
  urldate = {2022-11-11},
  langid = {english},
  keywords = {Culture,Data management,Institutional funding of science,Language,Metadata,Open data,Publication ethics,Research ethics},
  file = {/home/skynet3/Zotero/storage/X9FAR96Y/Frassl et al. - 2018 - Ten simple rules for collaboratively writing a mul.pdf;/home/skynet3/Zotero/storage/2SCT35VJ/article.html}
}

@misc{freiBenignOverfittingLinearity2022,
  title = {Benign {{Overfitting}} without {{Linearity}}: {{Neural Network Classifiers Trained}} by {{Gradient Descent}} for {{Noisy Linear Data}}},
  shorttitle = {Benign {{Overfitting}} without {{Linearity}}},
  author = {Frei, Spencer and Chatterji, Niladri S. and Bartlett, Peter L.},
  date = {2022-09-20},
  number = {arXiv:2202.05928},
  eprint = {2202.05928},
  eprinttype = {arxiv},
  primaryclass = {cs, math, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2202.05928},
  url = {http://arxiv.org/abs/2202.05928},
  urldate = {2022-11-11},
  abstract = {Benign overfitting, the phenomenon where interpolating models generalize well in the presence of noisy data, was first observed in neural network models trained with gradient descent. To better understand this empirical observation, we consider the generalization error of two-layer neural networks trained to interpolation by gradient descent on the logistic loss following random initialization. We assume the data comes from well-separated class-conditional log-concave distributions and allow for a constant fraction of the training labels to be corrupted by an adversary. We show that in this setting, neural networks exhibit benign overfitting: they can be driven to zero training error, perfectly fitting any noisy training labels, and simultaneously achieve minimax optimal test error. In contrast to previous work on benign overfitting that require linear or kernel-based predictors, our analysis holds in a setting where both the model and learning dynamics are fundamentally nonlinear.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Mathematics - Statistics Theory,Statistics - Machine Learning},
  file = {/home/skynet3/Zotero/storage/47VRU3YN/Frei et al. - 2022 - Benign Overfitting without Linearity Neural Netwo.pdf;/home/skynet3/Zotero/storage/YAVGENT6/2202.html}
}

@article{frickerAssessingStatisticalAnalyses2019,
  ids = {frickerAssessingStatisticalAnalyses2019a},
  title = {Assessing the {{Statistical Analyses Used}} in {{Basic}} and {{Applied Social Psychology After Their}} P-{{Value Ban}}},
  author = {Fricker, Ronald D. and Burke, Katherine and Han, Xiaoyan and Woodall, William H.},
  date = {2019-03-29},
  journaltitle = {The American Statistician},
  volume = {73},
  pages = {374--384},
  publisher = {{Taylor \& Francis}},
  issn = {0003-1305},
  doi = {10.1080/00031305.2018.1537892},
  url = {https://doi.org/10.1080/00031305.2018.1537892},
  urldate = {2022-11-11},
  abstract = {In this article, we assess the 31 articles published in Basic and Applied Social Psychology (BASP) in 2016, which is one full year after the BASP editors banned the use of inferential statistics. We discuss how the authors collected their data, how they reported and summarized their data, and how they used their data to reach conclusions. We found multiple instances of authors overstating conclusions beyond what the data would support if statistical significance had been considered. Readers would be largely unable to recognize this because the necessary information to do so was not readily available.},
  issue = {sup1},
  keywords = {Effect size,Inference ban,NHST,Psychology,Statistical significance},
  annotation = {\_eprint: https://doi.org/10.1080/00031305.2018.1537892},
  file = {/home/skynet3/Zotero/storage/2225K7KD/Fricker et al. - 2019 - Assessing the Statistical Analyses Used in Basic a.pdf}
}

@misc{fryerShapleyValuesFeature2021,
  ids = {fryerShapleyValuesFeature2021a},
  title = {Shapley Values for Feature Selection: {{The}} Good, the Bad, and the Axioms},
  shorttitle = {Shapley Values for Feature Selection},
  author = {Fryer, Daniel and Strümke, Inga and Nguyen, Hien},
  date = {2021-02-22},
  number = {arXiv:2102.10936},
  eprint = {2102.10936},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2102.10936},
  url = {http://arxiv.org/abs/2102.10936},
  urldate = {2022-11-11},
  abstract = {The Shapley value has become popular in the Explainable AI (XAI) literature, thanks, to a large extent, to a solid theoretical foundation, including four "favourable and fair" axioms for attribution in transferable utility games. The Shapley value is provably the only solution concept satisfying these axioms. In this paper, we introduce the Shapley value and draw attention to its recent uses as a feature selection tool. We call into question this use of the Shapley value, using simple, abstract "toy" counterexamples to illustrate that the axioms may work against the goals of feature selection. From this, we develop a number of insights that are then investigated in concrete simulation settings, with a variety of Shapley value formulations, including SHapley Additive exPlanations (SHAP) and Shapley Additive Global importancE (SAGE).},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/skynet3/Zotero/storage/KMZXNS9P/Fryer et al. - 2021 - Shapley values for feature selection The good, th.pdf;/home/skynet3/Zotero/storage/LPB929IQ/2102.html}
}

@article{fuEstimatingMisclassificationError2005,
  ids = {fuEstimatingMisclassificationError2005a},
  title = {Estimating Misclassification Error with Small Samples via Bootstrap Cross-Validation},
  author = {Fu, Wenjiang J. and Carroll, Raymond J. and Wang, Suojin},
  date = {2005-05-01},
  journaltitle = {Bioinformatics},
  shortjournal = {Bioinformatics},
  volume = {21},
  number = {9},
  pages = {1979--1986},
  issn = {1367-4803},
  doi = {10.1093/bioinformatics/bti294},
  url = {https://doi.org/10.1093/bioinformatics/bti294},
  urldate = {2022-11-11},
  abstract = {Motivation: Estimation of misclassification error has received increasing attention in clinical diagnosis and bioinformatics studies, especially in small sample studies with microarray data. Current error estimation methods are not satisfactory because they either have large variability (such as leave-one-out cross-validation) or large bias (such as resubstitution and leave-one-out bootstrap). While small sample size remains one of the key features of costly clinical investigations or of microarray studies that have limited resources in funding, time and tissue materials, accurate and easy-to-implement error estimation methods for small samples are desirable and will be beneficial.Results: A bootstrap cross-validation method is studied. It achieves accurate error estimation through a simple procedure with bootstrap resampling and only costs computer CPU time. Simulation studies and applications to microarray data demonstrate that it performs consistently better than its competitors. This method possesses several attractive properties: (1) it is implemented through a simple procedure; (2) it performs well for small samples with sample size, as small as 16; (3) it is not restricted to any particular classification rules and thus applies to many parametric or non-parametric methods.Contact:wfu@stat.tamu.edu},
  file = {/home/skynet3/Zotero/storage/MC2I83HD/Fu et al. - 2005 - Estimating misclassification error with small samp.pdf;/home/skynet3/Zotero/storage/GUC5BPE8/409121.html}
}

@article{gabrielCausalBoundsOutcomedependent2022,
  ids = {gabrielCausalBoundsOutcomedependent2022a},
  title = {Causal Bounds for Outcome-Dependent Sampling in Observational Studies},
  author = {Gabriel, Erin E. and Sachs, Michael C. and Sjölander, Arvid},
  date = {2022-04-03},
  journaltitle = {Journal of the American Statistical Association},
  shortjournal = {Journal of the American Statistical Association},
  volume = {117},
  number = {538},
  eprint = {2002.10519},
  eprinttype = {arxiv},
  primaryclass = {math, stat},
  pages = {939--950},
  issn = {0162-1459, 1537-274X},
  doi = {10.1080/01621459.2020.1832502},
  url = {http://arxiv.org/abs/2002.10519},
  urldate = {2022-11-11},
  abstract = {Outcome-dependent sampling designs are common in many different scientific fields including epidemiology, ecology, and economics. As with all observational studies, such designs often suffer from unmeasured confounding, which generally precludes the nonparametric identification of causal effects. Nonparametric bounds can provide a way to narrow the range of possible values for a nonidentifiable causal effect without making additional untestable assumptions. The nonparametric bounds literature has almost exclusively focused on settings with random sampling, and the bounds have often been derived with a particular linear programming method. We derive novel bounds for the causal risk difference, often referred to as the average treatment effect, in six settings with outcome-dependent sampling and unmeasured confounding for a binary outcome and exposure. Our derivations of the bounds illustrate two approaches that may be applicable in other settings where the bounding problem cannot be directly stated as a system of linear constraints. We illustrate our derived bounds in a real data example involving the effect of vitamin D concentration on mortality.},
  archiveprefix = {arXiv},
  keywords = {Mathematics - Statistics Theory,Statistics - Methodology},
  file = {/home/skynet3/Zotero/storage/8SQZLAE5/Gabriel et al. - 2022 - Causal bounds for outcome-dependent sampling in ob.pdf;/home/skynet3/Zotero/storage/E4FUA2RJ/2002.html}
}

@online{GaiBoKeYiGuoQi,
  title = {该博客已过期},
  url = {http://spark.leanote.com/post/2f0b4e93097c},
  urldate = {2022-11-11}
}

@online{GAMPredictiveModeling,
  title = {{{GAM}}: {{The Predictive Modeling Silver Bullet}} | {{Stitch Fix Technology}} – {{Multithreaded}}},
  shorttitle = {{{GAM}}},
  url = {https://multithreaded.stitchfix.com/blog/2015/07/30/gam/},
  urldate = {2022-11-11},
  abstract = {Imagine that you step into a room of data scientists; the dress code is casual and the scent of strong coffee is hanging in the air. You ask the data scienti...},
  file = {/home/skynet3/Zotero/storage/HBI6TLSC/gam.html}
}

@online{GANLabPlay,
  title = {{{GAN Lab}}: {{Play}} with {{Generative Adversarial Networks}} in {{Your Browser}}!},
  url = {https://poloclub.github.io/ganlab/},
  urldate = {2022-11-11},
  file = {/home/skynet3/Zotero/storage/JRJ93NBR/ganlab.html}
}

@article{gaoComputationalSocioeconomics2019,
  title = {Computational {{Socioeconomics}}},
  author = {Gao, Jian and Zhang, Yi-Cheng and Zhou, Tao},
  date = {2019-07},
  journaltitle = {Physics Reports},
  shortjournal = {Physics Reports},
  volume = {817},
  eprint = {1905.06166},
  eprinttype = {arxiv},
  primaryclass = {physics, q-fin},
  pages = {1--104},
  issn = {03701573},
  doi = {10.1016/j.physrep.2019.05.002},
  url = {http://arxiv.org/abs/1905.06166},
  urldate = {2022-11-11},
  abstract = {Uncovering the structure of socioeconomic systems and timely estimation of socioeconomic status are significant for economic development. The understanding of socioeconomic processes provides foundations to quantify global economic development, to map regional industrial structure, and to infer individual socioeconomic status. In this review, we will make a brief manifesto about a new interdisciplinary research field named Computational Socioeconomics, followed by detailed introduction about data resources, computational tools, data-driven methods, theoretical models and novel applications at multiple resolutions, including the quantification of global economic inequality and complexity, the map of regional industrial structure and urban perception, the estimation of individual socioeconomic status and demographic, and the real-time monitoring of emergent events. This review, together with pioneering works we have highlighted, will draw increasing interdisciplinary attentions and induce a methodological shift in future socioeconomic studies.},
  archiveprefix = {arXiv},
  keywords = {Economics - General Economics,Physics - Physics and Society},
  file = {/home/skynet3/Zotero/storage/LKUKNKK6/Gao et al. - 2019 - Computational Socioeconomics.pdf;/home/skynet3/Zotero/storage/V9V7SQCF/1905.html}
}

@online{GaospecialGgVennDiagramGgplot2,
  title = {Gaospecial/{{ggVennDiagram}}: {{A}} 'ggplot2' Implement of {{Venn Diagram}}.},
  shorttitle = {Gaospecial/{{ggVennDiagram}}},
  url = {https://github.com/gaospecial/ggVennDiagram},
  urldate = {2022-11-11},
  abstract = {A 'ggplot2' implement of Venn Diagram. Contribute to gaospecial/ggVennDiagram development by creating an account on GitHub.},
  langid = {english},
  organization = {{GitHub}},
  file = {/home/skynet3/Zotero/storage/W5J5Z555/ggVennDiagram.html}
}

@misc{garreauExplainingExplainerFirst2020,
  title = {Explaining the {{Explainer}}: {{A First Theoretical Analysis}} of {{LIME}}},
  shorttitle = {Explaining the {{Explainer}}},
  author = {Garreau, Damien and von Luxburg, Ulrike},
  options = {useprefix=true},
  date = {2020-01-13},
  number = {arXiv:2001.03447},
  eprint = {2001.03447},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2001.03447},
  url = {http://arxiv.org/abs/2001.03447},
  urldate = {2022-11-11},
  abstract = {Machine learning is used more and more often for sensitive applications, sometimes replacing humans in critical decision-making processes. As such, interpretability of these algorithms is a pressing need. One popular algorithm to provide interpretability is LIME (Local Interpretable Model-Agnostic Explanation). In this paper, we provide the first theoretical analysis of LIME. We derive closed-form expressions for the coefficients of the interpretable model when the function to explain is linear. The good news is that these coefficients are proportional to the gradient of the function to explain: LIME indeed discovers meaningful features. However, our analysis also reveals that poor choices of parameters can lead LIME to miss important features.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/skynet3/Zotero/storage/EDCJSV5A/Garreau and von Luxburg - 2020 - Explaining the Explainer A First Theoretical Anal.pdf;/home/skynet3/Zotero/storage/P9WR9BNR/2001.html}
}

@online{GaussianDistributionsAre2017,
  title = {Gaussian {{Distributions}} Are {{Soap Bubbles}}},
  date = {2017-11-09T14:57:57},
  url = {https://www.inference.vc/high-dimensional-gaussian-distributions-are-soap-bubble/},
  urldate = {2022-11-11},
  abstract = {This post is just a quick note on some of the pitfalls we encounter when dealing with high-dimensional problems, even when working with something as simple as a Gaussian distribution.  Last week I wrote about mixup a new data-augmentation scheme that achieves good performance in a few domains. I illustrated...},
  langid = {english},
  organization = {{inFERENCe}},
  file = {/home/skynet3/Zotero/storage/JSIEGWJQ/high-dimensional-gaussian-distributions-are-soap-bubble.html}
}

@article{gelmanBayesianAnalysisTests,
  title = {Bayesian Analysis of Tests with Unknown Specificity and Sensitivity},
  author = {Gelman, Andrew and Carpenter, Bob},
  pages = {18},
  abstract = {When testing for a rare disease, prevalence estimates can be highly sensitive to uncertainty in the specificity and sensitivity of the test. Bayesian inference is a natural way to propagate these uncertainties, with hierarchical modeling capturing variation in these parameters across experiments. Another concern is the people in the sample not being representative of the general population. Statistical adjustment cannot without strong assumptions correct for selection bias in an opt-in sample, but multilevel regression and poststratification can at least adjust for known differences between the sample and the population. We demonstrate hierarchical regression and poststratification models with code in Stan and discuss their application to a controversial recent study of SARS-CoV-2 antibodies in a sample of people from the Stanford University area. Wide posterior intervals make it impossible to evaluate the quantitative claims of that study regarding the number of unreported infections. For future studies, the methods described here should facilitate more accurate estimates of disease prevalence from imperfect tests performed on non-representative samples.},
  langid = {english},
  file = {/home/skynet3/Zotero/storage/XR7FMVL6/Gelman and Carpenter - Bayesian analysis of tests with unknown specificit.pdf}
}

@misc{gelmanBayesianAnalysisTests2020,
  title = {Bayesian Analysis of Tests with Unknown Specificity and Sensitivity},
  author = {Gelman, Andrew and Carpenter, Bob},
  date = {2020-07-02},
  pages = {2020.05.22.20108944},
  publisher = {{medRxiv}},
  doi = {10.1101/2020.05.22.20108944},
  url = {https://www.medrxiv.org/content/10.1101/2020.05.22.20108944v3},
  urldate = {2022-11-11},
  abstract = {When testing for a rare disease, prevalence estimates can be highly sensitive to uncertainty in the specificity and sensitivity of the test. Bayesian inference is a natural way to propagate these uncertainties, with hierarchical modeling capturing variation in these parameters across experiments. Another concern is the people in the sample not being representative of the general population. Statistical adjustment cannot with- out strong assumptions correct for selection bias in an opt-in sample, but multilevel regression and poststratification can at least adjust for known differences between the sample and the population. We demonstrate hierarchical regression and poststratification models with code in Stan and discuss their application to a controversial recent study of SARS-CoV-2 antibodies in a sample of people from the Stanford University area. Wide posterior intervals make it impossible to evaluate the quantitative claims of that study regarding the number of unreported infections. For future studies, the methods described here should facilitate more accurate estimates of disease prevalence from imperfect tests performed on non-representative samples.},
  langid = {english},
  file = {/home/skynet3/Zotero/storage/69XLXRCB/Gelman and Carpenter - 2020 - Bayesian analysis of tests with unknown specificit.pdf;/home/skynet3/Zotero/storage/FL3FIUVM/2020.05.22.html}
}

@article{gelmanBayesianAnalysisTestsa,
  title = {Bayesian Analysis of Tests with Unknown Specificity and Sensitivity},
  author = {Gelman, Andrew and Carpenter, Bob},
  pages = {18},
  abstract = {When testing for a rare disease, prevalence estimates can be highly sensitive to uncertainty in the specificity and sensitivity of the test. Bayesian inference is a natural way to propagate these uncertainties, with hierarchical modeling capturing variation in these parameters across experiments. Another concern is the people in the sample not being representative of the general population. Statistical adjustment cannot without strong assumptions correct for selection bias in an opt-in sample, but multilevel regression and poststratification can at least adjust for known differences between the sample and the population. We demonstrate hierarchical regression and poststratification models with code in Stan and discuss their application to a controversial recent study of SARS-CoV-2 antibodies in a sample of people from the Stanford University area. Wide posterior intervals make it impossible to evaluate the quantitative claims of that study regarding the number of unreported infections. For future studies, the methods described here should facilitate more accurate estimates of disease prevalence from imperfect tests performed on non-representative samples.},
  langid = {english},
  file = {/home/skynet3/Zotero/storage/A8PTYE2B/Gelman and Carpenter - Bayesian analysis of tests with unknown specificit.pdf}
}

@misc{gelmanBayesianWorkflow2020,
  ids = {gelmanBayesianWorkflow2020a},
  title = {Bayesian {{Workflow}}},
  author = {Gelman, Andrew and Vehtari, Aki and Simpson, Daniel and Margossian, Charles C. and Carpenter, Bob and Yao, Yuling and Kennedy, Lauren and Gabry, Jonah and Bürkner, Paul-Christian and Modrák, Martin},
  date = {2020-11-03},
  number = {arXiv:2011.01808},
  eprint = {2011.01808},
  eprinttype = {arxiv},
  primaryclass = {stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2011.01808},
  url = {http://arxiv.org/abs/2011.01808},
  urldate = {2022-11-11},
  abstract = {The Bayesian approach to data analysis provides a powerful way to handle uncertainty in all observations, model parameters, and model structure using probability theory. Probabilistic programming languages make it easier to specify and fit Bayesian models, but this still leaves us with many options regarding constructing, evaluating, and using these models, along with many remaining challenges in computation. Using Bayesian inference to solve real-world problems requires not only statistical skills, subject matter knowledge, and programming, but also awareness of the decisions made in the process of data analysis. All of these aspects can be understood as part of a tangled workflow of applied Bayesian statistics. Beyond inference, the workflow also includes iterative model building, model checking, validation and troubleshooting of computational problems, model understanding, and model comparison. We review all these aspects of workflow in the context of several examples, keeping in mind that in practice we will be fitting many models for any given problem, even if only a subset of them will ultimately be relevant for our conclusions.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Methodology},
  file = {/home/skynet3/Zotero/storage/LRI393EA/Gelman et al. - 2020 - Bayesian Workflow.pdf;/home/skynet3/Zotero/storage/R2UXGS8A/2011.html}
}

@article{gelmanGardenForkingPaths2013,
  title = {The Garden of Forking Paths: {{Why}} Multiple Comparisons Can Be a Problem, Even When There Is No “Fishing Expedition” or “p-Hacking” and the Research Hypothesis Was Posited Ahead of Time},
  shorttitle = {The Garden of Forking Paths},
  author = {Gelman, Andrew and Loken, Eric},
  date = {2013},
  journaltitle = {Department of Statistics, Columbia University},
  volume = {348},
  pages = {1--17},
  file = {/home/skynet3/Zotero/storage/B7TMEZ3X/Gelman and Loken - 2013 - The garden of forking paths Why multiple comparis.pdf}
}

@article{gelmanPowerCalculationsAssessing2014,
  ids = {gelmanPowerCalculationsAssessing2014a},
  title = {Beyond {{Power Calculations}}: {{Assessing Type S}} ({{Sign}}) and {{Type M}} ({{Magnitude}}) {{Errors}}},
  shorttitle = {Beyond {{Power Calculations}}},
  author = {Gelman, Andrew and Carlin, John},
  date = {2014-11},
  journaltitle = {Perspectives on Psychological Science: A Journal of the Association for Psychological Science},
  shortjournal = {Perspect Psychol Sci},
  volume = {9},
  number = {6},
  eprint = {26186114},
  eprinttype = {pmid},
  pages = {641--651},
  issn = {1745-6924},
  doi = {10.1177/1745691614551642},
  abstract = {Statistical power analysis provides the conventional approach to assess error rates when designing a research study. However, power analysis is flawed in that a narrow emphasis on statistical significance is placed as the primary focus of study design. In noisy, small-sample settings, statistically significant results can often be misleading. To help researchers address this problem in the context of their own studies, we recommend design calculations in which (a) the probability of an estimate being in the wrong direction (Type S [sign] error) and (b) the factor by which the magnitude of an effect might be overestimated (Type M [magnitude] error or exaggeration ratio) are estimated. We illustrate with examples from recent published research and discuss the largest challenge in a design calculation: coming up with reasonable estimates of plausible effect sizes based on external information.},
  langid = {english},
  keywords = {Beauty,Data Interpretation; Statistical,design calculation,exaggeration ratio,Female,Humans,Male,Menstrual Cycle,Politics,power analysis,Psychology,replication crisis,Research Design,Sex Ratio,statistical significance,Type M error,Type S error}
}

@book{gelmanRegressionOtherStories2020,
  title = {Regression and {{Other Stories}}},
  author = {Gelman, Andrew and Hill, Jennifer and Vehtari, Aki},
  date = {2020-07-23},
  edition = {1},
  publisher = {{Cambridge University Press}},
  doi = {10.1017/9781139161879},
  url = {https://www.cambridge.org/highereducation/product/9781139161879/book},
  urldate = {2022-11-12},
  abstract = {Most textbooks on regression focus on theory and the simplest of examples. Real statistical problems, however, are complex and subtle. This is not a book about the theory of regression. It is about using regression to solve real problems of comparison, estimation, prediction, and causal inference. Unlike other books, it focuses on practical issues such as sample size and missing data and a wide range of goals and techniques. It jumps right in to methods and computer code you can use immediately. Real examples, real stories from the authors' experience demonstrate what regression can do and its limitations, with practical advice for understanding assumptions and implementing methods for experiments and observational studies. They make a smooth transition to logistic regression and GLM. The emphasis is on computation in R and Stan rather than derivations, with code available online. Graphics and presentation aid understanding of the models and model fitting.},
  isbn = {978-1-139-16187-9 978-1-107-02398-7 978-1-107-67651-0}
}

@misc{gelmanWhatAreMost2021,
  title = {What Are the Most Important Statistical Ideas of the Past 50 Years?},
  author = {Gelman, Andrew and Vehtari, Aki},
  date = {2021-06-03},
  number = {arXiv:2012.00174},
  eprint = {2012.00174},
  eprinttype = {arxiv},
  primaryclass = {stat},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2012.00174},
  urldate = {2022-11-11},
  abstract = {We review the most important statistical ideas of the past half century, which we categorize as: counterfactual causal inference, bootstrapping and simulation-based inference, overparameterized models and regularization, Bayesian multilevel models, generic computation algorithms, adaptive decision analysis, robust inference, and exploratory data analysis. We discuss key contributions in these subfields, how they relate to modern computing and big data, and how they might be developed and extended in future decades. The goal of this article is to provoke thought and discussion regarding the larger themes of research in statistics and data science.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Methodology},
  file = {/home/skynet3/Zotero/storage/HDQQM8QW/Gelman and Vehtari - 2021 - What are the most important statistical ideas of t.pdf;/home/skynet3/Zotero/storage/78IDM7G2/2012.html}
}

@article{gelmanWhyHighOrderPolynomials2019,
  ids = {gelmanWhyHighOrderPolynomials2019a},
  title = {Why {{High-Order Polynomials Should Not Be Used}} in {{Regression Discontinuity Designs}}},
  author = {Gelman, Andrew and Imbens, Guido},
  date = {2019-07-03},
  journaltitle = {Journal of Business \& Economic Statistics},
  volume = {37},
  number = {3},
  pages = {447--456},
  publisher = {{Taylor \& Francis}},
  issn = {0735-0015},
  doi = {10.1080/07350015.2017.1366909},
  url = {https://doi.org/10.1080/07350015.2017.1366909},
  urldate = {2022-11-11},
  abstract = {It is common in regression discontinuity analysis to control for third, fourth, or higher-degree polynomials of the forcing variable. There appears to be a perception that such methods are theoretically justified, even though they can lead to evidently nonsensical results. We argue that controlling for global high-order polynomials in regression discontinuity analysis is a flawed approach with three major problems: it leads to noisy estimates, sensitivity to the degree of the polynomial, and poor coverage of confidence intervals. We recommend researchers instead use estimators based on local linear or quadratic polynomials or other smooth functions.},
  keywords = {Causal identification,Policy analysis,Polynomial regression,Regression discontinuity,Uncertainty.},
  annotation = {\_eprint: https://doi.org/10.1080/07350015.2017.1366909}
}

@online{GeneralizedPrincipalComponent2019,
  title = {Generalized {{Principal Component Analysis}}},
  date = {2019-07-03T04:18:31+00:00},
  url = {https://deepai.org/publication/generalized-principal-component-analysis},
  urldate = {2022-11-11},
  abstract = {07/03/19 - Generalized principal component analysis (GLM-PCA) facilitates dimension reduction of non-normally distributed data. We provide a ...},
  organization = {{DeepAI}},
  file = {/home/skynet3/Zotero/storage/AXMG6BFA/generalized-principal-component-analysis.html}
}

@online{GenieclustFastRobust,
  title = {Genieclust: {{Fast}} and {{Robust Hierarchical Clustering}} with {{Noise Point Detection}} — {{Python}} and {{R Package}} Genieclust},
  url = {https://genieclust.gagolewski.com/},
  urldate = {2022-11-11}
}

@online{GeometricIntuitionTraining2019,
  title = {Geometric {{Intuition}} for {{Training Neural Networks}}},
  date = {2019-11-25T19:49:22+00:00},
  url = {https://sea-adl.org/2019/11/25/geometric-intuition-for-training-neural-networks/},
  urldate = {2022-11-11},
  abstract = {Leo Dirac explains what it looks like to train a neural network through geometric intuition.},
  langid = {english},
  organization = {{Seattle Applied Deep Learning}},
  file = {/home/skynet3/Zotero/storage/H9VZD75B/geometric-intuition-for-training-neural-networks.html}
}

@article{gerbingMeaningWithinFactorCorrelated1984,
  ids = {gerbingMeaningWithinFactorCorrelated1984a},
  title = {On the {{Meaning}} of {{Within-Factor Correlated Measurement Errors}}},
  author = {Gerbing, David W. and Anderson, James C.},
  date = {1984-06-01},
  journaltitle = {Journal of Consumer Research},
  shortjournal = {Journal of Consumer Research},
  volume = {11},
  number = {1},
  pages = {572--580},
  issn = {0093-5301},
  doi = {10.1086/208993},
  url = {https://doi.org/10.1086/208993},
  urldate = {2022-11-11},
  abstract = {The meaning of correlated measurement errors is discussed within a hierarchical framework of error terms provided by true score, first-order factor, and second-order factor models: random error, indicator specific error, and group specific error, respectively. Group specific error can be represented either as extraneous first-order factors or as unwanted components of first-order factors that define a second-order factor. The uncritical use of correlated measurement errors without theoretical justification is shown to lead merely to more acceptable fit while obfuscating a more meaningful theoretical structure.},
  file = {/home/skynet3/Zotero/storage/XRLCTLVY/1822756.html}
}

@misc{gerhardusHighrecallCausalDiscovery2021,
  title = {High-Recall Causal Discovery for Autocorrelated Time Series with Latent Confounders},
  author = {Gerhardus, Andreas and Runge, Jakob},
  date = {2021-02-01},
  number = {arXiv:2007.01884},
  eprint = {2007.01884},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2007.01884},
  url = {http://arxiv.org/abs/2007.01884},
  urldate = {2022-11-11},
  abstract = {We present a new method for linear and nonlinear, lagged and contemporaneous constraint-based causal discovery from observational time series in the presence of latent confounders. We show that existing causal discovery methods such as FCI and variants suffer from low recall in the autocorrelated time series case and identify low effect size of conditional independence tests as the main reason. Information-theoretical arguments show that effect size can often be increased if causal parents are included in the conditioning sets. To identify parents early on, we suggest an iterative procedure that utilizes novel orientation rules to determine ancestral relationships already during the edge removal phase. We prove that the method is order-independent, and sound and complete in the oracle case. Extensive simulation studies for different numbers of variables, time lags, sample sizes, and further cases demonstrate that our method indeed achieves much higher recall than existing methods for the case of autocorrelated continuous variables while keeping false positives at the desired level. This performance gain grows with stronger autocorrelation. At https://github.com/jakobrunge/tigramite we provide Python code for all methods involved in the simulation studies.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning,Statistics - Methodology},
  file = {/home/skynet3/Zotero/storage/UKV88XDJ/Gerhardus and Runge - 2021 - High-recall causal discovery for autocorrelated ti.pdf;/home/skynet3/Zotero/storage/GS6CI88F/2007.html}
}

@article{gertlerHowMakeReplication2018,
  title = {How to Make Replication the Norm},
  author = {Gertler, Paul and Galiani, Sebastian and Romero, Mauricio},
  date = {2018-02},
  journaltitle = {Nature},
  volume = {554},
  number = {7693},
  pages = {417--419},
  publisher = {{Nature Publishing Group}},
  doi = {10.1038/d41586-018-02108-9},
  url = {https://www.nature.com/articles/d41586-018-02108-9},
  urldate = {2022-11-11},
  abstract = {The publishing system builds in resistance to replication. Paul Gertler, Sebastian Galiani and Mauricio Romero surveyed economics journals to find out how to fix it.},
  issue = {7693},
  langid = {english},
  keywords = {Publishing,Research data,Research management},
  annotation = {Bandiera\_abtest: a Cg\_type: Comment Subject\_term: Research data, Research management, Publishing},
  file = {/home/skynet3/Zotero/storage/3SEXN6BU/Gertler et al. - 2018 - How to make replication the norm.pdf}
}

@online{gesmannFittingDistributions2011,
  title = {Fitting Distributions with {{R}}},
  author = {Gesmann, Markus},
  date = {2011-12-01T17:40:00+00:00},
  url = {https://www.magesblog.com/post/2011-12-01-fitting-distributions-with-r/},
  urldate = {2022-11-11},
  abstract = {Fitting distribution with R is something I have to do once in a while, but where do I start? A good starting point to learn more about distribution fitting with R is Vito Ricci’s tutorial on CRAN. I also find the vignettes~of the actuar~and~fitdistrplus package a good read. I haven’t looked into the recently published Handbook of fitting statistical distributions with R, by Z. Karian and E.},
  langid = {british},
  organization = {{mages' blog}}
}

@online{GgdistCheatSheetslabinterval,
  title = {Ggdist/Cheat\_sheet-Slabinterval.Pdf at Master · Mjskay/Ggdist},
  url = {https://github.com/mjskay/ggdist},
  urldate = {2022-11-11},
  abstract = {Visualizations of distributions and uncertainty. Contribute to mjskay/ggdist development by creating an account on GitHub.},
  langid = {english},
  organization = {{GitHub}},
  file = {/home/skynet3/Zotero/storage/JXIXKJ47/cheat_sheet-slabinterval.html}
}

@misc{ghojoghTheoryOverfittingCross2019,
  title = {The {{Theory Behind Overfitting}}, {{Cross Validation}}, {{Regularization}}, {{Bagging}}, and {{Boosting}}: {{Tutorial}}},
  shorttitle = {The {{Theory Behind Overfitting}}, {{Cross Validation}}, {{Regularization}}, {{Bagging}}, and {{Boosting}}},
  author = {Ghojogh, Benyamin and Crowley, Mark},
  date = {2019-05-28},
  number = {arXiv:1905.12787},
  eprint = {1905.12787},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1905.12787},
  url = {http://arxiv.org/abs/1905.12787},
  urldate = {2022-11-11},
  abstract = {In this tutorial paper, we first define mean squared error, variance, covariance, and bias of both random variables and classification/predictor models. Then, we formulate the true and generalization errors of the model for both training and validation/test instances where we make use of the Stein's Unbiased Risk Estimator (SURE). We define overfitting, underfitting, and generalization using the obtained true and generalization errors. We introduce cross validation and two well-known examples which are \$K\$-fold and leave-one-out cross validations. We briefly introduce generalized cross validation and then move on to regularization where we use the SURE again. We work on both \$\textbackslash ell\_2\$ and \$\textbackslash ell\_1\$ norm regularizations. Then, we show that bootstrap aggregating (bagging) reduces the variance of estimation. Boosting, specifically AdaBoost, is introduced and it is explained as both an additive model and a maximum margin model, i.e., Support Vector Machine (SVM). The upper bound on the generalization error of boosting is also provided to show why boosting prevents from overfitting. As examples of regularization, the theory of ridge and lasso regressions, weight decay, noise injection to input/weights, and early stopping are explained. Random forest, dropout, histogram of oriented gradients, and single shot multi-box detector are explained as examples of bagging in machine learning and computer vision. Finally, boosting tree and SVM models are mentioned as examples of boosting.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/skynet3/Zotero/storage/SKTGJA9E/Ghojogh and Crowley - 2019 - The Theory Behind Overfitting, Cross Validation, R.pdf;/home/skynet3/Zotero/storage/LIMATIQZ/1905.html}
}

@article{ghoshBayesianEstimationCorrelation2021,
  title = {Bayesian {{Estimation}} of {{Correlation Matrices}} of {{Longitudinal Data}}},
  author = {Ghosh, Riddhi Pratim and Mallick, Bani and Pourahmadi, Mohsen},
  date = {2021-09},
  journaltitle = {Bayesian Analysis},
  volume = {16},
  number = {3},
  pages = {1039--1058},
  publisher = {{International Society for Bayesian Analysis}},
  issn = {1936-0975, 1931-6690},
  doi = {10.1214/20-BA1237},
  url = {https://projecteuclid.org/journals/bayesian-analysis/volume-16/issue-3/Bayesian-Estimation-of-Correlation-Matrices-of-Longitudinal-Data/10.1214/20-BA1237.full},
  urldate = {2022-11-11},
  abstract = {Estimation of correlation matrices is a challenging problem due to the notorious positive-definiteness constraint and high-dimensionality. Reparameterizing Cholesky factors of correlation matrices in terms of angles or hyperspherical coordinates where the angles vary freely in the range [0,π) has become popular in the last two decades. However, it has not been used in Bayesian estimation of correlation matrices perhaps due to lack of clear statistical relevance and suitable priors for the angles. In this paper, we show for the first time that for longitudinal data these angles are the inverse cosine of the semi-partial correlations (SPCs). This simple connection makes it possible to introduce physically meaningful selection and shrinkage priors on the angles or correlation matrices with emphasis on selection (sparsity) and shrinking towards longitudinal structure. Our method deals effectively with the positive-definiteness constraint in posterior computation. We compare the performance of our Bayesian estimation based on angles with some recent methods based on partial autocorrelations through simulation and apply the method to a data related to clinical trial on smoking.},
  keywords = {angular parameterization,Cholesky decomposition,longitudinal data,selection,shrinkage,structured correlation matrix},
  file = {/home/skynet3/Zotero/storage/54RAFGRX/Ghosh et al. - 2021 - Bayesian Estimation of Correlation Matrices of Lon.pdf}
}

@misc{giffinInstrumentalVariablesSpatial2021,
  title = {Instrumental Variables, Spatial Confounding and Interference},
  author = {Giffin, Andrew and Reich, Brian J. and Yang, Shu and Rappold, Ana G.},
  date = {2021-02-27},
  number = {arXiv:2103.00304},
  eprint = {2103.00304},
  eprinttype = {arxiv},
  primaryclass = {stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2103.00304},
  url = {http://arxiv.org/abs/2103.00304},
  urldate = {2022-11-11},
  abstract = {Unobserved spatial confounding variables are prevalent in environmental and ecological applications where the system under study is complex and the data are often observational. Instrumental variables (IVs) are a common way to address unobserved confounding; however, the efficacy of using IVs on spatial confounding is largely unknown. This paper explores the effectiveness of IVs in this situation -- with particular attention paid to the spatial scale of the instrument. We show that, in case of spatially-dependent treatments, IVs are most effective when they vary at a finer spatial resolution than the treatment. We investigate IV performance in extensive simulations and apply the model in the example of long term trends in the air pollution and cardiovascular mortality in the United States over 1990-2010. Finally, the IV approach is also extended to the spatial interference setting, in which treatments can affect nearby responses.},
  archiveprefix = {arXiv},
  keywords = {62-02,Statistics - Methodology},
  file = {/home/skynet3/Zotero/storage/4TPW3NH8/Giffin et al. - 2021 - Instrumental variables, spatial confounding and in.pdf;/home/skynet3/Zotero/storage/38EUBSWG/2103.html}
}

@article{gignacDunningKrugerEffectMostly2020,
  ids = {gignacDunningKrugerEffectMostly2020a},
  title = {The {{Dunning-Kruger}} Effect Is (Mostly) a Statistical Artefact: {{Valid}} Approaches to Testing the Hypothesis with Individual Differences Data},
  shorttitle = {The {{Dunning-Kruger}} Effect Is (Mostly) a Statistical Artefact},
  author = {Gignac, Gilles E. and Zajenkowski, Marcin},
  date = {2020-05-01},
  journaltitle = {Intelligence},
  shortjournal = {Intelligence},
  volume = {80},
  pages = {101449},
  issn = {0160-2896},
  doi = {10.1016/j.intell.2020.101449},
  url = {https://www.sciencedirect.com/science/article/pii/S0160289620300271},
  urldate = {2022-11-11},
  abstract = {The Dunning-Kruger hypothesis states that the degree to which people can estimate their ability accurately depends, in part, upon possessing the ability in question. Consequently, people with lower levels of the ability tend to self-assess their ability less well than people who have relatively higher levels of the ability. The most common method used to test the Dunning-Kruger hypothesis involves plotting the self-assessed and objectively assessed means across four categories (quartiles) of objective ability. However, this method has been argued to be confounded by the better-than-average effect and regression toward the mean. In this investigation, it is argued that the Dunning-Kruger hypothesis can be tested validly with two inferential statistical techniques: the Glejser test of heteroscedasticity and nonlinear (quadratic) regression. On the basis of a sample of 929 general community participants who completed a self-assessment of intelligence and the Advanced Raven's Progressive Matrices, we failed to identify statistically significant heteroscedasticity, contrary to the Dunning-Kruger hypothesis. Additionally, the association between objectively measured intelligence and self-assessed intelligence was found to be essentially entirely linear, again, contrary to the Dunning-Kruger hypothesis. It is concluded that, although the phenomenon described by the Dunning-Kruger hypothesis may be to some degree plausible for some skills, the magnitude of the effect may be much smaller than reported previously.},
  langid = {english},
  keywords = {Dunning-Kruger effect,Intelligence,Self-assessed intelligence},
  file = {/home/skynet3/Zotero/storage/UGD75FDL/S0160289620300271.html}
}

@video{giphyAnimatedGIFFind,
  ids = {giphyAnimatedGIFFinda},
  title = {Animated {{GIF}} - {{Find}} \& {{Share}} on {{GIPHY}}},
  editor = {GIPHY},
  url = {https://media1.giphy.com/media/Vaq10GafGxmflhKISt/giphy.gif?cid=dda24d5020e32858cab674bfaa3d3c2101818feecc5de538&rid=giphy.gif&ct=g},
  urldate = {2022-11-11},
  abstract = {Discover \& share this Animated GIF with everyone you know. GIPHY is how you search, share, discover, and create GIFs.},
  editortype = {director},
  file = {/home/skynet3/Zotero/storage/D6WSN936/Vaq10GafGxmflhKISt.html}
}

@online{GitDocumentation,
  title = {Git - {{Documentation}}},
  url = {https://git-scm.com/doc},
  urldate = {2022-11-11}
}

@misc{globus-harrisAlgorithmicFrameworkBias2022,
  title = {An {{Algorithmic Framework}} for {{Bias Bounties}}},
  author = {Globus-Harris, Ira and Kearns, Michael and Roth, Aaron},
  date = {2022-05-09},
  number = {arXiv:2201.10408},
  eprint = {2201.10408},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2201.10408},
  url = {http://arxiv.org/abs/2201.10408},
  urldate = {2022-11-11},
  abstract = {We propose and analyze an algorithmic framework for "bias bounties": events in which external participants are invited to propose improvements to a trained model, akin to bug bounty events in software and security. Our framework allows participants to submit arbitrary subgroup improvements, which are then algorithmically incorporated into an updated model. Our algorithm has the property that there is no tension between overall and subgroup accuracies, nor between different subgroup accuracies, and it enjoys provable convergence to either the Bayes optimal model or a state in which no further improvements can be found by the participants. We provide formal analyses of our framework, experimental evaluation, and findings from a preliminary bias bounty event.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computers and Society,Computer Science - Data Structures and Algorithms,Computer Science - Machine Learning},
  file = {/home/skynet3/Zotero/storage/4TNUA4LT/Globus-Harris et al. - 2022 - An Algorithmic Framework for Bias Bounties.pdf;/home/skynet3/Zotero/storage/EBPWEFWJ/2201.html}
}

@article{glynnEcologicalInferenceSocial2010,
  title = {Ecological {{Inference}} in the {{Social Sciences}}},
  author = {Glynn, Adam and Wakefield, Jon},
  date = {2010-05-01},
  journaltitle = {Statistical methodology},
  shortjournal = {Stat Methodol},
  volume = {7},
  number = {3},
  eprint = {20563299},
  eprinttype = {pmid},
  pages = {307--322},
  issn = {1572-3127},
  doi = {10.1016/j.stamet.2009.09.003},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2885825/},
  urldate = {2022-11-11},
  abstract = {Ecological inference is a problem of partial identification, and therefore reliable precise conclusions are rarely possible without the collection of individual level (identifying) data. Without such data, sensitivity analyses provide the only recourse. In this paper we review and critique approaches to ecological inference in the social sciences, and describe in detail hierarchical models, which allow both sensitivity analysis and the incorporation of individual level data into an ecological analysis. A crucial element of a sensitivity analysis in such models is prior specification, and we detail how this may be carried out. Furthermore, we demonstrate how the inclusion of a small amount of individual level data can dramatically improve the properties of such estimates.},
  pmcid = {PMC2885825},
  file = {/home/skynet3/Zotero/storage/Z4BQL8DG/Glynn and Wakefield - 2010 - Ecological Inference in the Social Sciences.pdf}
}

@article{gohdesFirstThingsFirst2013,
  ids = {gohdesFirstThingsFirst2013a},
  title = {First {{Things First}}: {{Assessing Data Quality}} before {{Model Quality}}},
  shorttitle = {First {{Things First}}},
  author = {Gohdes, Anita and Price, Megan},
  date = {2013-12-01},
  journaltitle = {Journal of Conflict Resolution},
  volume = {57},
  number = {6},
  pages = {1090--1108},
  publisher = {{SAGE Publications Inc}},
  issn = {0022-0027},
  doi = {10.1177/0022002712459708},
  url = {https://doi.org/10.1177/0022002712459708},
  urldate = {2022-11-11},
  abstract = {We address weaknesses in the Peace Research Insitute Oslo (PRIO) Battle Deaths Dataset, and as a result draw contradicting conclusions to those presented by Lacina and Gleditsch. Our analysis focuses on the availability of data on battle deaths within specific conflict-years and problems encountered when data from multiple types of sources are combined. We repeat Lacina, Gleditsch, and Russett?s analysis of battle deaths over time, with an attempt to provide a more robust model and incorporate an estimate of the uncertainty present in the PRIO Battle Deaths Dataset. This reanalysis reveals that the data used to establish the PRIO Battle Deaths Dataset does not offer a clear answer as to whether battle deaths have decreased or increased since the end of the Second World War. We contend that while the PRIO Battle Deaths Dataset offers the most comprehensive assembly of battle deaths data available to date, it is not suitable for analysis across countries or over time.},
  langid = {english}
}

@book{gohelUsingFlextablePackage,
  title = {Using the Flextable {{R}} Package},
  author = {Gohel, David},
  url = {https://ardata-fr.github.io/flextable-book/},
  urldate = {2022-11-11},
  abstract = {Flextable documentation, an R package for generating reporting tables from R in Word, HTML, PDF and PowerPoint formats.}
}

@online{goldfeldEvenRandomizationMediation2019,
  title = {Even with Randomization, Mediation Analysis Can Still Be Confounded | {{R-bloggers}}},
  author = {Goldfeld, Keith},
  date = {2019-04-16T00:00:00+00:00},
  url = {https://www.r-bloggers.com/2019/04/even-with-randomization-mediation-analysis-can-still-be-confounded/},
  urldate = {2022-11-11},
  abstract = {Randomization is super useful because it usually eliminates the risk that confounding will lead to a biased estimate of a treatment effect. However, this only goes so far. If you are conducting a meditation analysis in the hopes of understanding the underlying causal mechanism of a treatment, it is important to remember that the mediator has not been randomized, only the treatment. This means that the estimated mediation effect is still at risk of being confounded. I never fail to mention this when a researcher tells me they are interested in doing a mediation analysis (and it seems like more and more folks are interested in including this analysis as part of their studies). So, when my son brought up the fact that the lead investigator on his experimental psychology project wanted to include a mediation analysis, I, of course, had to pipe up. “You have to be careful, you know.” But, he wasn’t buying it, wondering why randomization didn’t take care of the confounding; surely, the potential confounders would be balanced across treatment groups. Maybe I’d had a little too much wine, as I considered he might have a point. But no - I’d quickly come to my senses - it doesn’t matter that the confounder is balanced across treatment groups (which it very well could be), it would still be unbalanced across the different levels of the mediator, which is what really matters if we are estimating the effect of the mediator. I proposed to do a simulation of this phenomenon. My son was not impressed, but I went ahead and did it anyways, and I am saving it here in case he wants to take a look. Incidentally, this is effectively a brief follow-up to an earlier post on mediation. So, if the way in which I am generating the data seems a bit opaque, you might want to take a look at what I did earlier. The data generating process Here is a DAG that succinctly describes how I will generate the data. You can see clearly that \textbackslash (U\_2\textbackslash ) is a confounder of the relationship between the mediator \textbackslash (M\textbackslash ) and the outcome \textbackslash (Y\textbackslash ). (It should be noted that if we were only interested in is the causal effect of \textbackslash (A\textbackslash ) on \textbackslash (Y\textbackslash ), \textbackslash (U\_2\textbackslash ) is not a confounder, so we wouldn’t need to control for \textbackslash (U\_2\textbackslash ).) As I did in the earlier simulation of mediation, I am simulating the potential outcomes so that we can see the “truth” that we are trying to measure. defU},
  langid = {american},
  file = {/home/skynet3/Zotero/storage/UCI4FGQU/even-with-randomization-mediation-analysis-can-still-be-confounded.html}
}

@book{gomez-rubioBayesianInferenceINLA,
  title = {Bayesian Inference with {{INLA}}},
  author = {Gómez-Rubio, Virgilio},
  url = {http://becarioprecario.bitbucket.io/inla-gitbook/index.html},
  urldate = {2022-11-11},
  abstract = {This book introduces the integrated nested Laplace approximation (INLA) for Bayesian inference and its associated R package R-INLA.},
  file = {/home/skynet3/Zotero/storage/PGLUQKPI/index.html}
}

@misc{goodman-baconUsingDifferenceinDifferencesIdentify2020,
  type = {SSRN Scholarly Paper},
  title = {Using {{Difference-in-Differences}} to {{Identify Causal Effects}} of {{COVID-19 Policies}}},
  author = {Goodman-Bacon, Andrew and Marcus, Jan},
  date = {2020-05-01},
  number = {3603970},
  location = {{Rochester, NY}},
  doi = {10.2139/ssrn.3603970},
  url = {https://papers.ssrn.com/abstract=3603970},
  urldate = {2022-11-11},
  abstract = {Policymakers have implemented a wide range of non-pharmaceutical interventions to fight the spread of COVID-19. Variation in policies across jurisdictions and over time strongly suggests a difference-in-differences (DD) research design to estimate causal effects of counter-COVID measures. We discuss threats to the validity of these DD designs and make recommendations about how researchers can avoid bias, interpret results accurately, and provide sound guidance to policymakers seeking to protect public health and facilitate an eventual economic recovery.},
  langid = {english},
  keywords = {causal inference,COVID-19,Difference-in-differences,non-pharmaceutical interventions},
  file = {/home/skynet3/Zotero/storage/NTD6LDKP/Goodman-Bacon and Marcus - 2020 - Using Difference-in-Differences to Identify Causal.pdf;/home/skynet3/Zotero/storage/MRVFBCRK/papers.html}
}

@article{gordonPredictingReplicabilityAnalysis2021,
  title = {Predicting Replicability—{{Analysis}} of Survey and Prediction Market Data from Large-Scale Forecasting Projects},
  author = {Gordon, Michael and Viganola, Domenico and Dreber, Anna and Johannesson, Magnus and Pfeiffer, Thomas},
  date = {2021-04-14},
  journaltitle = {PLOS ONE},
  shortjournal = {PLOS ONE},
  volume = {16},
  number = {4},
  pages = {e0248780},
  publisher = {{Public Library of Science}},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0248780},
  url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0248780},
  urldate = {2022-11-11},
  abstract = {The reproducibility of published research has become an important topic in science policy. A number of large-scale replication projects have been conducted to gauge the overall reproducibility in specific academic fields. Here, we present an analysis of data from four studies which sought to forecast the outcomes of replication projects in the social and behavioural sciences, using human experts who participated in prediction markets and answered surveys. Because the number of findings replicated and predicted in each individual study was small, pooling the data offers an opportunity to evaluate hypotheses regarding the performance of prediction markets and surveys at a higher power. In total, peer beliefs were elicited for the replication outcomes of 103 published findings. We find there is information within the scientific community about the replicability of scientific findings, and that both surveys and prediction markets can be used to elicit and aggregate this information. Our results show prediction markets can determine the outcomes of direct replications with 73\% accuracy (n = 103). Both the prediction market prices, and the average survey responses are correlated with outcomes (0.581 and 0.564 respectively, both p {$<$} .001). We also found a significant relationship between p-values of the original findings and replication outcomes. The dataset is made available through the R package “pooledmaRket” and can be used to further study community beliefs towards replications outcomes as elicited in the surveys and prediction markets.},
  langid = {english},
  keywords = {Experimental economics,Experimental psychology,Forecasting,Metaanalysis,Scientific publishing,Social sciences,Survey research,Surveys},
  file = {/home/skynet3/Zotero/storage/H5KULGUN/Gordon et al. - 2021 - Predicting replicability—Analysis of survey and pr.pdf;/home/skynet3/Zotero/storage/FBSNWDDL/article.html}
}

@misc{gorshkovOntologybasedIndustrialData2021,
  ids = {gorshkovOntologybasedIndustrialData2021a},
  title = {Ontology-Based Industrial Data Management Platform},
  author = {Gorshkov, Sergey and Grebeshkov, Alexander and Shebalov, Roman},
  date = {2021-03-09},
  number = {arXiv:2103.05538},
  eprint = {2103.05538},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2103.05538},
  url = {http://arxiv.org/abs/2103.05538},
  urldate = {2022-11-11},
  abstract = {Relational and noSQL storages are developed for the fast processing of the large data sets having a stable structure, while the ontologies are used to rep-resent complex and dynamic sets of information of a limited size. In the in-dustrial applications it is often needed to maintain the large warehouses of data consolidated from various sources. The ontologies are useful to repre-sent the structure of that data, but RDF triple stores are not well suitable for storing it. We offer an approach and a system allowing to use the opportuni-ties of fast storage engines along with the flexibility of ontology-based data management tools, including SPARQL queries. The system implements a multi-model data abstraction layer which allows working with the data as if it is situated in RDF triple store, executes SPARQL queries over it and ap-plies SHACL constraints and rules.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Databases,Computer Science - Software Engineering},
  file = {/home/skynet3/Zotero/storage/H6TEKA8J/Gorshkov et al. - 2021 - Ontology-based industrial data management platform.pdf;/home/skynet3/Zotero/storage/FIW2TRUH/2103.html}
}

@article{gortlerVisualExplorationGaussian2019,
  ids = {gortlerVisualExplorationGaussian2019a},
  title = {A {{Visual Exploration}} of {{Gaussian Processes}}},
  author = {Görtler, Jochen and Kehlbeck, Rebecca and Deussen, Oliver},
  date = {2019-04-02},
  journaltitle = {Distill},
  shortjournal = {Distill},
  volume = {4},
  number = {4},
  pages = {e17},
  issn = {2476-0757},
  doi = {10.23915/distill.00017},
  url = {https://distill.pub/2019/visual-exploration-gaussian-processes},
  urldate = {2022-11-11},
  abstract = {How to turn a collection of small building blocks into a versatile tool for solving regression problems.},
  langid = {english},
  file = {/home/skynet3/Zotero/storage/TNCYBCMI/visual-exploration-gaussian-processes.html}
}

@misc{gosiewskaEPPInterpretableScore2019,
  title = {{{EPP}}: Interpretable Score of Model Predictive Power},
  shorttitle = {{{EPP}}},
  author = {Gosiewska, Alicja and Bakala, Mateusz and Woznica, Katarzyna and Zwolinski, Maciej and Biecek, Przemyslaw},
  date = {2019-08-24},
  number = {arXiv:1908.09213},
  eprint = {1908.09213},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1908.09213},
  url = {http://arxiv.org/abs/1908.09213},
  urldate = {2022-11-11},
  abstract = {The most important part of model selection and hyperparameter tuning is the evaluation of model performance. The most popular measures, such as AUC, F1, ACC for binary classification, or RMSE, MAD for regression, or cross-entropy for multilabel classification share two common weaknesses. First is, that they are not on an interval scale. It means that the difference in performance for the two models has no direct interpretation. It makes no sense to compare such differences between datasets. Second is, that for k-fold cross-validation, the model performance is in most cases calculated as an average performance from particular folds, which neglects the information how stable is the performance for different folds. In this talk, we introduce a new EPP rating system for predictive models. We also demonstrate numerous advantages for this system, First, differences in EPP scores have probabilistic interpretation. Based on it we can assess the probability that one model will achieve better performance than another. Second, EPP scores can be directly compared between datasets. Third, they can be used for navigated hyperparameter tuning and model selection. Forth, we can create embeddings for datasets based on EPP scores.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/skynet3/Zotero/storage/A39682RL/Gosiewska et al. - 2019 - EPP interpretable score of model predictive power.pdf;/home/skynet3/Zotero/storage/4TWN527V/1908.html}
}

@misc{goyalFeatureInteractionsXGBoost2020,
  title = {Feature {{Interactions}} in {{XGBoost}}},
  author = {Goyal, Kshitij and Dumancic, Sebastijan and Blockeel, Hendrik},
  date = {2020-07-11},
  number = {arXiv:2007.05758},
  eprint = {2007.05758},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2007.05758},
  url = {http://arxiv.org/abs/2007.05758},
  urldate = {2022-11-11},
  abstract = {In this paper, we investigate how feature interactions can be identified to be used as constraints in the gradient boosting tree models using XGBoost's implementation. Our results show that accurate identification of these constraints can help improve the performance of baseline XGBoost model significantly. Further, the improvement in the model structure can also lead to better interpretability.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/skynet3/Zotero/storage/9CIEYKV6/Goyal et al. - 2020 - Feature Interactions in XGBoost.pdf;/home/skynet3/Zotero/storage/SKZYMW2F/2007.html}
}

@online{GradientBoostingExplained,
  title = {Gradient {{Boosting}} Explained by {{Alex Rogozhnikov}}},
  url = {http://arogozhnikov.github.io/2016/06/24/gradient_boosting_explained.html},
  urldate = {2022-11-11},
  abstract = {Understanding gradient boosting with interactive 3d-demonstrations}
}

@misc{grahamDyadicRegression2019,
  title = {Dyadic {{Regression}}},
  author = {Graham, Bryan S.},
  date = {2019-08-23},
  number = {arXiv:1908.09029},
  eprint = {1908.09029},
  eprinttype = {arxiv},
  primaryclass = {econ, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1908.09029},
  url = {http://arxiv.org/abs/1908.09029},
  urldate = {2022-11-11},
  abstract = {Dyadic data, where outcomes reflecting pairwise interaction among sampled units are of primary interest, arise frequently in social science research. Regression analyses with such data feature prominently in many research literatures (e.g., gravity models of trade). The dependence structure associated with dyadic data raises special estimation and, especially, inference issues. This chapter reviews currently available methods for (parametric) dyadic regression analysis and presents guidelines for empirical researchers.},
  archiveprefix = {arXiv},
  keywords = {62F12,Economics - Econometrics,Statistics - Applications},
  file = {/home/skynet3/Zotero/storage/HKWVPBTU/Graham - 2019 - Dyadic Regression.pdf;/home/skynet3/Zotero/storage/MY2XEU2H/1908.html}
}

@misc{grahamInternationalPoliticalEconomy2016,
  type = {SSRN Scholarly Paper},
  title = {The {{International Political Economy Data Resource}}},
  author = {Graham, Benjamin A. T. and Tucker, Jacob},
  date = {2016-09-14},
  number = {2534067},
  location = {{Rochester, NY}},
  doi = {10.2139/ssrn.2534067},
  url = {https://papers.ssrn.com/abstract=2534067},
  urldate = {2022-11-11},
  abstract = {Quantitative scholars in international relations often draw repeatedly on the same sources of country-year data across a diverse range of projects. The IPE Data Resource seeks to provide a public good to the field by standardizing and merging together 951 variables from 78 core IPE data sources into a single dataset, increasing efficiency and reducing the risk of data management errors. Easier access to data both encourages researchers to perform more robustness checks than they otherwise might and makes it easier for teachers of quantitative research methods to assign realistic exercises to their students. This resource will be updated and expanded annually and is available via the Harvard Dataverse Network.},
  langid = {english},
  keywords = {data,International Political Economy,IPE,time-series-cross-national},
  file = {/home/skynet3/Zotero/storage/MT44UJZV/Graham and Tucker - 2016 - The International Political Economy Data Resource.pdf;/home/skynet3/Zotero/storage/GEY8ABWD/papers.html}
}

@online{GrammarAnimatedGraphics,
  title = {A {{Grammar}} of {{Animated Graphics}}},
  url = {https://gganimate.com/},
  urldate = {2022-11-11},
  abstract = {The grammar of graphics as implemented in the ggplot2 package has     been successful in providing a powerful API for creating static      visualisation. In order to extend the API for animated graphics this package     provides a completely new set of grammar, fully compatible with ggplot2      for specifying transitions and animations in a flexible and extensible way.},
  langid = {english},
  file = {/home/skynet3/Zotero/storage/V7RPMLQQ/gganimate.com.html}
}

@online{GrammarGraphicsPython,
  title = {A {{Grammar}} of {{Graphics}} for {{Python}} — Plotnine 0.10.1 Documentation},
  url = {https://plotnine.readthedocs.io/en/stable/},
  urldate = {2022-11-11},
  file = {/home/skynet3/Zotero/storage/FF84Y8I5/stable.html}
}

@online{GraphicalDataAnalysis,
  title = {Graphical {{Data Analysis}} with {{R}}},
  url = {http://www.gradaanwr.net/},
  urldate = {2022-11-11},
  langid = {british}
}

@online{GraphicalModelsIntro,
  title = {3.1 - {{Graphical Models}} ({{Intro}} and {{Outline}}) - {{YouTube}}},
  url = {https://www.youtube.com/watch?v=Go4EkHN_PcA&list=PLoazKTcS0Rzb6bb9L508cyJ1z-U9iWkA0&index=20},
  urldate = {2022-11-15},
  file = {/home/skynet3/Zotero/storage/LJPYX6VW/watch.html}
}

@article{grauPRROCComputingVisualizing2015,
  ids = {grauPRROCComputingVisualizing2015a},
  title = {{{PRROC}}: Computing and Visualizing Precision-Recall and Receiver Operating Characteristic Curves in {{R}}},
  shorttitle = {{{PRROC}}},
  author = {Grau, Jan and Grosse, Ivo and Keilwagen, Jens},
  date = {2015-08-01},
  journaltitle = {Bioinformatics},
  shortjournal = {Bioinformatics},
  volume = {31},
  number = {15},
  pages = {2595--2597},
  issn = {1367-4803, 1460-2059},
  doi = {10.1093/bioinformatics/btv153},
  url = {https://academic.oup.com/bioinformatics/article-lookup/doi/10.1093/bioinformatics/btv153},
  urldate = {2022-11-11},
  langid = {english},
  file = {/home/skynet3/Zotero/storage/MJPL24QB/Grau et al. - 2015 - PRROC computing and visualizing precision-recall .pdf}
}

@article{greeneMOVEITLOSE2019,
  title = {{{MOVE IT OR LOSE IT}}: {{INTRODUCING PSEUDO-EARTH MOVER DIVERGENCE AS A CONTEXT-SENSITIVE METRIC FOR EVALUATING AND IMPROVING FORECASTING AND PREDICTION SYSTEMS}}},
  author = {Greene, Kevin and Hegre, Håvard and Hoyles, Frederick and Colaresi, Michael},
  date = {2019},
  pages = {16},
  abstract = {Prediction systems necessarily utilize a suite of metrics to evaluate and compare models and ultimately guide inferences about the world. However, conventional measures of model performance, ranging from accuracy, Brier scores, AUROC, and AUPR solely utilize observation by observation comparisons between the distribution of predictions and the actual observations. This bin-by-bin default approach assumes predictions are not embedded within a context of connections between observations. Yet, there are emerging areas of research, including political violence forecasting, where the spatial and temporal connections between observations have enormous implications for the overall costs and benefits of utilizing forecasting systems. We propose a new network-based contextsensitive performance metric, pseued-Earth Mover Divergence (pEMDiv) to accelerate progress and discoveries in these, and related, areas. pEMDiv nests other Earth Mover Distance calculations as special cases and for the first time simultaneously allows mis-calibrated predictions as well as asymmetric distance calculations. To illustrate the usefulness of our approach, we use pEMDiv, as well as other conventional metrics, to compare the performance of models from the Violence Early Warning System (ViEWS) Project. Our results suggest that not only can pEMDiv serve as a helpful complement to bin-by-bin measures of model performance, but can also be used to visually inspect the mistakes models are making in an human-interpretable context.},
  langid = {english},
  file = {/home/skynet3/Zotero/storage/82WFDSNP/Greene et al. - 2019 - MOVE IT OR LOSE IT INTRODUCING PSEUDO-EARTH MOVER.pdf}
}

@article{greenhillSeparationPlotNew2011,
  title = {The {{Separation Plot}}: {{A New Visual Method}} for {{Evaluating}} the {{Fit}} of {{Binary Models}}},
  shorttitle = {The {{Separation Plot}}},
  author = {Greenhill, Brian and Ward, Michael D. and Sacks, Audrey},
  date = {2011},
  journaltitle = {American Journal of Political Science},
  volume = {55},
  number = {4},
  pages = {991--1002},
  issn = {1540-5907},
  doi = {10.1111/j.1540-5907.2011.00525.x},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1540-5907.2011.00525.x},
  urldate = {2022-11-11},
  abstract = {We present a visual method for assessing the predictive power of models with binary outcomes. This technique allows the analyst to evaluate model fit based upon the models’ ability to consistently match high-probability predictions to actual occurrences of the event of interest, and low-probability predictions to nonoccurrences of the event of interest. Unlike existing methods for assessing predictive power for logit and probit models such as Percent Correctly Predicted statistics, Brier scores, and the ROC plot, our “separation plot” has the advantage of producing a visual display that is informative and easy to explain to a general audience, while also remaining insensitive to the often arbitrary probability thresholds that are used to distinguish between predicted events and nonevents. We demonstrate the effectiveness of this technique in building predictive models in a number of different areas of political research.},
  langid = {english},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1540-5907.2011.00525.x},
  file = {/home/skynet3/Zotero/storage/FZE2YL6P/j.1540-5907.2011.00525.html}
}

@article{greenlandStatisticalTestsValues2016,
  title = {Statistical Tests, {{P}} Values, Confidence Intervals, and Power: A Guide to Misinterpretations},
  shorttitle = {Statistical Tests, {{P}} Values, Confidence Intervals, and Power},
  author = {Greenland, Sander and Senn, Stephen J. and Rothman, Kenneth J. and Carlin, John B. and Poole, Charles and Goodman, Steven N. and Altman, Douglas G.},
  date = {2016},
  journaltitle = {European Journal of Epidemiology},
  shortjournal = {Eur J Epidemiol},
  volume = {31},
  eprint = {27209009},
  eprinttype = {pmid},
  pages = {337--350},
  issn = {0393-2990},
  doi = {10.1007/s10654-016-0149-3},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4877414/},
  urldate = {2022-11-11},
  abstract = {Misinterpretation and abuse of statistical tests, confidence intervals, and statistical power have been decried for decades, yet remain rampant. A key problem is that there are no interpretations of these concepts that are at once simple, intuitive, correct, and foolproof. Instead, correct use and interpretation of these statistics requires an attention to detail which seems to tax the patience of working scientists. This high cognitive demand has led to an epidemic of shortcut definitions and interpretations that are simply wrong, sometimes disastrously so—and yet these misinterpretations dominate much of the scientific literature. In light of this problem, we provide definitions and a discussion of basic statistics that are more general and critical than typically found in traditional introductory expositions. Our goal is to provide a resource for instructors, researchers, and consumers of statistics whose knowledge of statistical theory and technique may be limited but who wish to avoid and spot misinterpretations. We emphasize how violation of often unstated analysis protocols (such as selecting analyses for presentation based on the P values they produce) can lead to small P values even if the declared test hypothesis is correct, and can lead to large P values even if that hypothesis is incorrect. We then provide an explanatory list of 25 misinterpretations of P values, confidence intervals, and power. We conclude with guidelines for improving statistical interpretation and reporting.},
  pmcid = {PMC4877414},
  file = {/home/skynet3/Zotero/storage/LJ5CUI3J/Greenland et al. - 2016 - Statistical tests, P values, confidence intervals,.pdf}
}

@software{greggDedupe2022,
  title = {Dedupe},
  author = {Gregg, Forest and Eder, Derek},
  date = {2022-01},
  origdate = {2012-04-20T14:57:36Z},
  url = {https://github.com/dedupeio/dedupe},
  urldate = {2022-11-11},
  abstract = {:id: A python library for accurate and scalable fuzzy matching, record deduplication and entity-resolution.},
  version = {2.0.11}
}

@misc{greseleCausalInferenceStructural2022,
  title = {Causal {{Inference Through}} the {{Structural Causal Marginal Problem}}},
  author = {Gresele, Luigi and von Kügelgen, Julius and Kübler, Jonas M. and Kirschbaum, Elke and Schölkopf, Bernhard and Janzing, Dominik},
  options = {useprefix=true},
  date = {2022-07-14},
  number = {arXiv:2202.01300},
  eprint = {2202.01300},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2202.01300},
  url = {http://arxiv.org/abs/2202.01300},
  urldate = {2022-11-11},
  abstract = {We introduce an approach to counterfactual inference based on merging information from multiple datasets. We consider a causal reformulation of the statistical marginal problem: given a collection of marginal structural causal models (SCMs) over distinct but overlapping sets of variables, determine the set of joint SCMs that are counterfactually consistent with the marginal ones. We formalise this approach for categorical SCMs using the response function formulation and show that it reduces the space of allowed marginal and joint SCMs. Our work thus highlights a new mode of falsifiability through additional variables, in contrast to the statistical one via additional data.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/home/skynet3/Zotero/storage/XCHID3WK/Gresele et al. - 2022 - Causal Inference Through the Structural Causal Mar.pdf;/home/skynet3/Zotero/storage/YZVRXWNW/2202.html}
}

@article{griffithColliderBiasUndermines2020,
  title = {Collider Bias Undermines Our Understanding of {{COVID-19}} Disease Risk and Severity},
  author = {Griffith, Gareth J. and Morris, Tim T. and Tudball, Matthew J. and Herbert, Annie and Mancano, Giulia and Pike, Lindsey and Sharp, Gemma C. and Sterne, Jonathan and Palmer, Tom M. and Davey Smith, George and Tilling, Kate and Zuccolo, Luisa and Davies, Neil M. and Hemani, Gibran},
  date = {2020-11-12},
  journaltitle = {Nature Communications},
  shortjournal = {Nat Commun},
  volume = {11},
  number = {1},
  pages = {5749},
  publisher = {{Nature Publishing Group}},
  issn = {2041-1723},
  doi = {10.1038/s41467-020-19478-2},
  url = {https://www.nature.com/articles/s41467-020-19478-2},
  urldate = {2022-11-11},
  abstract = {Numerous observational studies have attempted to identify risk factors for infection with SARS-CoV-2 and COVID-19 disease outcomes. Studies have used datasets sampled from patients admitted to hospital, people tested for active infection, or people who volunteered to participate. Here, we highlight the challenge of interpreting observational evidence from such non-representative samples. Collider bias can induce associations between two or more variables which affect the likelihood of an individual being sampled, distorting associations between these variables in the sample. Analysing UK Biobank data, compared to the wider cohort the participants tested for COVID-19 were highly selected for a range of genetic, behavioural, cardiovascular, demographic, and anthropometric traits. We discuss the mechanisms inducing these problems, and approaches that could help mitigate them. While collider bias should be explored in existing studies, the optimal way to mitigate the problem is to use appropriate sampling strategies at the study design stage.},
  issue = {1},
  langid = {english},
  keywords = {Epidemiology,Infectious diseases,Risk factors,Statistical methods},
  file = {/home/skynet3/Zotero/storage/JQ9FAN6V/Griffith et al. - 2020 - Collider bias undermines our understanding of COVI.pdf;/home/skynet3/Zotero/storage/Z7BW3EV3/s41467-020-19478-2.html}
}

@misc{grinsztajnWhyTreebasedModels2022,
  ids = {grinsztajnWhyTreebasedModels2022a},
  title = {Why Do Tree-Based Models Still Outperform Deep Learning on Tabular Data?},
  author = {Grinsztajn, Léo and Oyallon, Edouard and Varoquaux, Gaël},
  date = {2022-07-18},
  number = {arXiv:2207.08815},
  eprint = {2207.08815},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2207.08815},
  url = {http://arxiv.org/abs/2207.08815},
  urldate = {2022-11-11},
  abstract = {While deep learning has enabled tremendous progress on text and image datasets, its superiority on tabular data is not clear. We contribute extensive benchmarks of standard and novel deep learning methods as well as tree-based models such as XGBoost and Random Forests, across a large number of datasets and hyperparameter combinations. We define a standard set of 45 datasets from varied domains with clear characteristics of tabular data and a benchmarking methodology accounting for both fitting models and finding good hyperparameters. Results show that tree-based models remain state-of-the-art on medium-sized data (\$\textbackslash sim\$10K samples) even without accounting for their superior speed. To understand this gap, we conduct an empirical investigation into the differing inductive biases of tree-based models and Neural Networks (NNs). This leads to a series of challenges which should guide researchers aiming to build tabular-specific NNs: 1. be robust to uninformative features, 2. preserve the orientation of the data, and 3. be able to easily learn irregular functions. To stimulate research on tabular architectures, we contribute a standard benchmark and raw data for baselines: every point of a 20 000 compute hours hyperparameter search for each learner.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning,Statistics - Methodology},
  file = {/home/skynet3/Zotero/storage/P9DBQSMU/Grinsztajn et al. - 2022 - Why do tree-based models still outperform deep lea.pdf;/home/skynet3/Zotero/storage/Z7FEJ3NC/2207.html}
}

@book{grolemundHandsOnProgramming,
  title = {Hands-{{On Programming}} with {{R}}},
  author = {Grolemund, Garrett},
  url = {https://rstudio-education.github.io/hopr/},
  urldate = {2022-11-11},
  abstract = {This book will teach you how to program in R, with hands-on examples. I wrote it for non-programmers to provide a friendly introduction to the R language. You’ll learn how to load data, assemble and disassemble data objects, navigate R’s environment system, write your own functions, and use all of R’s programming tools. Throughout the book, you’ll use your newfound skills to solve practical data science problems.},
  file = {/home/skynet3/Zotero/storage/NMY4T86G/hopr.html}
}

@book{grolemundMarkdownDefinitiveGuide,
  title = {R {{Markdown}}: {{The Definitive Guide}}},
  shorttitle = {R {{Markdown}}},
  author = {Grolemund, J. J. Allaire, Garrett, Yihui Xie},
  url = {https://bookdown.org/yihui/rmarkdown/},
  urldate = {2022-11-11},
  abstract = {The first official book authored by the core R Markdown developers that provides a comprehensive and accurate reference to the R Markdown ecosystem. With R Markdown, you can easily create reproducible data analysis reports, presentations, dashboards, interactive applications, books, dissertations, websites, and journal articles, while enjoying the simplicity of Markdown and the great power of R and other languages.},
  file = {/home/skynet3/Zotero/storage/2QJB43HJ/rmarkdown.html}
}

@book{grolemundTidyverseCookbook,
  title = {The {{Tidyverse Cookbook}}},
  author = {Grolemund, Edited by Garrett},
  url = {https://rstudio-education.github.io/tidyverse-cookbook/},
  urldate = {2022-11-11},
  abstract = {How to solve common data science tasks with R’s Tidyverse},
  file = {/home/skynet3/Zotero/storage/YIVGGMQ2/tidyverse-cookbook.html}
}

@online{groverCustomLossFunctions2018,
  title = {Custom {{Loss Functions}} for {{Gradient Boosting}}},
  author = {Grover, Prince},
  date = {2018-09-27T13:41:40},
  url = {https://towardsdatascience.com/custom-loss-functions-for-gradient-boosting-f79c1b40466d},
  urldate = {2022-11-11},
  abstract = {Optimize what matters},
  langid = {english},
  organization = {{Medium}},
  file = {/home/skynet3/Zotero/storage/S3TP22VY/custom-loss-functions-for-gradient-boosting-f79c1b40466d.html}
}

@misc{gruberBetterUncertaintyCalibration2022,
  ids = {gruberBetterUncertaintyCalibration2022a},
  title = {Better {{Uncertainty Calibration}} via {{Proper Scores}} for {{Classification}} and {{Beyond}}},
  author = {Gruber, Sebastian and Buettner, Florian},
  date = {2022-09-19},
  number = {arXiv:2203.07835},
  eprint = {2203.07835},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2203.07835},
  url = {http://arxiv.org/abs/2203.07835},
  urldate = {2022-11-11},
  abstract = {With model trustworthiness being crucial for sensitive real-world applications, practitioners are putting more and more focus on improving the uncertainty calibration of deep neural networks. Calibration errors are designed to quantify the reliability of probabilistic predictions but their estimators are usually biased and inconsistent. In this work, we introduce the framework of proper calibration errors, which relates every calibration error to a proper score and provides a respective upper bound with optimal estimation properties. This relationship can be used to reliably quantify the model calibration improvement. We theoretically and empirically demonstrate the shortcomings of commonly used estimators compared to our approach. Due to the wide applicability of proper scores, this gives a natural extension of recalibration beyond classification.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/skynet3/Zotero/storage/ICNVUI9G/Gruber and Buettner - 2022 - Better Uncertainty Calibration via Proper Scores f.pdf;/home/skynet3/Zotero/storage/XSBCBLKD/2203.html}
}

@misc{grunwaldSafeTesting2021,
  ids = {grunwaldSafeTesting2021a},
  title = {Safe {{Testing}}},
  author = {Grünwald, Peter and de Heide, Rianne and Koolen, Wouter},
  options = {useprefix=true},
  date = {2021-12-06},
  number = {arXiv:1906.07801},
  eprint = {1906.07801},
  eprinttype = {arxiv},
  primaryclass = {cs, math, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1906.07801},
  url = {http://arxiv.org/abs/1906.07801},
  urldate = {2022-11-11},
  abstract = {We develop the theory of hypothesis testing based on the E-value, a notion of evidence that, unlike the p-value, allows for effortlessly combining results from several studies in the common scenario where the decision to perform a new study may depend on previous outcomes. Tests based on E-values are safe, i.e. they preserve Type-I error guarantees, under such optional continuation. We define growth-rate optimality (GRO) as an analogue of power in an optional continuation context, and we show how to construct GRO E-variables for general testing problems with composite null and alternative, emphasizing models with nuisance parameters. GRO E-values take the form of Bayes factors with special priors. We illustrate the theory using several classic examples including a one-sample safe t-test (in which the right Haar prior turns out to be GRO) and the 2x2 contingency table (in which the GRO prior is different from standard priors). Sharing Fisherian, Neymanian and Jeffreys-Bayesian interpretations, E-values and the corresponding tests may provide a methodology acceptable to adherents of all three schools.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Information Theory,Computer Science - Machine Learning,Mathematics - Statistics Theory,Statistics - Methodology},
  file = {/home/skynet3/Zotero/storage/AJE4CSZU/Grünwald et al. - 2021 - Safe Testing.pdf;/home/skynet3/Zotero/storage/3U4GNNHI/1906.html}
}

@online{GRWebsite,
  ids = {GRWebsitea},
  title = {{{GR}}'s {{Website}}},
  url = {https://grodri.github.io/glms/notes/c7s4},
  urldate = {2022-11-11},
  file = {/home/skynet3/Zotero/storage/FYVXJQ5Y/c7s4.html}
}

@article{guanPostModelFitting2020,
  ids = {guanPostModelFitting2020a},
  title = {Post Model‐fitting Exploration via a “{{Next}}‐{{Door}}” Analysis},
  author = {Guan, Leying and Tibshirani, Robert},
  date = {2020-09},
  journaltitle = {Canadian Journal of Statistics},
  shortjournal = {Can J Statistics},
  volume = {48},
  number = {3},
  pages = {447--470},
  issn = {0319-5724, 1708-945X},
  doi = {10.1002/cjs.11542},
  url = {https://onlinelibrary.wiley.com/doi/10.1002/cjs.11542},
  urldate = {2022-11-11},
  langid = {english},
  file = {/home/skynet3/Zotero/storage/M7BCH9J5/Guan and Tibshirani - 2020 - Post model‐fitting exploration via a “Next‐Door” a.pdf}
}

@online{gudeDataScienceInterviews2020,
  title = {Data {{Science Interviews During}} the 2020 {{Pandemic}}},
  author = {Gude, Alexander},
  date = {2020-09-21T00:00:00+00:00},
  url = {https://alexgude.com/blog/interviewing-for-data-science-positions-in-2020/},
  urldate = {2022-11-11},
  abstract = {In the middle of the COVID-19 pandemic, I found myself looking for a data science job for the third time in my life. This post covers what I learned.},
  langid = {english},
  organization = {{Alex Gude}},
  file = {/home/skynet3/Zotero/storage/9CI9IF8H/interviewing-for-data-science-positions-in-2020.html}
}

@online{gudeTechInterviewsRespect2017,
  title = {Tech {{Interviews}}: {{Respect Everyone}}’s {{Time}}},
  shorttitle = {Tech {{Interviews}}},
  author = {Gude, Alexander},
  date = {2017-09-18T00:00:00+00:00},
  url = {https://alexgude.com/blog/interviews-respect-time/},
  urldate = {2022-11-11},
  abstract = {Interviewing is notoriously agonizing for both the candidate and the company! But it could be much better! Here I propose one guiding principle to make it easier on everyone.},
  langid = {english},
  organization = {{Alex Gude}},
  file = {/home/skynet3/Zotero/storage/2YHC3G39/interviews-respect-time.html}
}

@misc{guoConfounderSelectionObjectives2022,
  title = {Confounder {{Selection}}: {{Objectives}} and {{Approaches}}},
  shorttitle = {Confounder {{Selection}}},
  author = {Guo, F. Richard and Lundborg, Anton Rask and Zhao, Qingyuan},
  date = {2022-08-29},
  number = {arXiv:2208.13871},
  eprint = {2208.13871},
  eprinttype = {arxiv},
  primaryclass = {math, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2208.13871},
  url = {http://arxiv.org/abs/2208.13871},
  urldate = {2022-11-12},
  abstract = {Confounder selection is perhaps the most important step in the design of observational studies. A number of criteria, often with different objectives and approaches, have been proposed, and their validity and practical value have been debated in the literature. Here, we provide a unified review of these criteria and the assumptions behind them. We list several objectives that confounder selection methods aim to achieve and discuss the amount of structural knowledge required by different approaches. Finally, we discuss limitations of the existing approaches and implications for practitioners.},
  archiveprefix = {arXiv},
  keywords = {Mathematics - Statistics Theory,Statistics - Methodology},
  file = {/home/skynet3/Zotero/storage/KP74NNKP/Guo et al. - 2022 - Confounder Selection Objectives and Approaches.pdf;/home/skynet3/Zotero/storage/S73TUZPB/2208.html}
}

@misc{guoSurveyAutomatedFactChecking2022,
  title = {A {{Survey}} on {{Automated Fact-Checking}}},
  author = {Guo, Zhijiang and Schlichtkrull, Michael and Vlachos, Andreas},
  date = {2022-06-06},
  number = {arXiv:2108.11896},
  eprint = {2108.11896},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2108.11896},
  url = {http://arxiv.org/abs/2108.11896},
  urldate = {2022-11-11},
  abstract = {Fact-checking has become increasingly important due to the speed with which both information and misinformation can spread in the modern media ecosystem. Therefore, researchers have been exploring how fact-checking can be automated, using techniques based on natural language processing, machine learning, knowledge representation, and databases to automatically predict the veracity of claims. In this paper, we survey automated fact-checking stemming from natural language processing, and discuss its connections to related tasks and disciplines. In this process, we present an overview of existing datasets and models, aiming to unify the various definitions given and identify common concepts. Finally, we highlight challenges for future research.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/home/skynet3/Zotero/storage/HCW53UAF/Guo et al. - 2022 - A Survey on Automated Fact-Checking.pdf;/home/skynet3/Zotero/storage/ITIAPAQQ/2108.html}
}

@online{guptaNeverTrustRownames,
  title = {Never Trust Rownames of a Dataframe},
  author = {Gupta, Ankur},
  url = {https://www.perfectlyrandom.org/2015/06/16/never-trust-the-row-names-of-a-dataframe-in-R/},
  urldate = {2022-11-11},
  abstract = {Introduction Dataframes in R have both column names and row names. Column names, which are used frequently, give the dataframes in R their characteristic distinction. Row names, on the other hand, are rarely used. Usually, row names appear to be the same as row numbers but this is not the case. This quick blog post demonstrates that row names are not the same as row numbers. This is something most experienced R users are well aware of.},
  organization = {{Perfectly Random}}
}

@inproceedings{guThreeLevelsGeneralization2021,
  ids = {guThreeLevelsGeneralization2021a},
  title = {Beyond {{I}}.{{I}}.{{D}}.: {{Three Levels}} of {{Generalization}} for {{Question Answering}} on {{Knowledge Bases}}},
  shorttitle = {Beyond {{I}}.{{I}}.{{D}}.},
  booktitle = {Proceedings of the {{Web Conference}} 2021},
  author = {Gu, Yu and Kase, Sue and Vanni, Michelle and Sadler, Brian and Liang, Percy and Yan, Xifeng and Su, Yu},
  date = {2021-04-19},
  eprint = {2011.07743},
  eprinttype = {arxiv},
  primaryclass = {cs},
  pages = {3477--3488},
  doi = {10.1145/3442381.3449992},
  url = {http://arxiv.org/abs/2011.07743},
  urldate = {2022-11-11},
  abstract = {Existing studies on question answering on knowledge bases (KBQA) mainly operate with the standard i.i.d assumption, i.e., training distribution over questions is the same as the test distribution. However, i.i.d may be neither reasonably achievable nor desirable on large-scale KBs because 1) true user distribution is hard to capture and 2) randomly sample training examples from the enormous space would be highly data-inefficient. Instead, we suggest that KBQA models should have three levels of built-in generalization: i.i.d, compositional, and zero-shot. To facilitate the development of KBQA models with stronger generalization, we construct and release a new large-scale, high-quality dataset with 64,331 questions, GrailQA, and provide evaluation settings for all three levels of generalization. In addition, we propose a novel BERT-based KBQA model. The combination of our dataset and model enables us to thoroughly examine and demonstrate, for the first time, the key role of pre-trained contextual embeddings like BERT in the generalization of KBQA.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning,I.2.7},
  file = {/home/skynet3/Zotero/storage/D4WXEB7J/Gu et al. - 2021 - Beyond I.I.D. Three Levels of Generalization for .pdf;/home/skynet3/Zotero/storage/38DLZPKX/2011.html}
}

@misc{gutmannPenPaperExercises2022,
  title = {Pen and {{Paper Exercises}} in {{Machine Learning}}},
  author = {Gutmann, Michael U.},
  date = {2022-06-27},
  number = {arXiv:2206.13446},
  eprint = {2206.13446},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2206.13446},
  url = {http://arxiv.org/abs/2206.13446},
  urldate = {2022-11-11},
  abstract = {This is a collection of (mostly) pen-and-paper exercises in machine learning. The exercises are on the following topics: linear algebra, optimisation, directed graphical models, undirected graphical models, expressive power of graphical models, factor graphs and message passing, inference for hidden Markov models, model-based learning (including ICA and unnormalised models), sampling and Monte-Carlo integration, and variational inference.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/skynet3/Zotero/storage/JUVVR3AB/Gutmann - 2022 - Pen and Paper Exercises in Machine Learning.pdf;/home/skynet3/Zotero/storage/CU6SLPMW/2206.html}
}

@article{guzeyHowLifeSciences2019,
  title = {How {{Life Sciences Actually Work}}: {{Findings}} of a {{Year-Long Investigation}}},
  shorttitle = {How {{Life Sciences Actually Work}}},
  author = {Guzey, Alexey},
  date = {2019-08-15},
  journaltitle = {guzey.com},
  url = {https://guzey.com/how-life-sciences-actually-work/},
  urldate = {2022-11-11},
  abstract = {See discussion on /r/slatestarcodex (a) (25 comments), Hacker News (a) (17 comments), Twitter (1 (a), 2 (a)) ({$>$}300 retweets), Marginal Revolution (a) (39 comments)  Summary: academia has a lot of problems and it could work much better. However, these problems are not as catastrophic as an outside perspective would suggest. My (contrarian, I guess) intuition is that scientific progress in biology is not slowing down. Specific parts of academia that seem to be problematic: rigid, punishing for …},
  langid = {english}
}

@article{haberCausalLanguageStrength2018,
  title = {Causal Language and Strength of Inference in Academic and Media Articles Shared in Social Media ({{CLAIMS}}): {{A}} Systematic Review},
  shorttitle = {Causal Language and Strength of Inference in Academic and Media Articles Shared in Social Media ({{CLAIMS}})},
  author = {Haber, Noah and Smith, Emily R. and Moscoe, Ellen and Andrews, Kathryn and Audy, Robin and Bell, Winnie and Brennan, Alana T. and Breskin, Alexander and Kane, Jeremy C. and Karra, Mahesh and McClure, Elizabeth S. and Suarez, Elizabeth A. and {on behalf of the CLAIMS research team}},
  editor = {Dorta-González, Pablo},
  date = {2018-05-30},
  journaltitle = {PLOS ONE},
  shortjournal = {PLoS ONE},
  volume = {13},
  number = {5},
  pages = {e0196346},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0196346},
  url = {https://dx.plos.org/10.1371/journal.pone.0196346},
  urldate = {2022-11-11},
  langid = {english},
  file = {/home/skynet3/Zotero/storage/7ATAIR62/Haber et al. - 2018 - Causal language and strength of inference in acade.pdf}
}

@online{haberIntroducingStan2tfpLightweight2020,
  title = {Introducing {{Stan2tfp}} - a Lightweight Interface for the {{Stan-to-TensorFlow Probability}} Compiler},
  author = {Haber, Adam},
  date = {2020-05-21T00:00:00+00:00},
  url = {https://adamhaber.github.io/post/stan2tfp-post1/},
  urldate = {2022-11-11},
  abstract = {TL;DR The new Stan compiler has an alternative backend that allows you to do this: stan2tfp is a lightwight interface for this compiler, that allows you to do this with one line of code, and fit the model to data with another. Why stan2tfp In short - to get the convenience of Stan programs and the scalability of TensorFlow. The model is written in Stan, which means you get a lot of the benefits of having the Stan compiler behind your shoulder (types, bounds, etc).},
  langid = {american},
  organization = {{Adam Haber}}
}

@article{hafemanOpeningBlackBox2009,
  title = {Opening the {{Black Box}}: A Motivation for the Assessment of Mediation},
  shorttitle = {Opening the {{Black Box}}},
  author = {Hafeman, Danella M. and Schwartz, Sharon},
  date = {2009-06},
  journaltitle = {International Journal of Epidemiology},
  shortjournal = {Int J Epidemiol},
  volume = {38},
  number = {3},
  eprint = {19261660},
  eprinttype = {pmid},
  pages = {838--845},
  issn = {1464-3685},
  doi = {10.1093/ije/dyn372},
  abstract = {Recent criticism of epidemiologic methods has focused on the limitations of 'black box' epidemiology, a pejorative label given to the simple identification of exposure-disease relationships. The assessment of mediation is an important tool for addressing this criticism. By using mediation analysis to open the black box, underlying mechanisms of the observed associations can be described and causal inference improved. An explicit theoretical motivation for such an analysis has been missing from the epidemiological literature. To provide this motivation, we integrate literature from epidemiology and other social sciences to describe the reasons that an investigator might want to assess mediation. We then describe the connections between these reasons and specific measures of indirect and direct effects that have been previously described.},
  langid = {english},
  keywords = {Causality,Confounding Factors; Epidemiologic,Epidemiologic Research Design,Evidence-Based Medicine,Humans,Models; Theoretical,Motivation,Risk Assessment}
}

@online{hafenIntroducingGeofacet2018,
  title = {Introducing Geofacet},
  author = {Hafen, Ryan},
  date = {2018-03-10T00:00:00+00:00},
  url = {https://ryanhafen.com/blog/geofacet/},
  urldate = {2022-11-11},
  abstract = {I released an R package over 9 months ago called geofacet, and have long promised a blog post about the approach. This is the first post in what I plan to be a series of two or three posts. In this post I’ll introduce what the package does and compare it to some other approaches for visualizing geographic data.},
  langid = {american},
  organization = {{Ryan Hafen}},
  file = {/home/skynet3/Zotero/storage/P84ST6T5/geofacet.html}
}

@misc{haghifamUnifiedInformationTheoreticFramework2021,
  title = {Towards a {{Unified Information-Theoretic Framework}} for {{Generalization}}},
  author = {Haghifam, Mahdi and Dziugaite, Gintare Karolina and Moran, Shay and Roy, Daniel M.},
  date = {2021-11-17},
  number = {arXiv:2111.05275},
  eprint = {2111.05275},
  eprinttype = {arxiv},
  primaryclass = {cs, math, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2111.05275},
  url = {http://arxiv.org/abs/2111.05275},
  urldate = {2022-11-11},
  abstract = {In this work, we investigate the expressiveness of the "conditional mutual information" (CMI) framework of Steinke and Zakynthinou (2020) and the prospect of using it to provide a unified framework for proving generalization bounds in the realizable setting. We first demonstrate that one can use this framework to express non-trivial (but sub-optimal) bounds for any learning algorithm that outputs hypotheses from a class of bounded VC dimension. We prove that the CMI framework yields the optimal bound on the expected risk of Support Vector Machines (SVMs) for learning halfspaces. This result is an application of our general result showing that stable compression schemes Bousquet al. (2020) of size \$k\$ have uniformly bounded CMI of order \$O(k)\$. We further show that an inherent limitation of proper learning of VC classes contradicts the existence of a proper learner with constant CMI, and it implies a negative resolution to an open problem of Steinke and Zakynthinou (2020). We further study the CMI of empirical risk minimizers (ERMs) of class \$H\$ and show that it is possible to output all consistent classifiers (version space) with bounded CMI if and only if \$H\$ has a bounded star number (Hanneke and Yang (2015)). Moreover, we prove a general reduction showing that "leave-one-out" analysis is expressible via the CMI framework. As a corollary we investigate the CMI of the one-inclusion-graph algorithm proposed by Haussler et al. (1994). More generally, we show that the CMI framework is universal in the sense that for every consistent algorithm and data distribution, the expected risk vanishes as the number of samples diverges if and only if its evaluated CMI has sublinear growth with the number of samples.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Information Theory,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/skynet3/Zotero/storage/9EYL87LF/Haghifam et al. - 2021 - Towards a Unified Information-Theoretic Framework .pdf;/home/skynet3/Zotero/storage/YT4SJQYQ/2111.html}
}

@misc{hallArtScienceMachine2020,
  title = {On the {{Art}} and {{Science}} of {{Machine Learning Explanations}}},
  author = {Hall, Patrick},
  date = {2020-05-31},
  number = {arXiv:1810.02909},
  eprint = {1810.02909},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1810.02909},
  url = {http://arxiv.org/abs/1810.02909},
  urldate = {2022-11-11},
  abstract = {This text discusses several popular explanatory methods that go beyond the error measurements and plots traditionally used to assess machine learning models. Some of the explanatory methods are accepted tools of the trade while others are rigorously derived and backed by long-standing theory. The methods, decision tree surrogate models, individual conditional expectation (ICE) plots, local interpretable model-agnostic explanations (LIME), partial dependence plots, and Shapley explanations, vary in terms of scope, fidelity, and suitable application domain. Along with descriptions of these methods, this text presents real-world usage recommendations supported by a use case and public, in-depth software examples for reproducibility.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/skynet3/Zotero/storage/UZFX2EXU/Hall - 2020 - On the Art and Science of Machine Learning Explana.pdf;/home/skynet3/Zotero/storage/HZVCA3L7/1810.html}
}

@article{haltermanSyntheticallyGeneratedText,
  ids = {haltermanSyntheticallyGeneratedTexta},
  title = {Synthetically Generated Text for Supervised Text Analysis},
  author = {Halterman, Andrew},
  pages = {20},
  abstract = {Supervised text models are often the best tool for categorizing documents into known classes or for extracting information from within documents. However, supervised models are often difficult to employ because of the expense involved in hand-labeling documents, the difficulty of retrieving relevant documents for rare class annotation, and copyright and privacy concerns involved in sharing annotated documents. This paper proposes a partial solution to these three issues, in the form of controlled generation of synthetic text. Recent advances in text generation make it possible to create synthetic documents with desired class labels and in a form that can be broadly shared without copyright or licensing concerns. I demonstrate the usefulness of text generation techniques with three applications: using an off-the-shelf language model prompted with article headlines to generate synthetic news articles describing specified political events for training an event detection system, using a finetuned language model to generate synthetic tweets describing the fighting in Ukraine for named entity recognition labeling, and using a task description approach to generate a multilingual corpus of populist manifesto statements for training a sentence-level populism classifier. The article includes a discussion of the ethical concerns inherent in this work along with proposed guidelines for researchers.},
  langid = {english},
  file = {/home/skynet3/Zotero/storage/JXA9D2QW/Halterman - Synthetically generated text for supervised text a.pdf}
}

@misc{hamBenefitsCostsMatching2022,
  ids = {hamBenefitsCostsMatching2022a},
  title = {Benefits and Costs of Matching Prior to a {{Difference}} in {{Differences}} Analysis When Parallel Trends Does Not Hold},
  author = {Ham, Dae Woong and Miratrix, Luke},
  date = {2022-07-26},
  number = {arXiv:2205.08644},
  eprint = {2205.08644},
  eprinttype = {arxiv},
  primaryclass = {math, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2205.08644},
  url = {http://arxiv.org/abs/2205.08644},
  urldate = {2022-11-11},
  abstract = {The Difference in Difference (DiD) estimator is a popular estimator built on the "parallel trends" assumption. To increase the plausibility of this assumption, a natural idea is to match treated and control units prior to a DiD analysis. In this paper, we characterize the bias of matching prior to a DiD analysis under a linear structural model. Our framework allows for both observed and unobserved confounders that have time varying effects. Given this framework, we find that matching on baseline covariates reduces the bias associated with these covariates, when compared to the original DiD estimator. We further find that additionally matching on the pre-treatment outcomes has both cost and benefit. First, it mitigates the bias associated with unobserved confounders, since matching on pre-treatment outcomes partially balances these unobserved confounders. This reduction is proportional to the reliability of the outcome, a measure of how coupled the outcomes are with these latent covariates. On the other hand, we find that matching on the pre-treatment outcome undermines the second "difference" in a DiD estimate by forcing the treated and control group's pre-treatment outcomes to be equal. This injects bias into the final estimate, analogous to the case when parallel trends holds. We extend our bias results to multivariate confounders with multiple pre-treatment periods and find similar results. Finally, we provide heuristic guidelines to practitioners on whether to match prior to their DiD analysis, along with a method for roughly estimating the reduction in bias. We illustrate our guidelines by reanalyzing a recent empirical study that used matching prior to a DiD analysis to explore the impact of principal turnover on student achievement. We find that the authors' decision to match on the pre-treatment outcomes was crucial in making the estimated treatment effect more credible.},
  archiveprefix = {arXiv},
  keywords = {Mathematics - Statistics Theory,Statistics - Methodology},
  file = {/home/skynet3/Zotero/storage/PR9VK3Z7/Ham and Miratrix - 2022 - Benefits and costs of matching prior to a Differen.pdf;/home/skynet3/Zotero/storage/3UIIISUP/2205.html}
}

@online{HamiltonianMonteCarlo,
  title = {Hamiltonian {{Monte Carlo}} Explained by {{Alex Rogozhnikov}}},
  url = {https://arogozhnikov.github.io/2016/12/19/markov_chain_monte_carlo.html},
  urldate = {2022-11-11},
  abstract = {Understanding an effective way of sampling from complex distributions with 3d-demonstrations}
}

@book{hardtPatternsPredictionsActions2022,
  title = {Patterns, {{Predictions}}, and {{Actions}}: {{Foundations}} of {{Machine Learning}}},
  shorttitle = {Patterns, {{Predictions}}, and {{Actions}}},
  author = {Hardt, Moritz and Recht, Benjamin},
  date = {2022-10-18},
  eprint = {491xEAAAQBAJ},
  eprinttype = {googlebooks},
  publisher = {{Princeton University Press}},
  abstract = {An authoritative, up-to-date graduate textbook on machine learning that highlights its historical context and societal impactsPatterns, Predictions, and Actions introduces graduate students to the essentials of machine learning while offering invaluable perspective on its history and social implications. Beginning with the foundations of decision making, Moritz Hardt and Benjamin Recht explain how representation, optimization, and generalization are the constituents of supervised learning. They go on to provide self-contained discussions of causality, the practice of causal inference, sequential decision making, and reinforcement learning, equipping readers with the concepts and tools they need to assess the consequences that may arise from acting on statistical decisions.Provides a modern introduction to machine learning, showing how data patterns support predictions and consequential actionsPays special attention to societal impacts and fairness in decision makingTraces the development of machine learning from its origins to todayFeatures a novel chapter on machine learning benchmarks and datasetsInvites readers from all backgrounds, requiring some experience with probability, calculus, and linear algebraAn essential textbook for students and a guide for researchers},
  isbn = {978-0-691-23373-4},
  langid = {english},
  pagetotal = {320},
  keywords = {Computers / Data Science / Machine Learning,Computers / Mathematical & Statistical Software,Mathematics / Probability & Statistics / General}
}

@misc{hardwickeEstimatingPrevalenceTransparency2020,
  title = {Estimating the Prevalence of Transparency and Reproducibility-Related Research Practices in Psychology (2014-2017)},
  author = {Hardwicke, Tom E. and Thibault, Robert T. and Kosie, Jessica and Wallach, Joshua D. and Kidwell, Mallory C. and Ioannidis, John},
  date = {2020-01-02T08:39:15},
  publisher = {{MetaArXiv}},
  doi = {10.31222/osf.io/9sz2y},
  url = {https://osf.io/preprints/metaarxiv/9sz2y/},
  urldate = {2022-11-11},
  abstract = {Psychologists are navigating an unprecedented period of introspection about the credibility and utility of their discipline. Reform initiatives have emphasized the benefits of several transparency and reproducibility-related research practices; however, their adoption across the psychology literature is unknown. To estimate their prevalence, we manually examined a random sample of 250 psychology articles published between 2014-2017. Over half of the articles were publicly available (154/237, 65\% [95\% confidence interval, 59\%-71\%]); however, sharing of research materials (26/183, 14\% [10\%-19\%]), study protocols (0/188, 0\% [0\%-1\%]), raw data (4/188, 2\% [1\%-4\%]), and analysis scripts (1/188, 1\% [0\%-1\%]) was rare. Pre-registration was also uncommon (5/188, 3\% [1\%-5\%]). Many articles included a funding disclosure statement (142/228, 62\% [56\%-69\%]), but conflict of interest statements were less common (88/228, 39\% [32\%-45\%]). Replication studies were rare (10/188, 5\% [3\%-8\%]) and few studies were included in systematic reviews (21/183, 11\% [8\%-16\%]) or meta-analyses (12/183, 7\% [4\%-10\%]). Overall, the results suggest that transparency and reproducibility-related research practices were far from routine. These findings establish a baseline which can be used to assess future progress towards increasing the credibility and utility of psychology research.},
  langid = {american},
  keywords = {meta-research,open science,psychology,Psychology,reproducibility,Social and Behavioral Sciences,transparency},
  file = {/home/skynet3/Zotero/storage/96MAR4WR/Hardwicke et al. - 2020 - Estimating the prevalence of transparency and repr.pdf}
}

@misc{harozComparisonPreregistrationPlatforms2022,
  ids = {harozComparisonPreregistrationPlatforms2022a},
  title = {Comparison of {{Preregistration Platforms}}},
  author = {Haroz, Steve},
  date = {2022-02-24T14:42:51},
  publisher = {{MetaArXiv}},
  doi = {10.31222/osf.io/zry2u},
  url = {https://osf.io/preprints/metaarxiv/zry2u/},
  urldate = {2022-11-11},
  abstract = {Preregistration can force researchers to front-load a lot of decision-making to an early stage of a project. Choosing which preregistration platform to use must be therefore be one of those early decisions, and because a preregistration cannot be moved, that choice is permanent. This article aims to help researchers who are already interested in preregistration choose a platform by clarifying differences between them. Preregistration criteria and features are explained and analyzed for sites that cater to a broad range of research fields, including: GitHub, AsPredicted, Zenodo, the Open Science Framework (OSF), and an “open-ended” variant of OSF. While a private prespecification document can help mitigate self-deception, this guide considers publicly shared preregistrations that aim to improve credibility. It therefore defines three of the criteria (a timestamp, a registry, and persistence) as a bare minimum for a valid and reliable preregistration. GitHub and AsPredicted fail to meet all three. Zenodo and OSF meet the basic criteria and vary in which additional features they offer.},
  langid = {american},
  keywords = {preregistration,Social and Behavioral Sciences},
  file = {/home/skynet3/Zotero/storage/SHYZFJNQ/Haroz - 2022 - Comparison of Preregistration Platforms.pdf}
}

@inproceedings{hartfordDeepIVFlexible2017,
  title = {Deep {{IV}}: {{A Flexible Approach}} for {{Counterfactual Prediction}}},
  shorttitle = {Deep {{IV}}},
  booktitle = {Proceedings of the 34th {{International Conference}} on {{Machine Learning}}},
  author = {Hartford, Jason and Lewis, Greg and Leyton-Brown, Kevin and Taddy, Matt},
  date = {2017-07-17},
  pages = {1414--1423},
  publisher = {{PMLR}},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v70/hartford17a.html},
  urldate = {2022-11-15},
  abstract = {Counterfactual prediction requires understanding causal relationships between so-called treatment and outcome variables. This paper provides a recipe for augmenting deep learning methods to accurately characterize such relationships in the presence of instrument variables (IVs) – sources of treatment randomization that are conditionally independent from the outcomes. Our IV specification resolves into two prediction tasks that can be solved with deep neural nets: a first-stage network for treatment prediction and a second-stage network whose loss function involves integration over the conditional treatment distribution. This Deep IV framework allows us to take advantage of off-the-shelf supervised learning techniques to estimate causal effects by adapting the loss function. Experiments show that it outperforms existing machine learning approaches.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english},
  file = {/home/skynet3/Zotero/storage/QP7SJKMU/Hartford et al. - 2017 - Deep IV A Flexible Approach for Counterfactual Pr.pdf;/home/skynet3/Zotero/storage/XXFPXUHG/Hartford et al. - 2017 - Deep IV A Flexible Approach for Counterfactual Pr.pdf}
}

@misc{hartfordValidCausalInference2020,
  ids = {hartfordValidCausalInference2020a},
  title = {Valid {{Causal Inference}} with ({{Some}}) {{Invalid Instruments}}},
  author = {Hartford, Jason and Veitch, Victor and Sridhar, Dhanya and Leyton-Brown, Kevin},
  date = {2020-06-19},
  number = {arXiv:2006.11386},
  eprint = {2006.11386},
  eprinttype = {arxiv},
  primaryclass = {cs, econ, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2006.11386},
  url = {http://arxiv.org/abs/2006.11386},
  urldate = {2022-11-11},
  abstract = {Instrumental variable methods provide a powerful approach to estimating causal effects in the presence of unobserved confounding. But a key challenge when applying them is the reliance on untestable "exclusion" assumptions that rule out any relationship between the instrument variable and the response that is not mediated by the treatment. In this paper, we show how to perform consistent IV estimation despite violations of the exclusion assumption. In particular, we show that when one has multiple candidate instruments, only a majority of these candidates---or, more generally, the modal candidate-response relationship---needs to be valid to estimate the causal effect. Our approach uses an estimate of the modal prediction from an ensemble of instrumental variable estimators. The technique is simple to apply and is "black-box" in the sense that it may be used with any instrumental variable estimator as long as the treatment effect is identified for each valid instrument independently. As such, it is compatible with recent machine-learning based estimators that allow for the estimation of conditional average treatment effects (CATE) on complex, high dimensional data. Experimentally, we achieve accurate estimates of conditional average treatment effects using an ensemble of deep network-based estimators, including on a challenging simulated Mendelian Randomization problem.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Economics - Econometrics,Statistics - Machine Learning,Statistics - Methodology},
  file = {/home/skynet3/Zotero/storage/BXILV3ZZ/Hartford et al. - 2020 - Valid Causal Inference with (Some) Invalid Instrum.pdf;/home/skynet3/Zotero/storage/5UMKP7LD/2006.html}
}

@online{HeatmapIntegratedDecisionTree,
  title = {Heatmap-{{Integrated Decision Tree Visualizations}}},
  url = {https://trang1618.github.io/treeheatr/},
  urldate = {2022-11-11},
  abstract = {Creates interpretable decision tree visualizations with the     data represented as a heatmap at the trees leaf nodes.  treeheatr     utilizes the customizable ggparty' package for drawing decision     trees.},
  langid = {english},
  file = {/home/skynet3/Zotero/storage/L6KJGZEE/treeheatr.html}
}

@misc{heatonApplicationsDeepNeural2022,
  ids = {heatonApplicationsDeepNeural2022a},
  title = {Applications of {{Deep Neural Networks}} with {{Keras}}},
  author = {Heaton, Jeff},
  date = {2022-05-16},
  number = {arXiv:2009.05673},
  eprint = {2009.05673},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2009.05673},
  urldate = {2022-11-11},
  abstract = {Deep learning is a group of exciting new technologies for neural networks. Through a combination of advanced training techniques and neural network architectural components, it is now possible to create neural networks that can handle tabular data, images, text, and audio as both input and output. Deep learning allows a neural network to learn hierarchies of information in a way that is like the function of the human brain. This course will introduce the student to classic neural network structures, Convolution Neural Networks (CNN), Long Short-Term Memory (LSTM), Gated Recurrent Neural Networks (GRU), General Adversarial Networks (GAN), and reinforcement learning. Application of these architectures to computer vision, time series, security, natural language processing (NLP), and data generation will be covered. High-Performance Computing (HPC) aspects will demonstrate how deep learning can be leveraged both on graphical processing units (GPUs), as well as grids. Focus is primarily upon the application of deep learning to problems, with some introduction to mathematical foundations. Readers will use the Python programming language to implement deep learning using Google TensorFlow and Keras. It is not necessary to know Python prior to this book; however, familiarity with at least one programming language is assumed.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,I.2},
  file = {/home/skynet3/Zotero/storage/YL9WC4US/Heaton - 2022 - Applications of Deep Neural Networks with Keras.pdf;/home/skynet3/Zotero/storage/LIIC2IA2/2009.html}
}

@article{heesenPeerReviewGood2021,
  title = {Is {{Peer Review}} a {{Good Idea}}?},
  author = {Heesen, Remco and Bright, Liam Kofi},
  date = {2021-09},
  journaltitle = {The British Journal for the Philosophy of Science},
  volume = {72},
  number = {3},
  pages = {635--663},
  publisher = {{The University of Chicago Press}},
  issn = {0007-0882},
  doi = {10.1093/bjps/axz029},
  url = {https://www.journals.uchicago.edu/doi/10.1093/bjps/axz029},
  urldate = {2022-11-11},
  abstract = {Prepublication peer review should be abolished. We consider the effects that such a change will have on the social structure of science, paying particular attention to the changed incentive structure and the likely effects on the behaviour of individual scientists. We evaluate these changes from the perspective of epistemic consequentialism. We find that where the effects of abolishing prepublication peer review can be evaluated with a reasonable level of confidence based on presently available evidence, they are either positive or neutral. We conclude that on present evidence abolishing peer review weakly dominates the status quo.},
  file = {/home/skynet3/Zotero/storage/IGI4KDIT/Heesen and Bright - 2021 - Is Peer Review a Good Idea.pdf}
}

@online{heissGuideWorkingCountryyear2021,
  title = {A Guide to Working with Country-Year Panel Data and {{Bayesian}} Multilevel Models},
  author = {Heiss, Andrew},
  date = {2021-12-01T00:00:00+00:00},
  url = {https://www.andrewheiss.com/blog/2021/12/01/multilevel-models-panel-data-guide/},
  urldate = {2022-11-11},
  abstract = {How to use multilevel models with R and brms to work with country-year panel data.},
  langid = {english},
  file = {/home/skynet3/Zotero/storage/HUGMUBLJ/multilevel-models-panel-data-guide.html}
}

@misc{heistKnowledgeGraphsWeb2020,
  title = {Knowledge {{Graphs}} on the {{Web}} -- an {{Overview}}},
  author = {Heist, Nicolas and Hertling, Sven and Ringler, Daniel and Paulheim, Heiko},
  date = {2020-03-12},
  number = {arXiv:2003.00719},
  eprint = {2003.00719},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2003.00719},
  urldate = {2022-11-11},
  abstract = {Knowledge Graphs are an emerging form of knowledge representation. While Google coined the term Knowledge Graph first and promoted it as a means to improve their search results, they are used in many applications today. In a knowledge graph, entities in the real world and/or a business domain (e.g., people, places, or events) are represented as nodes, which are connected by edges representing the relations between those entities. While companies such as Google, Microsoft, and Facebook have their own, non-public knowledge graphs, there is also a larger body of publicly available knowledge graphs, such as DBpedia or Wikidata. In this chapter, we provide an overview and comparison of those publicly available knowledge graphs, and give insights into their contents, size, coverage, and overlap.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Databases},
  file = {/home/skynet3/Zotero/storage/R2KJDTMH/Heist et al. - 2020 - Knowledge Graphs on the Web -- an Overview.pdf;/home/skynet3/Zotero/storage/339JC76G/2003.html}
}

@article{heldAssessmentIntrinsicCredibility2019,
  title = {The {{Assessment}} of {{Intrinsic Credibility}} and a {{New Argument}} for P{$<$}0.005},
  author = {Held, Leonhard},
  date = {2019-03},
  journaltitle = {Royal Society Open Science},
  shortjournal = {R. Soc. open sci.},
  volume = {6},
  number = {3},
  eprint = {1803.10052},
  eprinttype = {arxiv},
  primaryclass = {stat},
  pages = {181534},
  issn = {2054-5703},
  doi = {10.1098/rsos.181534},
  url = {http://arxiv.org/abs/1803.10052},
  urldate = {2022-11-11},
  abstract = {The concept of intrinsic credibility has been recently introduced to check the credibility of "out of the blue" findings without any prior support. A significant result is deemed intrinsically credible if it is in conflict with a sceptical prior derived from the very same data that would make the effect non-significant. In this paper I propose to use Bayesian prior-predictive tail probabilities to assess intrinsic credibility. For the standard 5\% significance level, this leads to a new p-value threshold that is remarkably close to the recently proposed p{$<$}0.005 standard. I also introduce the credibility ratio, the ratio of the upper to the lower limit of a standard confidence interval for the corresponding effect size. I show that the credibility ratio has to be smaller than 5.8 such that a significant finding is also intrinsically credible. Finally, a p-value for intrinsic credibility is proposed that is a simple function of the ordinary p-value and has a direct frequentist interpretation in terms of the probability of replicating an effect.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Methodology},
  file = {/home/skynet3/Zotero/storage/EKBY3Z4I/Held - 2019 - The Assessment of Intrinsic Credibility and a New .pdf;/home/skynet3/Zotero/storage/A63DRW8B/1803.html}
}

@article{helskeEstimationCausalEffects2021,
  ids = {helskeEstimationCausalEffects2021a},
  title = {Estimation of Causal Effects with Small Data in the Presence of Trapdoor Variables},
  author = {Helske, Jouni and Tikka, Santtu and Karvanen, Juha},
  date = {2021-07},
  journaltitle = {Journal of the Royal Statistical Society: Series A (Statistics in Society)},
  shortjournal = {J. R. Stat. Soc. A},
  volume = {184},
  number = {3},
  eprint = {2003.03187},
  eprinttype = {arxiv},
  primaryclass = {stat},
  pages = {1030--1051},
  issn = {0964-1998, 1467-985X},
  doi = {10.1111/rssa.12699},
  url = {http://arxiv.org/abs/2003.03187},
  urldate = {2022-11-11},
  abstract = {We consider the problem of estimating causal effects of interventions from observational data when well-known back-door and front-door adjustments are not applicable. We show that when an identifiable causal effect is subject to an implicit functional constraint that is not deducible from conditional independence relations, the estimator of the causal effect can exhibit bias in small samples. This bias is related to variables that we call trapdoor variables. We use simulated data to study different strategies to account for trapdoor variables and suggest how the related trapdoor bias might be minimized. The importance of trapdoor variables in causal effect estimation is illustrated with real data from the Life Course 1971-2002 study. Using this dataset, we estimate the causal effect of education on income in the Finnish context. Bayesian modelling allows us to take the parameter uncertainty into account and to present the estimated causal effects as posterior distributions.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Computation,Statistics - Methodology},
  file = {/home/skynet3/Zotero/storage/TBDJXUK9/Helske et al. - 2021 - Estimation of causal effects with small data in th.pdf;/home/skynet3/Zotero/storage/NTIJUL72/2003.html}
}

@misc{hengReEvaluatingStrengthenedIVDesigns2021,
  title = {Re-{{Evaluating Strengthened-IV Designs}}: {{Asymptotic Efficiency}}, {{Bias Formula}}, and the {{Validity}} and {{Power}} of {{Sensitivity Analyses}}},
  shorttitle = {Re-{{Evaluating Strengthened-IV Designs}}},
  author = {Heng, Siyu and Zhang, Bo and Han, Xu and Lorch, Scott A. and Small, Dylan S.},
  date = {2021-10-15},
  number = {arXiv:1911.09171},
  eprint = {1911.09171},
  eprinttype = {arxiv},
  primaryclass = {stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1911.09171},
  url = {http://arxiv.org/abs/1911.09171},
  urldate = {2022-11-11},
  abstract = {Instrumental variables (IVs) are extensively used to estimate treatment effects when the treatment and outcome are confounded by unmeasured confounders; however, weak IVs are often encountered in empirical studies and may cause problems. Many studies have considered building a stronger IV from the original, possibly weak, IV in the design stage of a matched study at the cost of not using some of the samples in the analysis. It is widely accepted that strengthening an IV tends to render nonparametric tests more powerful and will increase the power of sensitivity analyses in large samples. In this article, we re-evaluate this conventional wisdom to bring new insights into this topic. We consider matched observational studies from three perspectives. First, we evaluate the trade-off between IV strength and sample size on nonparametric tests assuming the IV is valid and exhibit conditions under which strengthening an IV increases power and conversely conditions under which it decreases power. Second, we derive a necessary condition for a valid sensitivity analysis model with continuous doses. We show that the \$\textbackslash Gamma\$ sensitivity analysis model, which has been previously used to come to the conclusion that strengthening an IV increases the power of sensitivity analyses in large samples, does not apply to the continuous IV setting and thus this previously reached conclusion may be invalid. Third, we quantify the bias of the Wald estimator with a possibly invalid IV under an oracle and leverage it to develop a valid sensitivity analysis framework; under this framework, we show that strengthening an IV may amplify or mitigate the bias of the estimator, and may or may not increase the power of sensitivity analyses. We also discuss how to better adjust for the observed covariates when building an IV in matched studies.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Applications,Statistics - Methodology},
  file = {/home/skynet3/Zotero/storage/H5ERV9NQ/Heng et al. - 2021 - Re-Evaluating Strengthened-IV Designs Asymptotic .pdf;/home/skynet3/Zotero/storage/HRASKHXQ/1911.html}
}

@article{hernanDoesObesityShorten2008,
  title = {Does Obesity Shorten Life? {{The}} Importance of Well-Defined Interventions to Answer Causal Questions},
  shorttitle = {Does Obesity Shorten Life?},
  author = {Hernán, M. A. and Taubman, S. L.},
  date = {2008-08},
  journaltitle = {International Journal of Obesity},
  shortjournal = {Int J Obes},
  volume = {32},
  number = {3},
  pages = {S8-S14},
  publisher = {{Nature Publishing Group}},
  issn = {1476-5497},
  doi = {10.1038/ijo.2008.82},
  url = {https://www.nature.com/articles/ijo200882},
  urldate = {2022-11-15},
  abstract = {Many observational studies have estimated a strong effect of obesity on mortality. In this paper, we explicitly define the causal question that is asked by these studies and discuss the problems associated with it. We argue that observational studies of obesity and mortality violate the condition of consistency of counterfactual (potential) outcomes, a necessary condition for meaningful causal inference, because (1) they do not explicitly specify the interventions on body mass index (BMI) that are being compared and (2) different methods to modify BMI may lead to different counterfactual mortality outcomes, even if they lead to the same BMI value in a given person. Besides precluding the estimation of unambiguous causal effects, this violation of consistency affects the ability to address two additional conditions that are also necessary for causal inference: exchangeability and positivity. We conclude that consistency violations not only preclude the estimation of well-defined causal effects but also compromise our ability to estimate ill-defined causal effects.},
  issue = {3},
  langid = {english},
  keywords = {Epidemiology,general,Health Promotion and Disease Prevention,Internal Medicine,Medicine/Public Health,Metabolic Diseases,Public Health},
  file = {/home/skynet3/Zotero/storage/B67Q5IBI/Hernán and Taubman - 2008 - Does obesity shorten life The importance of well-.pdf;/home/skynet3/Zotero/storage/WYDPLBNW/ijo200882.html}
}

@book{hesterLetGitStarted,
  title = {Let’s {{Git}} Started | {{Happy Git}} and {{GitHub}} for the {{useR}}},
  author = {Hester, the STAT 545 TAs, Jim, Jenny Bryan},
  url = {https://happygitwithr.com/},
  urldate = {2022-11-11},
  abstract = {Using Git and GitHub with R, Rstudio, and R Markdown},
  langid = {english}
}

@online{HeuristicsToolsBased,
  title = {Heuristics {{Tools Based}} on {{Mutual Information}} for {{Variable Ranking}}},
  url = {https://www.math.uzh.ch/pages/varrank/index.html},
  urldate = {2022-11-11},
  abstract = {A computational toolbox of heuristics approaches for performing variable ranking and feature selection based on mutual information well adapted for multivariate system epidemiology datasets. The core function is a general implementation of the minimum redundancy maximum relevance model. R. Battiti (1994) {$<$}doi:10.1109/72.298224{$>$}. Continuous variables are discretized using a large choice of rule. Variables ranking can be learned with a sequential forward/backward search algorithm. The two main problems that can be addressed by this package is the selection of the most representative variable within a group of variables of interest (i.e. dimension reduction) and variable ranking with respect to a set of features of interest.},
  langid = {english}
}

@article{hillAcquiescenceBiasInflates,
  ids = {hillAcquiescenceBiasInflatesa},
  title = {Acquiescence {{Bias Inﬂates Estimates}} of {{Conspiratorial Beliefs}} and {{Political Misperceptions}}},
  author = {Hill, Seth J and Roberts, Margaret E},
  pages = {61},
  abstract = {Citizen beliefs about the political world interest scholars, pundits, and politicians. Political scientists use opinion surveys to study beliefs about facts, such the current unemployment rate, and more conspiratorial beliefs, such as whether Barack Obama was born abroad. We find that these studies often ignore acquiescence-response bias, the tendency for survey respondents to endorse any assertion made in a survey question regardless of content. Replicating recent scholarship we show that acquiescence bias inflates estimated incidence of conspiratorial beliefs and political misperceptions among survey respondents in the U.S. and China by up to 50\%. Acquiescence bias is disproportionately prevalent among more ideological respondents, inflating correlations between political ideology such as conservatism and endorsement of conspiracies or misperception of facts. We conclude that more attention ought to be paid to survey measurement, and propose and demonstrate two methods to correct for acquiescence bias.},
  langid = {english},
  file = {/home/skynet3/Zotero/storage/MK4DLMQ9/Hill and Roberts - Acquiescence Bias Inﬂates Estimates of Conspirator.pdf}
}

@article{hillBayesianAdditiveRegression2020,
  ids = {hillBayesianAdditiveRegression2020a},
  title = {Bayesian {{Additive Regression Trees}}: {{A Review}} and {{Look Forward}}},
  shorttitle = {Bayesian {{Additive Regression Trees}}},
  author = {Hill, Jennifer and Linero, Antonio and Murray, Jared},
  date = {2020},
  journaltitle = {Annual Review of Statistics and Its Application},
  volume = {7},
  number = {1},
  pages = {251--278},
  doi = {10.1146/annurev-statistics-031219-041110},
  url = {https://doi.org/10.1146/annurev-statistics-031219-041110},
  urldate = {2022-11-11},
  abstract = {Bayesian additive regression trees (BART) provides a flexible approach to fitting a variety of regression models while avoiding strong parametric assumptions. The sum-of-trees model is embedded in a Bayesian inferential framework to support uncertainty quantification and provide a principled approach to regularization through prior specification. This article presents the basic approach and discusses further development of the original algorithm that supports a variety of data structures and assumptions. We describe augmentations of the prior specification to accommodate higher dimensional data and smoother functions. Recent theoretical developments provide justifications for the performance observed in simulations and other settings. Use of BART in causal inference provides an additional avenue for extensions and applications. We discuss software options as well as challenges and future directions.},
  keywords = {Bayesian nonparametrics,causal inference,machine learning,regression,regularization},
  annotation = {\_eprint: https://doi.org/10.1146/annurev-statistics-031219-041110}
}

@online{HistoryScientificJournals,
  title = {A {{History}} of {{Scientific Journals}}},
  url = {https://www.uclpress.co.uk/products/187262},
  urldate = {2022-11-11},
  abstract = {Modern scientific research has changed so much since Isaac Newton’s day: it is more professional, collaborative and international, with more complicated equipment and a more diverse community of researchers. Yet the use of scientific journals to report, share and store results is a thread that runs through the history},
  langid = {english},
  organization = {{UCL Press}},
  file = {/home/skynet3/Zotero/storage/UZRML5L6/187262.html}
}

@online{HmsdbmiUpSetRImplementation,
  title = {Hms-Dbmi/{{UpSetR}}: {{An R}} Implementation of the {{UpSet}} Set Visualization Technique Published by {{Lex}}, {{Gehlenborg}}, et Al..},
  shorttitle = {Hms-Dbmi/{{UpSetR}}},
  url = {https://github.com/hms-dbmi/UpSetR},
  urldate = {2022-11-11},
  abstract = {An R implementation of the UpSet set visualization technique published by Lex, Gehlenborg, et al..   - hms-dbmi/UpSetR: An R implementation of the UpSet set visualization technique published by Lex...},
  langid = {english},
  organization = {{GitHub}},
  file = {/home/skynet3/Zotero/storage/KT2B2BJI/UpSetR.html}
}

@misc{hoangRandomizedPvaluesMultiple2020,
  title = {Randomized P-Values for Multiple Testing and Their Application in Replicability Analysis},
  author = {Hoang, Anh-Tuan and Dickhaus, Thorsten},
  date = {2020-02-25},
  number = {arXiv:1912.06982},
  eprint = {1912.06982},
  eprinttype = {arxiv},
  primaryclass = {stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1912.06982},
  url = {http://arxiv.org/abs/1912.06982},
  urldate = {2022-11-11},
  abstract = {We are concerned with testing replicability hypotheses for many endpoints simultaneously. This constitutes a multiple test problem with composite null hypotheses. Traditional \$p\$-values, which are computed under least favourable parameter configurations, are over-conservative in the case of composite null hypotheses. As demonstrated in prior work, this poses severe challenges in the multiple testing context, especially when one goal of the statistical analysis is to estimate the proportion \$\textbackslash pi\_0\$ of true null hypotheses. Randomized \$p\$-values have been proposed to remedy this issue. In the present work, we discuss the application of randomized \$p\$-values in replicability analysis. In particular, we introduce a general class of statistical models for which valid, randomized \$p\$-values can be calculated easily. By means of computer simulations, we demonstrate that their usage typically leads to a much more accurate estimation of \$\textbackslash pi\_0\$. Finally, we apply our proposed methodology to a real data example from genomics.},
  archiveprefix = {arXiv},
  keywords = {62J15; 62G30,Statistics - Methodology},
  file = {/home/skynet3/Zotero/storage/HYFKPE4L/Hoang and Dickhaus - 2020 - Randomized p-values for multiple testing and their.pdf;/home/skynet3/Zotero/storage/JIGCYBBG/1912.html}
}

@misc{hodgesStatisticalMethodsResearch2019,
  ids = {hodgesStatisticalMethodsResearch2019a,hodgesStatisticalMethodsResearch2019b},
  title = {Statistical Methods Research Done as Science Rather than Mathematics},
  author = {Hodges, James S.},
  date = {2019-05-20},
  number = {arXiv:1905.08381},
  eprint = {1905.08381},
  eprinttype = {arxiv},
  primaryclass = {stat},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/1905.08381},
  urldate = {2022-11-11},
  abstract = {This paper is about how we study statistical methods. As an example, it uses the random regressions model, in which the intercept and slope of cluster-specific regression lines are modeled as a bivariate random effect. Maximizing this model's restricted likelihood often gives a boundary value for the random effect correlation or variances. We argue that this is a problem; that it is a problem because our discipline has little understanding of how contemporary models and methods map data to inferential summaries; that we lack such understanding, even for models as simple as this, because of a near-exclusive reliance on mathematics as a means of understanding; and that math alone is no longer sufficient. We then argue that as a discipline, we can and should break open our black-box methods by mimicking the five steps that molecular biologists commonly use to break open Nature's black boxes: design a simple model system, formulate hypotheses using that system, test them in experiments on that system, iterate as needed to reformulate and test hypotheses, and finally test the results in an "in vivo" system. We demonstrate this by identifying conditions under which the random-regressions restricted likelihood is likely to be maximized at a boundary value. Resistance to this approach seems to arise from a view that it lacks the certainty or intellectual heft of mathematics, perhaps because simulation experiments in our literature rarely do more than measure a new method's operating characteristics in a small range of situations. We argue that such work can make useful contributions including, as in molecular biology, the findings themselves and sometimes the designs used in the five steps; that these contributions have as much practical value as mathematical results; and that therefore they merit publication as much as the mathematical results our discipline esteems so highly.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Other Statistics},
  file = {/home/skynet3/Zotero/storage/49QJ5HBC/Hodges - 2019 - Statistical methods research done as science rathe.pdf;/home/skynet3/Zotero/storage/DRIS3IWV/1905.html;/home/skynet3/Zotero/storage/UBNACCQJ/1905.html}
}

@article{hoekstraRobustMisinterpretationConfidence2014,
  title = {Robust Misinterpretation of Confidence Intervals},
  author = {Hoekstra, Rink and Morey, Richard D. and Rouder, Jeffrey N. and Wagenmakers, Eric-Jan},
  date = {2014-10},
  journaltitle = {Psychonomic Bulletin \& Review},
  shortjournal = {Psychon Bull Rev},
  volume = {21},
  number = {5},
  pages = {1157--1164},
  issn = {1069-9384, 1531-5320},
  doi = {10.3758/s13423-013-0572-3},
  url = {https://link.springer.com/10.3758/s13423-013-0572-3},
  urldate = {2022-11-11},
  abstract = {Null hypothesis significance testing (NHST) is undoubtedly the most common inferential technique used to justify claims in the social sciences. However, even staunch defenders of NHST agree that its outcomes are often misinterpreted. Confidence intervals (CIs) have frequently been proposed as a more useful alternative to NHST, and their use is strongly encouraged in the APA Manual. Nevertheless, little is known about how researchers interpret CIs. In this study, 120 researchers and 442 students—all in the field of psychology—were asked to assess the truth value of six particular statements involving different interpretations of a CI. Although all six statements were false, both researchers and students endorsed, on average, more than three statements, indicating a gross misunderstanding of CIs. Self-declared experience with statistics was not related to researchers’ performance, and, even more surprisingly, researchers hardly outperformed the students, even though the students had not received any education on statistical inference whatsoever. Our findings suggest that many researchers do not know the correct interpretation of a CI. The misunderstandings surrounding pvalues and CIs are particularly unfortunate because they constitute the main tools by which psychologists draw conclusions from data.},
  langid = {english},
  file = {/home/skynet3/Zotero/storage/HUYDQVQY/Hoekstra et al. - 2014 - Robust misinterpretation of confidence intervals.pdf}
}

@article{hoganKnowledgeGraphs2022,
  ids = {hoganKnowledgeGraphs2022a},
  title = {Knowledge {{Graphs}}},
  author = {Hogan, Aidan and Blomqvist, Eva and Cochez, Michael and d' Amato, Claudia and de Melo, Gerard and Gutierrez, Claudio and Gayo, José Emilio Labra and Kirrane, Sabrina and Neumaier, Sebastian and Polleres, Axel and Navigli, Roberto and Ngomo, Axel-Cyrille Ngonga and Rashid, Sabbir M. and Rula, Anisa and Schmelzeisen, Lukas and Sequeda, Juan and Staab, Steffen and Zimmermann, Antoine},
  options = {useprefix=true},
  date = {2022-05-31},
  journaltitle = {ACM Computing Surveys},
  shortjournal = {ACM Comput. Surv.},
  volume = {54},
  number = {4},
  eprint = {2003.02320},
  eprinttype = {arxiv},
  primaryclass = {cs},
  pages = {1--37},
  issn = {0360-0300, 1557-7341},
  doi = {10.1145/3447772},
  url = {http://arxiv.org/abs/2003.02320},
  urldate = {2022-11-11},
  abstract = {In this paper we provide a comprehensive introduction to knowledge graphs, which have recently garnered significant attention from both industry and academia in scenarios that require exploiting diverse, dynamic, large-scale collections of data. After some opening remarks, we motivate and contrast various graph-based data models and query languages that are used for knowledge graphs. We discuss the roles of schema, identity, and context in knowledge graphs. We explain how knowledge can be represented and extracted using a combination of deductive and inductive techniques. We summarise methods for the creation, enrichment, quality assessment, refinement, and publication of knowledge graphs. We provide an overview of prominent open knowledge graphs and enterprise knowledge graphs, their applications, and how they use the aforementioned techniques. We conclude with high-level future research directions for knowledge graphs.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Databases,Computer Science - Machine Learning},
  file = {/home/skynet3/Zotero/storage/A6EQN7RB/Hogan et al. - 2022 - Knowledge Graphs.pdf;/home/skynet3/Zotero/storage/AXISKW9D/2003.html}
}

@online{hogervorstHowUseCatboost,
  title = {How to {{Use Catboost}} with {{Tidymodels}}},
  author = {Hogervorst, Roel M.},
  url = {https://blog.rmhogervorst.nl/blog/2020/08/28/how-to-use-catboost-with-tidymodels/},
  urldate = {2022-11-11},
  abstract = {Treesnip standardizes everything},
  langid = {english},
  organization = {{Roel's R-tefacts}},
  file = {/home/skynet3/Zotero/storage/GDEZWIZK/how-to-use-catboost-with-tidymodels.html}
}

@online{Home,
  title = {Home},
  url = {https://techdevguide.withgoogle.com/},
  urldate = {2022-11-11},
  abstract = {Whether you’re a student, an educator, or otherwise interested in software engineering, newer to computer science or a more experienced coder, we hope there’s something for you here in Google’s Guide to Technical Development.},
  langid = {english},
  organization = {{Google Tech Dev Guide}}
}

@online{HomeColahBlog,
  title = {Home - Colah's Blog},
  url = {http://colah.github.io/},
  urldate = {2022-11-11}
}

@online{HomeFtfyFixes,
  title = {Home - Ftfy: Fixes Text for You},
  url = {https://ftfy.readthedocs.io/en/latest/},
  urldate = {2022-11-11},
  file = {/home/skynet3/Zotero/storage/2TB9ADUG/latest.html}
}

@online{Homepage,
  title = {Homepage},
  url = {https://codeocean.com/},
  urldate = {2022-11-11},
  abstract = {A Computational Research Platform for faster, collaborative, quality discoveries: automated developer tools, computing resources, preservation, collaboration and sharing.},
  langid = {american},
  organization = {{Code Ocean}}
}

@online{HomeSimsemSemTools,
  ids = {HomeSimsemSemToolsa},
  title = {Home · Simsem/{{semTools Wiki}}},
  url = {https://github.com/simsem/semTools},
  urldate = {2022-11-11},
  abstract = {Useful tools for structural equation modeling. Contribute to simsem/semTools development by creating an account on GitHub.},
  langid = {english},
  organization = {{GitHub}},
  file = {/home/skynet3/Zotero/storage/P4GBRHDH/wiki.html}
}

@article{hongAccuracyRandomforestbasedImputation2020,
  ids = {hongAccuracyRandomforestbasedImputation2020a},
  title = {Accuracy of Random-Forest-Based Imputation of Missing Data in the Presence of Non-Normality, Non-Linearity, and Interaction},
  author = {Hong, Shangzhi and Lynn, Henry S.},
  date = {2020-07-25},
  journaltitle = {BMC Medical Research Methodology},
  shortjournal = {BMC Medical Research Methodology},
  volume = {20},
  number = {1},
  pages = {199},
  issn = {1471-2288},
  doi = {10.1186/s12874-020-01080-1},
  url = {https://doi.org/10.1186/s12874-020-01080-1},
  urldate = {2022-11-11},
  abstract = {Missing data are common in statistical analyses, and imputation methods based on random forests (RF) are becoming popular for handling missing data especially in biomedical research. Unlike standard imputation approaches, RF-based imputation methods do not assume normality or require specification of parametric models. However, it is still inconclusive how they perform for non-normally distributed data or when there are non-linear relationships or interactions.},
  keywords = {Imputation accuracy,Missing data imputation,Random forest},
  file = {/home/skynet3/Zotero/storage/7PETQBR6/Hong and Lynn - 2020 - Accuracy of random-forest-based imputation of miss.pdf;/home/skynet3/Zotero/storage/J6K29MMM/s12874-020-01080-1.html}
}

@misc{hongOverfittingPostselectionUncertainty2017,
  title = {On Overfitting and Post-Selection Uncertainty Assessments},
  author = {Hong, Liang and Kuffner, Todd A. and Martin, Ryan},
  date = {2017-12-06},
  number = {arXiv:1712.02379},
  eprint = {1712.02379},
  eprinttype = {arxiv},
  primaryclass = {math, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1712.02379},
  url = {http://arxiv.org/abs/1712.02379},
  urldate = {2022-11-11},
  abstract = {In a regression context, when the relevant subset of explanatory variables is uncertain, it is common to use a data-driven model selection procedure. Classical linear model theory, applied naively to the selected sub-model, may not be valid because it ignores the selected sub-model's dependence on the data. We provide an explanation of this phenomenon, in terms of overfitting, for a class of model selection criteria.},
  archiveprefix = {arXiv},
  keywords = {62F99,Mathematics - Statistics Theory,Statistics - Applications,Statistics - Methodology},
  file = {/home/skynet3/Zotero/storage/8NVYWFE2/Hong et al. - 2017 - On overfitting and post-selection uncertainty asse.pdf;/home/skynet3/Zotero/storage/YBKFZB8I/1712.html}
}

@online{HopfieldNetworksAll2020,
  title = {Hopfield {{Networks}} Is {{All You Need}}},
  date = {2020-08-25T07:03:39+00:00},
  url = {https://ml-jku.github.io/hopfield-layers/},
  urldate = {2022-11-11},
  abstract = {Blog post},
  langid = {american},
  organization = {{hopfield-layers}},
  file = {/home/skynet3/Zotero/storage/SBHTLL6R/hopfield-layers.html}
}

@article{horbachAbilityDifferentPeer2019,
  ids = {horbachAbilityDifferentPeer2019a},
  title = {The Ability of Different Peer Review Procedures to Flag Problematic Publications},
  author = {Horbach, S. P. J. M. and Halffman, W.},
  date = {2019-01-01},
  journaltitle = {Scientometrics},
  shortjournal = {Scientometrics},
  volume = {118},
  number = {1},
  pages = {339--373},
  issn = {1588-2861},
  doi = {10.1007/s11192-018-2969-2},
  url = {https://doi.org/10.1007/s11192-018-2969-2},
  urldate = {2022-11-11},
  abstract = {There is a mounting worry about erroneous and outright fraudulent research that gets published in the scientific literature. Although peer review’s ability to filter out such publications is contentious, several peer review innovations attempt to do just that. However, there is very little systematic evidence documenting the ability of different review procedures to flag problematic publications. In this article, we use survey data on peer review in a wide range of journals to compare the retraction rates of specific review procedures, using the Retraction Watch database. We were able to identify which peer review procedures were used since 2000 for 361 journals, publishing a total of 833,172 articles, of which 670 were retracted. After addressing the dual character of retractions, signalling both a failure to identify problems prior to publication, but also the willingness to correct mistakes, we empirically assess review procedures. With considerable conceptual caveats, we were able to identify peer review procedures that seem able to detect problematic research better than others. Results were verified for disciplinary differences and variation between reasons for retraction. This leads to informed recommendations for journal editors about strengths and weaknesses of specific peer review procedures, allowing them to select review procedures that address issues most relevant to their field.},
  langid = {english},
  keywords = {Peer review,Research integrity,Retractions,Scientific publishing},
  file = {/home/skynet3/Zotero/storage/63UAJZ7K/Horbach and Halffman - 2019 - The ability of different peer review procedures to.pdf}
}

@article{horraceResultsBiasInconsistency2006,
  title = {Results on the Bias and Inconsistency of Ordinary Least Squares for the Linear Probability Model},
  author = {Horrace, William C. and Oaxaca, Ronald L.},
  date = {2006-03-01},
  journaltitle = {Economics Letters},
  shortjournal = {Economics Letters},
  volume = {90},
  number = {3},
  pages = {321--327},
  issn = {0165-1765},
  doi = {10.1016/j.econlet.2005.08.024},
  url = {https://www.sciencedirect.com/science/article/pii/S0165176505003150},
  urldate = {2022-11-14},
  abstract = {This note formalizes bias and inconsistency results for ordinary least squares (OLS) on the linear probability model and provides sufficient conditions for unbiasedness and consistency to hold. The conditions suggest that a “trimming estimator” may reduce OLS bias.},
  langid = {english},
  keywords = {Consistency,LPM,OLS,Unbiased},
  file = {/home/skynet3/Zotero/storage/JY94A85G/Horrace and Oaxaca - 2006 - Results on the bias and inconsistency of ordinary .pdf;/home/skynet3/Zotero/storage/4QU5R76T/S0165176505003150.html}
}

@article{howardTimeuniformNonparametricNonasymptotic2021,
  title = {Time-Uniform, Nonparametric, Nonasymptotic Confidence Sequences},
  author = {Howard, Steven R. and Ramdas, Aaditya and McAuliffe, Jon and Sekhon, Jasjeet},
  date = {2021-04-01},
  journaltitle = {The Annals of Statistics},
  shortjournal = {Ann. Statist.},
  volume = {49},
  number = {2},
  eprint = {1810.08240},
  eprinttype = {arxiv},
  primaryclass = {math, stat},
  issn = {0090-5364},
  doi = {10.1214/20-AOS1991},
  url = {http://arxiv.org/abs/1810.08240},
  urldate = {2022-11-11},
  abstract = {A confidence sequence is a sequence of confidence intervals that is uniformly valid over an unbounded time horizon. Our work develops confidence sequences whose widths go to zero, with nonasymptotic coverage guarantees under nonparametric conditions. We draw connections between the Cram\textbackslash 'er-Chernoff method for exponential concentration, the law of the iterated logarithm (LIL), and the sequential probability ratio test -- our confidence sequences are time-uniform extensions of the first; provide tight, nonasymptotic characterizations of the second; and generalize the third to nonparametric settings, including sub-Gaussian and Bernstein conditions, self-normalized processes, and matrix martingales. We illustrate the generality of our proof techniques by deriving an empirical-Bernstein bound growing at a LIL rate, as well as a novel upper LIL for the maximum eigenvalue of a sum of random matrices. Finally, we apply our methods to covariance matrix estimation and to estimation of sample average treatment effect under the Neyman-Rubin potential outcomes model.},
  archiveprefix = {arXiv},
  keywords = {Mathematics - Probability,Mathematics - Statistics Theory,Statistics - Methodology},
  file = {/home/skynet3/Zotero/storage/MGW7RUEJ/Howard et al. - 2021 - Time-uniform, nonparametric, nonasymptotic confide.pdf;/home/skynet3/Zotero/storage/TB3ZSBGP/1810.html}
}

@inproceedings{howeDeformablePartModels2019,
  title = {Deformable {{Part Models}} for {{Automatically Georeferencing Historical Map Images}}},
  booktitle = {Proceedings of the 27th {{ACM SIGSPATIAL International Conference}} on {{Advances}} in {{Geographic Information Systems}}},
  author = {Howe, Nicholas R. and Weinman, Jerod and Gouwar, John and Shamji, Aabid},
  date = {2019-11-05},
  pages = {540--543},
  publisher = {{ACM}},
  location = {{Chicago IL USA}},
  doi = {10.1145/3347146.3359367},
  url = {https://dl.acm.org/doi/10.1145/3347146.3359367},
  urldate = {2022-11-11},
  eventtitle = {{{SIGSPATIAL}} '19: 27th {{ACM SIGSPATIAL International Conference}} on {{Advances}} in {{Geographic Information Systems}}},
  isbn = {978-1-4503-6909-1},
  langid = {english},
  file = {/home/skynet3/Zotero/storage/T3ZPJB8H/Howe et al. - 2019 - Deformable Part Models for Automatically Georefere.pdf}
}

@online{HowInstallPostgreSQL,
  title = {How {{To Install PostgreSQL}} on {{Ubuntu}} 20.04 [{{Quickstart}}] | {{DigitalOcean}}},
  url = {https://www.digitalocean.com/community/tutorials/how-to-install-postgresql-on-ubuntu-20-04-quickstart},
  urldate = {2022-11-11},
  abstract = {PostgreSQL, or Postgres, is a relational database management system that provides an implementation of the SQL querying language. This quickstart guide demon…},
  langid = {english}
}

@software{HowLimeExplains,
  title = {How Lime Explains Stuff},
  url = {https://cran.r-project.org/web/packages/lime/vignettes/Understanding_lime.html},
  urldate = {2022-11-11},
  abstract = {In order to be able to understand the explanations produced by lime it is necessary to have at least some knowledge of how these explanations are achieved. To this end, you are encouraged to read through the article that introduced the lime framework as well as the additional resources linked to from the original Python repository. This vignette will provide an overview to allow you to get up to speed on the framework and let you efficiently understand the output it produces.}
}

@online{HowRegularizationWorks,
  title = {How {{Regularization Works}}},
  url = {https://e2eml.school/regularization.html},
  urldate = {2022-11-11},
  file = {/home/skynet3/Zotero/storage/AHAT6M35/regularization.html}
}

@online{HowSpeedXGBoost,
  title = {How to {{Speed Up XGBoost Model Training}}},
  url = {https://www.anyscale.com/blog/how-to-speed-up-xgboost-model-training},
  urldate = {2022-11-11},
  abstract = {XGBoost is an open-source implementation of gradient boosting designed for speed and performance. However, even XGBoost training can sometimes be slow. This post reviews some approaches for accelerating this process like changing tree construction method, leveraging cloud computing, distributed XGBoost on Ray.},
  langid = {american},
  organization = {{Anyscale}},
  file = {/home/skynet3/Zotero/storage/6CUIQJ8H/how-to-speed-up-xgboost-model-training.html}
}

@misc{hoyleAutomatedTopicModel2021,
  title = {Is {{Automated Topic Model Evaluation Broken}}?: {{The Incoherence}} of {{Coherence}}},
  shorttitle = {Is {{Automated Topic Model Evaluation Broken}}?},
  author = {Hoyle, Alexander and Goel, Pranav and Peskov, Denis and Hian-Cheong, Andrew and Boyd-Graber, Jordan and Resnik, Philip},
  date = {2021-10-27},
  number = {arXiv:2107.02173},
  eprint = {2107.02173},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2107.02173},
  url = {http://arxiv.org/abs/2107.02173},
  urldate = {2022-11-11},
  abstract = {Topic model evaluation, like evaluation of other unsupervised methods, can be contentious. However, the field has coalesced around automated estimates of topic coherence, which rely on the frequency of word co-occurrences in a reference corpus. Contemporary neural topic models surpass classical ones according to these metrics. At the same time, topic model evaluation suffers from a validation gap: automated coherence, developed for classical models, has not been validated using human experimentation for neural models. In addition, a meta-analysis of topic modeling literature reveals a substantial standardization gap in automated topic modeling benchmarks. To address the validation gap, we compare automated coherence with the two most widely accepted human judgment tasks: topic rating and word intrusion. To address the standardization gap, we systematically evaluate a dominant classical model and two state-of-the-art neural models on two commonly used datasets. Automated evaluations declare a winning model when corresponding human evaluations do not, calling into question the validity of fully automatic evaluations independent of human judgments.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/skynet3/Zotero/storage/WMIXL6GI/Hoyle et al. - 2021 - Is Automated Topic Model Evaluation Broken The I.pdf;/home/skynet3/Zotero/storage/TGAM26YR/2107.html}
}

@online{https://jakubnowosad.comGeocomputationGuideReproducible2022,
  ids = {https://jakubnowosad.comGeocomputationGuideReproducible2022a},
  title = {Geocomputation with {{R}}’s Guide to Reproducible Spatial Data Analysis},
  author = {{https://jakubnowosad.com}, Jakub Nowosad},
  date = {2022-08-30},
  url = {https://jakubnowosad.com/ogh2022/#/title-slide},
  urldate = {2022-11-11},
  langid = {english},
  file = {/home/skynet3/Zotero/storage/6BRA6ZSF/ogh2022.html}
}

@misc{hunermundCausalInferenceData2021,
  ids = {hunermundCausalInferenceData2021a},
  title = {Causal {{Inference}} and {{Data Fusion}} in {{Econometrics}}},
  author = {Hünermund, Paul and Bareinboim, Elias},
  date = {2021-03-10},
  number = {arXiv:1912.09104},
  eprint = {1912.09104},
  eprinttype = {arxiv},
  primaryclass = {econ},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1912.09104},
  url = {http://arxiv.org/abs/1912.09104},
  urldate = {2022-11-11},
  abstract = {Learning about cause and effect is arguably the main goal in applied econometrics. In practice, the validity of these causal inferences is contingent on a number of critical assumptions regarding the type of data that has been collected and the substantive knowledge that is available. For instance, unobserved confounding factors threaten the internal validity of estimates, data availability is often limited to non-random, selection-biased samples, causal effects need to be learned from surrogate experiments with imperfect compliance, and causal knowledge has to be extrapolated across structurally heterogeneous populations. A powerful causal inference framework is required to tackle these challenges, which plague most data analysis to varying degrees. Building on the structural approach to causality introduced by Haavelmo (1943) and the graph-theoretic framework proposed by Pearl (1995), the artificial intelligence (AI) literature has developed a wide array of techniques for causal learning that allow to leverage information from various imperfect, heterogeneous, and biased data sources (Bareinboim and Pearl, 2016). In this paper, we discuss recent advances in this literature that have the potential to contribute to econometric methodology along three dimensions. First, they provide a unified and comprehensive framework for causal inference, in which the aforementioned problems can be addressed in full generality. Second, due to their origin in AI, they come together with sound, efficient, and complete algorithmic criteria for automatization of the corresponding identification task. And third, because of the nonparametric description of structural models that graph-theoretic approaches build on, they combine the strengths of both structural econometrics as well as the potential outcomes framework, and thus offer an effective middle ground between these two literature streams.},
  archiveprefix = {arXiv},
  keywords = {Economics - Econometrics},
  file = {/home/skynet3/Zotero/storage/W2T6V9AT/Hünermund and Bareinboim - 2021 - Causal Inference and Data Fusion in Econometrics.pdf;/home/skynet3/Zotero/storage/KBVBT7EL/1912.html}
}

@book{huntington-kleinEffectIntroductionResearch2021,
  title = {The {{Effect}}: {{An Introduction}} to {{Research Design}} and {{Causality}}},
  shorttitle = {The {{Effect}}},
  author = {Huntington-Klein, Nick},
  date = {2021-12-20},
  publisher = {{Chapman and Hall/CRC}},
  location = {{Boca Raton}},
  doi = {10.1201/9781003226055},
  abstract = {The Effect: An Introduction to Research Design and Causality is about research design, specifically concerning research that uses observational data to make a causal inference. It is separated into two halves, each with different approaches to that subject. The first half goes through the concepts of causality, with very little in the way of estimation. It introduces the concept of identification thoroughly and clearly and discusses it as a process of trying to isolate variation that has a causal interpretation. Subjects include heavy emphasis on data-generating processes and causal diagrams. Concepts are demonstrated with a heavy emphasis on graphical intuition and the question of what we do to data. When we “add a control variable” what does that actually do? Key Features: ~• Extensive code examples in R, Stata, and Python • Chapters on overlooked topics in econometrics classes: heterogeneous treatment effects, simulation and power analysis, new cutting-edge methods, and uncomfortable ignored assumptions • An easy-to-read conversational tone • Up-to-date coverage of methods with fast-moving literatures like difference-in-differences},
  isbn = {978-1-00-322605-5},
  pagetotal = {646}
}

@article{huntington-kleinInfluenceHiddenResearcher2020,
  ids = {huntington-kleinInfluenceHiddenResearcher2020a,huntington-kleinInfluenceHiddenResearcher2021},
  title = {The {{Influence}} of {{Hidden Researcher Decisions}} in {{Applied Microeconomics}}},
  author = {Huntington-Klein, Nick and Arenas, Andreu and Beam, Emily and Bertoni, Marco and Bloem, Jeffrey R and Burli, Pralhad and Chen, Naibin and Greico, Paul and Ekpe, Godwin and Pugatch, Todd and Saavedra, Martin and Stopnitzky, Yaniv},
  date = {2020},
  pages = {42},
  langid = {english},
  keywords = {metascience,replication,research},
  file = {/home/skynet3/Zotero/storage/N523GK4J/Huntington-Klein et al. - 2021 - The influence of hidden researcher decisions in ap.pdf;/home/skynet3/Zotero/storage/Q92IHNNT/Huntington-Klein et al. - 2020 - The Influence of Hidden Researcher Decisions in Ap.pdf;/home/skynet3/Zotero/storage/LXZ26NK4/ecin.html}
}

@article{husseyMethodStreamlinePhacking2021,
  title = {A Method to Streamline P-Hacking},
  author = {Hussey, Ian},
  date = {2021-04-01},
  journaltitle = {Meta-Psychology},
  volume = {5},
  issn = {2003-2714},
  doi = {10.15626/MP.2020.2529},
  url = {https://open.lnu.se/index.php/metapsychology/article/view/2529},
  urldate = {2022-11-11},
  abstract = {The analytic strategy of p-hacking has rapidly accelerated the achievement of psychological scientists’ goals (e.g., publications \&amp; tenure), but has suffered a number of setbacks in recent years. In order to remediate this, this article presents a statistical inference measure that can greatly accelerate and streamline the p-hacking process: generating random numbers that are \&lt; .05. I refer to this approach as pointless. Results of a simulation study are presented and an R script is provided for others to use. In the absence of systemic changes to modal p-hacking practices within psychological science (e.g., worrying trends such as preregistration and replication), I argue that vast amounts of time and research funding could be saved through the widespread adoption of this innovative approach.},
  langid = {english},
  keywords = {NHST,p-hacking},
  file = {/home/skynet3/Zotero/storage/CZNNR9Q7/Hussey - 2021 - A method to streamline p-hacking.pdf}
}

@online{IBM67Billion,
  title = {Inside {{IBM}}'s \$67 Billion {{SAGE}}, the Largest Computer Ever Built - {{ExtremeTech}}},
  url = {https://www.extremetech.com/computing/151980-inside-ibms-67-billion-sage-the-largest-computer-ever-built},
  urldate = {2022-11-11}
}

@inreference{ID3Algorithm2022,
  ids = {ID3Algorithm2022a},
  title = {{{ID3}} Algorithm},
  booktitle = {Wikipedia},
  date = {2022-09-19T06:29:52Z},
  url = {https://en.wikipedia.org/w/index.php?title=ID3_algorithm&oldid=1111083600},
  urldate = {2022-11-11},
  abstract = {In decision tree learning, ID3 (Iterative Dichotomiser 3) is an algorithm invented by Ross Quinlan used to generate a decision tree from a dataset. ID3 is the precursor to the C4.5 algorithm, and is typically used in the machine learning and natural language processing domains.},
  langid = {english},
  annotation = {Page Version ID: 1111083600},
  file = {/home/skynet3/Zotero/storage/XIRUJZUA/ID3_algorithm.html}
}

@online{IdentificationMissingData2019,
  title = {Identification {{In Missing Data Models Represented By Directed Acyclic Graphs}}},
  date = {2019-06-29T17:17:52+00:00},
  url = {https://deepai.org/publication/identification-in-missing-data-models-represented-by-directed-acyclic-graphs},
  urldate = {2022-11-11},
  abstract = {06/29/19 - Missing data is a pervasive problem in data analyses, resulting in datasets that contain censored realizations of a target distrib...},
  organization = {{DeepAI}},
  file = {/home/skynet3/Zotero/storage/6N3IIZC5/identification-in-missing-data-models-represented-by-directed-acyclic-graphs.html}
}

@online{IdentityCrisis,
  title = {Identity {{Crisis}}},
  url = {https://betanalpha.github.io/assets/case_studies/identifiability.html},
  urldate = {2022-11-11}
}

@online{IgraphMartinBroadhurst,
  title = {Igraph | {{Martin Broadhurst}}},
  url = {http://www.martinbroadhurst.com/tag/igraph},
  urldate = {2022-11-11},
  langid = {american}
}

@article{iiMonteCarloStudy,
  ids = {iiMonteCarloStudya},
  title = {A {{Monte Carlo}} Study on Methods for Handling Class Imbalance},
  author = {Ii, Mark H White},
  pages = {10},
  langid = {english},
  file = {/home/skynet3/Zotero/storage/TH2XLG6H/Ii - A Monte Carlo study on methods for handling class .pdf}
}

@article{imaiStatisticalMethodEmpirical2012,
  ids = {imaiStatisticalMethodEmpirical2012a},
  title = {A {{Statistical Method}} for {{Empirical Testing}} of {{Competing Theories}}: {{EMPIRICAL TESTING OF COMPETING THEORIES}}},
  shorttitle = {A {{Statistical Method}} for {{Empirical Testing}} of {{Competing Theories}}},
  author = {Imai, Kosuke and Tingley, Dustin},
  date = {2012-01},
  journaltitle = {American Journal of Political Science},
  volume = {56},
  number = {1},
  pages = {218--236},
  issn = {00925853},
  doi = {10.1111/j.1540-5907.2011.00555.x},
  url = {https://onlinelibrary.wiley.com/doi/10.1111/j.1540-5907.2011.00555.x},
  urldate = {2022-11-11},
  langid = {english},
  file = {/home/skynet3/Zotero/storage/JB36UB72/Imai and Tingley - 2012 - A Statistical Method for Empirical Testing of Comp.pdf}
}

@article{imaiUseTwoWayFixed2021,
  ids = {imaiUseTwoWayFixed2021a},
  title = {On the {{Use}} of {{Two-Way Fixed Effects Regression Models}} for {{Causal Inference}} with {{Panel Data}}},
  author = {Imai, Kosuke and Kim, In Song},
  date = {2021-07},
  journaltitle = {Political Analysis},
  shortjournal = {Polit. Anal.},
  volume = {29},
  number = {3},
  pages = {405--415},
  issn = {1047-1987, 1476-4989},
  doi = {10.1017/pan.2020.33},
  url = {https://www.cambridge.org/core/product/identifier/S1047198720000339/type/journal_article},
  urldate = {2022-11-11},
  abstract = {The two-way linear fixed effects regression ( FE) has become a default method for estimating causal effects from panel data. Many applied researchers use the FE estimator to adjust for unobserved unit-specific and time-specific confounders at the same time. Unfortunately, we demonstrate that the ability of the FE model to simultaneously adjust for these two types of unobserved confounders critically relies upon the assumption of linear additive effects. Another common justification for the use of the FE estimator is based on its equivalence to the difference-in-differences estimator under the simplest setting with two groups and two time periods. We show that this equivalence does not hold under more general settings commonly encountered in applied research. Instead, we prove that the multi-period difference-in-differences estimator is equivalent to the weighted FE estimator with some observations having negative weights. These analytical results imply that in contrast to the popular belief, the FE estimator does not represent a designbased, nonparametric estimation strategy for causal inference. Instead, its validity fundamentally rests on the modeling assumptions.},
  langid = {english},
  file = {/home/skynet3/Zotero/storage/98IUZG3G/Imai and Kim - 2021 - On the Use of Two-Way Fixed Effects Regression Mod.pdf}
}

@article{imaiWhenShouldWe2019,
  ids = {imaiWhenShouldWe2019a},
  title = {When {{Should We Use Unit Fixed Effects Regression Models}} for {{Causal Inference}} with {{Longitudinal Data}}?},
  author = {Imai, Kosuke and Kim, In Song},
  date = {2019},
  journaltitle = {American Journal of Political Science},
  volume = {63},
  number = {2},
  pages = {467--490},
  issn = {1540-5907},
  doi = {10.1111/ajps.12417},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/ajps.12417},
  urldate = {2022-11-11},
  abstract = {Many researchers use unit fixed effects regression models as their default methods for causal inference with longitudinal data. We show that the ability of these models to adjust for unobserved time-invariant confounders comes at the expense of dynamic causal relationships, which are permitted under an alternative selection-on-observables approach. Using the nonparametric directed acyclic graph, we highlight two key causal identification assumptions of unit fixed effects models: Past treatments do not directly influence current outcome, and past outcomes do not affect current treatment. Furthermore, we introduce a new nonparametric matching framework that elucidates how various unit fixed effects models implicitly compare treated and control observations to draw causal inference. By establishing the equivalence between matching and weighted unit fixed effects estimators, this framework enables a diverse set of identification strategies to adjust for unobservables in the absence of dynamic causal relationships between treatment and outcome variables. We illustrate the proposed methodology through its application to the estimation of GATT membership effects on dyadic trade volume.},
  langid = {english},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/ajps.12417},
  file = {/home/skynet3/Zotero/storage/FUN9LMAN/Imai and Kim - 2019 - When Should We Use Unit Fixed Effects Regression M.pdf}
}

@article{imbensStatisticalSignificancePValues2021,
  title = {Statistical {{Significance}}, {$<$}em{$>$}p{$<$}/Em{$>$}-{{Values}}, and the {{Reporting}} of {{Uncertainty}}},
  author = {Imbens, Guido W.},
  date = {2021-08},
  journaltitle = {Journal of Economic Perspectives},
  volume = {35},
  number = {3},
  pages = {157--174},
  issn = {0895-3309},
  doi = {10.1257/jep.35.3.157},
  url = {https://www.aeaweb.org/articles?id=10.1257/jep.35.3.157},
  urldate = {2022-11-11},
  abstract = {-hacking and publication bias. The use of pre-analysis plans and replication studies, in combination with lowering the emphasis on statistical  significance may help address these problems.},
  langid = {english},
  keywords = {Criteria for Decision-Making under Risk and Uncertainty,Econometrics,Estimation: General,Hypothesis Testing: General},
  file = {/home/skynet3/Zotero/storage/XJ8SUME4/Imbens - 2021 - Statistical Significance, p-Values, and t.pdf;/home/skynet3/Zotero/storage/MFTFTH4U/articles.html}
}

@online{InceptionTimeFindingAlexNet2019,
  title = {{{InceptionTime}}: {{Finding AlexNet}} for {{Time Series Classification}}},
  shorttitle = {{{InceptionTime}}},
  date = {2019-09-11T09:32:40+00:00},
  url = {https://deepai.org/publication/inceptiontime-finding-alexnet-for-time-series-classification},
  urldate = {2022-11-11},
  abstract = {09/11/19 - Time series classification (TSC) is the area of machine learning interested in learning how to assign labels to time series. The l...},
  organization = {{DeepAI}},
  file = {/home/skynet3/Zotero/storage/FWJGAYSH/inceptiontime-finding-alexnet-for-time-series-classification.html}
}

@online{InfluenceHiddenResearcher,
  title = {The {{Influence}} of {{Hidden Researcher Decisions}} in {{Applied Microeconomics}}},
  url = {https://www.iza.org/publications/dp/13233/the-influence-of-hidden-researcher-decisions-in-applied-microeconomics},
  urldate = {2022-11-11},
  abstract = {Researchers make hundreds of decisions about data collection, preparation, and analysis in their research. We use a many-analysts approach to measure...},
  langid = {english}
}

@misc{ingModelSelectionHighdimensional2019,
  title = {Model Selection for High-Dimensional Linear Regression with Dependent Observations},
  author = {Ing, Ching-Kang},
  date = {2019-06-18},
  number = {arXiv:1906.07395},
  eprint = {1906.07395},
  eprinttype = {arxiv},
  primaryclass = {cs, math, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1906.07395},
  url = {http://arxiv.org/abs/1906.07395},
  urldate = {2022-11-11},
  abstract = {We investigate the prediction capability of the orthogonal greedy algorithm (OGA) in high-dimensional regression models with dependent observations. The rates of convergence of the prediction error of OGA are obtained under a variety of sparsity conditions. To prevent OGA from overfitting, we introduce a high-dimensional Akaike's information criterion (HDAIC) to determine the number of OGA iterations. A key contribution of this work is to show that OGA, used in conjunction with HDAIC, can achieve the optimal convergence rate without knowledge of how sparse the underlying high-dimensional model is.},
  archiveprefix = {arXiv},
  keywords = {63M30; 62F07; 62F12,Computer Science - Machine Learning,Mathematics - Statistics Theory,Statistics - Machine Learning},
  file = {/home/skynet3/Zotero/storage/B3THEE6Q/Ing - 2019 - Model selection for high-dimensional linear regres.pdf;/home/skynet3/Zotero/storage/TN5TZ4V5/1906.html}
}

@online{InstituteReplication,
  title = {Institute for {{Replication}}},
  url = {https://i4replication.org/index.html},
  urldate = {2022-11-11}
}

@online{InteractiveGuideFourier,
  title = {An {{Interactive Guide To The Fourier Transform}} – {{BetterExplained}}},
  url = {https://betterexplained.com/articles/an-interactive-guide-to-the-fourier-transform/},
  urldate = {2022-11-11},
  file = {/home/skynet3/Zotero/storage/U37WENXC/an-interactive-guide-to-the-fourier-transform.html}
}

@online{InterviewWarmup,
  title = {Interview {{Warmup}}},
  url = {https://grow.google/certificates/interview-warmup/},
  urldate = {2022-11-11},
  abstract = {A quick way to prepare for your next interview. Practice key questions, get insights about your answers, and get more comfortable interviewing.},
  langid = {english},
  file = {/home/skynet3/Zotero/storage/35SZV3WC/interview-warmup.html}
}

@online{IntroductionCodingEconomists,
  title = {Introduction — {{Coding}} for {{Economists}}},
  url = {https://aeturrell.github.io/coding-for-economists/intro.html},
  urldate = {2022-11-11}
}

@online{IntroductionDiDMultiple,
  title = {Introduction to {{DiD}} with {{Multiple Time Periods}}},
  url = {https://bcallaway11.github.io/did/articles/multi-period-did.html},
  urldate = {2022-11-11},
  abstract = {did},
  langid = {english},
  file = {/home/skynet3/Zotero/storage/UMQ36QWY/multi-period-did.html}
}

@online{IntroductionFacebookAI,
  title = {Introduction to {{Facebook AI Similarity Search}} ({{Faiss}})},
  url = {https://www.pinecone.io/learn/faiss-tutorial/},
  urldate = {2022-11-11},
  abstract = {Learn how Facebook AI Similarity Search changes — search.},
  langid = {english},
  organization = {{Pinecone}},
  file = {/home/skynet3/Zotero/storage/QYQXR9VS/faiss-tutorial.html}
}

@online{IntroductionGitHow,
  title = {An Introduction to {{Git}} and How to Use It with {{RStudio}} · {{The R}} Class},
  url = {http://r-bio.github.io/intro-git-rstudio/},
  urldate = {2022-11-11},
  file = {/home/skynet3/Zotero/storage/TLARGGQZ/intro-git-rstudio.html}
}

@online{IntroductionStan,
  title = {An {{Introduction}} to {{Stan}}},
  url = {https://betanalpha.github.io/assets/case_studies/stan_intro.html},
  urldate = {2022-11-11},
  file = {/home/skynet3/Zotero/storage/434NU7MI/stan_intro.html}
}

@online{InverseCDFMethod,
  title = {The {{Inverse CDF Method}}},
  url = {https://dk81.github.io/dkmathstats_site/prob-inverse-cdf.html},
  urldate = {2022-11-11}
}

@online{InversetransformMethodGenerating,
  title = {The Inverse-Transform Method for Generating Random Variables in {{R}} - Heds.Nz},
  url = {https://heds.nz/posts/inverse-transform/},
  urldate = {2022-11-11}
}

@online{ivanov37ReasonsWhy2019,
  title = {37 {{Reasons}} Why Your {{Neural Network}} Is Not Working},
  author = {Ivanov, Slav},
  date = {2019-04-16T09:44:40},
  url = {https://blog.slavv.com/37-reasons-why-your-neural-network-is-not-working-4020854bd607},
  urldate = {2022-11-11},
  abstract = {The network had been training for the last 12 hours. It all looked good: the gradients were flowing and the loss was decreasing. But then…},
  langid = {english},
  organization = {{Medium}},
  file = {/home/skynet3/Zotero/storage/PS4TZJ3A/37-reasons-why-your-neural-network-is-not-working-4020854bd607.html}
}

@inproceedings{iyyerPoliticalIdeologyDetection2014,
  title = {Political {{Ideology Detection Using Recursive Neural Networks}}},
  booktitle = {Proceedings of the 52nd {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Iyyer, Mohit and Enns, Peter and Boyd-Graber, Jordan and Resnik, Philip},
  date = {2014-06},
  pages = {1113--1122},
  publisher = {{Association for Computational Linguistics}},
  location = {{Baltimore, Maryland}},
  doi = {10.3115/v1/P14-1105},
  url = {https://aclanthology.org/P14-1105},
  urldate = {2022-11-11},
  eventtitle = {{{ACL}} 2014},
  file = {/home/skynet3/Zotero/storage/DBVARQY3/Iyyer et al. - 2014 - Political Ideology Detection Using Recursive Neura.pdf}
}

@software{ja-thomasAutoxgboostAutomaticTuning2022,
  title = {Autoxgboost - {{Automatic}} Tuning and Fitting of Xgboost.},
  author = {ja- {thomas}},
  options = {useprefix=true},
  date = {2022-11-09T01:45:54Z},
  origdate = {2017-02-11T15:39:19Z},
  url = {https://github.com/ja-thomas/autoxgboost},
  urldate = {2022-11-11}
}

@misc{jaegerWhenImputeImputation2020,
  title = {When to {{Impute}}? {{Imputation}} before and during Cross-Validation},
  shorttitle = {When to {{Impute}}?},
  author = {Jaeger, Byron C. and Tierney, Nicholas J. and Simon, Noah R.},
  date = {2020-10-01},
  number = {arXiv:2010.00718},
  eprint = {2010.00718},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2010.00718},
  urldate = {2022-11-11},
  abstract = {Cross-validation (CV) is a technique used to estimate generalization error for prediction models. For pipeline modeling algorithms (i.e. modeling procedures with multiple steps), it has been recommended the entire sequence of steps be carried out during each replicate of CV to mimic the application of the entire pipeline to an external testing set. While theoretically sound, following this recommendation can lead to high computational costs when a pipeline modeling algorithm includes computationally expensive operations, e.g. imputation of missing values. There is a general belief that unsupervised variable selection (i.e. ignoring the outcome) can be applied before conducting CV without incurring bias, but there is less consensus for unsupervised imputation of missing values. We empirically assessed whether conducting unsupervised imputation prior to CV would result in biased estimates of generalization error or result in poorly selected tuning parameters and thus degrade the external performance of downstream models. Results show that despite optimistic bias, the reduced variance of imputation before CV compared to imputation during each replicate of CV leads to a lower overall root mean squared error for estimation of the true external R-squared and the performance of models tuned using CV with imputation before versus during each replication is minimally different. In conclusion, unsupervised imputation before CV appears valid in certain settings and may be a helpful strategy that enables analysts to use more flexible imputation techniques without incurring high computational costs.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Computation,Statistics - Machine Learning},
  file = {/home/skynet3/Zotero/storage/956C8HNB/Jaeger et al. - 2020 - When to Impute Imputation before and during cross.pdf;/home/skynet3/Zotero/storage/RP7M5HIQ/2010.html}
}

@misc{janzingCausalRegularization2019,
  ids = {janzingCausalRegularization2019a},
  title = {Causal {{Regularization}}},
  author = {Janzing, Dominik},
  date = {2019-06-28},
  number = {arXiv:1906.12179},
  eprint = {1906.12179},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1906.12179},
  url = {http://arxiv.org/abs/1906.12179},
  urldate = {2022-11-11},
  abstract = {I argue that regularizing terms in standard regression methods not only help against overfitting finite data, but sometimes also yield better causal models in the infinite sample regime. I first consider a multi-dimensional variable linearly influencing a target variable with some multi-dimensional unobserved common cause, where the confounding effect can be decreased by keeping the penalizing term in Ridge and Lasso regression even in the population limit. Choosing the size of the penalizing term, is however challenging, because cross validation is pointless. Here it is done by first estimating the strength of confounding via a method proposed earlier, which yielded some reasonable results for simulated and real data. Further, I prove a `causal generalization bound' which states (subject to a particular model of confounding) that the error made by interpreting any non-linear regression as causal model can be bounded from above whenever functions are taken from a not too rich class. In other words, the bound guarantees "generalization" from observational to interventional distributions, which is usually not subject of statistical learning theory (and is only possible due to the underlying symmetries of the confounder model).},
  archiveprefix = {arXiv},
  keywords = {62J02,Computer Science - Machine Learning,G.3,Statistics - Machine Learning},
  file = {/home/skynet3/Zotero/storage/VDUKTLXX/Janzing - 2019 - Causal Regularization.pdf;/home/skynet3/Zotero/storage/DL9SADFU/1906.html}
}

@article{janzingQuantifyingCausalInfluences2013,
  title = {Quantifying Causal Influences},
  author = {Janzing, Dominik and Balduzzi, David and Grosse-Wentrup, Moritz and Schölkopf, Bernhard},
  date = {2013-10-01},
  journaltitle = {The Annals of Statistics},
  shortjournal = {Ann. Statist.},
  volume = {41},
  number = {5},
  eprint = {1203.6502},
  eprinttype = {arxiv},
  primaryclass = {math, stat},
  issn = {0090-5364},
  doi = {10.1214/13-AOS1145},
  url = {http://arxiv.org/abs/1203.6502},
  urldate = {2022-11-15},
  abstract = {Many methods for causal inference generate directed acyclic graphs (DAGs) that formalize causal relations between \$n\$ variables. Given the joint distribution on all these variables, the DAG contains all information about how intervening on one variable changes the distribution of the other \$n-1\$ variables. However, quantifying the causal influence of one variable on another one remains a nontrivial question. Here we propose a set of natural, intuitive postulates that a measure of causal strength should satisfy. We then introduce a communication scenario, where edges in a DAG play the role of channels that can be locally corrupted by interventions. Causal strength is then the relative entropy distance between the old and the new distribution. Many other measures of causal strength have been proposed, including average causal effect, transfer entropy, directed information, and information flow. We explain how they fail to satisfy the postulates on simple DAGs of \$\textbackslash leq3\$ nodes. Finally, we investigate the behavior of our measure on time-series, supporting our claims with experiments on simulated data.},
  archiveprefix = {arXiv},
  keywords = {Mathematics - Statistics Theory},
  file = {/home/skynet3/Zotero/storage/S5IEWRX3/Janzing et al. - 2013 - Quantifying causal influences.pdf;/home/skynet3/Zotero/storage/N3TC8MAY/1203.html}
}

@online{Jason718AwesomeselfsupervisedlearningCurated,
  title = {Jason718/Awesome-Self-Supervised-Learning: {{A}} Curated List of Awesome Self-Supervised Methods},
  shorttitle = {Jason718/Awesome-Self-Supervised-Learning},
  url = {https://github.com/jason718/awesome-self-supervised-learning},
  urldate = {2022-11-11},
  abstract = {A curated list of awesome self-supervised methods. Contribute to jason718/awesome-self-supervised-learning development by creating an account on GitHub.},
  langid = {english},
  organization = {{GitHub}},
  file = {/home/skynet3/Zotero/storage/NZ7SY3HX/awesome-self-supervised-learning.html}
}

@misc{jasonMeasuringAverageTreatment2019,
  title = {Measuring {{Average Treatment Effect}} from {{Heavy-tailed Data}}},
  author = {Jason and Wang and Burke, Pauline},
  date = {2019-05-22},
  number = {arXiv:1905.09252},
  eprint = {1905.09252},
  eprinttype = {arxiv},
  primaryclass = {stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1905.09252},
  url = {http://arxiv.org/abs/1905.09252},
  urldate = {2022-11-11},
  abstract = {Heavy-tailed metrics are common and often critical to product evaluation in the online world. While we may have samples large enough for Central Limit Theorem to kick in, experimentation is challenging due to the wide confidence interval of estimation. We demonstrate the pressure by running A/A simulations with customer spending data from a large-scale Ecommerce site. Solutions are then explored. On one front we address the heavy tail directly and highlight the often ignored nuances of winsorization. In particular, the legitimacy of false positive rate could be at risk. We are further inspired by the idea of robust statistics and introduce Huber regression as a better way to measure treatment effect. On another front covariates from pre-experiment period are exploited. Although they are independent to assignment and potentially explain the variation of response well, concerns are that models are learned against prediction error rather than the bias of parameter. We find the framework of orthogonal learning useful, matching not raw observations but residuals from two predictions, one towards the response and the other towards the assignment. Robust regression is readily integrated, together with cross-fitting. The final design is proven highly effective in driving down variance at the same time controlling bias. It is empowering our daily practice and hopefully can also benefit other applications in the industry.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Applications},
  file = {/home/skynet3/Zotero/storage/K97QLVPM/Jason et al. - 2019 - Measuring Average Treatment Effect from Heavy-tail.pdf;/home/skynet3/Zotero/storage/HMI7YGUK/1905.html}
}

@software{JAXAutogradXLA2022,
  title = {{{JAX}}: {{Autograd}} and {{XLA}}},
  shorttitle = {{{JAX}}},
  date = {2022-11-11T17:26:57Z},
  origdate = {2018-10-25T21:25:02Z},
  url = {https://github.com/google/jax},
  urldate = {2022-11-11},
  abstract = {Composable transformations of Python+NumPy programs: differentiate, vectorize, JIT to GPU/TPU, and more},
  organization = {{Google}},
  keywords = {jax}
}

@article{jenningsElectionPollingErrors2018,
  title = {Election Polling Errors across Time and Space},
  author = {Jennings, Will and Wlezien, Christopher},
  date = {2018-04},
  journaltitle = {Nature Human Behaviour},
  shortjournal = {Nat Hum Behav},
  volume = {2},
  number = {4},
  pages = {276--283},
  publisher = {{Nature Publishing Group}},
  issn = {2397-3374},
  doi = {10.1038/s41562-018-0315-6},
  url = {https://www.nature.com/articles/s41562-018-0315-6},
  urldate = {2022-11-11},
  abstract = {Are election polling misses becoming more prevalent? Are they more likely in some contexts than others? Here we undertake an over-time and cross-national assessment of prediction errors in pre-election polls. Our analysis draws on more than 30,000 national polls from 351 general elections in 45 countries between 1942 and 2017. We proceed in the following way. First, building on previous studies, we show how errors in national polls evolve in a structured way over the election timeline. Second, we examine errors in polls in the final week of the election campaign to assess performance across election years. Third, we undertake a pooled analysis of polling errors—controlling for a number of institutional and party features—that enables us to test whether poll errors have increased or decreased over time. We find that, contrary to conventional wisdom, the recent performance of polls has not been outside the ordinary. However, the performance of polls does vary across political contexts and in understandable ways.},
  issue = {4},
  langid = {english},
  keywords = {Politics,Society},
  file = {/home/skynet3/Zotero/storage/SXTUD8CS/Jennings and Wlezien - 2018 - Election polling errors across time and space.pdf;/home/skynet3/Zotero/storage/P45IU876/s41562-018-0315-6.html}
}

@article{jesseeDonKnowResponses2017,
  title = {“{{Don}}’t {{Know}}” {{Responses}}, {{Personality}}, and the {{Measurement}} of {{Political Knowledge}}*},
  author = {Jessee, Stephen A.},
  date = {2017-10},
  journaltitle = {Political Science Research and Methods},
  volume = {5},
  number = {4},
  pages = {711--731},
  publisher = {{Cambridge University Press}},
  issn = {2049-8470, 2049-8489},
  doi = {10.1017/psrm.2015.23},
  url = {https://www.cambridge.org/core/journals/political-science-research-and-methods/article/abs/dont-know-responses-personality-and-the-measurement-of-political-knowledge/C28B2FF6AD8181F9F60651C0933E5620},
  urldate = {2022-11-11},
  abstract = {A prominent worry in the measurement of political knowledge is that respondents who say they don’t know the answer to a survey question may have partial knowledge about the topic—more than respondents who answer incorrectly but less than those who answer correctly. It has also been asserted that differentials in respondents’ willingness to guess, driven strongly by personality, can bias traditional knowledge measures. Using a multinomial probit item response model, I show that, contrary to previous claims that “don’t know” responses to political knowledge questions conceal a good deal of “hidden knowledge,” these responses are actually reflective of less knowledge, not only than correct responses but also than incorrect answers. Furthermore, arguments that the meaning of “don’t know” responses varies strongly by respondent personality type are incorrect. In fact, these results hold for high- and low-trait respondents on each of the five most commonly used core personality measures.},
  langid = {english},
  file = {/home/skynet3/Zotero/storage/XRW6UIDN/C28B2FF6AD8181F9F60651C0933E5620.html}
}

@online{jfeliwebKnightFoundationSchool,
  title = {Knight {{Foundation School}} of {{Computing}} and {{Information Sciences}} | {{CREATING FLORIDA}}'{{S NEXT GENERATION OF COMPUTING PROFESSIONALS}}},
  author = {JFeliWeb},
  url = {https://www.cis.fiu.edu/},
  urldate = {2022-11-11},
  abstract = {FIU's School of Computing and Information Sciences offer degrees in B.A., B.S., \& M.S. in Computer Science and in Information Technology. Classes can be taken online, in class or both at your convince. CREATING FLORIDA'S NEXT GENERATION OF COMPUTING PROFESSIONALS},
  langid = {american},
  organization = {{Knight Foundation School of Computing and Information Sciences}},
  file = {/home/skynet3/Zotero/storage/FGG744R8/www.cis.fiu.edu.html}
}

@online{jiaminBackpropagationNotJust2018,
  title = {Backpropagation Is Not Just the Chain Rule},
  author = {{jiamin}, Author},
  date = {2018-04-28T01:35:15+00:00},
  url = {https://wordpress.cs.vt.edu/optml/2018/04/28/backpropagation-is-not-just-the-chain-rule/},
  urldate = {2022-11-11},
  abstract = {Author: Jiamin Wang and Spencer Jenkins Motivation Basically, machine learning problem is about function approximation. If we have datasets (x, y),~x~is the variables and y~is the response, and we …},
  langid = {american},
  organization = {{Optimization in Machine Learning}}
}

@article{jiangEcologicalRegressionPartial2019,
  ids = {jiangEcologicalRegressionPartial2019a},
  title = {Ecological {{Regression}} with {{Partial Identification}}},
  author = {Jiang, Wenxin and King, Gary and Schmaltz, Allen and Tanner, Martin A.},
  date = {2019},
  journaltitle = {Political Analysis},
  volume = {28},
  number = {1},
  pages = {1--22},
  file = {/home/skynet3/Zotero/storage/P692YIRA/ecological-regression-partial-identification.html}
}

@misc{jinSensitivityAnalysisIndividual2022,
  ids = {jinSensitivityAnalysisIndividual2022a},
  title = {Sensitivity {{Analysis}} of {{Individual Treatment Effects}}: {{A Robust Conformal Inference Approach}}},
  shorttitle = {Sensitivity {{Analysis}} of {{Individual Treatment Effects}}},
  author = {Jin, Ying and Ren, Zhimei and Candès, Emmanuel J.},
  date = {2022-04-24},
  number = {arXiv:2111.12161},
  eprint = {2111.12161},
  eprinttype = {arxiv},
  primaryclass = {stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2111.12161},
  url = {http://arxiv.org/abs/2111.12161},
  urldate = {2022-11-11},
  abstract = {We propose a model-free framework for sensitivity analysis of individual treatment effects (ITEs), building upon ideas from conformal inference. For any unit, our procedure reports the \$\textbackslash Gamma\$-value, a number which quantifies the minimum strength of confounding needed to explain away the evidence for ITE. Our approach rests on the reliable predictive inference of counterfactuals and ITEs in situations where the training data is confounded. Under the marginal sensitivity model of Tan (2006), we characterize the shift between the distribution of the observations and that of the counterfactuals. We first develop a general method for predictive inference of test samples from a shifted distribution; we then leverage this to construct covariate-dependent prediction sets for counterfactuals. No matter the value of the shift, these prediction sets (resp. approximately) achieve marginal coverage if the propensity score is known exactly (resp. estimated). We describe a distinct procedure also attaining coverage, however, conditional on the training data. In the latter case, we prove a sharpness result showing that for certain classes of prediction problems, the prediction intervals cannot possibly be tightened. We verify the validity and performance of the new methods via simulation studies and apply them to analyze real datasets.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Methodology},
  file = {/home/skynet3/Zotero/storage/48M6MXPA/Jin et al. - 2022 - Sensitivity Analysis of Individual Treatment Effec.pdf;/home/skynet3/Zotero/storage/W5RL9SCU/2111.html}
}

@article{jiSurveyKnowledgeGraphs2022,
  title = {A {{Survey}} on {{Knowledge Graphs}}: {{Representation}}, {{Acquisition}} and {{Applications}}},
  shorttitle = {A {{Survey}} on {{Knowledge Graphs}}},
  author = {Ji, Shaoxiong and Pan, Shirui and Cambria, Erik and Marttinen, Pekka and Yu, Philip S.},
  date = {2022-02},
  journaltitle = {IEEE Transactions on Neural Networks and Learning Systems},
  shortjournal = {IEEE Trans. Neural Netw. Learning Syst.},
  volume = {33},
  number = {2},
  eprint = {2002.00388},
  eprinttype = {arxiv},
  primaryclass = {cs},
  pages = {494--514},
  issn = {2162-237X, 2162-2388},
  doi = {10.1109/TNNLS.2021.3070843},
  url = {http://arxiv.org/abs/2002.00388},
  urldate = {2022-11-11},
  abstract = {Human knowledge provides a formal understanding of the world. Knowledge graphs that represent structural relations between entities have become an increasingly popular research direction towards cognition and human-level intelligence. In this survey, we provide a comprehensive review of knowledge graph covering overall research topics about 1) knowledge graph representation learning, 2) knowledge acquisition and completion, 3) temporal knowledge graph, and 4) knowledge-aware applications, and summarize recent breakthroughs and perspective directions to facilitate future research. We propose a full-view categorization and new taxonomies on these topics. Knowledge graph embedding is organized from four aspects of representation space, scoring function, encoding models, and auxiliary information. For knowledge acquisition, especially knowledge graph completion, embedding methods, path inference, and logical rule reasoning, are reviewed. We further explore several emerging topics, including meta relational learning, commonsense reasoning, and temporal knowledge graphs. To facilitate future research on knowledge graphs, we also provide a curated collection of datasets and open-source libraries on different tasks. In the end, we have a thorough outlook on several promising research directions.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/home/skynet3/Zotero/storage/73HQPCS6/Ji et al. - 2022 - A Survey on Knowledge Graphs Representation, Acqu.pdf;/home/skynet3/Zotero/storage/2NBVFXX7/2002.html}
}

@online{jmountTipUseIsTRUE2018,
  title = {R {{Tip}}: Use {{isTRUE}}()},
  shorttitle = {R {{Tip}}},
  author = {{jmount}},
  date = {2018-06-11T15:55:45+00:00},
  url = {https://win-vector.com/2018/06/11/r-tip-use-istrue/},
  urldate = {2022-11-11},
  abstract = {R Tip: use isTRUE(). A lot of R functions are type unstable, which means they return different types or classes depending on details of their values. For example consider all.equal(), it returns th…},
  langid = {american},
  organization = {{Win Vector LLC}},
  file = {/home/skynet3/Zotero/storage/3E6WNJLU/r-tip-use-istrue.html}
}

@article{joffeInvitedCommentaryPropensity1999,
  title = {Invited {{Commentary}}: {{Propensity Scores}}},
  shorttitle = {Invited {{Commentary}}},
  author = {Joffe, Marshall M. and Rosenbaum, Paul R.},
  date = {1999-08-15},
  journaltitle = {American Journal of Epidemiology},
  shortjournal = {American Journal of Epidemiology},
  volume = {150},
  number = {4},
  pages = {327--333},
  issn = {0002-9262},
  doi = {10.1093/oxfordjournals.aje.a010011},
  url = {https://doi.org/10.1093/oxfordjournals.aje.a010011},
  urldate = {2022-11-11},
  abstract = {The propensity score is the conditional probability of exposure to a treatment given observed covariates. In a cohort study, matching or stratifying treated and control subjects on a single variable, the propensity score, tends to balance all of the observed covariates; however, unlike random assignment of treatments, the propensity score may not also balance unobserved covariates. The authors review the uses and limitations of propensity scores and provide a brief outline of associated statistical theory. They also present a new result of using propensity scores in case-cohort studies.Am J Epidemiol 1999; 150: 327–33.},
  file = {/home/skynet3/Zotero/storage/S95WYJ8V/Joffe and Rosenbaum - 1999 - Invited Commentary Propensity Scores.pdf;/home/skynet3/Zotero/storage/FSYRU2SK/98791.html}
}

@article{joffeSelectiveIgnorabilityAssumptions2010,
  title = {Selective {{Ignorability Assumptions}} in {{Causal Inference}}},
  author = {Joffe, Marshall M. and Yang, Wei Peter and Feldman, Harold I.},
  date = {2010-03-05},
  journaltitle = {The International Journal of Biostatistics},
  volume = {6},
  number = {2},
  publisher = {{De Gruyter}},
  issn = {1557-4679},
  doi = {10.2202/1557-4679.1199},
  url = {https://www.degruyter.com/document/doi/10.2202/1557-4679.1199/html},
  urldate = {2022-11-11},
  abstract = {Most attempts at causal inference in observational studies are based on assumptions that treatment assignment is ignorable. Such assumptions are usually made casually, largely because they justify the use of available statistical methods and not because they are truly believed. It will often be the case that it is plausible that conditional independence holds at least approximately for a subset but not all of the experience giving rise to one's data. Such selective ignorability assumptions may be used to derive valid causal inferences in conjunction with structural nested models. In this paper, we outline selective ignorability assumptions mathematically and sketch how they may be used along with otherwise standard G-estimation or likelihood-based methods to obtain inference on structural nested models. We also consider use of these assumptions in the presence of selective measurement error or missing data when the missingness is not at random. We motivate and illustrate our development by considering an analysis of an observational database to estimate the effect of erythropoietin use on mortality among hemodialysis patients.},
  langid = {english},
  keywords = {anemia,causal inference,end-stage renal disease,ignorability},
  file = {/home/skynet3/Zotero/storage/KG6LDI6I/Joffe et al. - 2010 - Selective Ignorability Assumptions in Causal Infer.pdf}
}

@article{johanssonInferenceExperimentsConditional2022,
  title = {Inference in {{Experiments Conditional}} on {{Observed Imbalances}} in {{Covariates}}},
  author = {Johansson, Per and Nordin, Mattias},
  date = {2022-10-02},
  journaltitle = {The American Statistician},
  volume = {76},
  number = {4},
  pages = {394--404},
  publisher = {{Taylor \& Francis}},
  issn = {0003-1305},
  doi = {10.1080/00031305.2022.2054859},
  url = {https://doi.org/10.1080/00031305.2022.2054859},
  urldate = {2022-11-11},
  abstract = {Double-blind randomized controlled trials are traditionally seen as the gold standard for causal inferences as the difference-in-means estimator is an unbiased estimator of the average treatment effect in the experiment. The fact that this estimator is unbiased over all possible randomizations does not, however, mean that any given estimate is close to the true treatment effect. Similarly, while predetermined covariates will be balanced between treatment and control groups on average, large imbalances may be observed in a given experiment and the researcher may therefore want to condition on such covariates using linear regression. This article studies the theoretical properties of both the difference-in-means and OLS estimators conditional on observed differences in covariates. By deriving the statistical properties of the conditional estimators, we can establish guidance for how to deal with covariate imbalances.},
  keywords = {Conditional inference,Covariate balance,Experimental design,P-hacking,Randomization inference},
  annotation = {\_eprint: https://doi.org/10.1080/00031305.2022.2054859},
  file = {/home/skynet3/Zotero/storage/Y8A59IRN/Johansson and Nordin - 2022 - Inference in Experiments Conditional on Observed I.pdf}
}

@article{johanssonPreprintsUnderutilizedMechanism2018,
  title = {Preprints: {{An}} Underutilized Mechanism to Accelerate Outbreak Science},
  shorttitle = {Preprints},
  author = {Johansson, Michael A. and Reich, Nicholas G. and Meyers, Lauren Ancel and Lipsitch, Marc},
  date = {2018-04-03},
  journaltitle = {PLOS Medicine},
  shortjournal = {PLOS Medicine},
  volume = {15},
  number = {4},
  pages = {e1002549},
  publisher = {{Public Library of Science}},
  issn = {1549-1676},
  doi = {10.1371/journal.pmed.1002549},
  url = {https://journals.plos.org/plosmedicine/article?id=10.1371/journal.pmed.1002549},
  urldate = {2022-11-11},
  abstract = {In an Essay, Michael Johansson and colleagues advocate the posting of research studies addressing infectious disease outbreaks as preprints.},
  langid = {english},
  keywords = {Epidemiology,Infectious diseases,Medical journals,Peer review,Public and occupational health,Scientific publishing,Scientists,Zika fever},
  file = {/home/skynet3/Zotero/storage/JVMR4C9Q/Johansson et al. - 2018 - Preprints An underutilized mechanism to accelerat.pdf;/home/skynet3/Zotero/storage/5HPFMEBF/article.html}
}

@book{johnsonFeatureEngineeringSelection,
  title = {Feature {{Engineering}} and {{Selection}}: {{A Practical Approach}} for {{Predictive Models}}},
  shorttitle = {Feature {{Engineering}} and {{Selection}}},
  author = {Johnson, Max Kuhn {and} Kjell},
  url = {https://bookdown.org/max/FES/},
  urldate = {2022-11-11},
  abstract = {A primary goal of predictive modeling is to find a reliable and effective predic- tive relationship between an available set of features and an outcome. This book provides an extensive set of techniques for uncovering effective representations of the features for modeling the outcome and for finding an optimal subset of features to improve a model’s predictive performance.}
}

@online{JoinOurSlack,
  ids = {JoinOurSlacka},
  title = {Join Our {{Slack}}},
  url = {https://datatalks.club/slack},
  urldate = {2022-11-11},
  abstract = {DataTalks.Club – the place to talk about data},
  langid = {english},
  organization = {{DataTalks.Club}},
  file = {/home/skynet3/Zotero/storage/JEZEW525/slack.html}
}

@online{Joke2kFakerFaker,
  title = {Joke2k/Faker: {{Faker}} Is a {{Python}} Package That Generates Fake Data for You.},
  shorttitle = {Joke2k/Faker},
  url = {https://github.com/joke2k/faker},
  urldate = {2022-11-11},
  abstract = {Faker is a Python package that generates fake data for you. - joke2k/faker: Faker is a Python package that generates fake data for you.},
  langid = {english},
  organization = {{GitHub}},
  file = {/home/skynet3/Zotero/storage/UW9EC673/faker.html}
}

@online{jonesReparameterizationTrick2019,
  title = {The {{Reparameterization}} “{{Trick}}”},
  author = {Jones, Llion},
  date = {2019-06-13T06:12:27},
  url = {https://medium.com/@llionj/the-reparameterization-trick-4ff30fe92954},
  urldate = {2022-11-11},
  abstract = {As Simple as Possible in TensorFlow},
  langid = {english},
  organization = {{Medium}},
  file = {/home/skynet3/Zotero/storage/ZKMXANFK/the-reparameterization-trick-4ff30fe92954.html}
}

@online{JosephmisitiAwesomemachinelearningCurated,
  title = {Josephmisiti/Awesome-Machine-Learning: {{A}} Curated List of Awesome {{Machine Learning}} Frameworks, Libraries and Software.},
  shorttitle = {Josephmisiti/Awesome-Machine-Learning},
  url = {https://github.com/josephmisiti/awesome-machine-learning},
  urldate = {2022-11-11},
  abstract = {A curated list of awesome Machine Learning frameworks, libraries and software. - josephmisiti/awesome-machine-learning: A curated list of awesome Machine Learning frameworks, libraries and software.},
  langid = {english},
  organization = {{GitHub}},
  file = {/home/skynet3/Zotero/storage/4K8CY2LR/awesome-machine-learning.html}
}

@online{jrWorkflow2022,
  title = {R {{Workflow}}},
  author = {Jr, Frank E. Harrell},
  date = {2022-11-09},
  url = {http://hbiostat.org/rflow/},
  urldate = {2022-11-11},
  langid = {english}
}

@online{JtextorDagittyGraphical,
  title = {Jtextor/Dagitty: {{Graphical}} Analysis of Structural Causal Models / Graphical Causal Models.},
  shorttitle = {Jtextor/Dagitty},
  url = {https://github.com/jtextor/dagitty},
  urldate = {2022-11-11},
  abstract = {Graphical analysis of structural causal models / graphical causal models. - jtextor/dagitty: Graphical analysis of structural causal models / graphical causal models.},
  langid = {english},
  organization = {{GitHub}},
  file = {/home/skynet3/Zotero/storage/SBCIDZTJ/dagitty.html}
}

@online{JupyterNotebookViewer,
  title = {Jupyter {{Notebook Viewer}}},
  url = {https://nbviewer.org/github/bckenstler/dsb17-walkthrough/blob/master/Part%201.%20DSB17%20Preprocessing.ipynb},
  urldate = {2022-11-11}
}

@misc{kahn-langPromisePitfallsDifferencesinDifferences2018,
  type = {Working Paper},
  title = {The {{Promise}} and {{Pitfalls}} of {{Differences-in-Differences}}: {{Reflections}} on ‘16 and {{Pregnant}}’ and {{Other Applications}}},
  shorttitle = {The {{Promise}} and {{Pitfalls}} of {{Differences-in-Differences}}},
  author = {Kahn-Lang, Ariella and Lang, Kevin},
  date = {2018-07},
  series = {Working {{Paper Series}}},
  number = {24857},
  publisher = {{National Bureau of Economic Research}},
  doi = {10.3386/w24857},
  url = {https://www.nber.org/papers/w24857},
  urldate = {2022-11-11},
  abstract = {We use the exchange between Kearney/Levine and Jaeger/Joyce/Kaestner on “16 and Pregnant” to reexamine the use of DiD as a response to the failure of nature to properly design an experiment for us. We argue that 1) any DiD paper should address why the original levels of the experimental and control groups differed, and why this would not impact trends, 2) the parallel trends argument requires a justification of the chosen functional form and that the use of the interaction coefficients in probit and logit may be justified in some cases, and 3) parallel trends in the period prior to treatment is suggestive of counterfactual parallel trends, but parallel pre-trends is neither necessary nor sufficient for the parallel counterfactual trends condition to hold. Importantly, the purely statistical approach uses pretesting and thus generates the wrong standard errors. Moreover, we underline the dangers of implicitly or explicitly accepting the null hypothesis when failing to reject the absence of a differential pre-trend.},
  file = {/home/skynet3/Zotero/storage/JIBINZGI/Kahn-Lang and Lang - 2018 - The Promise and Pitfalls of Differences-in-Differe.pdf}
}

@misc{kalainathanCausalDiscoveryToolbox2019,
  title = {Causal {{Discovery Toolbox}}: {{Uncover}} Causal Relationships in {{Python}}},
  shorttitle = {Causal {{Discovery Toolbox}}},
  author = {Kalainathan, Diviyan and Goudet, Olivier},
  date = {2019-03-06},
  number = {arXiv:1903.02278},
  eprint = {1903.02278},
  eprinttype = {arxiv},
  primaryclass = {stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1903.02278},
  url = {http://arxiv.org/abs/1903.02278},
  urldate = {2022-11-11},
  abstract = {This paper presents a new open source Python framework for causal discovery from observational data and domain background knowledge, aimed at causal graph and causal mechanism modeling. The 'cdt' package implements the end-to-end approach, recovering the direct dependencies (the skeleton of the causal graph) and the causal relationships between variables. It includes algorithms from the 'Bnlearn' and 'Pcalg' packages, together with algorithms for pairwise causal discovery such as ANM. 'cdt' is available under the MIT License at https://github.com/Diviyan-Kalainathan/CausalDiscoveryToolbox.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Computation,Statistics - Machine Learning},
  file = {/home/skynet3/Zotero/storage/IJXBEU9N/Kalainathan and Goudet - 2019 - Causal Discovery Toolbox Uncover causal relations.pdf;/home/skynet3/Zotero/storage/MX8MX3HP/1903.html}
}

@article{kandulaReappraisingUtilityGoogle2019,
  title = {Reappraising the Utility of {{Google Flu Trends}}},
  author = {Kandula, Sasikiran and Shaman, Jeffrey},
  date = {2019-08-02},
  journaltitle = {PLOS Computational Biology},
  shortjournal = {PLOS Computational Biology},
  volume = {15},
  number = {8},
  pages = {e1007258},
  publisher = {{Public Library of Science}},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1007258},
  url = {https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1007258},
  urldate = {2022-11-11},
  abstract = {Estimation of influenza-like illness (ILI) using search trends activity was intended to supplement traditional surveillance systems, and was a motivation behind the development of Google Flu Trends (GFT). However, several studies have previously reported large errors in GFT estimates of ILI in the US. Following recent release of time-stamped surveillance data, which better reflects real-time operational scenarios, we reanalyzed GFT errors. Using three data sources—GFT: an archive of weekly ILI estimates from Google Flu Trends; ILIf: fully-observed ILI rates from ILINet; and, ILIp: ILI rates available in real-time based on partial reporting—five influenza seasons were analyzed and mean square errors (MSE) of GFT and ILIp as estimates of ILIf were computed. To correct GFT errors, a random forest regression model was built with ILI and GFT rates from the previous three weeks as predictors. An overall reduction in error of 44\% was observed and the errors of the corrected GFT are lower than those of ILIp. An 80\% reduction in error during 2012/13, when GFT had large errors, shows that extreme failures of GFT could have been avoided. Using autoregressive integrated moving average (ARIMA) models, one- to four-week ahead forecasts were generated with two separate data streams: ILIp alone, and with both ILIp and corrected GFT. At all forecast targets and seasons, and for all but two regions, inclusion of GFT lowered MSE. Results from two alternative error measures, mean absolute error and mean absolute proportional error, were largely consistent with results from MSE. Taken together these findings provide an error profile of GFT in the US, establish strong evidence for the adoption of search trends based 'nowcasts' in influenza forecast systems, and encourage reevaluation of the utility of this data source in diverse domains.},
  langid = {english},
  keywords = {Archives,Epidemiology,Forecasting,Infectious disease surveillance,Influenza,Mathematical functions,Outpatients,United States},
  file = {/home/skynet3/Zotero/storage/Y2UXGGUN/Kandula and Shaman - 2019 - Reappraising the utility of Google Flu Trends.pdf;/home/skynet3/Zotero/storage/GUW88G55/article.html}
}

@article{kaplanLikelihoodNullEffects2015,
  title = {Likelihood of {{Null Effects}} of {{Large NHLBI Clinical Trials Has Increased}} over {{Time}}},
  author = {Kaplan, Robert M. and Irvin, Veronica L.},
  date = {2015-08-05},
  journaltitle = {PLOS ONE},
  shortjournal = {PLOS ONE},
  volume = {10},
  number = {8},
  pages = {e0132382},
  publisher = {{Public Library of Science}},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0132382},
  url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0132382},
  urldate = {2022-11-11},
  abstract = {Background We explore whether the number of null results in large National Heart Lung, and Blood Institute (NHLBI) funded trials has increased over time. Methods We identified all large NHLBI supported RCTs between 1970 and 2012 evaluating drugs or dietary supplements for the treatment or prevention of cardiovascular disease. Trials were included if direct costs {$>\$$}500,000/year, participants were adult humans, and the primary outcome was cardiovascular risk, disease or death. The 55 trials meeting these criteria were coded for whether they were published prior to or after the year 2000, whether they registered in clinicaltrials.gov prior to publication, used active or placebo comparator, and whether or not the trial had industry co-sponsorship. We tabulated whether the study reported a positive, negative, or null result on the primary outcome variable and for total mortality. Results 17 of 30 studies (57\%) published prior to 2000 showed a significant benefit of intervention on the primary outcome in comparison to only 2 among the 25 (8\%) trials published after 2000 (χ2=12.2,df= 1, p=0.0005). There has been no change in the proportion of trials that compared treatment to placebo versus active comparator. Industry co-sponsorship was unrelated to the probability of reporting a significant benefit. Pre-registration in clinical trials.gov was strongly associated with the trend toward null findings. Conclusions The number NHLBI trials reporting positive results declined after the year 2000. Prospective declaration of outcomes in RCTs, and the adoption of transparent reporting standards, as required by clinicaltrials.gov, may have contributed to the trend toward null findings.},
  langid = {english},
  keywords = {Cardiovascular diseases,Cardiovascular therapy,Coronary heart disease,Drug therapy,Myocardial infarction,Randomized controlled trials,Sudden cardiac death,Women's health},
  file = {/home/skynet3/Zotero/storage/6GZZDHGK/Kaplan and Irvin - 2015 - Likelihood of Null Effects of Large NHLBI Clinical.pdf;/home/skynet3/Zotero/storage/LI7W6ZXS/article.html}
}

@article{kapoorIrReproducibleMachine,
  ids = {kapoorIrReproducibleMachinea},
  title = {({{Ir}}){{Reproducible Machine Learning}}: {{A Case Study}}},
  author = {Kapoor, Sayash and Narayanan, Arvind},
  pages = {7},
  abstract = {The use of Machine Learning (ML) methods for prediction and forecasting has become widespread across the quantitative sciences. However, there are many known methodological pitfalls in ML-based research. As a case study of these pitfalls, we examine the subfield of civil war prediction in Political Science. Our main finding is that several recent studies published in top Political Science journals claiming superior performance of ML models over Logistic Regression models fail to reproduce. Our results provide two reasons to be skeptical of the use of ML methods in this research area, by both questioning their usefulness and highlighting the pitfalls of applying them correctly. Results identifying pitfalls in studies that use ML methods have appeared in at least eight quantitative science fields. However, we go farther than most previous research to investigate whether the claims made in the reviewed studies survive once the errors are corrected. We argue that there is a reproducibility crisis brewing in research fields that use ML methods and discuss a few systemic interventions that could help resolve it.},
  langid = {english},
  file = {/home/skynet3/Zotero/storage/3HJUG9VP/Kapoor and Narayanan - (Ir)Reproducible Machine Learning A Case Study.pdf}
}

@misc{kapoorLeakageReproducibilityCrisis2022,
  ids = {kapoorLeakageReproducibilityCrisis2022a},
  title = {Leakage and the {{Reproducibility Crisis}} in {{ML-based Science}}},
  author = {Kapoor, Sayash and Narayanan, Arvind},
  date = {2022-07-14},
  number = {arXiv:2207.07048},
  eprint = {2207.07048},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2207.07048},
  urldate = {2022-11-11},
  abstract = {The use of machine learning (ML) methods for prediction and forecasting has become widespread across the quantitative sciences. However, there are many known methodological pitfalls, including data leakage, in ML-based science. In this paper, we systematically investigate reproducibility issues in ML-based science. We show that data leakage is indeed a widespread problem and has led to severe reproducibility failures. Specifically, through a survey of literature in research communities that adopted ML methods, we find 17 fields where errors have been found, collectively affecting 329 papers and in some cases leading to wildly overoptimistic conclusions. Based on our survey, we present a fine-grained taxonomy of 8 types of leakage that range from textbook errors to open research problems. We argue for fundamental methodological changes to ML-based science so that cases of leakage can be caught before publication. To that end, we propose model info sheets for reporting scientific claims based on ML models that would address all types of leakage identified in our survey. To investigate the impact of reproducibility errors and the efficacy of model info sheets, we undertake a reproducibility study in a field where complex ML models are believed to vastly outperform older statistical models such as Logistic Regression (LR): civil war prediction. We find that all papers claiming the superior performance of complex ML models compared to LR models fail to reproduce due to data leakage, and complex ML models don't perform substantively better than decades-old LR models. While none of these errors could have been caught by reading the papers, model info sheets would enable the detection of leakage in each case.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Methodology},
  file = {/home/skynet3/Zotero/storage/TBNMXDS2/Kapoor and Narayanan - 2022 - Leakage and the Reproducibility Crisis in ML-based.pdf;/home/skynet3/Zotero/storage/Q3GZJFLJ/2207.html}
}

@inproceedings{karpinskaPerilsUsingMechanical2021,
  ids = {karpinskaPerilsUsingMechanical2021a},
  title = {The {{Perils}} of {{Using Mechanical Turk}} to {{Evaluate Open-Ended Text Generation}}},
  booktitle = {Proceedings of the 2021 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Karpinska, Marzena and Akoury, Nader and Iyyer, Mohit},
  date = {2021-11},
  pages = {1265--1285},
  publisher = {{Association for Computational Linguistics}},
  location = {{Online and Punta Cana, Dominican Republic}},
  doi = {10.18653/v1/2021.emnlp-main.97},
  url = {https://aclanthology.org/2021.emnlp-main.97},
  urldate = {2022-11-11},
  abstract = {Recent text generation research has increasingly focused on open-ended domains such as story and poetry generation. Because models built for such tasks are difficult to evaluate automatically, most researchers in the space justify their modeling choices by collecting crowdsourced human judgments of text quality (e.g., Likert scores of coherence or grammaticality) from Amazon Mechanical Turk (AMT). In this paper, we first conduct a survey of 45 open-ended text generation papers and find that the vast majority of them fail to report crucial details about their AMT tasks, hindering reproducibility. We then run a series of story evaluation experiments with both AMT workers and English teachers and discover that even with strict qualification filters, AMT workers (unlike teachers) fail to distinguish between model-generated text and human-generated references. We show that AMT worker judgments improve when they are shown model-generated output alongside human-generated references, which enables the workers to better calibrate their ratings. Finally, interviews with the English teachers provide deeper insights into the challenges of the evaluation process, particularly when rating model-generated text.},
  eventtitle = {{{EMNLP}} 2021},
  file = {/home/skynet3/Zotero/storage/GQDDMC3X/Karpinska et al. - 2021 - The Perils of Using Mechanical Turk to Evaluate Op.pdf}
}

@online{KarthikRstudio2019Resources,
  title = {Karthik/Rstudio2019: {{Resources}} from My {{Rstudio}}::Conf 2019 Talk},
  shorttitle = {Karthik/Rstudio2019},
  url = {https://github.com/karthik/rstudio2019},
  urldate = {2022-11-11},
  abstract = {Resources from my Rstudio::conf 2019 talk. Contribute to karthik/rstudio2019 development by creating an account on GitHub.},
  langid = {english},
  organization = {{GitHub}},
  file = {/home/skynet3/Zotero/storage/BAEHHWGB/rstudio2019.html}
}

@misc{kashaniDeepLearningInterviews2022,
  title = {Deep {{Learning Interviews}}: {{Hundreds}} of Fully Solved Job Interview Questions from a Wide Range of Key Topics in {{AI}}},
  shorttitle = {Deep {{Learning Interviews}}},
  author = {Kashani, Shlomo and Ivry, Amir},
  date = {2022-01-04},
  number = {arXiv:2201.00650},
  eprint = {2201.00650},
  eprinttype = {arxiv},
  primaryclass = {cs, math},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2201.00650},
  url = {http://arxiv.org/abs/2201.00650},
  urldate = {2022-11-11},
  abstract = {The second edition of Deep Learning Interviews is home to hundreds of fully-solved problems, from a wide range of key topics in AI. It is designed to both rehearse interview or exam specific topics and provide machine learning MSc / PhD. students, and those awaiting an interview a well-organized overview of the field. The problems it poses are tough enough to cut your teeth on and to dramatically improve your skills-but they're framed within thought-provoking questions and engaging stories. That is what makes the volume so specifically valuable to students and job seekers: it provides them with the ability to speak confidently and quickly on any relevant topic, to answer technical questions clearly and correctly, and to fully understand the purpose and meaning of interview questions and answers. Those are powerful, indispensable advantages to have when walking into the interview room. The book's contents is a large inventory of numerous topics relevant to DL job interviews and graduate level exams. That places this work at the forefront of the growing trend in science to teach a core set of practical mathematical and computational skills. It is widely accepted that the training of every computer scientist must include the fundamental theorems of ML, and AI appears in the curriculum of nearly every university. This volume is designed as an excellent reference for graduates of such programs.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Information Theory,Computer Science - Machine Learning},
  file = {/home/skynet3/Zotero/storage/RMFR7M8R/Kashani and Ivry - 2022 - Deep Learning Interviews Hundreds of fully solved.pdf;/home/skynet3/Zotero/storage/RHP2YNU9/2201.html}
}

@online{KassambaraSurvminerSurvival,
  title = {Kassambara/Survminer: {{Survival Analysis}} and {{Visualization}}},
  shorttitle = {Kassambara/Survminer},
  url = {https://github.com/kassambara/survminer},
  urldate = {2022-11-11},
  abstract = {Survival Analysis and Visualization. Contribute to kassambara/survminer development by creating an account on GitHub.},
  langid = {english},
  organization = {{GitHub}},
  file = {/home/skynet3/Zotero/storage/VIVI4NFR/survminer.html}
}

@software{kayGgdistVisualizationsDistributions2022,
  title = {Ggdist: {{Visualizations}} of Distributions and Uncertainty},
  shorttitle = {Ggdist},
  author = {Kay, Matthew},
  date = {2022-11-02T17:03:42Z},
  origdate = {2020-06-05T02:26:40Z},
  url = {https://github.com/mjskay/ggdist/blob/fe1f5c4d3d06354384872b1d4006e3bbc9ec6c7e/figures-source/cheat_sheet-slabinterval.pdf},
  urldate = {2022-11-11},
  abstract = {Visualizations of distributions and uncertainty}
}

@misc{kazemiRepresentationLearningDynamic2019,
  ids = {kazemiRepresentationLearningDynamic2019a},
  title = {Representation {{Learning}} for {{Dynamic Graphs}}: {{A Survey}}},
  shorttitle = {Representation {{Learning}} for {{Dynamic Graphs}}},
  author = {Kazemi, Seyed Mehran and Goel, Rishab and Jain, Kshitij and Kobyzev, Ivan and Sethi, Akshay and Forsyth, Peter and Poupart, Pascal},
  date = {2019-05-27},
  number = {arXiv:1905.11485},
  eprint = {1905.11485},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/1905.11485},
  urldate = {2022-11-11},
  abstract = {Graphs arise naturally in many real-world applications including social networks, recommender systems, ontologies, biology, and computational finance. Traditionally, machine learning models for graphs have been mostly designed for static graphs. However, many applications involve evolving graphs. This introduces important challenges for learning and inference since nodes, attributes, and edges change over time. In this survey, we review the recent advances in representation learning for dynamic graphs, including dynamic knowledge graphs. We describe existing models from an encoder-decoder perspective, categorize these encoders and decoders based on the techniques they employ, and analyze the approaches in each category. We also review several prominent applications and widely used datasets and highlight directions for future research.},
  archiveprefix = {arXiv},
  version = {1},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/skynet3/Zotero/storage/MPWTX5K5/Kazemi et al. - 2019 - Representation Learning for Dynamic Graphs A Surv.pdf;/home/skynet3/Zotero/storage/2623AZ6X/1905.html}
}

@online{KeepUsingPlate,
  title = {Keep Using Plate Notation},
  url = {https://davidrushingdewhurst.com/blog/2020-07-28keep-using-plate-notation.html},
  urldate = {2022-11-11},
  file = {/home/skynet3/Zotero/storage/J2NXQ35I/2020-07-28keep-using-plate-notation.html}
}

@misc{keLearningInduceCausal2022,
  title = {Learning to {{Induce Causal Structure}}},
  author = {Ke, Nan Rosemary and Chiappa, Silvia and Wang, Jane and Goyal, Anirudh and Bornschein, Jorg and Rey, Melanie and Weber, Theophane and Botvinic, Matthew and Mozer, Michael and Rezende, Danilo Jimenez},
  date = {2022-10-07},
  number = {arXiv:2204.04875},
  eprint = {2204.04875},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2204.04875},
  url = {http://arxiv.org/abs/2204.04875},
  urldate = {2022-11-11},
  abstract = {The fundamental challenge in causal induction is to infer the underlying graph structure given observational and/or interventional data. Most existing causal induction algorithms operate by generating candidate graphs and evaluating them using either score-based methods (including continuous optimization) or independence tests. In our work, we instead treat the inference process as a black box and design a neural network architecture that learns the mapping from both observational and interventional data to graph structures via supervised training on synthetic graphs. The learned model generalizes to new synthetic graphs, is robust to train-test distribution shifts, and achieves state-of-the-art performance on naturalistic graphs for low sample complexity.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/skynet3/Zotero/storage/PGUX3D8N/Ke et al. - 2022 - Learning to Induce Causal Structure.pdf;/home/skynet3/Zotero/storage/4ZUMLLIA/2204.html}
}

@inproceedings{keLightGBMHighlyEfficient2017,
  ids = {keLightGBMHighlyEfficient2017a},
  title = {{{LightGBM}}: {{A Highly Efficient Gradient Boosting Decision Tree}}},
  shorttitle = {{{LightGBM}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Ke, Guolin and Meng, Qi and Finley, Thomas and Wang, Taifeng and Chen, Wei and Ma, Weidong and Ye, Qiwei and Liu, Tie-Yan},
  date = {2017},
  volume = {30},
  publisher = {{Curran Associates, Inc.}},
  url = {https://papers.nips.cc/paper/2017/hash/6449f44a102fde848669bdd9eb6b76fa-Abstract.html},
  urldate = {2022-11-11},
  abstract = {Gradient Boosting Decision Tree (GBDT) is a popular machine learning algorithm, and has quite a few effective implementations such as XGBoost and pGBRT. Although many engineering optimizations have been adopted in these implementations, the efficiency and scalability are still unsatisfactory when the feature dimension is high and data size is large. A major reason is that for each feature, they need to scan all the data instances to estimate the information gain of all possible split points, which is very time consuming. To tackle this problem, we propose two novel techniques: \textbackslash emph\{Gradient-based One-Side Sampling\} (GOSS) and \textbackslash emph\{Exclusive Feature Bundling\} (EFB). With GOSS, we exclude a significant proportion of data instances with small gradients, and only use the rest to estimate the information gain. We prove that, since the data instances with larger gradients play a more important role in the computation of information gain, GOSS can obtain quite accurate estimation of the information gain with a much smaller data size. With EFB, we bundle mutually exclusive features (i.e., they rarely take nonzero values simultaneously), to reduce the number of features. We prove that finding the optimal bundling of exclusive features is NP-hard, but a greedy algorithm can achieve quite good approximation ratio (and thus can effectively reduce the number of features without hurting the accuracy of split point determination by much). We call our new GBDT implementation with GOSS and EFB \textbackslash emph\{LightGBM\}. Our experiments on multiple public datasets show that, LightGBM speeds up the training process of conventional GBDT by up to over 20 times while achieving almost the same accuracy.},
  file = {/home/skynet3/Zotero/storage/LZB8JCPS/Ke et al. - 2017 - LightGBM A Highly Efficient Gradient Boosting Dec.pdf}
}

@online{kellybrendelTimeAssumeThat2021,
  title = {Time to Assume That Health Research Is Fraudulent until Proven Otherwise?},
  author = {{kellybrendel}},
  date = {2021-07-05T16:41:45+00:00},
  url = {https://blogs.bmj.com/bmj/2021/07/05/time-to-assume-that-health-research-is-fraudulent-until-proved-otherwise/},
  urldate = {2022-11-11},
  abstract = {Health research is based on trust. Health professionals and journal editors reading the results of a clinical trial assume that the trial happened and that the results were honestly reported. [...]More...},
  langid = {american},
  organization = {{The BMJ}},
  file = {/home/skynet3/Zotero/storage/7YXBVSLY/time-to-assume-that-health-research-is-fraudulent-until-proved-otherwise.html}
}

@misc{kellyStandardErrorsPersistence2019,
  type = {SSRN Scholarly Paper},
  ids = {kellyStandardErrorsPersistence2019a},
  title = {The {{Standard Errors}} of {{Persistence}}},
  author = {Kelly, Morgan},
  date = {2019-06-03},
  number = {3398303},
  location = {{Rochester, NY}},
  doi = {10.2139/ssrn.3398303},
  url = {https://papers.ssrn.com/abstract=3398303},
  urldate = {2022-11-11},
  abstract = {A large literature on persistence finds that many modern outcomes strongly reflect characteristics of the same places in the distant past. However, alongside unusually high t statistics, these regressions display severe spatial autocorrelation in residuals, and the purpose of this paper is to examine whether these two properties might be connected. We start by running artificial regressions where both variables are spatial noise and find that, even for modest ranges of spatial correlation between points, t statistics become severely inflated leading to significance levels that are in error by several orders of magnitude. We analyse 27 persistence studies in leading journals and find that in most cases if we replace the main explanatory variable with spatial noise the fit of the regression commonly improves; and if we replace the dependent variable with spatial noise, the persistence variable can still explain it at high significance levels. We can predict in advance which persistence results might be the outcome of fitting spatial noise from the degree of spatial autocorrelation in their residuals measured by a standard Moran statistic. Our findings suggest that the results of persistence studies, and of spatial regressions more generally, might be treated with some caution in the absence of reported Moran statistics and noise simulations.},
  langid = {english},
  keywords = {Deep Origins,Inflated t Statistics,Persistence,Spatial Noise},
  file = {/home/skynet3/Zotero/storage/DVCHGTZ3/Kelly - 2019 - The Standard Errors of Persistence.pdf;/home/skynet3/Zotero/storage/N9Z5G6A2/papers.html}
}

@misc{kennedyNonparametricCausalEffects2018,
  title = {Nonparametric Causal Effects Based on Incremental Propensity Score Interventions},
  author = {Kennedy, Edward H.},
  date = {2018-06-18},
  number = {arXiv:1704.00211},
  eprint = {1704.00211},
  eprinttype = {arxiv},
  primaryclass = {stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1704.00211},
  url = {http://arxiv.org/abs/1704.00211},
  urldate = {2022-11-11},
  abstract = {Most work in causal inference considers deterministic interventions that set each unit's treatment to some fixed value. However, under positivity violations these interventions can lead to non-identification, inefficiency, and effects with little practical relevance. Further, corresponding effects in longitudinal studies are highly sensitive to the curse of dimensionality, resulting in widespread use of unrealistic parametric models. We propose a novel solution to these problems: incremental interventions that shift propensity score values rather than set treatments to fixed values. Incremental interventions have several crucial advantages. First, they avoid positivity assumptions entirely. Second, they require no parametric assumptions and yet still admit a simple characterization of longitudinal effects, independent of the number of timepoints. For example, they allow longitudinal effects to be visualized with a single curve instead of lists of coefficients. After characterizing these incremental interventions and giving identifying conditions for corresponding effects, we also develop general efficiency theory, propose efficient nonparametric estimators that can attain fast convergence rates even when incorporating flexible machine learning, and propose a bootstrap-based confidence band and simultaneous test of no treatment effect. Finally we explore finite-sample performance via simulation, and apply the methods to study time-varying sociological effects of incarceration on entry into marriage.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Methodology},
  file = {/home/skynet3/Zotero/storage/X7AJ9PIZ/Kennedy - 2018 - Nonparametric causal effects based on incremental .pdf;/home/skynet3/Zotero/storage/AUJE756Q/1704.html}
}

@article{kennedyOhNoGot2005,
  title = {Oh {{No}}! {{I Got}} the {{Wrong Sign}}! {{What Should I Do}}?},
  author = {Kennedy, Peter E.},
  date = {2005-01-01},
  journaltitle = {The Journal of Economic Education},
  volume = {36},
  number = {1},
  pages = {77--92},
  publisher = {{Routledge}},
  issn = {0022-0485},
  doi = {10.3200/JECE.36.1.77-92},
  url = {https://doi.org/10.3200/JECE.36.1.77-92},
  urldate = {2022-11-11},
  abstract = {Getting a "wrong" sign in empirical work is a common phenomenon. Remarkably, econometrics textbooks provide very little information to practitioners on how this problem can arise. The author exposits a long list of ways in which a wrong sign can occur and how it might be corrected.},
  keywords = {data mining,false significance,misspecification},
  annotation = {\_eprint: https://doi.org/10.3200/JECE.36.1.77-92},
  file = {/home/skynet3/Zotero/storage/4IMNLCCJ/Kennedy - 2005 - Oh No! I Got the Wrong Sign! What Should I Do.pdf}
}

@misc{kennedyOptimalDoublyRobust2022,
  title = {Towards Optimal Doubly Robust Estimation of Heterogeneous Causal Effects},
  author = {Kennedy, Edward H.},
  date = {2022-05-26},
  number = {arXiv:2004.14497},
  eprint = {2004.14497},
  eprinttype = {arxiv},
  primaryclass = {math, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2004.14497},
  url = {http://arxiv.org/abs/2004.14497},
  urldate = {2022-11-11},
  abstract = {Heterogeneous effect estimation plays a crucial role in causal inference, with applications across medicine and social science. Many methods for estimating conditional average treatment effects (CATEs) have been proposed in recent years, but there are important theoretical gaps in understanding if and when such methods are optimal. This is especially true when the CATE has nontrivial structure (e.g., smoothness or sparsity). Our work contributes in several main ways. First, we study a two-stage doubly robust CATE estimator and give a generic model-free error bound, which, despite its generality, yields sharper results than those in the current literature. We apply the bound to derive error rates in nonparametric models with smoothness or sparsity, and give sufficient conditions for oracle efficiency. Underlying our error bound is a general oracle inequality for regression with estimated or imputed outcomes, which is of independent interest; this is the second main contribution. The third contribution is aimed at understanding the fundamental statistical limits of CATE estimation. To that end, we propose and study a local polynomial adaptation of double-residual regression. We show that this estimator can be oracle efficient under even weaker conditions, if used with a specialized form of sample splitting and careful choices of tuning parameters. These are the weakest conditions currently found in the literature, and we conjecture that they are minimal in a minimax sense. We go on to give error bounds in the non-trivial regime where oracle rates cannot be achieved. Some finite-sample properties are explored with simulations.},
  archiveprefix = {arXiv},
  keywords = {Mathematics - Statistics Theory},
  file = {/home/skynet3/Zotero/storage/X3NPU75L/Kennedy - 2022 - Towards optimal doubly robust estimation of hetero.pdf;/home/skynet3/Zotero/storage/UEWY658F/2004.html}
}

@online{KeonAwesomenlpCurated,
  title = {Keon/Awesome-Nlp: {{A}} Curated List of Resources Dedicated to {{Natural Language Processing}} ({{NLP}})},
  shorttitle = {Keon/Awesome-Nlp},
  url = {https://github.com/keon/awesome-nlp},
  urldate = {2022-11-11},
  abstract = {:book: A curated list of resources dedicated to Natural Language Processing (NLP) - keon/awesome-nlp: A curated list of resources dedicated to Natural Language Processing (NLP)},
  langid = {english},
  organization = {{GitHub}},
  file = {/home/skynet3/Zotero/storage/CDYUQ94L/awesome-nlp.html}
}

@article{kepesQuestionableResearchPractices2022,
  title = {Questionable Research Practices among Researchers in the Most Research-Productive Management Programs},
  author = {Kepes, Sven and Keener, Sheila K. and McDaniel, Michael A. and Hartman, Nathan S.},
  date = {2022},
  journaltitle = {Journal of Organizational Behavior},
  volume = {43},
  number = {7},
  pages = {1190--1208},
  issn = {1099-1379},
  doi = {10.1002/job.2623},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/job.2623},
  urldate = {2022-11-11},
  abstract = {Questionable research practices (QRPs) among researchers have been a source of concern in many fields of study. QRPs are often used to enhance the probability of achieving statistical significance which affects the likelihood of a paper being published. Using a sample of researchers from 10 top research-productive management programs, we compared hypotheses tested in dissertations to those tested in journal articles derived from those dissertations to draw inferences concerning the extent of engagement in QRPs. Results indicated that QRPs related to changes in sample size and covariates were associated with unsupported dissertation hypotheses becoming supported in journal articles. Researchers also tended to exclude unsupported dissertation hypotheses from journal articles. Likewise, results suggested that many article hypotheses may have been created after the results were known (i.e., HARKed). Articles from prestigious journals contained a higher percentage of potentially HARKed hypotheses than those from less well-regarded journals. Finally, articles published in prestigious journals were associated with more QRP usage than less prestigious journals. QRPs increase in the percentage of supported hypotheses and result in effect sizes that likely overestimate population parameters. As such, results reported in articles published in our most prestigious journals may be less credible than previously believed.},
  langid = {english},
  keywords = {Chrysalis Effect,HARKing,questionable research practices,research integrity},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/job.2623},
  file = {/home/skynet3/Zotero/storage/XX5U9KQ7/Kepes et al. - 2022 - Questionable research practices among researchers .pdf;/home/skynet3/Zotero/storage/L7SVSHLS/job.html}
}

@article{khaliliaPredictingDiseaseRisks2011,
  title = {Predicting Disease Risks from Highly Imbalanced Data Using Random Forest},
  author = {Khalilia, Mohammed and Chakraborty, Sounak and Popescu, Mihail},
  date = {2011-07-29},
  journaltitle = {BMC Medical Informatics and Decision Making},
  shortjournal = {BMC Med Inform Decis Mak},
  volume = {11},
  eprint = {21801360},
  eprinttype = {pmid},
  pages = {51},
  issn = {1472-6947},
  doi = {10.1186/1472-6947-11-51},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3163175/},
  urldate = {2022-11-11},
  abstract = {Background We present a method utilizing Healthcare Cost and Utilization Project (HCUP) dataset for predicting disease risk of individuals based on their medical diagnosis history. The presented methodology may be incorporated in a variety of applications such as risk management, tailored health communication and decision support systems in healthcare. Methods We employed the National Inpatient Sample (NIS) data, which is publicly available through Healthcare Cost and Utilization Project (HCUP), to train random forest classifiers for disease prediction. Since the HCUP data is highly imbalanced, we employed an ensemble learning approach based on repeated random sub-sampling. This technique divides the training data into multiple sub-samples, while ensuring that each sub-sample is fully balanced. We compared the performance of support vector machine (SVM), bagging, boosting and RF to predict the risk of eight chronic diseases. Results We predicted eight disease categories. Overall, the RF ensemble learning method outperformed SVM, bagging and boosting in terms of the area under the receiver operating characteristic (ROC) curve (AUC). In addition, RF has the advantage of computing the importance of each variable in the classification process. Conclusions In combining repeated random sub-sampling with RF, we were able to overcome the class imbalance problem and achieve promising results. Using the national HCUP data set, we predicted eight disease categories with an average AUC of 88.79\%.},
  pmcid = {PMC3163175},
  file = {/home/skynet3/Zotero/storage/GU55J8GN/Khalilia et al. - 2011 - Predicting disease risks from highly imbalanced da.pdf}
}

@software{kharratzadehSplinesStan2022,
  title = {Splines in {{Stan}}},
  author = {Kharratzadeh, Milad},
  date = {2022-02-22T16:32:17Z},
  origdate = {2017-02-06T14:54:59Z},
  url = {https://github.com/milkha/Splines_in_Stan/blob/912ba64276360893195c442dacc1c61dd682b033/splines_in_stan.pdf},
  urldate = {2022-11-11},
  abstract = {Implementation of B-Splines in Stan}
}

@misc{kimCausalGraphicalViews2019,
  title = {Causal {{Graphical Views}} of {{Fixed Effects}} and {{Random Effects Models}}},
  author = {Kim, Yongnam and Steiner, Peter M.},
  date = {2019-07-28T04:04:18},
  publisher = {{PsyArXiv}},
  doi = {10.31234/osf.io/cxd2n},
  url = {https://psyarxiv.com/cxd2n/},
  urldate = {2022-11-11},
  abstract = {Despite the long-standing discussion on fixed effects (FE) and random effects (RE) models, how and under which conditions both methods can eliminate unmeasured confounding bias have not yet been widely understood in practice. Using a simple pretest-posttest design in a linear setting, this article translates the conventional algebraic formalization of FE and RE models into causal graphs and provides intuitively accessible graphical explanations about their data-generating and bias-removing processes. The proposed causal graphs highlight that FE and RE models consider different data-generating models. RE models presume a data-generating model that is identical to a randomized controlled trial while FE models allow for unobserved time-invariant treatment-outcome confounding. Augmenting regular causal graphs that describe data-generating processes by adding the computational structures of FE and RE estimators, the article visualizes how FE estimators (gain score and deviation score estimators) and RE estimators (quasi-deviation score estimator) offset unmeasured confounding bias. In contrast to standard regression or matching estimators that reduce confounding bias by blocking non-causal paths via conditioning, FE and RE estimators offset confounding bias by deliberately creating new non-causal paths and associations of opposite sign. Though FE and RE estimators are similar in their bias-offsetting mechanisms, the augmented graphs reveal their subtle differences that can result in different biases in observational studies.},
  langid = {american},
  keywords = {bias-offsetting,causal graph,demeaning,Educational Psychology,fixed effect,gain score,Quantitative Methods,random effect,Social and Behavioral Sciences,Statistical Methods},
  file = {/home/skynet3/Zotero/storage/KWL3CMMY/Kim and Steiner - 2019 - Causal Graphical Views of Fixed Effects and Random.pdf}
}

@article{kingReplicationReplication1995,
  ids = {kingReplicationReplication1995a},
  title = {Replication, {{Replication}}},
  author = {King, Gary},
  date = {1995},
  journaltitle = {PS: Political Science and Politics},
  volume = {28},
  pages = {444--452},
  file = {/home/skynet3/Zotero/storage/F88ISU2K/replication-abs.html}
}

@article{kingWhyPropensityScores2019,
  ids = {kingWhyPropensityScores2019a},
  title = {Why {{Propensity Scores Should Not Be Used}} for {{Matching}}},
  author = {King, Gary and Nielsen, Richard},
  date = {2019},
  journaltitle = {Political Analysis},
  volume = {27},
  number = {4},
  pages = {435--454},
  file = {/home/skynet3/Zotero/storage/GKM958KV/why-propensity-scores-should-not-be-used-formatching.html}
}

@misc{kiniAnalyticStudyDouble2020,
  ids = {kiniAnalyticStudyDouble2020a},
  title = {Analytic {{Study}} of {{Double Descent}} in {{Binary Classification}}: {{The Impact}} of {{Loss}}},
  shorttitle = {Analytic {{Study}} of {{Double Descent}} in {{Binary Classification}}},
  author = {Kini, Ganesh and Thrampoulidis, Christos},
  date = {2020-01-30},
  number = {arXiv:2001.11572},
  eprint = {2001.11572},
  eprinttype = {arxiv},
  primaryclass = {cs, eess, math, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2001.11572},
  url = {http://arxiv.org/abs/2001.11572},
  urldate = {2022-11-11},
  abstract = {Extensive empirical evidence reveals that, for a wide range of different learning methods and datasets, the risk curve exhibits a double-descent (DD) trend as a function of the model size. In a recent paper [Zeyu,Kammoun,Thrampoulidis,2019] the authors studied binary linear classification models and showed that the test error of gradient descent (GD) with logistic loss undergoes a DD. In this paper, we complement these results by extending them to GD with square loss. We show that the DD phenomenon persists, but we also identify several differences compared to logistic loss. This emphasizes that crucial features of DD curves (such as their transition threshold and global minima) depend both on the training data and on the learning algorithm. We further study the dependence of DD curves on the size of the training set. Similar to our earlier work, our results are analytic: we plot the DD curves by first deriving sharp asymptotics for the test error under Gaussian features. Albeit simple, the models permit a principled study of DD features, the outcomes of which theoretically corroborate related empirical findings occurring in more complex learning tasks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Information Theory,Computer Science - Machine Learning,Electrical Engineering and Systems Science - Signal Processing,Statistics - Machine Learning},
  file = {/home/skynet3/Zotero/storage/GY5LW3AL/Kini and Thrampoulidis - 2020 - Analytic Study of Double Descent in Binary Classif.pdf;/home/skynet3/Zotero/storage/STWJU8C9/2001.html}
}

@misc{kleinComparingPublishedScientific2016,
  title = {Comparing {{Published Scientific Journal Articles}} to {{Their Pre-print Versions}}},
  author = {Klein, Martin and Broadwell, Peter and Farb, Sharon E. and Grappone, Todd},
  date = {2016-04-18},
  number = {arXiv:1604.05363},
  eprint = {1604.05363},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/1604.05363},
  urldate = {2022-11-11},
  abstract = {Academic publishers claim that they add value to scholarly communications by coordinating reviews and contributing and enhancing text during publication. These contributions come at a considerable cost: U.S. academic libraries paid \$1.7 billion for serial subscriptions in 2008 alone. Library budgets, in contrast, are flat and not able to keep pace with serial price inflation. We have investigated the publishers' value proposition by conducting a comparative study of pre-print papers and their final published counterparts. This comparison had two working assumptions: 1) if the publishers' argument is valid, the text of a pre-print paper should vary measurably from its corresponding final published version, and 2) by applying standard similarity measures, we should be able to detect and quantify such differences. Our analysis revealed that the text contents of the scientific papers generally changed very little from their pre-print to final published versions. These findings contribute empirical indicators to discussions of the added value of commercial publishers and therefore should influence libraries' economic decisions regarding access to scholarly publications.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Digital Libraries},
  file = {/home/skynet3/Zotero/storage/8CMUQUYM/Klein et al. - 2016 - Comparing Published Scientific Journal Articles to.pdf;/home/skynet3/Zotero/storage/7VX3PINC/1604.html}
}

@online{KoalaverseVipVariable,
  title = {Koalaverse/Vip: {{Variable Importance Plots}} ({{VIPs}})},
  shorttitle = {Koalaverse/Vip},
  url = {https://github.com/koalaverse/vip},
  urldate = {2022-11-11},
  abstract = {Variable Importance Plots (VIPs). Contribute to koalaverse/vip development by creating an account on GitHub.},
  langid = {english},
  organization = {{GitHub}},
  file = {/home/skynet3/Zotero/storage/UB4ZH5T8/vip.html}
}

@misc{kohlerOverparametrizedDeepNeural2020,
  title = {Over-Parametrized Deep Neural Networks Do Not Generalize Well},
  author = {Kohler, Michael and Krzyzak, Adam},
  date = {2020-01-14},
  number = {arXiv:1912.03925},
  eprint = {1912.03925},
  eprinttype = {arxiv},
  primaryclass = {math, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1912.03925},
  url = {http://arxiv.org/abs/1912.03925},
  urldate = {2022-11-11},
  abstract = {Recently it was shown in several papers that backpropagation is able to find the global minimum of the empirical risk on the training data using over-parametrized deep neural networks. In this paper a similar result is shown for deep neural networks with the sigmoidal squasher activation function in a regression setting, and a lower bound is presented which proves that these networks do not generalize well on a new data in the sense that they do not achieve the optimal minimax rate of convergence for estimation of smooth regression functions.},
  archiveprefix = {arXiv},
  keywords = {Mathematics - Statistics Theory},
  file = {/home/skynet3/Zotero/storage/LXJWY2ZU/Kohler and Krzyzak - 2020 - Over-parametrized deep neural networks do not gene.pdf;/home/skynet3/Zotero/storage/QVVT7X7S/1912.html}
}

@article{koltunHindexNoLonger2021,
  title = {The H-Index Is No Longer an Effective Correlate of Scientific Reputation},
  author = {Koltun, Vladlen and Hafner, David},
  date = {2021-06-28},
  journaltitle = {PLOS ONE},
  shortjournal = {PLOS ONE},
  volume = {16},
  number = {6},
  pages = {e0253397},
  publisher = {{Public Library of Science}},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0253397},
  url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0253397},
  urldate = {2022-11-11},
  abstract = {The impact of individual scientists is commonly quantified using citation-based measures. The most common such measure is the h-index. A scientist’s h-index affects hiring, promotion, and funding decisions, and thus shapes the progress of science. Here we report a large-scale study of scientometric measures, analyzing millions of articles and hundreds of millions of citations across four scientific fields and two data platforms. We find that the correlation of the h-index with awards that indicate recognition by the scientific community has substantially declined. These trends are associated with changing authorship patterns. We show that these declines can be mitigated by fractional allocation of citations among authors, which has been discussed in the literature but not implemented at scale. We find that a fractional analogue of the h-index outperforms other measures as a correlate and predictor of scientific awards. Our results suggest that the use of the h-index in ranking scientists should be reconsidered, and that fractional allocation measures such as h-frac provide more robust alternatives.},
  langid = {english},
  keywords = {Bibliometrics,Careers,Citation analysis,Computer and information sciences,Econometrics,Physicists,Scientists,Scientometrics},
  file = {/home/skynet3/Zotero/storage/9FQ247DD/Koltun and Hafner - 2021 - The h-index is no longer an effective correlate of.pdf}
}

@misc{kowalskiHowExamineExternal2018,
  type = {Working Paper},
  title = {How to {{Examine External Validity Within}} an {{Experiment}}},
  author = {Kowalski, Amanda E.},
  date = {2018-07},
  series = {Working {{Paper Series}}},
  number = {24834},
  publisher = {{National Bureau of Economic Research}},
  doi = {10.3386/w24834},
  url = {https://www.nber.org/papers/w24834},
  urldate = {2022-11-11},
  abstract = {A fundamental concern for researchers who analyze and design experiments is that the estimate obtained from the experiment might not be externally valid for other policies of interest. Researchers often attempt to assess external validity by comparing data from an experiment to external data. In this paper, I discuss approaches from the treatment effects literature that researchers can use to begin the examination of external validity internally, within the data from a single experiment. I focus on presenting the approaches simply using stylized examples.},
  file = {/home/skynet3/Zotero/storage/IFUPAA8Z/Kowalski - 2018 - How to Examine External Validity Within an Experim.pdf}
}

@article{kowsariTextClassificationAlgorithms2019,
  title = {Text {{Classification Algorithms}}: {{A Survey}}},
  shorttitle = {Text {{Classification Algorithms}}},
  author = {Kowsari, Kamran and Meimandi, Kiana Jafari and Heidarysafa, Mojtaba and Mendu, Sanjana and Barnes, Laura E. and Brown, Donald E.},
  date = {2019-04-23},
  journaltitle = {Information},
  shortjournal = {Information},
  volume = {10},
  number = {4},
  eprint = {1904.08067},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  pages = {150},
  issn = {2078-2489},
  doi = {10.3390/info10040150},
  url = {http://arxiv.org/abs/1904.08067},
  urldate = {2022-11-11},
  abstract = {In recent years, there has been an exponential growth in the number of complex documents and texts that require a deeper understanding of machine learning methods to be able to accurately classify texts in many applications. Many machine learning approaches have achieved surpassing results in natural language processing. The success of these learning algorithms relies on their capacity to understand complex models and non-linear relationships within data. However, finding suitable structures, architectures, and techniques for text classification is a challenge for researchers. In this paper, a brief overview of text classification algorithms is discussed. This overview covers different text feature extractions, dimensionality reduction methods, existing algorithms and techniques, and evaluations methods. Finally, the limitations of each technique and their application in the real-world problem are discussed.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Information Retrieval,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/skynet3/Zotero/storage/HJZMLX28/Kowsari et al. - 2019 - Text Classification Algorithms A Survey.pdf;/home/skynet3/Zotero/storage/J9STADN8/1904.html}
}

@article{kreutzerQualityGlanceAudit2022,
  title = {Quality at a {{Glance}}: {{An Audit}} of {{Web-Crawled Multilingual Datasets}}},
  shorttitle = {Quality at a {{Glance}}},
  author = {Kreutzer, Julia and Caswell, Isaac and Wang, Lisa and Wahab, Ahsan and van Esch, Daan and Ulzii-Orshikh, Nasanbayar and Tapo, Allahsera and Subramani, Nishant and Sokolov, Artem and Sikasote, Claytone and Setyawan, Monang and Sarin, Supheakmungkol and Samb, Sokhar and Sagot, Benoît and Rivera, Clara and Rios, Annette and Papadimitriou, Isabel and Osei, Salomey and Suarez, Pedro Ortiz and Orife, Iroro and Ogueji, Kelechi and Rubungo, Andre Niyongabo and Nguyen, Toan Q. and Müller, Mathias and Müller, André and Muhammad, Shamsuddeen Hassan and Muhammad, Nanda and Mnyakeni, Ayanda and Mirzakhalov, Jamshidbek and Matangira, Tapiwanashe and Leong, Colin and Lawson, Nze and Kudugunta, Sneha and Jernite, Yacine and Jenny, Mathias and Firat, Orhan and Dossou, Bonaventure F. P. and Dlamini, Sakhile and de Silva, Nisansa and Ballı, Sakine Çabuk and Biderman, Stella and Battisti, Alessia and Baruwa, Ahmed and Bapna, Ankur and Baljekar, Pallavi and Azime, Israel Abebe and Awokoya, Ayodele and Ataman, Duygu and Ahia, Orevaoghene and Ahia, Oghenefego and Agrawal, Sweta and Adeyemi, Mofetoluwa},
  options = {useprefix=true},
  date = {2022-01-31},
  journaltitle = {Transactions of the Association for Computational Linguistics},
  volume = {10},
  eprint = {2103.12028},
  eprinttype = {arxiv},
  primaryclass = {cs},
  pages = {50--72},
  issn = {2307-387X},
  doi = {10.1162/tacl_a_00447},
  url = {http://arxiv.org/abs/2103.12028},
  urldate = {2022-11-11},
  abstract = {With the success of large-scale pre-training and multilingual modeling in Natural Language Processing (NLP), recent years have seen a proliferation of large, web-mined text datasets covering hundreds of languages. We manually audit the quality of 205 language-specific corpora released with five major public datasets (CCAligned, ParaCrawl, WikiMatrix, OSCAR, mC4). Lower-resource corpora have systematic issues: At least 15 corpora have no usable text, and a significant fraction contains less than 50\% sentences of acceptable quality. In addition, many are mislabeled or use nonstandard/ambiguous language codes. We demonstrate that these issues are easy to detect even for non-proficient speakers, and supplement the human audit with automatic analyses. Finally, we recommend techniques to evaluate and improve multilingual corpora and discuss potential risks that come with low-quality data releases.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/home/skynet3/Zotero/storage/TRGI5Q9P/Kreutzer et al. - 2022 - Quality at a Glance An Audit of Web-Crawled Multi.pdf;/home/skynet3/Zotero/storage/SQTHNGAX/2103.html}
}

@article{kruschkeBayesianEstimationSupersedes2013,
  title = {Bayesian Estimation Supersedes the t Test},
  author = {Kruschke, John K.},
  date = {2013-05},
  journaltitle = {Journal of Experimental Psychology. General},
  shortjournal = {J Exp Psychol Gen},
  volume = {142},
  number = {2},
  eprint = {22774788},
  eprinttype = {pmid},
  pages = {573--603},
  issn = {1939-2222},
  doi = {10.1037/a0029146},
  abstract = {Bayesian estimation for 2 groups provides complete distributions of credible values for the effect size, group means and their difference, standard deviations and their difference, and the normality of the data. The method handles outliers. The decision rule can accept the null value (unlike traditional t tests) when certainty in the estimate is high (unlike Bayesian model comparison using Bayes factors). The method also yields precise estimates of statistical power for various research goals. The software and programs are free and run on Macintosh, Windows, and Linux platforms.},
  langid = {english},
  keywords = {Bayes Theorem,Confidence Intervals,Software},
  file = {/home/skynet3/Zotero/storage/XDQWLLEV/Kruschke - 2013 - Bayesian estimation supersedes the t test.pdf}
}

@misc{kuchibhotlaAllLinearRegression2019,
  title = {All of {{Linear Regression}}},
  author = {Kuchibhotla, Arun K. and Brown, Lawrence D. and Buja, Andreas and Cai, Junhui},
  date = {2019-10-14},
  number = {arXiv:1910.06386},
  eprint = {1910.06386},
  eprinttype = {arxiv},
  primaryclass = {math, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1910.06386},
  url = {http://arxiv.org/abs/1910.06386},
  urldate = {2022-11-11},
  abstract = {Least squares linear regression is one of the oldest and widely used data analysis tools. Although the theoretical analysis of the ordinary least squares (OLS) estimator is as old, several fundamental questions are yet to be answered. Suppose regression observations \$(X\_1,Y\_1),\textbackslash ldots,(X\_n,Y\_n)\textbackslash in\textbackslash mathbb\{R\}\^d\textbackslash times\textbackslash mathbb\{R\}\$ (not necessarily independent) are available. Some of the questions we deal with are as follows: under what conditions, does the OLS estimator converge and what is the limit? What happens if the dimension is allowed to grow with \$n\$? What happens if the observations are dependent with dependence possibly strengthening with \$n\$? How to do statistical inference under these kinds of misspecification? What happens to the OLS estimator under variable selection? How to do inference under misspecification and variable selection? We answer all the questions raised above with one simple deterministic inequality which holds for any set of observations and any sample size. This implies that all our results are a finite sample (non-asymptotic) in nature. In the end, one only needs to bound certain random quantities under specific settings of interest to get concrete rates and we derive these bounds for the case of independent observations. In particular, the problem of inference after variable selection is studied, for the first time, when \$d\$, the number of covariates increases (almost exponentially) with sample size \$n\$. We provide comments on the ``right'' statistic to consider for inference under variable selection and efficient computation of quantiles.},
  archiveprefix = {arXiv},
  keywords = {Mathematics - Statistics Theory,Statistics - Methodology},
  file = {/home/skynet3/Zotero/storage/PSVG95WN/Kuchibhotla et al. - 2019 - All of Linear Regression.pdf;/home/skynet3/Zotero/storage/M2U5RXRZ/1910.html}
}

@article{kucukelbirAutomaticDifferentiationVariational,
  title = {Automatic {{Diﬀerentiation Variational Inference}}},
  author = {Kucukelbir, Alp},
  pages = {45},
  abstract = {Probabilistic modeling is iterative. A scientist posits a simple model, fits it to her data, refines it according to her analysis, and repeats. However, fitting complex models to large data is a bottleneck in this process. Deriving algorithms for new models can be both mathematically and computationally challenging, which makes it difficult to efficiently cycle through the steps. To this end, we develop automatic differentiation variational inference ( ). Using our method, the scientist only provides a probabilistic model and a dataset, nothing else. automatically derives an efficient variational inference algorithm, freeing the scientist to refine and explore many models.},
  langid = {english},
  file = {/home/skynet3/Zotero/storage/4JXM52S9/Kucukelbir - Automatic Diﬀerentiation Variational Inference.pdf}
}

@article{kucukStanceDetectionSurvey2020,
  ids = {kucukStanceDetectionSurvey2020a},
  title = {Stance {{Detection}}: {{A Survey}}},
  shorttitle = {Stance {{Detection}}},
  author = {Küçük, Dilek and Can, Fazli},
  date = {2020-02-06},
  journaltitle = {ACM Computing Surveys},
  shortjournal = {ACM Comput. Surv.},
  volume = {53},
  number = {1},
  pages = {12:1--12:37},
  issn = {0360-0300},
  doi = {10.1145/3369026},
  url = {https://doi.org/10.1145/3369026},
  urldate = {2022-11-11},
  abstract = {Automatic elicitation of semantic information from natural language texts is an important research problem with many practical application areas. Especially after the recent proliferation of online content through channels such as social media sites, news portals, and forums; solutions to problems such as sentiment analysis, sarcasm/controversy/veracity/rumour/fake news detection, and argument mining gained increasing impact and significance, revealed with large volumes of related scientific publications. In this article, we tackle an important problem from the same family and present a survey of stance detection in social media posts and (online) regular texts. Although stance detection is defined in different ways in different application settings, the most common definition is “automatic classification of the stance of the producer of a piece of text, towards a target, into one of these three classes: \{Favor, Against, Neither\}.” Our survey includes definitions of related problems and concepts, classifications of the proposed approaches so far, descriptions of the relevant datasets and tools, and related outstanding issues. Stance detection is a recent natural language processing topic with diverse application areas, and our survey article on this newly emerging topic will act as a significant resource for interested researchers and practitioners.},
  keywords = {deep learning,social media analysis,Stance detection,Twitter},
  file = {/home/skynet3/Zotero/storage/SL5Q4MRQ/Küçük and Can - 2020 - Stance Detection A Survey.pdf}
}

@book{kuhnCaretPackage,
  title = {The Caret {{Package}}},
  author = {Kuhn, Max},
  url = {https://topepo.github.io/caret/index.html},
  urldate = {2022-11-11},
  abstract = {Documentation for the caret package.},
  file = {/home/skynet3/Zotero/storage/FJSYVXDU/index.html}
}

@online{KullbackLeiblerDivergenceExplained,
  ids = {KullbackLeiblerDivergenceExplaineda},
  title = {Kullback-{{Leibler Divergence Explained}}},
  url = {http://www.countbayesie.com/blog/2017/5/9/kullback-leibler-divergence-explained},
  urldate = {2022-11-11},
  abstract = {Kullback–Leibler divergence is a very useful way to measure the difference between two probability distributions. In this post we'll go over a simple example to help you better grasp this interesting tool from information theory.},
  langid = {american},
  organization = {{Count Bayesie}},
  file = {/home/skynet3/Zotero/storage/34EXDE3I/kullback-leibler-divergence-explained.html}
}

@article{kumarShapleyResidualsQuantifying2020,
  title = {Shapley {{Residuals}}: {{Quantifying}} the Limits of the {{Shapley}} Value for Explanations.},
  shorttitle = {Shapley {{Residuals}}},
  author = {Kumar, I. E. and Scheidegger, C. and Venkatasubramanian, S. and Friedler, S.},
  date = {2020-01},
  journaltitle = {ICML Workshop on Workshop on Human Interpretability in Machine Learning (WHI)},
  url = {https://par.nsf.gov/biblio/10187138-shapley-residuals-quantifying-limits-shapley-value-explanations},
  urldate = {2022-11-11},
  abstract = {Popular feature importance techniques compute additive approximations to nonlinear models by first defining a cooperative game describing the value of different subsets of the model’s features, then calculating the resulting game’s Shapley values to attribute credit additively between the features. However, the specific modeling settings in which the Shapley values are a poor approximation for the true game have not been well-described. In this paper we utilize an interpretation of Shapley values as the result of an orthogonal projection between vector spaces to calculate a residual representing the kernel component of that projection. We provide an algorithm for computing these residuals, characterize different modeling settings based on the value of the residuals, and demonstrate that they capture information about model predictions that Shapley values cannot.},
  langid = {english},
  file = {/home/skynet3/Zotero/storage/MS4ZI5WD/Kumar et al. - 2020 - Shapley Residuals Quantifying the limits of the S.pdf;/home/skynet3/Zotero/storage/KILVBREN/10187138-shapley-residuals-quantifying-limits-shapley-value-explanations.html}
}

@article{kumorEfficientIdentificationLinear,
  title = {Efficient {{Identiﬁcation}} in {{Linear Structural Causal Models}} with {{Instrumental Cutsets}}},
  author = {Kumor, Daniel and Chen, Bryant and Bareinboim, Elias},
  pages = {25},
  abstract = {One of the most common mistakes made when performing data analysis is attributing causal meaning to regression coefficients. Formally, a causal effect can only be computed if it is identifiable from a combination of observational data and structural knowledge about the domain under investigation (Pearl, 2000, Ch. 5). Building on the literature of instrumental variables (IVs), a plethora of methods has been developed to identify causal effects in linear systems. Almost invariably, however, the most powerful such methods rely on exponential-time procedures. In this paper, we investigate graphical conditions to allow efficient identification in arbitrary linear structural causal models (SCMs). In particular, we develop a method to efficiently find unconditioned instrumental subsets, which are generalizations of IVs that can be used to tame the complexity of many canonical algorithms found in the literature. Further, we prove that determining whether an effect can be identified with TSID (Weihs et al., 2017), a method more powerful than unconditioned instrumental sets and other efficient identification algorithms, is NP-Complete. Finally, building on the idea of flow constraints, we introduce a new and efficient criterion called Instrumental Cutsets (IC), which is able to solve for parameters missed by all other existing polynomial-time algorithms.},
  langid = {english},
  file = {/home/skynet3/Zotero/storage/GR97ZS46/Kumor et al. - Efﬁcient Identiﬁcation in Linear Structural Causal.pdf}
}

@online{kuninSeeingTheory,
  ids = {kuninSeeingTheorya},
  title = {Seeing {{Theory}}},
  author = {Kunin, Daniel},
  url = {http://seeingtheory.io},
  urldate = {2022-11-11},
  abstract = {A visual introduction to probability and statistics.},
  langid = {english},
  file = {/home/skynet3/Zotero/storage/S6UZ7H7R/index.html}
}

@book{kurzStatisticalRethinkingBrms,
  title = {Statistical Rethinking with Brms, Ggplot2, and the Tidyverse: {{Second}} Edition},
  shorttitle = {Statistical Rethinking with Brms, Ggplot2, and the Tidyverse},
  author = {Kurz, A. Solomon},
  url = {https://bookdown.org/content/4857/},
  urldate = {2022-11-11},
  abstract = {This book is an attempt to re-express the code in the second edition of McElreath’s textbook, ‘Statistical rethinking.’ His models are re-fit in brms, plots are redone with ggplot2, and the general data wrangling code predominantly follows the tidyverse style.},
  file = {/home/skynet3/Zotero/storage/VRL97QVW/4857.html}
}

@article{kvarvenComparingMetaanalysesPreregistered2020,
  ids = {kvarvenComparingMetaanalysesPreregistered2020a},
  title = {Comparing Meta-Analyses and Preregistered Multiple-Laboratory Replication Projects},
  author = {Kvarven, Amanda and Strømland, Eirik and Johannesson, Magnus},
  date = {2020-04},
  journaltitle = {Nature Human Behaviour},
  shortjournal = {Nat Hum Behav},
  volume = {4},
  number = {4},
  eprint = {31873200},
  eprinttype = {pmid},
  pages = {423--434},
  issn = {2397-3374},
  doi = {10.1038/s41562-019-0787-z},
  abstract = {Many researchers rely on meta-analysis to summarize research evidence. However, there is a concern that publication bias and selective reporting may lead to biased meta-analytic effect sizes. We compare the results of meta-analyses to large-scale preregistered replications in psychology carried out at multiple laboratories. The multiple-laboratory replications provide precisely estimated effect sizes that do not suffer from publication bias or selective reporting. We searched the literature and identified 15\,meta-analyses on the same topics as multiple-laboratory replications. We find that meta-analytic effect sizes are significantly different from replication effect sizes for 12 out of the 15\,meta-replication pairs. These differences are systematic and, on average, meta-analytic effect sizes are almost three times as large as replication effect sizes. We also implement three methods of correcting meta-analysis for bias, but these methods do not substantively improve the meta-analytic results.},
  langid = {english},
  keywords = {Behavioral Research,Bias,Data Interpretation; Statistical,Humans,Meta-Analysis as Topic,Reproducibility of Results,Statistics as Topic}
}

@article{l.havenPreregisteringQualitativeResearch2019,
  ids = {l.havenPreregisteringQualitativeResearch2019a},
  title = {Preregistering Qualitative Research},
  author = {L. Haven, Tamarinde and Van Grootel, Dr. Leonie},
  date = {2019-04-03},
  journaltitle = {Accountability in Research},
  volume = {26},
  number = {3},
  eprint = {30741570},
  eprinttype = {pmid},
  pages = {229--244},
  publisher = {{Taylor \& Francis}},
  issn = {0898-9621},
  doi = {10.1080/08989621.2019.1580147},
  url = {https://doi.org/10.1080/08989621.2019.1580147},
  urldate = {2022-11-11},
  abstract = {The threat to reproducibility and awareness of current rates of research misbehavior sparked initiatives to better academic science. One initiative is preregistration of quantitative research. We investigate whether the preregistration format could also be used to boost the credibility of qualitative research. A crucial distinction underlying preregistration is that between prediction and postdiction. In qualitative research, data are used to decide which way interpretation should move forward, using data to generate hypotheses and new research questions. Qualitative research is thus a real-life example of postdiction research. Some may object to the idea of preregistering qualitative studies because qualitative research generally does not test hypotheses, and because qualitative research design is typically flexible and subjective. We rebut these objections, arguing that making hypotheses explicit is just one feature of preregistration, that flexibility can be tracked using preregistration, and that preregistration would provide a check on subjectivity. We then contextualize preregistrations alongside another initiative to enhance credibility in qualitative research: the confirmability audit. Besides, preregistering qualitative studies is practically useful to combating dissemination bias and could incentivize qualitative researchers to report constantly on their study's development. We conclude with suggested modifications to the Open Science Framework preregistration form to tailor it for qualitative studies.},
  keywords = {Preregistration,qualitative research,transparency},
  annotation = {\_eprint: https://doi.org/10.1080/08989621.2019.1580147},
  file = {/home/skynet3/Zotero/storage/6PDJ6AMU/L. Haven and Van Grootel - 2019 - Preregistering qualitative research.pdf}
}

@online{L2RegularizationBatch,
  title = {L2 {{Regularization}} and {{Batch Norm}}},
  url = {https://blog.janestreet.com/l2-regularization-and-batch-norm/},
  urldate = {2022-11-11},
  abstract = {This blog post is about an interesting detail about machine learningthat I came across as a researcher at Jane Street - that of the interaction between L2 re...},
  organization = {{Jane Street Tech Blog}},
  file = {/home/skynet3/Zotero/storage/HKCCG82S/l2-regularization-and-batch-norm.html}
}

@online{labMethodsBites,
  ids = {labMethodsBitesa},
  title = {Methods {{Bites}}},
  author = {Lab, MZES Social Science Data},
  url = {https://www.mzes.uni-mannheim.de/socialsciencedatalab},
  urldate = {2022-11-11},
  abstract = {Blog of the MZES Social Science Data Lab},
  langid = {english},
  organization = {{Methods Bites}},
  file = {/home/skynet3/Zotero/storage/9N6EB33L/applied-bayesian-statistics.html}
}

@online{labsDistrictDataLabs,
  title = {District {{Data Labs}} - {{Time Maps}}: {{Visualizing Discrete Events Across Many Timescales}}},
  shorttitle = {District {{Data Labs}} - {{Time Maps}}},
  author = {Labs, District Data},
  url = {https://districtdatalabs.silvrback.com/time-maps-visualizing-discrete-events-across-many-timescales},
  urldate = {2022-11-11},
  abstract = {Time Maps: Visualizing Discrete Events Across Many Timescales}
}

@online{labWeakSupervisionNew2019,
  title = {Weak {{Supervision}}: {{A New Programming Paradigm}} for {{Machine Learning}}},
  shorttitle = {Weak {{Supervision}}},
  author = {Lab{$<$}/a{$>$}, {$<$}a href='https://www paroma xyz'{$>$}Paroma Varma{$<$}/a{$>$}, {$<$}a href='https://www bradenhancock com'{$>$}Braden Hancock{$<$}/a{$>$}, {$<$}a href='https://cs stanford edu/people/chrismre/'{$>$}Chris Ré{$<$}/a{$>$}, {and} {$<$}a href='https://cs stanford edu/people/chrismre/\#students'{$>$}other members of Hazy, {$<$}a href='https://ajratner github io'{$>$}Alex Ratner{$<$}/a{$>$}},
  date = {2019-03-10T00:00:00-08:00},
  url = {http://ai.stanford.edu/blog/weak-supervision/},
  urldate = {2022-11-11},
  abstract = {In recent years, the real-world impact of machine learning (ML) has grown in leaps and bounds. In large part, this is due to the advent of deep learning models, which allow practitioners to get state-of-the-art scores on benchmark datasets without any hand-engineered features. Given the availability of multiple open-source ML frameworks like TensorFlow and PyTorch, and an abundance of available state-of-the-art models, it can be argued that high-quality ML models are almost a commoditized resource now. There is a hidden catch, however: the reliance of these models on massive sets of hand-labeled training data.},
  organization = {{SAIL Blog}}
}

@misc{lakensSampleSizeJustification2021,
  ids = {lakensSampleSizeJustification2021a},
  title = {Sample {{Size Justification}}},
  author = {Lakens, Daniel},
  date = {2021-01-04T07:32:17},
  publisher = {{PsyArXiv}},
  doi = {10.31234/osf.io/9d3yf},
  url = {https://psyarxiv.com/9d3yf/},
  urldate = {2022-11-11},
  abstract = {An important step when designing an empirical study is to justify the sample size that will be collected. The key aim of a sample size justification for such studies is to explain how the collected data is expected to provide valuable information given the inferential goals of the researcher. In this overview article six approaches are discussed to justify the sample size in a quantitative empirical study: 1) collecting data from (almost) the entire population, 2) choosing a sample size based on resource constraints, 3) performing an a-priori power analysis, 4) planning for a desired accuracy, 5) using heuristics, or 6) explicitly acknowledging the absence of a justification. An important question to consider when justifying sample sizes is which effect sizes are deemed interesting, and the extent to which the data that is collected informs inferences about these effect sizes. Depending on the sample size justification chosen, researchers could consider 1) what the smallest effect size of interest is, 2) which minimal effect size will be statistically significant, 3) which effect sizes they expect (and what they base these expectations on), 4) which effect sizes would be rejected based on a confidence interval around the effect size, 5) which ranges of effects a study has sufficient power to detect based on a sensitivity power analysis, and 6) which effect sizes are expected in a specific research area. Researchers can use the guidelines presented in this article, for example by using the interactive form in the accompanying online Shiny app, to improve their sample size justification, and hopefully, align the informational value of a study with their inferential goals.},
  langid = {american},
  keywords = {Experimental Design and Sample Surveys,power analysis,Quantitative Methods,sample size justification,Social and Behavioral Sciences,study design,value of information},
  file = {/home/skynet3/Zotero/storage/YV5XID4E/Lakens - 2021 - Sample Size Justification.pdf}
}

@article{lalHowMuchShould2021,
  ids = {lalHowMuchShould2021a},
  title = {How {{Much Should We Trust Instrumental Variable Estimates}} in {{Political Science}}? {{Practical Advice}} Based on {{Over}} 60 {{Replicated Studies}}},
  shorttitle = {How {{Much Should We Trust Instrumental Variable Estimates}} in {{Political Science}}?},
  author = {Lal, Apoorva and Lockhart, Mackenzie William and Xu, Yiqing and Zu, Ziwen},
  date = {2021},
  journaltitle = {SSRN Electronic Journal},
  shortjournal = {SSRN Journal},
  issn = {1556-5068},
  doi = {10.2139/ssrn.3905329},
  url = {https://www.ssrn.com/abstract=3905329},
  urldate = {2022-11-11},
  abstract = {Instrumental variable (IV) strategies are commonly used in political science to establish causal relationships, yet the identifying assumptions required by an IV design are demanding and it remains challenging for researchers to evaluate their plausibility. We replicate 61 papers published in three top journals in political science from the past decade (2010-2020) and document several troubling patterns: (1) researchers often miscalculate the first-stage F statistics, overestimating the strength of their IVs; (2) most researchers rely on classical asymptotic standard errors, which often severely underestimate the uncertainties around the two-stage-least-squares (2SLS) estimates; (3) in the majority of the replicated studies, the 2SLS estimates are much bigger than the ordinary-least-squares estimates, and their ratio is negatively correlated with the strength of the IVs in studies where the IVs are not experimentally generated, suggesting potential violations of the exclusion restriction. To improve practice, we provide a checklist for researchers to avoid these pitfalls and recommend a zero-first-stage test and a local-to-zero procedure to guard against failures of the identifying assumptions.},
  langid = {english},
  file = {/home/skynet3/Zotero/storage/IKBLGASJ/Lal et al. - 2021 - How Much Should We Trust Instrumental Variable Est.pdf}
}

@article{landgraveNameBasedTreatmentsViolate2022,
  title = {Do {{Name-Based Treatments Violate Information Equivalence}}? {{Evidence}} from a {{Correspondence Audit Experiment}}},
  shorttitle = {Do {{Name-Based Treatments Violate Information Equivalence}}?},
  author = {Landgrave, Michelangelo and Weller, Nicholas},
  date = {2022-01},
  journaltitle = {Political Analysis},
  volume = {30},
  number = {1},
  pages = {142--148},
  publisher = {{Cambridge University Press}},
  issn = {1047-1987, 1476-4989},
  doi = {10.1017/pan.2020.52},
  url = {https://www.cambridge.org/core/journals/political-analysis/article/abs/do-namebased-treatments-violate-information-equivalence-evidence-from-a-correspondence-audit-experiment/56C6846518DDADE6EAF92DAE11552BDF},
  urldate = {2022-11-11},
  abstract = {Name-based treatments have been used in observational studies and experiments to study the differential effect of identity—commonly race or ethnic minority status. These treatments are typically assumed to signal only a single characteristic. If names unintentionally signal other characteristics, then the treatment can violate information equivalence, and estimated treatment effects cannot be attributed to the desired characteristic alone. Using results from a name perception study paired with an original correspondence audit experiment of U.S. state legislators, we show that names manipulate perceptions of minority status, socioeconomic status (SES), and migrant status. Our audit study shows that low SES status is related to reply rates both across and within each racial category. These results provide evidence that discrimination cannot be easily attributed singularly to the intended treatment of minority status but rather reflect a more multifaceted form of discrimination. More generally, our results provide an example of how name-based treatments manipulate more than the intended characteristic, which means that estimated treatment effects cannot be interpreted as being manipulated solely by the desired characteristic. Future studies with name-based or other informational treatments should account for the potential violation of information equivalence in their research design and interpretation of results.},
  langid = {english},
  keywords = {audit study,discrimination,experimental design,information equivalence}
}

@video{LargeScaleStudyCuriosityDriven,
  title = {Large-{{Scale Study}} of {{Curiosity-Driven Learning}}},
  url = {https://pathak22.github.io/large-scale-curiosity/},
  urldate = {2022-11-11},
  abstract = {Burda*, Edwards*, Pathak* et.al. (* equal contribution, alphabetical order) Large-Scale Study of Curiosity-Driven Learning. 2018.},
  file = {/home/skynet3/Zotero/storage/4JP2UVVF/large-scale-curiosity.html}
}

@misc{lattimoreReplacingDocalculusBayes2021,
  ids = {lattimoreReplacingDocalculusBayes2021a},
  title = {Replacing the Do-Calculus with {{Bayes}} Rule},
  author = {Lattimore, Finnian and Rohde, David},
  date = {2021-12-10},
  number = {arXiv:1906.07125},
  eprint = {1906.07125},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/1906.07125},
  urldate = {2022-11-11},
  abstract = {The concept of causality has a controversial history. The question of whether it is possible to represent and address causal problems with probability theory, or if fundamentally new mathematics such as the do calculus is required has been hotly debated, e.g. Pearl (2001) states "the building blocks of our scientific and everyday knowledge are elementary facts such as "mud does not cause rain" and "symptoms do not cause disease" and those facts, strangely enough, cannot be expressed in the vocabulary of probability calculus". This has lead to a dichotomy between advocates of causal graphical modeling and the do calculus, and researchers applying Bayesian methods. In this paper we demonstrate that, while it is critical to explicitly model our assumptions on the impact of intervening in a system, provided we do so, estimating causal effects can be done entirely within the standard Bayesian paradigm. The invariance assumptions underlying causal graphical models can be encoded in ordinary Probabilistic graphical models, allowing causal estimation with Bayesian statistics, equivalent to the do calculus. Elucidating the connections between these approaches is a key step toward enabling the insights provided by each to be combined to solve real problems.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/skynet3/Zotero/storage/4R6TYQTW/Lattimore and Rohde - 2021 - Replacing the do-calculus with Bayes rule.pdf;/home/skynet3/Zotero/storage/778CT7VS/1906.html}
}

@inproceedings{lauDesignSpaceComputational2020,
  ids = {lauDesignSpaceComputational2020a},
  title = {The {{Design Space}} of {{Computational Notebooks}}: {{An Analysis}} of 60 {{Systems}} in {{Academia}} and {{Industry}}},
  shorttitle = {The {{Design Space}} of {{Computational Notebooks}}},
  booktitle = {2020 {{IEEE Symposium}} on {{Visual Languages}} and {{Human-Centric Computing}} ({{VL}}/{{HCC}})},
  author = {Lau, Sam and Drosos, Ian and Markel, Julia M. and Guo, Philip J.},
  date = {2020-08},
  pages = {1--11},
  publisher = {{IEEE}},
  location = {{Dunedin, New Zealand}},
  doi = {10.1109/VL/HCC50065.2020.9127201},
  url = {https://ieeexplore.ieee.org/document/9127201/},
  urldate = {2022-11-11},
  abstract = {Computational notebooks such as Jupyter are now used by millions of data scientists, machine learning engineers, and computational researchers to do exploratory and end-user programming. In recent years, dozens of different notebook systems have been developed across academia and industry. However, we still lack an understanding of how their individual designs relate to one another and what their tradeoffs are. To provide a holistic view of this rapidly-emerging landscape, we performed, to our knowledge, the first comprehensive design analysis of dozens of notebook systems. We analyzed 60 notebooks (16 academic papers, 29 industry products, and 15 experimental/R\&D projects) and formulated a design space that succinctly captures variations in system features. Our design space covers 10 dimensions that include diverse ways of importing data, editing code and prose, running code, and publishing notebook outputs. We conclude by suggesting ways for researchers to push future projects beyond the current bounds of this space.},
  eventtitle = {2020 {{IEEE Symposium}} on {{Visual Languages}} and {{Human-Centric Computing}} ({{VL}}/{{HCC}})},
  isbn = {978-1-72816-901-9},
  langid = {english},
  file = {/home/skynet3/Zotero/storage/WESXYUIY/Lau et al. - 2020 - The Design Space of Computational Notebooks An An.pdf}
}

@online{lauraeXgboostHiGamma2017,
  title = {Xgboost: “{{Hi I}}’m {{Gamma}}. {{What}} Can {{I}} Do for You?” — And the Tuning of Regularization},
  shorttitle = {Xgboost},
  author = {Laurae},
  date = {2017-01-03T19:41:23},
  url = {https://medium.com/data-design/xgboost-hi-im-gamma-what-can-i-do-for-you-and-the-tuning-of-regularization-a42ea17e6ab6},
  urldate = {2022-11-11},
  abstract = {Laurae: This post is about tuning the regularization in the tree-based xgboost (Maximum Depth, Minimum Child Weight, Gamma). It also…},
  langid = {english},
  organization = {{Data Science \& Design}},
  file = {/home/skynet3/Zotero/storage/J9M532EZ/xgboost-hi-im-gamma-what-can-i-do-for-you-and-the-tuning-of-regularization-a42ea17e6ab6.html}
}

@misc{laurinavichyuteShareCodeNot2021,
  title = {Share the Code, Not Just the Data: {{A}} Case Study of the Reproducibility of Articles Published in the {{Journal}} of {{Memory}} and {{Language}} under the Open Data Policy},
  shorttitle = {Share the Code, Not Just the Data},
  author = {Laurinavichyute, Anna and Yadav, Himanshu and Vasishth, Shravan},
  date = {2021-09-30T15:33:45},
  publisher = {{PsyArXiv}},
  doi = {10.31234/osf.io/hf297},
  url = {https://psyarxiv.com/hf297/},
  urldate = {2022-11-11},
  abstract = {In 2019 the Journal of Memory and Language instituted an open data and code policy; this policy requires that, as a rule, code and data be released at the latest upon publication. How effective is this policy? We compared 59 papers published before, and 59 papers published after, the policy took effect. After the policy was in place, the rate of data sharing increased by more than 50\%. We further looked at whether papers published under the open data policy were reproducible, in the sense that the published results should be possible to regenerate given the data, and given the code, when code was provided. For 8 out of the 59 papers, data sets were inaccessible. The reproducibility rate ranged from 34\% to 56\%, depending on the reproducibility criteria. The strongest predictor of whether an attempt to reproduce would be successful is the presence of the analysis code: it increases the probability of reproducing reported results by almost 40\%. We propose two simple steps that can increase the reproducibility of published papers: share the analysis code, and attempt to reproduce one’s own analysis using only the shared materials.},
  langid = {american},
  keywords = {journal policy,Linguistics,meta-research,Meta-science,open data,open science,Psycholinguistics and Neurolinguistics,reproducibility,reproducible statistical analyses,Social and Behavioral Sciences},
  file = {/home/skynet3/Zotero/storage/WYXUF9L3/Laurinavichyute et al. - 2021 - Share the code, not just the data A case study of.pdf}
}

@article{lawrenceLessonIvermectinMetaanalyses2021,
  title = {The Lesson of Ivermectin: Meta-Analyses Based on Summary Data Alone Are Inherently Unreliable},
  shorttitle = {The Lesson of Ivermectin},
  author = {Lawrence, Jack M. and Meyerowitz-Katz, Gideon and Heathers, James A. J. and Brown, Nicholas J. L. and Sheldrick, Kyle A.},
  date = {2021-11},
  journaltitle = {Nature Medicine},
  shortjournal = {Nat Med},
  volume = {27},
  number = {11},
  pages = {1853--1854},
  publisher = {{Nature Publishing Group}},
  issn = {1546-170X},
  doi = {10.1038/s41591-021-01535-y},
  url = {https://www.nature.com/articles/s41591-021-01535-y},
  urldate = {2022-11-11},
  issue = {11},
  langid = {english},
  keywords = {Medical research,Randomized controlled trials},
  file = {/home/skynet3/Zotero/storage/KKB9XM2V/Lawrence et al. - 2021 - The lesson of ivermectin meta-analyses based on s.pdf;/home/skynet3/Zotero/storage/9NIJVIHB/s41591-021-01535-y.html}
}

@online{LearningNeuralCausal2019,
  title = {Learning {{Neural Causal Models}} from {{Unknown Interventions}}},
  date = {2019-10-02T16:50:15+00:00},
  url = {https://deepai.org/publication/learning-neural-causal-models-from-unknown-interventions},
  urldate = {2022-11-11},
  abstract = {10/02/19 - Meta-learning over a set of distributions can be interpreted as learning different types of parameters corresponding to short-term...},
  organization = {{DeepAI}},
  file = {/home/skynet3/Zotero/storage/JVCCRXMV/learning-neural-causal-models-from-unknown-interventions.html}
}

@online{LearningPositiveUnlabeled2020,
  title = {Learning from {{Positive}} and {{Unlabeled Data}} by {{Identifying}} the {{Annotation Process}}},
  date = {2020-03-02T17:57:12+00:00},
  url = {https://deepai.org/publication/learning-from-positive-and-unlabeled-data-by-identifying-the-annotation-process},
  urldate = {2022-11-11},
  abstract = {03/02/20 - In binary classification, Learning from Positive and Unlabeled data (LePU) is semi-supervised learning but with labeled elements f...},
  organization = {{DeepAI}},
  file = {/home/skynet3/Zotero/storage/KKWIPFT3/learning-from-positive-and-unlabeled-data-by-identifying-the-annotation-process.html}
}

@online{LeavefutureoutCrossvalidationTimeseries2020,
  title = {Leave-Future-out Cross-Validation for Time-Series Models - {{Modeling}}},
  date = {2020-02-04T10:52:50+00:00},
  url = {https://discourse.mc-stan.org/t/leave-future-out-cross-validation-for-time-series-models/12954/2},
  urldate = {2022-11-11},
  abstract = {The link you provided for Stan website has title " Leave-one-out cross-validation for non-factorizable models". Did you mean to link to http://mc-stan.org/loo/articles/loo2-lfo.html which has title " Approximate leave-future-out cross-validation for Bayesian time series models" ?   If we have a model p(y\_i|f\_i,\textbackslash phi) and joint time series prior for (f\_1,...,f\_T) then p(y\_i|f\_i,\textbackslash phi) can be considered independent given f\_i and \textbackslash phi and likelihood is factorizable. This is true often and the past v...},
  langid = {english},
  organization = {{The Stan Forums}},
  file = {/home/skynet3/Zotero/storage/74QRAE8N/2.html}
}

@misc{leeDimensionalAnalysisStatistical2021,
  ids = {leeDimensionalAnalysisStatistical2021a},
  title = {Dimensional {{Analysis}} in {{Statistical Modelling}}},
  author = {Lee, Tae Yoon and Zidek, James V. and Heckman, Nancy},
  date = {2021-09-05},
  number = {arXiv:2002.11259},
  eprint = {2002.11259},
  eprinttype = {arxiv},
  primaryclass = {math, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2002.11259},
  url = {http://arxiv.org/abs/2002.11259},
  urldate = {2022-11-11},
  abstract = {Building on recent work in statistical science, the paper presents a theory for modelling natural phenomena that unifies physical and statistical paradigms based on the underlying principle that a model must be nondimensionalizable. After all, such phenomena cannot depend on how the experimenter chooses to assess them. Yet the model itself must be comprised of quantities that can be determined theoretically or empirically. Hence, the underlying principle requires that the model represents these natural processes correctly no matter what scales and units of measurement are selected. This goal was realized for physical modelling through the celebrated theories of Buckingham and Bridgman and for statistical modellers through the invariance principle of Hunt and Stein. Building on recent research in statistical science, the paper shows how the latter can embrace and extend the former. The invariance principle is extended to encompass the Bayesian paradigm, thereby enabling an assessment of model uncertainty. The paper covers topics not ordinarily seen in statistical science regarding dimensions, scales, and units of quantities in statistical modelling. It shows the special difficulties that can arise when models involve transcendental functions, such as the logarithm which is used e.g. in likelihood analysis and is a singularity in the family of Box-Cox family of transformations. Further, it demonstrates the importance of the scale of measurement, in particular how differently modellers must handle ratio- and interval-scales},
  archiveprefix = {arXiv},
  keywords = {62A01; 00A71; 97F70,Mathematics - Statistics Theory},
  file = {/home/skynet3/Zotero/storage/JZAHY7CZ/Lee et al. - 2021 - Dimensional Analysis in Statistical Modelling.pdf;/home/skynet3/Zotero/storage/AFY39XFN/2002.html}
}

@misc{leeEconometricPerspectiveAlgorithmic2020,
  title = {An {{Econometric Perspective}} on {{Algorithmic Subsampling}}},
  author = {Lee, Sokbae and Ng, Serena},
  date = {2020-04-30},
  number = {arXiv:1907.01954},
  eprint = {1907.01954},
  eprinttype = {arxiv},
  primaryclass = {econ, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1907.01954},
  url = {http://arxiv.org/abs/1907.01954},
  urldate = {2022-11-11},
  abstract = {Datasets that are terabytes in size are increasingly common, but computer bottlenecks often frustrate a complete analysis of the data. While more data are better than less, diminishing returns suggest that we may not need terabytes of data to estimate a parameter or test a hypothesis. But which rows of data should we analyze, and might an arbitrary subset of rows preserve the features of the original data? This paper reviews a line of work that is grounded in theoretical computer science and numerical linear algebra, and which finds that an algorithmically desirable sketch, which is a randomly chosen subset of the data, must preserve the eigenstructure of the data, a property known as a subspace embedding. Building on this work, we study how prediction and inference can be affected by data sketching within a linear regression setup. We show that the sketching error is small compared to the sample size effect which a researcher can control. As a sketch size that is algorithmically optimal may not be suitable for prediction and inference, we use statistical arguments to provide 'inference conscious' guides to the sketch size. When appropriately implemented, an estimator that pools over different sketches can be nearly as efficient as the infeasible one using the full sample.},
  archiveprefix = {arXiv},
  keywords = {Economics - Econometrics,Statistics - Computation},
  file = {/home/skynet3/Zotero/storage/2FXAXA5X/Lee and Ng - 2020 - An Econometric Perspective on Algorithmic Subsampl.pdf;/home/skynet3/Zotero/storage/CBUWS2B2/1907.html}
}

@article{leekStatisticsValuesAre2015,
  title = {Statistics: {{P}} Values Are Just the Tip of the Iceberg},
  shorttitle = {Statistics},
  author = {Leek, Jeffrey T. and Peng, Roger D.},
  date = {2015-04},
  journaltitle = {Nature},
  volume = {520},
  number = {7549},
  pages = {612--612},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  doi = {10.1038/520612a},
  url = {https://www.nature.com/articles/520612a},
  urldate = {2022-11-11},
  abstract = {Ridding science of shoddy statistics will require scrutiny of every step, not merely the last one, say Jeffrey T. Leek and Roger D. Peng.},
  issue = {7549},
  langid = {english},
  keywords = {Careers,Mathematics and computing,Research management},
  file = {/home/skynet3/Zotero/storage/AY7PLKQ8/Leek and Peng - 2015 - Statistics P values are just the tip of the icebe.pdf;/home/skynet3/Zotero/storage/YL6V3WXU/520612a.html}
}

@article{leeRegressionDiscontinuityDesigns2010,
  title = {Regression {{Discontinuity Designs}} in {{Economics}}},
  author = {Lee, David S and Lemieux, Thomas},
  date = {2010-06-01},
  journaltitle = {Journal of Economic Literature},
  shortjournal = {Journal of Economic Literature},
  volume = {48},
  number = {2},
  pages = {281--355},
  issn = {0022-0515},
  doi = {10.1257/jel.48.2.281},
  url = {https://pubs.aeaweb.org/doi/10.1257/jel.48.2.281},
  urldate = {2022-11-15},
  abstract = {This paper provides an introduction and “user guide” to Regression Discontinuity (RD) designs for empirical researchers. It presents the basic theory behind the research design, details when RD is likely to be valid or invalid given economic incentives, explains why it is considered a “quasi-experimental” design, and summarizes different ways (with their advantages and disadvantages) of estimating RD designs and the limitations of interpreting these estimates. Concepts are discussed using examples drawn from the growing body of empirical research using RD. (JEL C21, C31)},
  langid = {english},
  file = {/home/skynet3/Zotero/storage/NJCVV2N4/Lee and Lemieux - 2010 - Regression Discontinuity Designs in Economics.pdf}
}

@misc{leeuwenbergComparingMethodsAddressing2021,
  title = {Comparing Methods Addressing Multi-Collinearity When Developing Prediction Models},
  author = {Leeuwenberg, Artuur M. and van Smeden, Maarten and Langendijk, Johannes A. and van der Schaaf, Arjen and Mauer, Murielle E. and Moons, Karel G. M. and Reitsma, Johannes B. and Schuit, Ewoud},
  options = {useprefix=true},
  date = {2021-01-05},
  number = {arXiv:2101.01603},
  eprint = {2101.01603},
  eprinttype = {arxiv},
  primaryclass = {stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2101.01603},
  url = {http://arxiv.org/abs/2101.01603},
  urldate = {2022-11-11},
  abstract = {Clinical prediction models are developed widely across medical disciplines. When predictors in such models are highly collinear, unexpected or spurious predictor-outcome associations may occur, thereby potentially reducing face-validity and explainability of the prediction model. Collinearity can be dealt with by exclusion of collinear predictors, but when there is no a priori motivation (besides collinearity) to include or exclude specific predictors, such an approach is arbitrary and possibly inappropriate. We compare different methods to address collinearity, including shrinkage, dimensionality reduction, and constrained optimization. The effectiveness of these methods is illustrated via simulations. In the conducted simulations, no effect of collinearity was observed on predictive outcomes. However, a negative effect of collinearity on the stability of predictor selection was found, affecting all compared methods, but in particular methods that perform strong predictor selection (e.g., Lasso).\vphantom\{\}},
  archiveprefix = {arXiv},
  keywords = {60,G.3,Statistics - Methodology},
  file = {/home/skynet3/Zotero/storage/FTR57XQY/Leeuwenberg et al. - 2021 - Comparing methods addressing multi-collinearity wh.pdf;/home/skynet3/Zotero/storage/FWAPMY2P/2101.html}
}

@book{leglerMultipleLinearRegression,
  title = {Beyond {{Multiple Linear Regression}}},
  author = {Legler, Paul Roback {and} Julie},
  url = {https://bookdown.org/roback/bookdown-bysh/},
  urldate = {2022-11-11},
  abstract = {Expired site for BYSH}
}

@misc{leiGeometricUnderstandingDeep2018,
  title = {Geometric {{Understanding}} of {{Deep Learning}}},
  author = {Lei, Na and Luo, Zhongxuan and Yau, Shing-Tung and Gu, David Xianfeng},
  date = {2018-05-30},
  number = {arXiv:1805.10451},
  eprint = {1805.10451},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1805.10451},
  url = {http://arxiv.org/abs/1805.10451},
  urldate = {2022-11-11},
  abstract = {Deep learning is the mainstream technique for many machine learning tasks, including image recognition, machine translation, speech recognition, and so on. It has outperformed conventional methods in various fields and achieved great successes. Unfortunately, the understanding on how it works remains unclear. It has the central importance to lay down the theoretic foundation for deep learning. In this work, we give a geometric view to understand deep learning: we show that the fundamental principle attributing to the success is the manifold structure in data, namely natural high dimensional data concentrates close to a low-dimensional manifold, deep learning learns the manifold and the probability distribution on it. We further introduce the concepts of rectified linear complexity for deep neural network measuring its learning capability, rectified linear complexity of an embedding manifold describing the difficulty to be learned. Then we show for any deep neural network with fixed architecture, there exists a manifold that cannot be learned by the network. Finally, we propose to apply optimal mass transportation theory to control the probability distribution in the latent space.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/skynet3/Zotero/storage/4B5EZPMQ/Lei et al. - 2018 - Geometric Understanding of Deep Learning.pdf;/home/skynet3/Zotero/storage/GA8PS3U3/1805.html}
}

@article{leipzigRoleMetadataReproducible2021,
  title = {The Role of Metadata in Reproducible Computational Research},
  author = {Leipzig, Jeremy and Nüst, Daniel and Hoyt, Charles Tapley and Ram, Karthik and Greenberg, Jane},
  date = {2021-09-10},
  journaltitle = {Patterns},
  shortjournal = {Patterns},
  volume = {2},
  number = {9},
  pages = {100322},
  issn = {2666-3899},
  doi = {10.1016/j.patter.2021.100322},
  url = {https://www.sciencedirect.com/science/article/pii/S2666389921001707},
  urldate = {2022-11-11},
  abstract = {Reproducible computational research (RCR) is the keystone of the scientific method for in silico analyses, packaging the transformation of raw data to published results. In addition to its role in research integrity, improving the reproducibility of scientific studies can accelerate evaluation and reuse. This potential and wide support for the FAIR principles have motivated interest in metadata standards supporting reproducibility. Metadata provide context and provenance to raw data and methods and are essential to both discovery and validation. Despite this shared connection with scientific data, few studies have explicitly described how metadata enable reproducible computational research. This review employs a functional content analysis to identify metadata standards that support reproducibility across an analytic stack consisting of input data, tools, notebooks, pipelines, and publications. Our review provides background context, explores gaps, and discovers component trends of embeddedness and methodology weight from which we derive recommendations for future work.},
  langid = {english},
  keywords = {containers,FAIR,metadata,notebooks,ontologies,pipelines,provenance,RCR,replicability,reproducibility,reproducible computational research,reproducible research,semantic,software dependencies,workflows},
  file = {/home/skynet3/Zotero/storage/8JEQ8M36/Leipzig et al. - 2021 - The role of metadata in reproducible computational.pdf;/home/skynet3/Zotero/storage/WGLS3T4V/S2666389921001707.html}
}

@article{lelorierDiscrepanciesMetaanalysesSubsequent1997,
  title = {Discrepancies between Meta-Analyses and Subsequent Large Randomized, Controlled Trials},
  author = {LeLorier, J. and Grégoire, G. and Benhaddad, A. and Lapierre, J. and Derderian, F.},
  date = {1997-08-21},
  journaltitle = {The New England Journal of Medicine},
  shortjournal = {N Engl J Med},
  volume = {337},
  number = {8},
  eprint = {9262498},
  eprinttype = {pmid},
  pages = {536--542},
  issn = {0028-4793},
  doi = {10.1056/NEJM199708213370806},
  abstract = {BACKGROUND: Meta-analyses are now widely used to provide evidence to support clinical strategies. However, large randomized, controlled trials are considered the gold standard in evaluating the efficacy of clinical interventions. METHODS: We compared the results of large randomized, controlled trials (involving 1000 patients or more) that were published in four journals (the New England Journal of Medicine, the Lancet, the Annals of Internal Medicine, and the Journal of the American Medical Association) with the results of meta-analyses published earlier on the same topics. Regarding the principal and secondary outcomes, we judged whether the findings of the randomized trials agreed with those of the corresponding meta-analyses, and we determined whether the study results were positive (indicating that treatment improved the outcome) or negative (indicating that the outcome with treatment was the same or worse than without it) at the conventional level of statistical significance (P{$<$}0.05). RESULTS: We identified 12 large randomized, controlled trials and 19 meta-analyses addressing the same questions. For a total of 40 primary and secondary outcomes, agreement between the meta-analyses and the large clinical trials was only fair (kappa= 0.35; 95 percent confidence interval, 0.06 to 0.64). The positive predictive value of the meta-analyses was 68 percent, and the negative predictive value 67 percent. However, the difference in point estimates between the randomized trials and the meta-analyses was statistically significant for only 5 of the 40 comparisons (12 percent). Furthermore, in each case of disagreement a statistically significant effect of treatment was found by one method, whereas no statistically significant effect was found by the other. CONCLUSIONS: The outcomes of the 12 large randomized, controlled trials that we studied were not predicted accurately 35 percent of the time by the meta-analyses published previously on the same topics.},
  langid = {english},
  keywords = {Humans,Meta-Analysis as Topic,Odds Ratio,Predictive Value of Tests,Randomized Controlled Trials as Topic,Sensitivity and Specificity}
}

@article{lenzAchievingStatisticalSignificance2021,
  title = {Achieving {{Statistical Significance}} with {{Control Variables}} and {{Without Transparency}}},
  author = {Lenz, Gabriel S. and Sahn, Alexander},
  date = {2021-07},
  journaltitle = {Political Analysis},
  volume = {29},
  number = {3},
  pages = {356--369},
  publisher = {{Cambridge University Press}},
  issn = {1047-1987, 1476-4989},
  doi = {10.1017/pan.2020.31},
  url = {https://www.cambridge.org/core/journals/political-analysis/article/abs/achieving-statistical-significance-with-control-variables-and-without-transparency/1E867C357835019E0C9322B918414045},
  urldate = {2022-11-11},
  abstract = {How often do articles depend on suppression effects for their findings? How often do they disclose this fact? By suppression effects, we mean control-variable-induced increases in estimated effect sizes. Researchers generally scrutinize suppression effects as they want reassurance that authors have a strong explanation for them, especially when the statistical significance of the key finding depends on them. In a reanalysis of observational studies from a leading journal, we find that over 30\% of articles depend on suppression effects for statistical significance. Although increases in key effect estimates from including control variables are of course potentially justifiable, none of the articles justify or disclose them. These findings may point to a hole in the review process: journals are accepting articles that depend on suppression effects without readers, reviewers, or editors being made aware.},
  langid = {english},
  keywords = {controls,covariates,open science,regression,reproducibility,suppression effects,transparency}
}

@online{LessonsArchivesStrategies2019,
  title = {Lessons from {{Archives}}: {{Strategies}} for {{Collecting Sociocultural Data}} in {{Machine Learning}}},
  shorttitle = {Lessons from {{Archives}}},
  date = {2019-12-22T05:56:55+00:00},
  url = {https://deepai.org/publication/lessons-from-archives-strategies-for-collecting-sociocultural-data-in-machine-learning},
  urldate = {2022-11-11},
  abstract = {12/22/19 - A growing body of work shows that many problems in fairness, accountability, transparency, and ethics in machine learning systems ...},
  organization = {{DeepAI}},
  file = {/home/skynet3/Zotero/storage/JCERIVML/lessons-from-archives-strategies-for-collecting-sociocultural-data-in-machine-learning.html}
}

@online{LessonsLearnedReproducing,
  title = {Lessons {{Learned Reproducing}} a {{Deep Reinforcement Learning Paper}}},
  url = {http://amid.fish/reproducing-deep-rl},
  urldate = {2022-11-11}
}

@online{leungHowEasilyDraw2022,
  title = {How to {{Easily Draw Neural Network Architecture Diagrams}}},
  author = {Leung, Kenneth},
  date = {2022-09-13T15:54:28},
  url = {https://towardsdatascience.com/how-to-easily-draw-neural-network-architecture-diagrams-a6b6138ed875},
  urldate = {2022-11-11},
  abstract = {Using the no-code diagrams.net tool to showcase your deep learning models with diagram visualizations},
  langid = {english},
  organization = {{Medium}},
  file = {/home/skynet3/Zotero/storage/NB8R7UH4/how-to-easily-draw-neural-network-architecture-diagrams-a6b6138ed875.html}
}

@misc{levyFundamentalMeasureTreatment2018,
  title = {A {{Fundamental Measure}} of {{Treatment Effect Heterogeneity}}},
  author = {Levy, Jonathan and van der Laan, Mark and Hubbard, Alan and Pirracchio, Romain},
  options = {useprefix=true},
  date = {2018-12-23},
  number = {arXiv:1811.03745},
  eprint = {1811.03745},
  eprinttype = {arxiv},
  primaryclass = {math, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1811.03745},
  url = {http://arxiv.org/abs/1811.03745},
  urldate = {2022-11-11},
  abstract = {We offer a non-parametric plug-in estimator for an important measure of treatment effect variability and provide minimum conditions under which the estimator is asymptotically efficient. The stratum specific treatment effect function or so-called blip function, is the average treatment effect for a randomly drawn stratum of confounders. The mean of the blip function is the average treatment effect (ATE), whereas the variance of the blip function (VTE), the main subject of this paper, measures overall clinical effect heterogeneity, perhaps providing a strong impetus to refine treatment based on the confounders. VTE is also an important measure for assessing reliability of the treatment for an individual. The CV-TMLE provides simultaneous plug-in estimates and inference for both ATE and VTE, guaranteeing asymptotic efficiency under one less condition than for TMLE. This condition is difficult to guarantee a priori, particularly when using highly adaptive machine learning that we need to employ in order to eliminate bias. Even in defiance of this condition, CV-TMLE sampling distributions maintain normality, not guaranteed for TMLE, and have a lower mean squared error than their TMLE counterparts. In addition to verifying the theoretical properties of TMLE and CV-TMLE through simulations, we point out some of the challenges in estimating VTE, which lacks double robustness and might be unavoidably biased if the true VTE is small and sample size insufficient. We will provide an application of the estimator on a data set for treatment of acute trauma patients.},
  archiveprefix = {arXiv},
  keywords = {Mathematics - Statistics Theory,Statistics - Applications,Statistics - Methodology},
  file = {/home/skynet3/Zotero/storage/WHH2YU78/Levy et al. - 2018 - A Fundamental Measure of Treatment Effect Heteroge.pdf;/home/skynet3/Zotero/storage/9PHGW5SZ/1811.html}
}

@article{lewbelUsingHeteroscedasticityIdentify2012,
  ids = {lewbelUsingHeteroscedasticityIdentify2012a},
  title = {Using {{Heteroscedasticity}} to {{Identify}} and {{Estimate Mismeasured}} and {{Endogenous Regressor Models}}},
  author = {Lewbel, Arthur},
  date = {2012-01-01},
  journaltitle = {Journal of Business \& Economic Statistics},
  volume = {30},
  number = {1},
  pages = {67--80},
  publisher = {{Taylor \& Francis}},
  issn = {0735-0015},
  doi = {10.1080/07350015.2012.643126},
  url = {https://doi.org/10.1080/07350015.2012.643126},
  urldate = {2022-11-11},
  abstract = {This article proposes a new method of obtaining identification in mismeasured regressor models, triangular systems, and simultaneous equation systems. The method may be used in applications where other sources of identification, such as instrumental variables or repeated measurements, are not available. Associated estimators take the form of two-stage least squares or generalized method of moments. Identification comes from a heteroscedastic covariance restriction that is shown to be a feature of many models of endogeneity or mismeasurement. Identification is also obtained for semiparametric partly linear models, and associated estimators are provided. Set identification bounds are derived for cases where point-identifying assumptions fail to hold. An empirical application estimating Engel curves is provided.},
  keywords = {Endogeneity,Heteroscedastic errors,Identification,Measurement error,Partly linear model,Simultaneous system},
  annotation = {\_eprint: https://doi.org/10.1080/07350015.2012.643126}
}

@misc{lewisPuzzlingRelationshipMultilab2020,
  title = {The Puzzling Relationship between Multi-Lab Replications and Meta-Analyses of the Rest of the Literature},
  author = {Lewis, Molly and Mathur, Maya and VanderWeele, Tyler and Frank, Michael C.},
  date = {2020-04-03T17:32:00},
  publisher = {{PsyArXiv}},
  doi = {10.31234/osf.io/pbrdk},
  url = {https://psyarxiv.com/pbrdk/},
  urldate = {2022-11-11},
  abstract = {What is the best way to estimate the size of important effects? Should we aggregate across disparate findings using statistical meta-analysis, or instead run large, multi-lab replications (MLR)? A recent paper by Kvarven, Strømland, and Johannesson (2020) compared effect size estimates derived from these two different methods for 15 different psychological phenomena. The authors report that, for the same phenomenon, the meta-analytic estimate tends to be about three times larger than the MLR estimate. These results pose an important puzzle: What is the relationship between these two estimates? Kvarven et al. suggest that their results undermine the value of meta-analysis. In contrast, we argue that both meta-analysis and MLR are informative, and that the discrepancy between estimates obtained via the two methods is in fact still unexplained. Informed by re-analyses of Kvarven et al.’s data and by other empirical evidence, we discuss possible sources of this discrepancy and argue that understanding the relationship between estimates obtained from these two methods is an important puzzle for future meta-scientific research.},
  langid = {american},
  keywords = {Meta-science},
  file = {/home/skynet3/Zotero/storage/6GZP289U/Lewis et al. - 2020 - The puzzling relationship between multi-lab replic.pdf}
}

@article{leykSpatialAllocationPopulation2019,
  ids = {leykSpatialAllocationPopulation2019a},
  title = {The Spatial Allocation of Population: A Review of Large-Scale Gridded Population Data Products and Their Fitness for Use},
  shorttitle = {The Spatial Allocation of Population},
  author = {Leyk, Stefan and Gaughan, Andrea E. and Adamo, Susana B. and de Sherbinin, Alex and Balk, Deborah and Freire, Sergio and Rose, Amy and Stevens, Forrest R. and Blankespoor, Brian and Frye, Charlie and Comenetz, Joshua and Sorichetta, Alessandro and MacManus, Kytt and Pistolesi, Linda and Levy, Marc and Tatem, Andrew J. and Pesaresi, Martino},
  options = {useprefix=true},
  date = {2019-09-11},
  journaltitle = {Earth System Science Data},
  shortjournal = {Earth Syst. Sci. Data},
  volume = {11},
  number = {3},
  pages = {1385--1409},
  issn = {1866-3516},
  doi = {10.5194/essd-11-1385-2019},
  url = {https://essd.copernicus.org/articles/11/1385/2019/},
  urldate = {2022-11-11},
  abstract = {Population data represent an essential component in studies focusing on human–nature interrelationships, disaster risk assessment and environmental health. Several recent efforts have produced global- and continental-extent gridded population data which are becoming increasingly popular among various research communities. However, these data products, which are of very different characteristics and based on different modeling assumptions, have never been systematically reviewed and compared, which may impede their appropriate use. This article fills this gap and presents, compares and discusses a set of large-scale (global and continental) gridded datasets representing population counts or densities. It focuses on data properties, methodological approaches and relative quality aspects that are important to fully understand the characteristics of the data with regard to the intended uses. Written by the data producers and members of the user community, through the lens of the “fitness for use” concept, the aim of this paper is to provide potential data users with the knowledge base needed to make informed decisions about the appropriateness of the data products available in relation to the target application and for critical analysis.},
  langid = {english},
  file = {/home/skynet3/Zotero/storage/GW4ZLC57/Leyk et al. - 2019 - The spatial allocation of population a review of .pdf}
}

@misc{liImprovingParameterEstimation2022,
  type = {SSRN Scholarly Paper},
  ids = {liImprovingParameterEstimation2022a},
  title = {Improving {{Parameter Estimation}} of {{Epidemic Models}}: {{Likelihood Functions}} and {{Kalman Filtering}}},
  shorttitle = {Improving {{Parameter Estimation}} of {{Epidemic Models}}},
  author = {Li, Tianyi and Rahmandad, Hazhir and Sterman, John},
  date = {2022-07-18},
  number = {4165188},
  location = {{Rochester, NY}},
  doi = {10.2139/ssrn.4165188},
  url = {https://papers.ssrn.com/abstract=4165188},
  urldate = {2022-11-11},
  abstract = {Projecting the course of infectious diseases and assessing the likely impact of policies to contain them require reliable estimates of the parameters in dynamic models of disease transmission. However, such estimation is difficult, especially for emerging diseases such as COVID-19, due to incomplete and delayed data, measurement error, and model specification error. Absent reliable estimation, model-based recommendations could be misleading. Here we conduct synthetic data experiments comparing the performance of various estimation methods for calibrating epidemic models, considering both process and measurement noise. We compare the performance of standard least squares against maximum likelihood methods including scaled-variance Gaussian, Poisson, and negative binomial likelihood functions, and assess the effectiveness of (extended) Kalman filtering. We explore the performance of these methods under different assumptions about data availability, from full information on the infection, symptom emergence, and removal rates to the more realistic setting where data are available only for symptom emergence. We find that widely-adopted naive estimation methods (least squares or incomplete Gaussian likelihood, without variance scaling and without Kalman filtering) perform poorly, especially in the estimation of confidence intervals. The negative binomial likelihood function performs well across a range of assumptions, scaling the error variance and Kalman filtering improve performance over the basic Gaussian estimator. We provide modelers with guidance to help choose estimation methods appropriate for their problem setting and data. Results may inform problems beyond the epidemic context, from  models of innovation diffusion to other feedback-rich managerial and policy settings with  limited data, process noise, and measurement error.},
  langid = {english},
  keywords = {epidemic model,Kalman filtering,likelihood function,negative binomial,parameter estimation},
  file = {/home/skynet3/Zotero/storage/5T2997M7/Li et al. - 2022 - Improving Parameter Estimation of Epidemic Models.pdf;/home/skynet3/Zotero/storage/GACWQQ9T/papers.html}
}

@article{limLearningInteractionsHierarchical2015,
  ids = {limLearningInteractionsHierarchical2015a},
  title = {Learning {{Interactions}} via {{Hierarchical Group-Lasso Regularization}}},
  author = {Lim, Michael and Hastie, Trevor},
  date = {2015-07-03},
  journaltitle = {Journal of Computational and Graphical Statistics},
  shortjournal = {Journal of Computational and Graphical Statistics},
  volume = {24},
  number = {3},
  pages = {627--654},
  issn = {1061-8600, 1537-2715},
  doi = {10.1080/10618600.2014.938812},
  url = {http://www.tandfonline.com/doi/full/10.1080/10618600.2014.938812},
  urldate = {2022-11-11},
  abstract = {We introduce a method for learning pairwise interactions in a linear regression or logistic regression model in a manner that satisfies strong hierarchy: whenever an interaction is estimated to be nonzero, both its associated main effects are also included in the model. We motivate our approach by modeling pairwise interactions for categorical variables with arbitrary numbers of levels, and then show how we can accommodate continuous variables as well. Our approach allows us to dispense with explicitly applying constraints on the main effects and interactions for identifiability, which results in interpretable interaction models. We compare our method with existing approaches on both simulated and real data, including a genome-wide association study, all using our R package glinternet.},
  langid = {english},
  file = {/home/skynet3/Zotero/storage/8EU82NPK/Lim and Hastie - 2015 - Learning Interactions via Hierarchical Group-Lasso.pdf}
}

@article{linClassimbalancedClassifiersHighdimensional2013,
  title = {Class-Imbalanced Classifiers for High-Dimensional Data},
  author = {Lin, Wei-Jiun and Chen, James J.},
  date = {2013-01-01},
  journaltitle = {Briefings in Bioinformatics},
  shortjournal = {Briefings in Bioinformatics},
  volume = {14},
  number = {1},
  pages = {13--26},
  issn = {1467-5463},
  doi = {10.1093/bib/bbs006},
  url = {https://doi.org/10.1093/bib/bbs006},
  urldate = {2022-11-11},
  abstract = {A class-imbalanced classifier is a decision rule to predict the class membership of new samples from an available data set where the class sizes differ considerably. When the class sizes are very different, most standard classification algorithms may favor the larger (majority) class resulting in poor accuracy in the minority class prediction. A class-imbalanced classifier typically modifies a standard classifier by a correction strategy or by incorporating a new strategy in the training phase to account for differential class sizes. This article reviews and evaluates some most important methods for class prediction of high-dimensional imbalanced data. The evaluation addresses the fundamental issues of the class-imbalanced classification problem: imbalance ratio, small disjuncts and overlap complexity, lack of data and feature selection. Four class-imbalanced classifiers are considered. The four classifiers include three standard classification algorithms each coupled with an ensemble correction strategy and one support vector machines (SVM)-based correction classifier. The three algorithms are (i) diagonal linear discriminant analysis (DLDA), (ii) random forests (RFs) and (ii) SVMs. The SVM-based correction classifier is SVM threshold adjustment (SVM-THR). A Monte–Carlo simulation and five genomic data sets were used to illustrate the analysis and address the issues. The SVM-ensemble classifier appears to perform the best when the class imbalance is not too severe. The SVM-THR performs well if the imbalance is severe and predictors are highly correlated. The DLDA with a feature selection can perform well without using the ensemble correction.},
  file = {/home/skynet3/Zotero/storage/DBR5FQUN/Lin and Chen - 2013 - Class-imbalanced classifiers for high-dimensional .pdf;/home/skynet3/Zotero/storage/NVUH5U8C/304457.html}
}

@article{lineroBayesianApproachesMissing2018,
  title = {Bayesian {{Approaches}} for {{Missing Not}} at {{Random Outcome Data}}: {{The Role}} of {{Identifying Restrictions}}},
  shorttitle = {Bayesian {{Approaches}} for {{Missing Not}} at {{Random Outcome Data}}},
  author = {Linero, Antonio R. and Daniels, Michael J.},
  date = {2018-05},
  journaltitle = {Statistical science : a review journal of the Institute of Mathematical Statistics},
  shortjournal = {Stat Sci},
  volume = {33},
  number = {2},
  eprint = {31889740},
  eprinttype = {pmid},
  pages = {198--213},
  issn = {0883-4237},
  doi = {10.1214/17-STS630},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6936760/},
  urldate = {2022-11-11},
  abstract = {Missing data is almost always present in real datasets, and introduces several statistical issues. One fundamental issue is that, in the absence of strong uncheckable assumptions, effects of interest are typically not nonparametrically identified. In this article, we review the generic approach of the use of identifying restrictions from a likelihood-based perspective, and provide points of contact for several recently proposed methods. An emphasis of this review is on restrictions for nonmonotone missingness, a subject that has been treated sparingly in the literature. We also present a general, fully-Bayesian, approach which is widely applicable and capable of handling a variety of identifying restrictions in a uniform manner.},
  pmcid = {PMC6936760},
  file = {/home/skynet3/Zotero/storage/43JAJYYZ/Linero and Daniels - 2018 - Bayesian Approaches for Missing Not at Random Outc.pdf}
}

@online{linschotenFiguringOutWhy2022,
  title = {Figuring out Why My Object Detection Model Is Underperforming with {{FiftyOne}}, a Great Tool You Probably Haven’t Heard of},
  author = {van Linschoten, Alex Strick},
  date = {2022-03-12T00:00:00-06:00},
  url = {https://mlops.systems/redactionmodel/computervision/tools/debugging/jupyter/2022/03/12/fiftyone-computervision.html},
  urldate = {2022-11-11},
  abstract = {I used the under-appreciated tool FiftyOne to analyse the ways that my object detection model is underperforming. For computer vision problems, it’s really useful to have visual debugging aids and FiftyOne is a well-documented and solid tool to help with that.},
  langid = {english},
  organization = {{mlops.systems}},
  file = {/home/skynet3/Zotero/storage/QYXXLZU3/fiftyone-computervision.html}
}

@software{lippeEfficientNeuralCausal2022,
  title = {Efficient {{Neural Causal Discovery}} without {{Acyclicity Constraints}}},
  author = {Lippe, Phillip},
  date = {2022-10-27T13:39:09Z},
  origdate = {2021-07-21T09:40:14Z},
  url = {https://github.com/phlippe/ENCO},
  urldate = {2022-11-11},
  abstract = {Official repository of the paper "Efficient Neural Causal Discovery without Acyclicity Constraints"},
  keywords = {causal-discovery,causality,pytorch,structure-learning}
}

@article{lipsitchNegativeControlsTool2010,
  title = {Negative {{Controls}}: {{A Tool}} for {{Detecting Confounding}} and {{Bias}} in {{Observational Studies}}},
  shorttitle = {Negative {{Controls}}},
  author = {Lipsitch, Marc and Tchetgen, Eric Tchetgen and Cohen, Ted},
  date = {2010-05},
  journaltitle = {Epidemiology (Cambridge, Mass.)},
  shortjournal = {Epidemiology},
  volume = {21},
  number = {3},
  eprint = {20335814},
  eprinttype = {pmid},
  pages = {383--388},
  issn = {1044-3983},
  doi = {10.1097/EDE.0b013e3181d61eeb},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3053408/},
  urldate = {2022-11-11},
  abstract = {Non-causal associations between exposures and outcomes are a threat to validity of causal inference in observational studies. Many techniques have been developed for study design and analysis to identify and eliminate such errors. Such problems are not expected to compromise experimental studies, where careful standardization of conditions (for laboratory work) and randomization (for population studies) should, if applied properly, eliminate most such non-causal associations. We argue, however, that a routine precaution taken in the design of biological laboratory experiments—the use of “negative controls”—is designed to detect both suspected and unsuspected sources of spurious causal inference. In epidemiology, analogous negative controls help to identify and resolve confounding as well as other sources of error, including recall bias or analytic flaws. We distinguish two types of negative controls (exposure controls and outcome controls), describe examples of each type from the epidemiologic literature, and identify the conditions for the use of such negative controls to detect confounding. We conclude that negative controls should be more commonly employed in observational studies, and that additional work is needed to specify the conditions under which negative controls will be sensitive detectors of other sources of error in observational studies.},
  pmcid = {PMC3053408},
  file = {/home/skynet3/Zotero/storage/X7WDNP75/Lipsitch et al. - 2010 - Negative Controls A Tool for Detecting Confoundin.pdf}
}

@misc{liptonMythosModelInterpretability2016,
  title = {The {{Mythos}} of {{Model Interpretability}}},
  author = {Lipton, Zachary C.},
  date = {2016-06-10},
  number = {arXiv:1606.03490},
  eprint = {1606.03490},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/1606.03490},
  urldate = {2022-11-11},
  abstract = {Supervised machine learning models boast remarkable predictive capabilities. But can you trust your model? Will it work in deployment? What else can it tell you about the world? We want models to be not only good, but interpretable. And yet the task of interpretation appears underspecified. Papers provide diverse and sometimes non-overlapping motivations for interpretability, and offer myriad notions of what attributes render models interpretable. Despite this ambiguity, many papers proclaim interpretability axiomatically, absent further explanation. In this paper, we seek to refine the discourse on interpretability. First, we examine the motivations underlying interest in interpretability, finding them to be diverse and occasionally discordant. Then, we address model properties and techniques thought to confer interpretability, identifying transparency to humans and post-hoc explanations as competing notions. Throughout, we discuss the feasibility and desirability of different notions, and question the oft-made assertions that linear models are interpretable and that deep neural networks are not.},
  archiveprefix = {arXiv},
  version = {1},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/home/skynet3/Zotero/storage/39TRNPHW/Lipton - 2016 - The Mythos of Model Interpretability.pdf;/home/skynet3/Zotero/storage/82QLMIZQ/1606.html}
}

@misc{liptonTroublingTrendsMachine2018,
  title = {Troubling {{Trends}} in {{Machine Learning Scholarship}}},
  author = {Lipton, Zachary C. and Steinhardt, Jacob},
  date = {2018-07-26},
  number = {arXiv:1807.03341},
  eprint = {1807.03341},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1807.03341},
  url = {http://arxiv.org/abs/1807.03341},
  urldate = {2022-11-11},
  abstract = {Collectively, machine learning (ML) researchers are engaged in the creation and dissemination of knowledge about data-driven algorithms. In a given paper, researchers might aspire to any subset of the following goals, among others: to theoretically characterize what is learnable, to obtain understanding through empirically rigorous experiments, or to build a working system that has high predictive accuracy. While determining which knowledge warrants inquiry may be subjective, once the topic is fixed, papers are most valuable to the community when they act in service of the reader, creating foundational knowledge and communicating as clearly as possible. Recent progress in machine learning comes despite frequent departures from these ideals. In this paper, we focus on the following four patterns that appear to us to be trending in ML scholarship: (i) failure to distinguish between explanation and speculation; (ii) failure to identify the sources of empirical gains, e.g., emphasizing unnecessary modifications to neural architectures when gains actually stem from hyper-parameter tuning; (iii) mathiness: the use of mathematics that obfuscates or impresses rather than clarifies, e.g., by confusing technical and non-technical concepts; and (iv) misuse of language, e.g., by choosing terms of art with colloquial connotations or by overloading established technical terms. While the causes behind these patterns are uncertain, possibilities include the rapid expansion of the community, the consequent thinness of the reviewer pool, and the often-misaligned incentives between scholarship and short-term measures of success (e.g., bibliometrics, attention, and entrepreneurial opportunity). While each pattern offers a corresponding remedy (don't do it), we also discuss some speculative suggestions for how the community might combat these trends.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/skynet3/Zotero/storage/R9XXX96D/Lipton and Steinhardt - 2018 - Troubling Trends in Machine Learning Scholarship.pdf;/home/skynet3/Zotero/storage/KFJ5896U/1807.html}
}

@online{ListFunctionRDocumentation,
  title = {List Function - {{RDocumentation}}},
  url = {https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/list},
  urldate = {2022-11-11}
}

@misc{liuFactorAnalysisHighDimensional2019,
  title = {Factor {{Analysis}} for {{High-Dimensional Time Series}} with {{Change Point}}},
  author = {Liu, Xialu and Zhang, Ting},
  date = {2019-07-22},
  number = {arXiv:1907.09522},
  eprint = {1907.09522},
  eprinttype = {arxiv},
  primaryclass = {stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1907.09522},
  url = {http://arxiv.org/abs/1907.09522},
  urldate = {2022-11-11},
  abstract = {We consider change-point latent factor models for high-dimensional time series, where a structural break may exist in the underlying factor structure. In particular, we propose consistent estimators for factor loading spaces before and after the change point, and the problem of estimating the change-point location is also considered. Compared with existing results on change-point factor analysis of high-dimensional time series, a distinguished feature of the current paper is that our results allow strong cross-sectional dependence in the noise process. To accommodate the unknown degree of cross-sectional dependence strength, we propose to use self-normalization to pivotalize the change-point test statistic. Numerical experiments including a Monte Carlo simulation study and a real data application are presented to illustrate the proposed methods.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Methodology},
  file = {/home/skynet3/Zotero/storage/38DTZ29N/Liu and Zhang - 2019 - Factor Analysis for High-Dimensional Time Series w.pdf;/home/skynet3/Zotero/storage/5DDVK8KF/1907.html}
}

@online{LiulchBpCausalPackage,
  title = {Liulch/{{bpCausal}}: {{A}} Package for {{Bayesian}} Causal Inference with Time-Series Cross-Sectional Data},
  shorttitle = {Liulch/{{bpCausal}}},
  url = {https://github.com/liulch/bpCausal},
  urldate = {2022-11-11},
  abstract = {A package for Bayesian causal inference with time-series cross-sectional data - liulch/bpCausal: A package for Bayesian causal inference with time-series cross-sectional data},
  langid = {english},
  organization = {{GitHub}},
  file = {/home/skynet3/Zotero/storage/QX398NRB/bpCausal.html}
}

@misc{liUnderstandingDisharmonyDropout2018,
  title = {Understanding the {{Disharmony}} between {{Dropout}} and {{Batch Normalization}} by {{Variance Shift}}},
  author = {Li, Xiang and Chen, Shuo and Hu, Xiaolin and Yang, Jian},
  date = {2018-01-16},
  number = {arXiv:1801.05134},
  eprint = {1801.05134},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/1801.05134},
  urldate = {2022-11-11},
  abstract = {This paper first answers the question "why do the two most powerful techniques Dropout and Batch Normalization (BN) often lead to a worse performance when they are combined together?" in both theoretical and statistical aspects. Theoretically, we find that Dropout would shift the variance of a specific neural unit when we transfer the state of that network from train to test. However, BN would maintain its statistical variance, which is accumulated from the entire learning procedure, in the test phase. The inconsistency of that variance (we name this scheme as "variance shift") causes the unstable numerical behavior in inference that leads to more erroneous predictions finally, when applying Dropout before BN. Thorough experiments on DenseNet, ResNet, ResNeXt and Wide ResNet confirm our findings. According to the uncovered mechanism, we next explore several strategies that modifies Dropout and try to overcome the limitations of their combination by avoiding the variance shift risks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/skynet3/Zotero/storage/H74YSW4F/Li et al. - 2018 - Understanding the Disharmony between Dropout and B.pdf;/home/skynet3/Zotero/storage/TSZG7UXF/1801.html}
}

@misc{liuOpenInformationExtraction2022,
  title = {Open {{Information Extraction}} from 2007 to 2022 -- {{A Survey}}},
  author = {Liu, Pai and Gao, Wenyang and Dong, Wenjie and Huang, Songfang and Zhang, Yue},
  date = {2022-08-18},
  number = {arXiv:2208.08690},
  eprint = {2208.08690},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2208.08690},
  url = {http://arxiv.org/abs/2208.08690},
  urldate = {2022-11-11},
  abstract = {Open information extraction is an important NLP task that targets extracting structured information from unstructured text without limitations on the relation type or the domain of the text. This survey paper covers open information extraction technologies from 2007 to 2022 with a focus on new models not covered by previous surveys. We propose a new categorization method from the source of information perspective to accommodate the development of recent OIE technologies. In addition, we summarize three major approaches based on task settings as well as current popular datasets and model evaluation metrics. Given the comprehensive review, several future directions are shown from datasets, source of information, output form, method, and evaluation metric aspects.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/home/skynet3/Zotero/storage/CVZGQAAN/Liu et al. - 2022 - Open Information Extraction from 2007 to 2022 -- A.pdf;/home/skynet3/Zotero/storage/GXEKBCXD/2208.html}
}

@misc{liuPretrainPromptPredict2021,
  title = {Pre-Train, {{Prompt}}, and {{Predict}}: {{A Systematic Survey}} of {{Prompting Methods}} in {{Natural Language Processing}}},
  shorttitle = {Pre-Train, {{Prompt}}, and {{Predict}}},
  author = {Liu, Pengfei and Yuan, Weizhe and Fu, Jinlan and Jiang, Zhengbao and Hayashi, Hiroaki and Neubig, Graham},
  date = {2021-07-28},
  number = {arXiv:2107.13586},
  eprint = {2107.13586},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2107.13586},
  urldate = {2022-11-11},
  abstract = {This paper surveys and organizes research works in a new paradigm in natural language processing, which we dub "prompt-based learning". Unlike traditional supervised learning, which trains a model to take in an input x and predict an output y as P(y|x), prompt-based learning is based on language models that model the probability of text directly. To use these models to perform prediction tasks, the original input x is modified using a template into a textual string prompt x' that has some unfilled slots, and then the language model is used to probabilistically fill the unfilled information to obtain a final string x, from which the final output y can be derived. This framework is powerful and attractive for a number of reasons: it allows the language model to be pre-trained on massive amounts of raw text, and by defining a new prompting function the model is able to perform few-shot or even zero-shot learning, adapting to new scenarios with few or no labeled data. In this paper we introduce the basics of this promising paradigm, describe a unified set of mathematical notations that can cover a wide variety of existing work, and organize existing work along several dimensions, e.g.the choice of pre-trained models, prompts, and tuning strategies. To make the field more accessible to interested beginners, we not only make a systematic review of existing works and a highly structured typology of prompt-based concepts, but also release other resources, e.g., a website http://pretrain.nlpedia.ai/ including constantly-updated survey, and paperlist.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/skynet3/Zotero/storage/K27EHRPS/Liu et al. - 2021 - Pre-train, Prompt, and Predict A Systematic Surve.pdf;/home/skynet3/Zotero/storage/RPFBSQTG/2107.html}
}

@misc{liuUnderstandingGrokkingEffective2022,
  ids = {liuUnderstandingGrokkingEffective2022a},
  title = {Towards {{Understanding Grokking}}: {{An Effective Theory}} of {{Representation Learning}}},
  shorttitle = {Towards {{Understanding Grokking}}},
  author = {Liu, Ziming and Kitouni, Ouail and Nolte, Niklas and Michaud, Eric J. and Tegmark, Max and Williams, Mike},
  date = {2022-10-14},
  number = {arXiv:2205.10343},
  eprint = {2205.10343},
  eprinttype = {arxiv},
  primaryclass = {cond-mat, physics:physics},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2205.10343},
  url = {http://arxiv.org/abs/2205.10343},
  urldate = {2022-11-11},
  abstract = {We aim to understand grokking, a phenomenon where models generalize long after overfitting their training set. We present both a microscopic analysis anchored by an effective theory and a macroscopic analysis of phase diagrams describing learning performance across hyperparameters. We find that generalization originates from structured representations whose training dynamics and dependence on training set size can be predicted by our effective theory in a toy setting. We observe empirically the presence of four learning phases: comprehension, grokking, memorization, and confusion. We find representation learning to occur only in a "Goldilocks zone" (including comprehension and grokking) between memorization and confusion. We find on transformers the grokking phase stays closer to the memorization phase (compared to the comprehension phase), leading to delayed generalization. The Goldilocks phase is reminiscent of "intelligence from starvation" in Darwinian evolution, where resource limitations drive discovery of more efficient solutions. This study not only provides intuitive explanations of the origin of grokking, but also highlights the usefulness of physics-inspired tools, e.g., effective theories and phase diagrams, for understanding deep learning.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Condensed Matter - Disordered Systems and Neural Networks,Condensed Matter - Statistical Mechanics,Physics - Classical Physics},
  file = {/home/skynet3/Zotero/storage/6Z5IDI9P/Liu et al. - 2022 - Towards Understanding Grokking An Effective Theor.pdf;/home/skynet3/Zotero/storage/J7PFJIJB/2205.html}
}

@online{LKJPriorUniform2020,
  title = {Is the {{LKJ}}(1) Prior Uniform? "{{Yes}}"},
  shorttitle = {Is the {{LKJ}}(1) Prior Uniform?},
  date = {2020-08-27T23:19:38+00:00},
  url = {http://srmart.in/is-the-lkj1-prior-uniform-yes/},
  urldate = {2022-11-11},
  langid = {american},
  organization = {{Stephen R. Martin, PhD}}
}

@online{LmcinnesUmapUniform,
  title = {Lmcinnes/Umap: {{Uniform Manifold Approximation}} and {{Projection}}},
  shorttitle = {Lmcinnes/Umap},
  url = {https://github.com/lmcinnes/umap},
  urldate = {2022-11-11},
  abstract = {Uniform Manifold Approximation and Projection. Contribute to lmcinnes/umap development by creating an account on GitHub.},
  langid = {english},
  organization = {{GitHub}},
  file = {/home/skynet3/Zotero/storage/DZV8KWPD/umap.html}
}

@online{LocalitySensitiveHashing2015,
  title = {Locality {{Sensitive Hashing}} in {{R}} · {{Data Science}} Notes},
  date = {2015-01-02T00:00:00Z},
  url = {http://dsnotes.com/post/locality-sensitive-hashing-in-r-part-1/},
  urldate = {2022-11-11},
  organization = {{Data Science notes}}
}

@article{loecherUnbiasedVariableImportance2022,
  title = {Unbiased Variable Importance for Random Forests},
  author = {Loecher, Markus},
  date = {2022-03-04},
  journaltitle = {Communications in Statistics - Theory and Methods},
  shortjournal = {Communications in Statistics - Theory and Methods},
  volume = {51},
  number = {5},
  eprint = {2003.02106},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  pages = {1413--1425},
  issn = {0361-0926, 1532-415X},
  doi = {10.1080/03610926.2020.1764042},
  url = {http://arxiv.org/abs/2003.02106},
  urldate = {2022-11-11},
  abstract = {The default variable-importance measure in random Forests, Gini importance, has been shown to suffer from the bias of the underlying Gini-gain splitting criterion. While the alternative permutation importance is generally accepted as a reliable measure of variable importance, it is also computationally demanding and suffers from other shortcomings. We propose a simple solution to the misleading/untrustworthy Gini importance which can be viewed as an overfitting problem: we compute the loss reduction on the out-of-bag instead of the in-bag training samples.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Applications,Statistics - Machine Learning},
  file = {/home/skynet3/Zotero/storage/ZXU4BVEV/Loecher - 2022 - Unbiased variable importance for random forests.pdf;/home/skynet3/Zotero/storage/9FK5BPR4/2003.html}
}

@article{lokenMeasurementErrorReplication2017,
  ids = {lokenMeasurementErrorReplication2017a},
  title = {Measurement Error and the Replication Crisis},
  author = {Loken, Eric and Gelman, Andrew},
  date = {2017-02-10},
  journaltitle = {Science},
  volume = {355},
  number = {6325},
  pages = {584--585},
  publisher = {{American Association for the Advancement of Science}},
  doi = {10.1126/science.aal3618},
  url = {https://www.science.org/doi/10.1126/science.aal3618},
  urldate = {2022-11-11},
  file = {/home/skynet3/Zotero/storage/7ZYZGECT/Loken and Gelman - 2017 - Measurement error and the replication crisis.pdf}
}

@misc{lonesHowAvoidMachine2022,
  title = {How to Avoid Machine Learning Pitfalls: A Guide for Academic Researchers},
  shorttitle = {How to Avoid Machine Learning Pitfalls},
  author = {Lones, Michael A.},
  date = {2022-09-06},
  number = {arXiv:2108.02497},
  eprint = {2108.02497},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2108.02497},
  urldate = {2022-11-11},
  abstract = {This document gives a concise outline of some of the common mistakes that occur when using machine learning techniques, and what can be done to avoid them. It is intended primarily as a guide for research students, and focuses on issues that are of particular concern within academic research, such as the need to do rigorous comparisons and reach valid conclusions. It covers five stages of the machine learning process: what to do before model building, how to reliably build models, how to robustly evaluate models, how to compare models fairly, and how to report results.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/home/skynet3/Zotero/storage/MAC5JIFW/Lones - 2022 - How to avoid machine learning pitfalls a guide fo.pdf;/home/skynet3/Zotero/storage/LNITAC3A/2108.html}
}

@article{longVariableSelectionPresence2015,
  title = {Variable Selection in the Presence of Missing Data: Resampling and Imputation},
  shorttitle = {Variable Selection in the Presence of Missing Data},
  author = {Long, Qi and Johnson, Brent A.},
  date = {2015-07},
  journaltitle = {Biostatistics (Oxford, England)},
  shortjournal = {Biostatistics},
  volume = {16},
  number = {3},
  eprint = {25694614},
  eprinttype = {pmid},
  pages = {596--610},
  issn = {1465-4644},
  doi = {10.1093/biostatistics/kxv003},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5156376/},
  urldate = {2022-11-11},
  abstract = {In the presence of missing data, variable selection methods need to be tailored to missing data mechanisms and statistical approaches used for handling missing data. We focus on the mechanism of missing at random and variable selection methods that can be combined with imputation. We investigate a general resampling approach (BI-SS) that combines bootstrap imputation and stability selection, the latter of which was developed for fully observed data. The proposed approach is general and can be applied to a wide range of settings. Our extensive simulation studies demonstrate that the performance of BI-SS is the best or close to the best and is relatively insensitive to tuning parameter values in terms of variable selection, compared with several existing methods for both low-dimensional and high-dimensional problems. The proposed approach is further illustrated using two applications, one for a low-dimensional problem and the other for a high-dimensional problem.},
  pmcid = {PMC5156376},
  file = {/home/skynet3/Zotero/storage/49DPM2FR/Long and Johnson - 2015 - Variable selection in the presence of missing data.pdf}
}

@misc{louppeUnderstandingRandomForests2015,
  title = {Understanding {{Random Forests}}: {{From Theory}} to {{Practice}}},
  shorttitle = {Understanding {{Random Forests}}},
  author = {Louppe, Gilles},
  date = {2015-06-03},
  number = {arXiv:1407.7502},
  eprint = {1407.7502},
  eprinttype = {arxiv},
  primaryclass = {stat},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/1407.7502},
  urldate = {2022-11-11},
  abstract = {Data analysis and machine learning have become an integrative part of the modern scientific methodology, offering automated procedures for the prediction of a phenomenon based on past observations, unraveling underlying patterns in data and providing insights about the problem. Yet, caution should avoid using machine learning as a black-box tool, but rather consider it as a methodology, with a rational thought process that is entirely dependent on the problem under study. In particular, the use of algorithms should ideally require a reasonable understanding of their mechanisms, properties and limitations, in order to better apprehend and interpret their results. Accordingly, the goal of this thesis is to provide an in-depth analysis of random forests, consistently calling into question each and every part of the algorithm, in order to shed new light on its learning capabilities, inner workings and interpretability. The first part of this work studies the induction of decision trees and the construction of ensembles of randomized trees, motivating their design and purpose whenever possible. Our contributions follow with an original complexity analysis of random forests, showing their good computational performance and scalability, along with an in-depth discussion of their implementation details, as contributed within Scikit-Learn. In the second part of this work, we analyse and discuss the interpretability of random forests in the eyes of variable importance measures. The core of our contributions rests in the theoretical characterization of the Mean Decrease of Impurity variable importance measure, from which we prove and derive some of its properties in the case of multiway totally randomized trees and in asymptotic conditions. In consequence of this work, our analysis demonstrates that variable importances [...].},
  archiveprefix = {arXiv},
  keywords = {Statistics - Machine Learning},
  file = {/home/skynet3/Zotero/storage/9N7KDRYH/Louppe - 2015 - Understanding Random Forests From Theory to Pract.pdf;/home/skynet3/Zotero/storage/D55ZIT6Y/1407.html}
}

@book{lovelaceWelcomeGeocomputation2019,
  title = {Welcome | {{Geocomputation}} with {{R}}},
  author = {Lovelace, Robin and Nowosad, Jakub and Muenchow, Jannes},
  date = {2019},
  url = {https://geocompr.robinlovelace.net/},
  urldate = {2022-11-11},
  abstract = {Welcome | Geocomputation with R is for people who want to analyze, visualize and model geographic data with open source software. It is based on R, a statistical programming language that has powerful data processing, visualization, and geospatial capabilities. The book equips you with the knowledge and skills to tackle a wide range of issues manifested in geographic data, including those with scientific, societal, and environmental implications. This book will interest people from many backgrounds, especially Geographic Information Systems (GIS) users interested in applying their domain-specific knowledge in a powerful open source language for data science, and R users interested in extending their skills to handle spatial data.},
  isbn = {978-0-203-73005-8},
  langid = {english},
  file = {/home/skynet3/Zotero/storage/RJFVRZFA/index.html}
}

@online{LrbergeFixestFixedeffects,
  title = {Lrberge/Fixest: {{Fixed-effects}} Estimations},
  shorttitle = {Lrberge/Fixest},
  url = {https://github.com/lrberge/fixest},
  urldate = {2022-11-11},
  abstract = {Fixed-effects estimations. Contribute to lrberge/fixest development by creating an account on GitHub.},
  langid = {english},
  organization = {{GitHub}},
  file = {/home/skynet3/Zotero/storage/GRHKCXCJ/fixest.html}
}

@article{luDyingReLUInitialization2020,
  title = {Dying {{ReLU}} and {{Initialization}}: {{Theory}} and {{Numerical Examples}}},
  shorttitle = {Dying {{ReLU}} and {{Initialization}}},
  author = {Lu, Lu and Shin, Yeonjong and Su, Yanhui and Karniadakis, George Em},
  date = {2020-06},
  journaltitle = {Communications in Computational Physics},
  shortjournal = {CiCP},
  volume = {28},
  number = {5},
  eprint = {1903.06733},
  eprinttype = {arxiv},
  primaryclass = {cs, math, stat},
  pages = {1671--1706},
  issn = {1815-2406, 1991-7120},
  doi = {10.4208/cicp.OA-2020-0165},
  url = {http://arxiv.org/abs/1903.06733},
  urldate = {2022-11-11},
  abstract = {The dying ReLU refers to the problem when ReLU neurons become inactive and only output 0 for any input. There are many empirical and heuristic explanations of why ReLU neurons die. However, little is known about its theoretical analysis. In this paper, we rigorously prove that a deep ReLU network will eventually die in probability as the depth goes to infinite. Several methods have been proposed to alleviate the dying ReLU. Perhaps, one of the simplest treatments is to modify the initialization procedure. One common way of initializing weights and biases uses symmetric probability distributions, which suffers from the dying ReLU. We thus propose a new initialization procedure, namely, a randomized asymmetric initialization. We prove that the new initialization can effectively prevent the dying ReLU. All parameters required for the new initialization are theoretically designed. Numerical examples are provided to demonstrate the effectiveness of the new initialization procedure.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Mathematics - Probability,Statistics - Machine Learning},
  file = {/home/skynet3/Zotero/storage/BBDHMG6B/Lu et al. - 2020 - Dying ReLU and Initialization Theory and Numerica.pdf;/home/skynet3/Zotero/storage/PTZKZZIE/1903.html}
}

@online{luiDirichletProcessGaussian2020,
  title = {Dirichlet {{Process Gaussian}} Mixture Model via the Stick-Breaking Construction in Various {{PPLs}}},
  author = {Lui, Arthur},
  date = {2020-07-06T00:00:00+00:00},
  url = {https://luiarthur.github.io/TuringBnpBenchmarks/dpsbgmm},
  urldate = {2022-11-11},
  abstract = {Benchmarks for Bayesian nonparametric models implemented in various PPLs.},
  langid = {english},
  organization = {{Turing BNP Benchmarks}},
  file = {/home/skynet3/Zotero/storage/F5H8D7S8/dpsbgmm.html}
}

@software{lundbergSlundbergShap2022,
  title = {Slundberg/Shap},
  author = {Lundberg, Scott},
  date = {2022-11-11T21:25:58Z},
  origdate = {2016-11-22T19:17:08Z},
  url = {https://github.com/slundberg/shap},
  urldate = {2022-11-11},
  abstract = {A game theoretic approach to explain the output of any machine learning model.},
  keywords = {deep-learning,explainability,gradient-boosting,interpretability,machine-learning,shap,shapley}
}

@article{lundbergWhatYourEstimand2021,
  ids = {lundbergWhatYourEstimand2021a,lundbergWhatYourEstimand2021b},
  title = {What {{Is Your Estimand}}? {{Defining}} the {{Target Quantity Connects Statistical Evidence}} to {{Theory}}},
  shorttitle = {What {{Is Your Estimand}}?},
  author = {Lundberg, Ian and Johnson, Rebecca and Stewart, Brandon M.},
  date = {2021-06-01},
  journaltitle = {American Sociological Review},
  shortjournal = {Am Sociol Rev},
  volume = {86},
  number = {3},
  pages = {532--565},
  publisher = {{SAGE Publications Inc}},
  issn = {0003-1224},
  doi = {10.1177/00031224211004187},
  url = {https://doi.org/10.1177/00031224211004187},
  urldate = {2022-11-07},
  abstract = {We make only one point in this article. Every quantitative study must be able to answer the question: what is your estimand? The estimand is the target quantity?the purpose of the statistical analysis. Much attention is already placed on how to do estimation; a similar degree of care should be given to defining the thing we are estimating. We advocate that authors state the central quantity of each analysis?the theoretical estimand?in precise terms that exist outside of any statistical model. In our framework, researchers do three things: (1) set a theoretical estimand, clearly connecting this quantity to theory; (2) link to an empirical estimand, which is informative about the theoretical estimand under some identification assumptions; and (3) learn from data. Adding precise estimands to research practice expands the space of theoretical questions, clarifies how evidence can speak to those questions, and unlocks new tools for estimation. By grounding all three steps in a precise statement of the target quantity, our framework connects statistical evidence to theory.},
  langid = {english},
  file = {/home/skynet3/Zotero/storage/7QEQBVDN/Lundberg et al. - 2021 - What Is Your Estimand Defining the Target Quantit.pdf;/home/skynet3/Zotero/storage/A7KA8KMD/Lundberg et al. - 2021 - What Is Your Estimand Defining the Target Quantit.pdf;/home/skynet3/Zotero/storage/LPJH4CWS/Lundberg et al. - 2021 - What Is Your Estimand Defining the Target Quantit.pdf}
}

@misc{lyonsLabellingUnsupervisedLearning2018,
  title = {Labelling as an Unsupervised Learning Problem},
  author = {Lyons, Terry and Arribas, Imanol Perez},
  date = {2018-05-30},
  number = {arXiv:1805.03911},
  eprint = {1805.03911},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1805.03911},
  url = {http://arxiv.org/abs/1805.03911},
  urldate = {2022-11-11},
  abstract = {Unravelling hidden patterns in datasets is a classical problem with many potential applications. In this paper, we present a challenge whose objective is to discover nonlinear relationships in noisy cloud of points. If a set of point satisfies a nonlinear relationship that is unlikely to be due to randomness, we will label the set with this relationship. Since points can satisfy one, many or no such nonlinear relationships, cloud of points will typically have one, multiple or no labels at all. This introduces the labelling problem that will be studied in this paper. The objective of this paper is to develop a framework for the labelling problem. We introduce a precise notion of a label, and we propose an algorithm to discover such labels in a given dataset, which is then tested in synthetic datasets. We also analyse, using tools from random matrix theory, the problem of discovering false labels in the dataset.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/skynet3/Zotero/storage/4YHUHQEQ/Lyons and Arribas - 2018 - Labelling as an unsupervised learning problem.pdf;/home/skynet3/Zotero/storage/J2FAT299/1805.html}
}

@article{macedoEntropicOutofDistributionDetection2022,
  title = {Entropic {{Out-of-Distribution Detection}}: {{Seamless Detection}} of {{Unknown Examples}}},
  shorttitle = {Entropic {{Out-of-Distribution Detection}}},
  author = {Macêdo, David and Ren, Tsang Ing and Zanchettin, Cleber and Oliveira, Adriano L. I. and Ludermir, Teresa},
  date = {2022-06},
  journaltitle = {IEEE Transactions on Neural Networks and Learning Systems},
  shortjournal = {IEEE Trans. Neural Netw. Learning Syst.},
  volume = {33},
  number = {6},
  eprint = {2006.04005},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  pages = {2350--2364},
  issn = {2162-237X, 2162-2388},
  doi = {10.1109/TNNLS.2021.3112897},
  url = {http://arxiv.org/abs/2006.04005},
  urldate = {2022-11-11},
  abstract = {In this paper, we argue that the unsatisfactory out-of-distribution (OOD) detection performance of neural networks is mainly due to the SoftMax loss anisotropy and propensity to produce low entropy probability distributions in disagreement with the principle of maximum entropy. Current out-of-distribution (OOD) detection approaches usually do not directly fix the SoftMax loss drawbacks, but rather build techniques to circumvent it. Unfortunately, those methods usually produce undesired side effects (e.g., classification accuracy drop, additional hyperparameters, slower inferences, and collecting extra data). In the opposite direction, we propose replacing SoftMax loss with a novel loss function that does not suffer from the mentioned weaknesses. The proposed IsoMax loss is isotropic (exclusively distance-based) and provides high entropy posterior probability distributions. Replacing the SoftMax loss by IsoMax loss requires no model or training changes. Additionally, the models trained with IsoMax loss produce as fast and energy-efficient inferences as those trained using SoftMax loss. Moreover, no classification accuracy drop is observed. The proposed method does not rely on outlier/background data, hyperparameter tuning, temperature calibration, feature extraction, metric learning, adversarial training, ensemble procedures, or generative models. Our experiments showed that IsoMax loss works as a seamless SoftMax loss drop-in replacement that significantly improves neural networks' OOD detection performance. Hence, it may be used as a baseline OOD detection approach to be combined with current or future OOD detection techniques to achieve even higher results.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/skynet3/Zotero/storage/DT88545J/Macêdo et al. - 2022 - Entropic Out-of-Distribution Detection Seamless D.pdf;/home/skynet3/Zotero/storage/FGSTAW4T/2006.html}
}

@online{MachineLearning,
  title = {Machine {{Learning}}},
  url = {https://developers.google.com/machine-learning},
  urldate = {2022-11-11},
  abstract = {Educational resources for machine learning.},
  langid = {english},
  organization = {{Google Developers}},
  file = {/home/skynet3/Zotero/storage/8TMYU4NL/machine-learning.html}
}

@online{MachineLearningFrameworks,
  title = {Machine {{Learning Frameworks Interoperability}} - {{Cheat Sheet}}},
  url = {http://bl.ocks.org/miguelusque/raw/f44a8e729896a96d0a3e4b07b5176af4/},
  urldate = {2022-11-11}
}

@online{MachineLearningPapers,
  ids = {MachineLearningPapersa},
  title = {Machine {{Learning Papers}}},
  url = {https://math.papers.bar/paper/347f685c018d11edb9b9d35608ee6155},
  urldate = {2022-11-11},
  file = {/home/skynet3/Zotero/storage/QSAY2HEJ/347f685c018d11edb9b9d35608ee6155.html}
}

@online{MachineLearningSets,
  title = {Machine {{Learning}} of {{Sets}}},
  url = {http://akosiorek.github.io/ml/2020/08/12/machine_learning_of_sets.html},
  urldate = {2022-11-11}
}

@online{MachineLearningTheory,
  title = {Machine Learning Theory - Part 3: Regularization and the Bias-variance Trade-off},
  shorttitle = {Machine Learning Theory - Part 3},
  url = {https://mostafa-samir.github.io/ml-theory-pt3/},
  urldate = {2022-11-11},
  abstract = {Wandering in a lifelong journey seeking after truth},
  langid = {brazilian},
  organization = {{Mostafa Samir's Blog}},
  file = {/home/skynet3/Zotero/storage/HV4GJZGK/ml-theory-pt3.html}
}

@misc{maclarenWhatCanBe2020,
  title = {What Can Be Estimated? {{Identifiability}}, Estimability, Causal Inference and Ill-Posed Inverse Problems},
  shorttitle = {What Can Be Estimated?},
  author = {Maclaren, Oliver J. and Nicholson, Ruanui},
  date = {2020-07-20},
  number = {arXiv:1904.02826},
  eprint = {1904.02826},
  eprinttype = {arxiv},
  primaryclass = {cs, math, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1904.02826},
  url = {http://arxiv.org/abs/1904.02826},
  urldate = {2022-11-11},
  abstract = {We consider basic conceptual questions concerning the relationship between statistical estimation and causal inference. Firstly, we show how to translate causal inference problems into an abstract statistical formalism without requiring any structure beyond an arbitrarily-indexed family of probability models. The formalism is simple but can incorporate a variety of causal modelling frameworks, including 'structural causal models', but also models expressed in terms of, e.g., differential equations. We focus primarily on the structural/graphical causal modelling literature, however. Secondly, we consider the extent to which causal and statistical concerns can be cleanly separated, examining the fundamental question: 'What can be estimated from data?'. We call this the problem of estimability. We approach this by analysing a standard formal definition of 'can be estimated' commonly adopted in the causal inference literature -- identifiability -- in our abstract statistical formalism. We use elementary category theory to show that identifiability implies the existence of a Fisher-consistent estimator, but also show that this estimator may be discontinuous, and thus unstable, in general. This difficulty arises because the causal inference problem is, in general, an ill-posed inverse problem. Inverse problems have three conditions which must be satisfied to be considered well-posed: existence, uniqueness, and stability of solutions. Here identifiability corresponds to the question of uniqueness; in contrast, we take estimability to mean satisfaction of all three conditions, i.e. well-posedness. Lack of stability implies that naive translation of a causally identifiable quantity into an achievable statistical estimation target may prove impossible. Our article is primarily expository and aimed at unifying ideas from multiple fields, though we provide new constructions and proofs.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Mathematics - Statistics Theory,Statistics - Machine Learning},
  file = {/home/skynet3/Zotero/storage/ZPWAZQEJ/Maclaren and Nicholson - 2020 - What can be estimated Identifiability, estimabili.pdf;/home/skynet3/Zotero/storage/WYIV63HQ/1904.html}
}

@article{maclehoseProbabilisticBiasAnalysis2012,
  title = {Is Probabilistic Bias Analysis Approximately {{Bayesian}}?},
  author = {MacLehose, Richard F. and Gustafson, Paul},
  date = {2012-01},
  journaltitle = {Epidemiology (Cambridge, Mass.)},
  shortjournal = {Epidemiology},
  volume = {23},
  number = {1},
  eprint = {22157311},
  eprinttype = {pmid},
  pages = {151--158},
  issn = {1044-3983},
  doi = {10.1097/EDE.0b013e31823b539c},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3257063/},
  urldate = {2022-11-11},
  abstract = {Case-control studies are particularly susceptible to differential exposure misclassification when exposure status is determined following incident case status. Probabilistic bias analysis methods have been developed as ways to adjust standard effect estimates based on the sensitivity and specificity of exposure misclassification. The iterative sampling method advocated in probabilistic bias analysis bears a distinct resemblance to a Bayesian adjustment; however, it is not identical. Furthermore, without a formal theoretical framework (Bayesian or frequentist), the results of a probabilistic bias analysis remain somewhat difficult to interpret. We describe, both theoretically and empirically, the extent to which probabilistic bias analysis can be viewed as approximately Bayesian. While the differences between probabilistic bias analysis and Bayesian approaches to misclassification can be substantial, these situations often involve unrealistic prior specifications and are relatively easy to detect. Outside of these special cases, probabilistic bias analysis and Bayesian approaches to exposure misclassification in case-control studies appear to perform equally well.},
  pmcid = {PMC3257063},
  file = {/home/skynet3/Zotero/storage/FSY88YU2/MacLehose and Gustafson - 2012 - Is probabilistic bias analysis approximately Bayes.pdf}
}

@article{macnamaraGrowthMindsetInterventions2022,
  ids = {macnamaraGrowthMindsetInterventions2022a},
  title = {Do Growth Mindset Interventions Impact Students’ Academic Achievement? {{A}} Systematic Review and Meta-Analysis with Recommendations for Best Practices},
  shorttitle = {Do Growth Mindset Interventions Impact Students’ Academic Achievement?},
  author = {Macnamara, Brooke N. and Burgoyne, Alexander P.},
  date = {2022},
  journaltitle = {Psychological Bulletin},
  pages = {No Pagination Specified-No Pagination Specified},
  publisher = {{American Psychological Association}},
  location = {{US}},
  issn = {1939-1455},
  doi = {10.1037/bul0000352},
  abstract = {According to mindset theory, students who believe their personal characteristics can change—that is, those who hold a growth mindset—will achieve more than students who believe their characteristics are fixed. Proponents of the theory have developed interventions to influence students’ mindsets, claiming that these interventions lead to large gains in academic achievement. Despite their popularity, the evidence for growth mindset intervention benefits has not been systematically evaluated considering both the quantity and quality of the evidence. Here, we provide such a review by (a) evaluating empirical studies’ adherence to a set of best practices essential for drawing causal conclusions and (b) conducting three meta-analyses. When examining all studies (63 studies, N = 97,672), we found major shortcomings in study design, analysis, and reporting, and suggestions of researcher and publication bias: Authors with a financial incentive to report positive findings published significantly larger effects than authors without this incentive. Across all studies, we observed a small overall effect: d¯ = 0.05, 95\% CI = [0.02, 0.09], which was nonsignificant after correcting for potential publication bias. No theoretically meaningful moderators were significant. When examining only studies demonstrating the intervention influenced students’ mindsets as intended (13 studies, N = 18,355), the effect was nonsignificant: d¯ = 0.04, 95\% CI = [−0.01, 0.10]. When examining the highest-quality evidence (6 studies, N = 13,571), the effect was nonsignificant: d¯ = 0.02, 95\% CI = [−0.06, 0.10]. We conclude that apparent effects of growth mindset interventions on academic achievement are likely attributable to inadequate study design, reporting flaws, and bias. (PsycInfo Database Record (c) 2022 APA, all rights reserved)},
  keywords = {Academic Achievement,Best Practices,Experimenter Bias,Monetary Incentives,Popularity,School Based Intervention,Student Characteristics,Theories},
  file = {/home/skynet3/Zotero/storage/D8T2IPIC/2023-14088-001.html}
}

@misc{maddoxRethinkingParameterCounting2020,
  title = {Rethinking {{Parameter Counting}} in {{Deep Models}}: {{Effective Dimensionality Revisited}}},
  shorttitle = {Rethinking {{Parameter Counting}} in {{Deep Models}}},
  author = {Maddox, Wesley J. and Benton, Gregory and Wilson, Andrew Gordon},
  date = {2020-05-25},
  number = {arXiv:2003.02139},
  eprint = {2003.02139},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2003.02139},
  url = {http://arxiv.org/abs/2003.02139},
  urldate = {2022-11-11},
  abstract = {Neural networks appear to have mysterious generalization properties when using parameter counting as a proxy for complexity. Indeed, neural networks often have many more parameters than there are data points, yet still provide good generalization performance. Moreover, when we measure generalization as a function of parameters, we see double descent behaviour, where the test error decreases, increases, and then again decreases. We show that many of these properties become understandable when viewed through the lens of effective dimensionality, which measures the dimensionality of the parameter space determined by the data. We relate effective dimensionality to posterior contraction in Bayesian deep learning, model selection, width-depth tradeoffs, double descent, and functional diversity in loss surfaces, leading to a richer understanding of the interplay between parameters and functions in deep models. We also show that effective dimensionality compares favourably to alternative norm- and flatness- based generalization measures.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/skynet3/Zotero/storage/ZLJ3T4KJ/Maddox et al. - 2020 - Rethinking Parameter Counting in Deep Models Effe.pdf;/home/skynet3/Zotero/storage/J9RHEJAG/2003.html}
}

@article{madlock-brownLackImpactRetraction2015,
  ids = {madlock-brownLackImpactRetraction2015a},
  title = {The (Lack of) Impact of Retraction on Citation Networks},
  author = {Madlock-Brown, Charisse R. and Eichmann, David},
  date = {2015-02},
  journaltitle = {Science and Engineering Ethics},
  shortjournal = {Sci Eng Ethics},
  volume = {21},
  number = {1},
  eprint = {24668038},
  eprinttype = {pmid},
  pages = {127--137},
  issn = {1471-5546},
  doi = {10.1007/s11948-014-9532-1},
  abstract = {Article retraction in research is rising, yet retracted articles continue to be cited at a disturbing rate. This paper presents an analysis of recent retraction patterns, with a unique emphasis on the role author self-cites play, to assist the scientific community in creating counter-strategies. This was accomplished by examining the following: (1) A categorization of retracted articles more complete than previously published work. (2) The relationship between citation counts and after-retraction self-cites from the authors of the work, and the distribution of self-cites across our retraction categories. (3) The distribution of retractions written by both the author and the editor across our retraction categories. (4) The trends for seven of our nine defined retraction categories over a 6-year period. (5) The average journal impact factor by category, and the relationship between impact factor, author self-cites, and overall citations. Our findings indicate new reasons for retractions have emerged in recent years, and more editors are penning retractions. The rates of increase for retraction varies by category, and there is statistically significant difference of average impact factor between many categories. 18~\% of authors self-cite retracted work post retraction with only 10~\% of those authors also citing the retraction notice. Further, there is a positive correlation between self-cites and after retraction citations.},
  langid = {english},
  keywords = {Authorship,Editorial Policies,Ethics; Research,Humans,Journal Impact Factor,Publishing,Retraction of Publication as Topic,Scientific Misconduct,Writing}
}

@misc{mahmoodHowMuchMore2022,
  ids = {mahmoodHowMuchMore2022a},
  title = {How {{Much More Data Do I Need}}? {{Estimating Requirements}} for {{Downstream Tasks}}},
  shorttitle = {How {{Much More Data Do I Need}}?},
  author = {Mahmood, Rafid and Lucas, James and Acuna, David and Li, Daiqing and Philion, Jonah and Alvarez, Jose M. and Yu, Zhiding and Fidler, Sanja and Law, Marc T.},
  date = {2022-07-13},
  number = {arXiv:2207.01725},
  eprint = {2207.01725},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2207.01725},
  url = {http://arxiv.org/abs/2207.01725},
  urldate = {2022-11-11},
  abstract = {Given a small training data set and a learning algorithm, how much more data is necessary to reach a target validation or test performance? This question is of critical importance in applications such as autonomous driving or medical imaging where collecting data is expensive and time-consuming. Overestimating or underestimating data requirements incurs substantial costs that could be avoided with an adequate budget. Prior work on neural scaling laws suggest that the power-law function can fit the validation performance curve and extrapolate it to larger data set sizes. We find that this does not immediately translate to the more difficult downstream task of estimating the required data set size to meet a target performance. In this work, we consider a broad class of computer vision tasks and systematically investigate a family of functions that generalize the power-law function to allow for better estimation of data requirements. Finally, we show that incorporating a tuned correction factor and collecting over multiple rounds significantly improves the performance of the data estimators. Using our guidelines, practitioners can accurately estimate data requirements of machine learning systems to gain savings in both development time and data acquisition costs.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/home/skynet3/Zotero/storage/ZBWPIEFW/Mahmood et al. - 2022 - How Much More Data Do I Need Estimating Requireme.pdf;/home/skynet3/Zotero/storage/YSYX9EM2/2207.html}
}

@online{mahrPlottingPartialPooling2017,
  ids = {mahrPlottingPartialPooling2017a},
  title = {Plotting Partial Pooling in Mixed-Effects Models},
  author = {Mahr, Tristan},
  date = {2017-06-22T00:00:00-05:00},
  url = {https://tjmahr.github.io/plotting-partial-pooling-in-mixed-effects-models/},
  urldate = {2022-11-11},
  abstract = {There are a lot of bilabial sounds in that title},
  langid = {english},
  organization = {{Higher Order Functions}},
  file = {/home/skynet3/Zotero/storage/ASVNV6BV/plotting-partial-pooling-in-mixed-effects-models.html}
}

@misc{maksymiukLandscapePackagesEXplainable2021,
  title = {Landscape of {{R}} Packages for {{eXplainable Artificial Intelligence}}},
  author = {Maksymiuk, Szymon and Gosiewska, Alicja and Biecek, Przemyslaw},
  date = {2021-03-26},
  number = {arXiv:2009.13248},
  eprint = {2009.13248},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2009.13248},
  urldate = {2022-11-11},
  abstract = {The growing availability of data and computing power fuels the development of predictive models. In order to ensure the safe and effective functioning of such models, we need methods for exploration, debugging, and validation. New methods and tools for this purpose are being developed within the eXplainable Artificial Intelligence (XAI) subdomain of machine learning. In this work (1) we present the taxonomy of methods for model explanations, (2) we identify and compare 27 packages available in R to perform XAI analysis, (3) we present an example of an application of particular packages, (4) we acknowledge recent trends in XAI. The article is primarily devoted to the tools available in R, but since it is easy to integrate the Python code, we will also show examples for the most popular libraries from Python.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/skynet3/Zotero/storage/9CSFFMIF/Maksymiuk et al. - 2021 - Landscape of R packages for eXplainable Artificial.pdf;/home/skynet3/Zotero/storage/UFSSTR7D/2009.html}
}

@misc{mallinarBenignTemperedCatastrophic2022,
  ids = {mallinarBenignTemperedCatastrophic2022a},
  title = {Benign, {{Tempered}}, or {{Catastrophic}}: {{A Taxonomy}} of {{Overfitting}}},
  shorttitle = {Benign, {{Tempered}}, or {{Catastrophic}}},
  author = {Mallinar, Neil and Simon, James B. and Abedsoltan, Amirhesam and Pandit, Parthe and Belkin, Mikhail and Nakkiran, Preetum},
  date = {2022-10-20},
  number = {arXiv:2207.06569},
  eprint = {2207.06569},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2207.06569},
  url = {http://arxiv.org/abs/2207.06569},
  urldate = {2022-11-11},
  abstract = {The practical success of overparameterized neural networks has motivated the recent scientific study of interpolating methods, which perfectly fit their training data. Certain interpolating methods, including neural networks, can fit noisy training data without catastrophically bad test performance, in defiance of standard intuitions from statistical learning theory. Aiming to explain this, a body of recent work has studied benign overfitting, a phenomenon where some interpolating methods approach Bayes optimality, even in the presence of noise. In this work we argue that while benign overfitting has been instructive and fruitful to study, many real interpolating methods like neural networks do not fit benignly: modest noise in the training set causes nonzero (but non-infinite) excess risk at test time, implying these models are neither benign nor catastrophic but rather fall in an intermediate regime. We call this intermediate regime tempered overfitting, and we initiate its systematic study. We first explore this phenomenon in the context of kernel (ridge) regression (KR) by obtaining conditions on the ridge parameter and kernel eigenspectrum under which KR exhibits each of the three behaviors. We find that kernels with powerlaw spectra, including Laplace kernels and ReLU neural tangent kernels, exhibit tempered overfitting. We then empirically study deep neural networks through the lens of our taxonomy, and find that those trained to interpolation are tempered, while those stopped early are benign. We hope our work leads to a more refined understanding of overfitting in modern learning.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/skynet3/Zotero/storage/H22FVYBI/Mallinar et al. - 2022 - Benign, Tempered, or Catastrophic A Taxonomy of O.pdf;/home/skynet3/Zotero/storage/HGF88YIH/2207.html}
}

@misc{malsiner-walliComparingSpikeSlab2018,
  ids = {malsiner-walliComparingSpikeSlab2018a},
  title = {Comparing {{Spike}} and {{Slab Priors}} for {{Bayesian Variable Selection}}},
  author = {Malsiner-Walli, Gertraud and Wagner, Helga},
  date = {2018-12-18},
  number = {arXiv:1812.07259},
  eprint = {1812.07259},
  eprinttype = {arxiv},
  primaryclass = {stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1812.07259},
  url = {http://arxiv.org/abs/1812.07259},
  urldate = {2022-11-11},
  abstract = {An important task in building regression models is to decide which regressors should be included in the final model. In a Bayesian approach, variable selection can be performed using mixture priors with a spike and a slab component for the effects subject to selection. As the spike is concentrated at zero, variable selection is based on the probability of assigning the corresponding regression effect to the slab component. These posterior inclusion probabilities can be determined by MCMC sampling. In this paper we compare the MCMC implementations for several spike and slab priors with regard to posterior inclusion probabilities and their sampling efficiency for simulated data. Further, we investigate posterior inclusion probabilities analytically for different slabs in two simple settings. Application of variable selection with spike and slab priors is illustrated on a data set of psychiatric patients where the goal is to identify covariates affecting metabolism.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Methodology},
  file = {/home/skynet3/Zotero/storage/DX5KH8ZU/Malsiner-Walli and Wagner - 2018 - Comparing Spike and Slab Priors for Bayesian Varia.pdf;/home/skynet3/Zotero/storage/AUF54LWX/1812.html}
}

@article{mansourniaPvalueCompatibilitySvalue2022,
  ids = {mansourniaPvalueCompatibilitySvalue2022a},
  title = {P-Value, Compatibility, and {{S-value}}},
  author = {Mansournia, Mohammad Ali and Nazemipour, Maryam and Etminan, Mahyar},
  date = {2022-12-01},
  journaltitle = {Global Epidemiology},
  shortjournal = {Global Epidemiology},
  volume = {4},
  pages = {100085},
  issn = {2590-1133},
  doi = {10.1016/j.gloepi.2022.100085},
  url = {https://www.sciencedirect.com/science/article/pii/S2590113322000153},
  urldate = {2022-11-11},
  abstract = {Misinterpretations of P-values and 95\% confidence intervals are ubiquitous in medical research. Specifically, the terms significance or confidence, extensively used in medical papers, ignore biases and violations of statistical assumptions and hence should be called overconfidence terms. In this paper, we present the compatibility view of P-values and confidence intervals; the P-value is interpreted as an index of compatibility between data and the model, including the test hypothesis and background assumptions, whereas a confidence interval is interpreted as the range of parameter values that are compatible with the data under background assumptions. We also suggest the use of a surprisal measure, often referred to as the S-value, a novel metric that transforms the P-value, for gauging compatibility in terms of an intuitive experiment of coin tossing.},
  langid = {english},
  keywords = {-value,Compatibility interval,Confidence interval,S-value,Significance},
  file = {/home/skynet3/Zotero/storage/4L7NM2BI/Mansournia et al. - 2022 - P-value, compatibility, and S-value.pdf;/home/skynet3/Zotero/storage/II28XNZS/S2590113322000153.html}
}

@online{MarkfairbanksTidypolarsTidy,
  title = {Markfairbanks/Tidypolars: {{Tidy}} Interface to Polars},
  shorttitle = {Markfairbanks/Tidypolars},
  url = {https://github.com/markfairbanks/tidypolars},
  urldate = {2022-11-11},
  abstract = {Tidy interface to polars. Contribute to markfairbanks/tidypolars development by creating an account on GitHub.},
  langid = {english},
  organization = {{GitHub}},
  file = {/home/skynet3/Zotero/storage/A2NMF4MX/tidypolars.html}
}

@online{martinInformativePriorsCorrelation2021,
  ids = {martinInformativePriorsCorrelation2021a},
  title = {Informative Priors for Correlation Matrices: {{An}} Easy Approach},
  shorttitle = {Informative Priors for Correlation Matrices},
  author = {Martin, Stephen},
  date = {2021-04-02T00:43:22+00:00},
  url = {http://srmart.in/informative-priors-for-correlation-matrices-an-easy-approach/},
  urldate = {2022-11-11},
  langid = {american},
  organization = {{Stephen R. Martin, PhD}}
}

@misc{masegosaLearningModelMisspecification2020,
  title = {Learning under {{Model Misspecification}}: {{Applications}} to {{Variational}} and {{Ensemble}} Methods},
  shorttitle = {Learning under {{Model Misspecification}}},
  author = {Masegosa, Andres R.},
  date = {2020-10-22},
  number = {arXiv:1912.08335},
  eprint = {1912.08335},
  eprinttype = {arxiv},
  primaryclass = {cs, math, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1912.08335},
  url = {http://arxiv.org/abs/1912.08335},
  urldate = {2022-11-11},
  abstract = {Virtually any model we use in machine learning to make predictions does not perfectly represent reality. So, most of the learning happens under model misspecification. In this work, we present a novel analysis of the generalization performance of Bayesian model averaging under model misspecification and i.i.d. data using a new family of second-order PAC-Bayes bounds. This analysis shows, in simple and intuitive terms, that Bayesian model averaging provides suboptimal generalization performance when the model is misspecified. In consequence, we provide strong theoretical arguments showing that Bayesian methods are not optimal for learning predictive models, unless the model class is perfectly specified. Using novel second-order PAC-Bayes bounds, we derive a new family of Bayesian-like algorithms, which can be implemented as variational and ensemble methods. The output of these algorithms is a new posterior distribution, different from the Bayesian posterior, which induces a posterior predictive distribution with better generalization performance. Experiments with Bayesian neural networks illustrate these findings.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Mathematics - Statistics Theory,Statistics - Machine Learning},
  file = {/home/skynet3/Zotero/storage/JVEVMFMG/Masegosa - 2020 - Learning under Model Misspecification Application.pdf;/home/skynet3/Zotero/storage/9CQVQNR4/1912.html}
}

@online{MassachusettsInstituteTechnology,
  ids = {MassachusettsInstituteTechnologya},
  title = {The {{Massachusetts Institute}} of {{Technology}} ({{MIT}})},
  url = {http://web.mit.edu},
  urldate = {2022-11-11},
  abstract = {The mission of MIT is to advance knowledge and educate students in science, technology and other areas of scholarship that will best serve the nation and the world in the 21st century.},
  langid = {english},
  organization = {{Massachusetts Institute of Technology}},
  file = {/home/skynet3/Zotero/storage/3A6B8WM4/www.mit.edu.html}
}

@online{MasurpSpecrConducting,
  title = {Masurp/Specr: {{Conducting}} and {{Visualizing Specification Curve Analyses}}},
  shorttitle = {Masurp/Specr},
  url = {https://github.com/masurp/specr},
  urldate = {2022-11-11},
  abstract = {Conducting and Visualizing Specification Curve Analyses - masurp/specr: Conducting and Visualizing Specification Curve Analyses},
  langid = {english},
  organization = {{GitHub}},
  file = {/home/skynet3/Zotero/storage/QYGJHRSR/specr.html}
}

@incollection{matilainenNumberSignalsMultivariate2018,
  title = {On the Number of Signals in Multivariate Time Series},
  author = {Matilainen, Markus and Nordhausen, Klaus and Virta, Joni},
  date = {2018},
  volume = {10891},
  eprint = {1801.04925},
  eprinttype = {arxiv},
  primaryclass = {stat},
  pages = {248--258},
  doi = {10.1007/978-3-319-93764-9_24},
  url = {http://arxiv.org/abs/1801.04925},
  urldate = {2022-11-11},
  abstract = {We assume a second-order source separation model where the observed multivariate time series is a linear mixture of latent, temporally uncorrelated time series with some components pure white noise. To avoid the modelling of noise, we extract the non-noise latent components using some standard method, allowing the modelling of the extracted univariate time series individually. An important question is the determination of which of the latent components are of interest in modelling and which can be considered as noise. Bootstrap-based methods have recently been used in determining the latent dimension in various methods of unsupervised and supervised dimension reduction and we propose a set of similar estimation strategies for second-order stationary time series. Simulation studies and a sound wave example are used to show the method's effectiveness.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Methodology},
  file = {/home/skynet3/Zotero/storage/BM2FBQIZ/Matilainen et al. - 2018 - On the number of signals in multivariate time seri.pdf;/home/skynet3/Zotero/storage/Y988C8DF/1801.html}
}

@online{MatrixFunctionRDocumentation,
  title = {Matrix Function - {{RDocumentation}}},
  url = {https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/matrix},
  urldate = {2022-11-11}
}

@inreference{MatrixMathematics2022,
  title = {Matrix (Mathematics)},
  booktitle = {Wikipedia},
  date = {2022-11-07T05:49:49Z},
  url = {https://en.wikipedia.org/w/index.php?title=Matrix_(mathematics)&oldid=1120475305},
  urldate = {2022-11-11},
  abstract = {In mathematics, a matrix (plural matrices) is a rectangular array or table of numbers, symbols, or expressions, arranged in rows and columns, which is used to represent a mathematical object or a property of such an object. For example,  is a matrix with two rows and three columns. This is often referred to as a "two by three matrix", a "2×3-matrix", or a matrix of dimension 2×3. Without further specifications, matrices represent linear maps, and allow explicit computations in linear algebra. Therefore, the study of matrices is a large part of linear algebra, and most properties and operations of abstract linear algebra can be expressed in terms of matrices. For example, matrix multiplication represents composition of linear maps. Not all matrices are related to linear algebra. This is, in particular, the case in graph theory, of incidence matrices, and adjacency matrices. This article focuses on matrices related to linear algebra, and, unless otherwise specified, all matrices represent linear maps or may be viewed as such. Square matrices, matrices with the same number of rows and columns, play a major role in matrix theory. Square matrices of a given dimension form a noncommutative ring, which is one of the most common examples of a noncommutative ring. The determinant of a square matrix is a number associated to the matrix, which is fundamental for the study of a square matrix; for example, a square matrix is invertible if and only if it has a nonzero determinant, and the eigenvalues of a square matrix are the roots of a polynomial determinant. In geometry, matrices are widely used for specifying and representing geometric transformations (for example rotations) and coordinate changes. In numerical analysis, many computational problems are solved by reducing them to a matrix computation, and this often involves computing with matrices of huge dimension. Matrices are used in most areas of mathematics and most scientific fields, either directly, or through their use in geometry and numerical analysis.},
  langid = {english},
  annotation = {Page Version ID: 1120475305}
}

@online{MatrixMultiplicationComposition,
  title = {Matrix Multiplication as Composition | {{Chapter}} 4, {{Essence}} of Linear Algebra - {{YouTube}}},
  url = {https://www.youtube.com/watch?v=XkY2DOUCWMU&list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab&index=5},
  urldate = {2022-11-11},
  file = {/home/skynet3/Zotero/storage/PUAAMUXJ/watch.html}
}

@online{MattCowgillGgannotateInteractively,
  title = {{{MattCowgill}}/Ggannotate: {{Interactively}} Annotate Ggplots},
  shorttitle = {{{MattCowgill}}/Ggannotate},
  url = {https://github.com/MattCowgill/ggannotate},
  urldate = {2022-11-11},
  abstract = {Interactively annotate ggplots. Contribute to MattCowgill/ggannotate development by creating an account on GitHub.},
  langid = {english},
  organization = {{GitHub}},
  file = {/home/skynet3/Zotero/storage/UYU2BHSB/ggannotate.html}
}

@misc{matteiParsimoniousTourBayesian2020,
  ids = {matteiParsimoniousTourBayesian2020a},
  title = {A {{Parsimonious Tour}} of {{Bayesian Model Uncertainty}}},
  author = {Mattei, Pierre-Alexandre},
  date = {2020-09-25},
  number = {arXiv:1902.05539},
  eprint = {1902.05539},
  eprinttype = {arxiv},
  primaryclass = {stat},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/1902.05539},
  urldate = {2022-11-11},
  abstract = {Modern statistical software and machine learning libraries are enabling semi-automated statistical inference. Within this context, it appears easier and easier to try and fit many models to the data at hand, reversing thereby the Fisherian way of conducting science by collecting data after the scientific hypothesis (and hence the model) has been determined. The renewed goal of the statistician becomes to help the practitioner choose within such large and heterogeneous families of models, a task known as model selection. The Bayesian paradigm offers a systematized way of assessing this problem. This approach, launched by Harold Jeffreys in his 1935 book Theory of Probability, has witnessed a remarkable evolution in the last decades, that has brought about several new theoretical and methodological advances. Some of these recent developments are the focus of this survey, which tries to present a unifying perspective on work carried out by different communities. In particular, we focus on non-asymptotic out-of-sample performance of Bayesian model selection and averaging techniques, and draw connections with penalized maximum likelihood. We also describe recent extensions to wider classes of probabilistic frameworks including high-dimensional, unidentifiable, or likelihood-free models.},
  archiveprefix = {arXiv},
  keywords = {62-01,I.2.6,Statistics - Methodology},
  file = {/home/skynet3/Zotero/storage/QXSST4XJ/Mattei - 2020 - A Parsimonious Tour of Bayesian Model Uncertainty.pdf;/home/skynet3/Zotero/storage/5TDIJRPB/1902.html}
}

@article{maxwellPersistenceUnderpoweredStudies2004,
  ids = {maxwellPersistenceUnderpoweredStudies2004a},
  title = {The {{Persistence}} of {{Underpowered Studies}} in {{Psychological Research}}: {{Causes}}, {{Consequences}}, and {{Remedies}}.},
  shorttitle = {The {{Persistence}} of {{Underpowered Studies}} in {{Psychological Research}}},
  author = {Maxwell, Scott E.},
  date = {2004},
  journaltitle = {Psychological Methods},
  shortjournal = {Psychological Methods},
  volume = {9},
  number = {2},
  pages = {147--163},
  issn = {1939-1463, 1082-989X},
  doi = {10.1037/1082-989X.9.2.147},
  url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/1082-989X.9.2.147},
  urldate = {2022-11-11},
  langid = {english},
  file = {/home/skynet3/Zotero/storage/PSKY6J6C/Maxwell - 2004 - The Persistence of Underpowered Studies in Psychol.pdf}
}

@book{mccallumBadDataHandbook2012,
  ids = {mccallumBadDataHandbook2012a},
  title = {Bad {{Data Handbook}}: {{Cleaning Up The Data So You Can Get Back To Work}}},
  shorttitle = {Bad {{Data Handbook}}},
  author = {McCallum, Q. Ethan},
  date = {2012-12-04},
  edition = {1st edition},
  publisher = {{O'Reilly Media}},
  location = {{Beijing, Köln}},
  isbn = {978-1-4493-2188-8},
  langid = {english},
  pagetotal = {264}
}

@misc{mccloskeyCriticalValuesRobust2022,
  title = {Critical {{Values Robust}} to {{P-hacking}}},
  author = {McCloskey, Adam and Michaillat, Pascal},
  date = {2022-06-15},
  number = {arXiv:2005.04141},
  eprint = {2005.04141},
  eprinttype = {arxiv},
  primaryclass = {econ, stat},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2005.04141},
  urldate = {2022-11-11},
  abstract = {P-hacking occurs when researchers engage in various behaviors that increase their chances of reporting statistically significant results. P-hacking is problematic because it reduces the informativeness of hypothesis tests -- by making significant results much more common than they are supposed to be in the absence of true significance. Despite its prevalence, p-hacking is not taken into account in hypothesis testing theory: the critical values used to determine significance assume no p-hacking. To address this problem, we build a model of p-hacking and use it to construct critical values such that, if these values are used to determine significance, and if researchers adjust their behavior to these new significance standards, then significant results occur with the desired frequency. Because such robust critical values allow for p-hacking, they are larger than classical critical values. As an illustration, we calibrate the model with evidence from the social and medical sciences. We find that the robust critical value for any test is the classical critical value for the same test with one fifth of the significance level -- a form of Bonferroni correction. For instance, for a \$z\$-test with a significance level of \$5\textbackslash\%\$, the robust critical value is \$2.31\$ instead of \$1.65\$ if the test is one-sided and \$2.57\$ instead of \$1.96\$ if the test is two-sided.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Economics - Econometrics,Statistics - Methodology},
  file = {/home/skynet3/Zotero/storage/DTYSX2LM/McCloskey and Michaillat - 2022 - Critical Values Robust to P-hacking.pdf}
}

@book{mcconvilleStatisticalInferenceData,
  ids = {mcconvilleStatisticalInferenceDataa},
  title = {Statistical {{Inference}} via {{Data Science}}},
  author = {McConville, Chester Ismay {and} Albert Y. Kim Foreword by Kelly S.},
  url = {https://moderndive.com/index.html},
  urldate = {2022-11-11},
  abstract = {An open-source and fully-reproducible electronic textbook for teaching statistical inference using tidyverse data science tools.},
  file = {/home/skynet3/Zotero/storage/YGQLADRB/index.html}
}

@book{mcelreathStatisticalRethinkingBayesian2020,
  title = {Statistical {{Rethinking}}: {{A Bayesian Course}} with {{Examples}} in {{R}} and {{STAN}}},
  shorttitle = {Statistical {{Rethinking}}},
  author = {McElreath, Richard},
  date = {2020-03-15},
  edition = {2},
  publisher = {{Chapman and Hall/CRC}},
  location = {{New York}},
  doi = {10.1201/9780429029608},
  abstract = {Statistical Rethinking: A Bayesian Course with Examples in R and Stan builds your knowledge of and confidence in making inferences from data. Reflecting the need for scripting in today's model-based statistics, the book pushes you to perform step-by-step calculations that are usually automated. This unique computational approach ensures that you understand enough of the details to make reasonable choices and interpretations in your own modeling work. The text presents causal inference and generalized linear multilevel models from a simple Bayesian perspective that builds on information theory and maximum entropy. The core material ranges from the basics of regression to advanced multilevel models. It also presents measurement error, missing data, and Gaussian process models for spatial and phylogenetic confounding. The second edition emphasizes the directed acyclic graph (DAG) approach to causal inference, integrating DAGs into many examples. The new edition also contains new material on the design of prior distributions, splines, ordered categorical predictors, social relations models, cross-validation, importance sampling, instrumental variables, and Hamiltonian Monte Carlo. It ends with an entirely new chapter that goes beyond generalized linear modeling, showing how domain-specific scientific models can be built into statistical analyses. Features Integrates working code into the main text Illustrates concepts through worked data analysis examples Emphasizes understanding assumptions and how assumptions are reflected in code Offers more detailed explanations of the mathematics in optional sections Presents examples of using the dagitty R package to analyze causal graphs Provides the rethinking R package on the author's website and on GitHub},
  isbn = {978-0-429-02960-8},
  pagetotal = {612}
}

@online{MCMCInteractiveGallery,
  title = {{{MCMC Interactive Gallery}}},
  url = {https://chi-feng.github.io/mcmc-demo/app.html},
  urldate = {2022-11-11}
}

@misc{mcmilinSelectionColliderBias2022,
  ids = {mcmilinSelectionColliderBias2022a},
  title = {Selection {{Collider Bias}} in {{Large Language Models}}},
  author = {McMilin, Emily},
  date = {2022-09-13},
  number = {arXiv:2208.10063},
  eprint = {2208.10063},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2208.10063},
  url = {http://arxiv.org/abs/2208.10063},
  urldate = {2022-11-11},
  abstract = {In this paper we motivate the causal mechanisms behind sample selection induced collider bias (selection collider bias) that can cause Large Language Models (LLMs) to learn unconditional dependence between entities that are unconditionally independent in the real world. We show that selection collider bias can become amplified in underspecified learning tasks, and although difficult to overcome, we describe a method to exploit the resulting spurious correlations for determination of when a model may be uncertain about its prediction. We demonstrate an uncertainty metric that matches human uncertainty in tasks with gender pronoun underspecification on an extended version of the Winogender Schemas evaluation set, and we provide an online demo where users can apply our uncertainty metric to their own texts and models.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/home/skynet3/Zotero/storage/BZA9UG87/McMilin - 2022 - Selection Collider Bias in Large Language Models.pdf;/home/skynet3/Zotero/storage/C39NG9DD/2208.html}
}

@online{Mean,
  title = {Mean},
  url = {https://www.wikidata.org/wiki/Q2796622},
  urldate = {2022-11-11},
  abstract = {general term for the several definitions of mean value, the sum divided by the count},
  langid = {english},
  file = {/home/skynet3/Zotero/storage/537TVKG6/Q2796622.html}
}

@inreference{Mean2022,
  title = {Mean},
  booktitle = {Wikipedia},
  date = {2022-11-01T16:23:09Z},
  url = {https://en.wikipedia.org/w/index.php?title=Mean&oldid=1119441617},
  urldate = {2022-11-11},
  abstract = {There are several kinds of mean in mathematics, especially in statistics. Each mean serves to summarize a given group of data, often to better understand the overall value (magnitude and sign) of a given data set.  For a data set, the arithmetic mean, also known as "arithmetic average", is a measure of central tendency of a finite set of numbers: specifically, the sum of the values divided by the number of values. The arithmetic mean of a set of numbers x1, x2, ..., xn is typically denoted using an overhead bar,                                                                x               ¯                                                  \{\textbackslash displaystyle \{\textbackslash bar \{x\}\}\}   . If the data set were based on a series of observations obtained by sampling from a statistical population, the arithmetic mean is the sample mean (                                                               x               ¯                                                  \{\textbackslash displaystyle \{\textbackslash bar \{x\}\}\}   ) to distinguish it from the mean, or expected value, of the underlying distribution, the population mean (denoted                         μ                 \{\textbackslash displaystyle \textbackslash mu \}    or                                    μ                        x                                     \{\textbackslash displaystyle \textbackslash mu \_\{x\}\}   ).Outside probability and statistics, a wide range of other notions of mean are often used in geometry and mathematical analysis; examples are given below.},
  langid = {english},
  annotation = {Page Version ID: 1119441617},
  file = {/home/skynet3/Zotero/storage/7YBZFVS7/Mean.html}
}

@inreference{Mean2022a,
  title = {Mean},
  booktitle = {Wikipedia},
  date = {2022-11-01T16:23:09Z},
  url = {https://en.wikipedia.org/w/index.php?title=Mean&oldid=1119441617},
  urldate = {2022-11-11},
  abstract = {There are several kinds of mean in mathematics, especially in statistics. Each mean serves to summarize a given group of data, often to better understand the overall value (magnitude and sign) of a given data set.  For a data set, the arithmetic mean, also known as "arithmetic average", is a measure of central tendency of a finite set of numbers: specifically, the sum of the values divided by the number of values. The arithmetic mean of a set of numbers x1, x2, ..., xn is typically denoted using an overhead bar,                                                                x               ¯                                                  \{\textbackslash displaystyle \{\textbackslash bar \{x\}\}\}   . If the data set were based on a series of observations obtained by sampling from a statistical population, the arithmetic mean is the sample mean (                                                               x               ¯                                                  \{\textbackslash displaystyle \{\textbackslash bar \{x\}\}\}   ) to distinguish it from the mean, or expected value, of the underlying distribution, the population mean (denoted                         μ                 \{\textbackslash displaystyle \textbackslash mu \}    or                                    μ                        x                                     \{\textbackslash displaystyle \textbackslash mu \_\{x\}\}   ).Outside probability and statistics, a wide range of other notions of mean are often used in geometry and mathematical analysis; examples are given below.},
  langid = {english},
  annotation = {Page Version ID: 1119441617},
  file = {/home/skynet3/Zotero/storage/RIESMUU8/Mean.html}
}

@online{MeanFunctionRDocumentation,
  title = {Mean Function - {{RDocumentation}}},
  url = {https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/mean},
  urldate = {2022-11-11}
}

@misc{mellonRidgeRegressionCan2022,
  type = {SSRN Scholarly Paper},
  ids = {mellonRidgeRegressionCan2022a},
  title = {Ridge {{Regression Can Produce Misleading Inferences}} in the {{Presence}} of {{Strong Confounders}}: {{The Case}} of {{Mass Polarization}}},
  shorttitle = {Ridge {{Regression Can Produce Misleading Inferences}} in the {{Presence}} of {{Strong Confounders}}},
  author = {Mellon, Jonathan and Prosser, Christopher},
  date = {2022-08-12},
  number = {4189012},
  location = {{Rochester, NY}},
  doi = {10.2139/ssrn.4189012},
  url = {https://papers.ssrn.com/abstract=4189012},
  urldate = {2022-11-11},
  abstract = {Cavari and Freedman (2022) argue that declining cooperation rates in surveys spuriously inflate the increase in polarization measured among the population. We show that their results rely on estimating linear models using ridge regression with a large shrinkage parameter rather than OLS (as they had done in previous work). Using simulations we show that their approach has 89.4\%-99.9\% chance of producing false positives if there were no true effect. When using appropriate methods, there are no longer any statistically significant effects and we would need 100-200 times as many observations to reliably detect the effect sizes claimed by Cavari and Freedman.},
  langid = {english},
  keywords = {non-response bias,polarization,ridge regression,spurious correlation,time trend},
  file = {/home/skynet3/Zotero/storage/J6I5MUEW/Mellon and Prosser - 2022 - Ridge Regression Can Produce Misleading Inferences.pdf;/home/skynet3/Zotero/storage/XYK45LFN/papers.html}
}

@article{meloAutomatedGeocodingTextual2017,
  ids = {meloAutomatedGeocodingTextual2017a},
  title = {Automated {{Geocoding}} of {{Textual Documents}}: {{A Survey}} of {{Current Approaches}}},
  shorttitle = {Automated {{Geocoding}} of {{Textual Documents}}},
  author = {Melo, Fernando and Martins, Bruno},
  date = {2017},
  journaltitle = {Transactions in GIS},
  volume = {21},
  number = {1},
  pages = {3--38},
  issn = {1467-9671},
  doi = {10.1111/tgis.12212},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/tgis.12212},
  urldate = {2022-11-11},
  abstract = {This survey article describes previous research addressing text-based document geocoding, i.e. the task of predicting the geospatial coordinates of latitude and longitude, that best correspond to an entire document, based on its textual contents. We describe (1) early document geocoding systems that use heuristics over place names mentioned in the text (e.g. names of cities and states), (2) probabilistic language modeling approaches, where generative models are built for different regions in the world (usually considering a discretization based on a rectangular grid) from the words occurring in a set of georeferenced training documents, which are then used to predict per-region probabilities for previously unseen test documents, (3) combinations of different models and heuristics, including clustering procedures, feature selection approaches, and/or language models built from different sources, and (4) recent approaches based on discriminative classification models.},
  langid = {english},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/tgis.12212},
  file = {/home/skynet3/Zotero/storage/HU2KAZ5Y/Melo and Martins - 2017 - Automated Geocoding of Textual Documents A Survey.pdf;/home/skynet3/Zotero/storage/G9ZI5JHV/tgis.html}
}

@online{MemorizingOverfittingBias2020,
  title = {Memorizing without Overfitting: {{Bias}}, Variance, and Interpolation in over-Parameterized Models},
  shorttitle = {Memorizing without Overfitting},
  date = {2020-10-26T22:31:04+00:00},
  url = {https://deepai.org/publication/memorizing-without-overfitting-bias-variance-and-interpolation-in-over-parameterized-models},
  urldate = {2022-11-11},
  abstract = {10/26/20 - The bias-variance trade-off is a central concept in supervised learning. In classical statistics, increasing the complexity of a m...},
  organization = {{DeepAI}},
  file = {/home/skynet3/Zotero/storage/AUZ59JGM/memorizing-without-overfitting-bias-variance-and-interpolation-in-over-parameterized-models.html}
}

@article{menezesNaturalScalesGeographical2017,
  title = {Natural {{Scales}} in {{Geographical Patterns}}},
  author = {Menezes, Telmo and Roth, Camille},
  date = {2017-04-04},
  journaltitle = {Scientific Reports},
  shortjournal = {Sci Rep},
  volume = {7},
  eprint = {28374825},
  eprinttype = {pmid},
  pages = {45823},
  issn = {2045-2322},
  doi = {10.1038/srep45823},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5379183/},
  urldate = {2022-11-11},
  abstract = {Human mobility is known to be distributed across several orders of magnitude of physical distances, which makes it generally difficult to endogenously find or define typical and meaningful scales. Relevant analyses, from movements to geographical partitions, seem to be relative to some ad-hoc scale, or no scale at all. Relying on geotagged data collected from photo-sharing social media, we apply community detection to movement networks constrained by increasing percentiles of the distance distribution. Using a simple parameter-free discontinuity detection algorithm, we discover clear phase transitions in the community partition space. The detection of these phases constitutes the first objective method of characterising endogenous, natural scales of human movement. Our study covers nine regions, ranging from cities to countries of various sizes and a transnational area. For all regions, the number of natural scales is remarkably low (2 or 3). Further, our results hint at scale-related behaviours rather than scale-related users. The partitions of the natural scales allow us to draw discrete multi-scale geographical boundaries, potentially capable of providing key insights in fields such as epidemiology or cultural contagion where the introduction of spatial boundaries is pivotal.},
  pmcid = {PMC5379183},
  file = {/home/skynet3/Zotero/storage/VXIFML69/Menezes and Roth - 2017 - Natural Scales in Geographical Patterns.pdf}
}

@article{mengStatisticalParadisesParadoxes2018,
  ids = {mengStatisticalParadisesParadoxes2018a},
  title = {Statistical Paradises and Paradoxes in Big Data ({{I}}): {{Law}} of Large Populations, Big Data Paradox, and the 2016 {{US}} Presidential Election},
  shorttitle = {Statistical Paradises and Paradoxes in Big Data ({{I}})},
  author = {Meng, Xiao-Li},
  date = {2018-06-01},
  journaltitle = {The Annals of Applied Statistics},
  shortjournal = {Ann. Appl. Stat.},
  volume = {12},
  number = {2},
  issn = {1932-6157},
  doi = {10.1214/18-AOAS1161SF},
  url = {https://projecteuclid.org/journals/annals-of-applied-statistics/volume-12/issue-2/Statistical-paradises-and-paradoxes-in-big-data-I--Law/10.1214/18-AOAS1161SF.full},
  urldate = {2022-11-11},
  langid = {english},
  file = {/home/skynet3/Zotero/storage/DEHT4F87/Meng - 2018 - Statistical paradises and paradoxes in big data (I.pdf}
}

@online{MetricsMondayYou2017,
  title = {‘{{Metrics Monday}}: {{You Can}}’t {{Compare OLS}} with {{2SLS}}},
  shorttitle = {‘{{Metrics Monday}}},
  date = {2017-11-20T11:00:46+00:00},
  url = {http://marcfbellemare.com/wordpress/12723},
  urldate = {2022-11-11},
  abstract = {Suppose you are interested in the effect of a treatment variable D on some outcome Y, and you have some controls X. You can thus estimate the following equation by ordinary least squares (OLS): (1)…},
  langid = {american},
  organization = {{Marc F. Bellemare}},
  file = {/home/skynet3/Zotero/storage/Z24PDW5S/12723.html}
}

@online{meyerQuartoPython2022,
  title = {Quarto with {{Python}}},
  author = {Meyer, Lucas A.},
  date = {2022-07-07},
  url = {https://www.meyerperin.com/using-quarto/},
  urldate = {2022-11-11},
  langid = {english}
}

@online{michaelSingleParameterModelsPyro2020,
  title = {Single-{{Parameter Models}} | {{Pyro}} vs. {{STAN}}},
  author = {Michael, Richard},
  date = {2020-07-15T20:49:40},
  url = {https://towardsdatascience.com/single-parameter-models-pyro-vs-stan-e7e69b45d95c},
  urldate = {2022-11-11},
  abstract = {Modeling U.S. cancer-death rates with two Bayesian approaches: MCMC in STAN and SVI in Pyro.},
  langid = {english},
  organization = {{Medium}},
  file = {/home/skynet3/Zotero/storage/RCUU6CYC/single-parameter-models-pyro-vs-stan-e7e69b45d95c.html}
}

@online{MicrosoftSPTAGDistributed,
  title = {Microsoft/{{SPTAG}}: {{A}} Distributed Approximate Nearest Neighborhood Search ({{ANN}}) Library Which Provides a High Quality Vector Index Build, Search and Distributed Online Serving Toolkits for Large Scale Vector Search Scenario.},
  shorttitle = {Microsoft/{{SPTAG}}},
  url = {https://github.com/microsoft/SPTAG},
  urldate = {2022-11-11},
  abstract = {A distributed approximate nearest neighborhood search (ANN) library which provides a high quality vector index build, search and distributed online serving toolkits for large scale vector search sc...},
  langid = {english},
  organization = {{GitHub}},
  file = {/home/skynet3/Zotero/storage/WTUFXNAD/SPTAG.html}
}

@article{miguelEvidenceResearchTransparency2021,
  title = {Evidence on {{Research Transparency}} in {{Economics}}},
  author = {Miguel, Edward},
  date = {2021-08},
  journaltitle = {Journal of Economic Perspectives},
  volume = {35},
  number = {3},
  pages = {193--214},
  issn = {0895-3309},
  doi = {10.1257/jep.35.3.193},
  url = {https://www.aeaweb.org/articles?id=10.1257/jep.35.3.193},
  urldate = {2022-11-11},
  abstract = {A decade ago, the term "research transparency" was not on economists' radar screen, but in a few short years a scholarly movement has emerged to bring new open  science practices, tools and norms into the mainstream of our discipline. The goal of this article is to lay out the evidence on the adoption of these approaches—in three  specific areas: open data, pre-registration and pre-analysis plans, and journal policies—and, more tentatively, begin to assess their impacts on the quality and credibility of  economics research. The evidence to date indicates that economics (and related quantitative social science fields) are in a period of rapid transition toward new  transparency-enhancing norms. While solid data on the benefits of these practices in economics is still limited, in part due to their relatively recent adoption, there is  growing reason to believe that critics' worst fears regarding onerous adoption costs have not been realized. Finally, the article presents a set of frontier questions and  potential innovations.},
  langid = {english},
  keywords = {Computer Programs: General,Computer Programs: General; Higher Education,Market for Economists; Hypothesis Testing: General; Data Collection and Data Estimation Methodology,Research Institutions; Role of Economics,Role of Economics,Role of Economists},
  file = {/home/skynet3/Zotero/storage/HSVS58N4/Miguel - 2021 - Evidence on Research Transparency in Economics.pdf;/home/skynet3/Zotero/storage/Q3YR22FX/articles.html}
}

@online{MilvusioMilvusVector,
  title = {Milvus-Io/Milvus: {{Vector}} Database for Scalable Similarity Search and {{AI}} Applications.},
  shorttitle = {Milvus-Io/Milvus},
  url = {https://github.com/milvus-io/milvus},
  urldate = {2022-11-11},
  abstract = {Vector database for scalable similarity search and AI applications. - milvus-io/milvus: Vector database for scalable similarity search and AI applications.},
  langid = {english},
  organization = {{GitHub}},
  file = {/home/skynet3/Zotero/storage/98CYZGK2/milvus.html}
}

@online{minhlabPaperTipIceberg2017,
  title = {A Paper Is the Tip of an Iceberg},
  author = {{minhlab}},
  date = {2017-03-18T01:06:41+00:00},
  url = {https://minhlab.wordpress.com/2017/03/18/a-paper-is-the-tip-of-the-iceberg/},
  urldate = {2022-11-11},
  abstract = {I was reading Clark and Manning (2016) and studying their code. The contrast is just amazing. This is what the paper has to say: This is what I found after 1 hour of reading a JSON file~and writing…},
  langid = {english},
  organization = {{Minh's virtual lab}},
  file = {/home/skynet3/Zotero/storage/SVZLCW3I/a-paper-is-the-tip-of-the-iceberg.html}
}

@online{MjskayTidybayesBayesian,
  title = {Mjskay/Tidybayes: {{Bayesian}} Analysis + Tidy Data + Geoms ({{R}} Package)},
  shorttitle = {Mjskay/Tidybayes},
  url = {https://github.com/mjskay/tidybayes},
  urldate = {2022-11-11},
  abstract = {Bayesian analysis + tidy data + geoms (R package). Contribute to mjskay/tidybayes development by creating an account on GitHub.},
  langid = {english},
  organization = {{GitHub}},
  file = {/home/skynet3/Zotero/storage/95BVKJJN/tidybayes.html}
}

@online{ModelAgnosticExplainers,
  title = {Model {{Agnostic Explainers}} for {{Individual Predictions}} • {{breakDown}}},
  url = {https://pbiecek.github.io/breakDown/},
  urldate = {2022-11-11},
  file = {/home/skynet3/Zotero/storage/FY49EVN6/breakDown.html}
}

@online{ModelOrientedEloMLPackage,
  title = {{{ModelOriented}}/{{EloML}}: {{R}} Package {{EloML}}: {{Elo}} Rating System for Machine Learning Models},
  shorttitle = {{{ModelOriented}}/{{EloML}}},
  url = {https://github.com/ModelOriented/EloML},
  urldate = {2022-11-11},
  abstract = {R package EloML: Elo rating system for machine learning models - ModelOriented/EloML: R package EloML: Elo rating system for machine learning models},
  langid = {english},
  organization = {{GitHub}},
  file = {/home/skynet3/Zotero/storage/Y9CU24WU/EloML.html}
}

@online{ModelSelectionTutorials,
  title = {Model Selection Tutorials and Talks},
  url = {https://avehtari.github.io/modelselection/index.html},
  urldate = {2022-11-11},
  file = {/home/skynet3/Zotero/storage/8I6EKEF6/index.html}
}

@article{mogensenEvaluatingRandomForests2012,
  title = {Evaluating {{Random Forests}} for {{Survival Analysis}} Using {{Prediction Error Curves}}},
  author = {Mogensen, Ulla B and Ishwaran, Hemant and Gerds, Thomas A},
  date = {2012-09},
  journaltitle = {Journal of statistical software},
  shortjournal = {J Stat Softw},
  volume = {50},
  number = {11},
  eprint = {25317082},
  eprinttype = {pmid},
  pages = {1--23},
  issn = {1548-7660},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4194196/},
  urldate = {2022-11-11},
  abstract = {Prediction error curves are increasingly used to assess and compare predictions in survival analysis. This article surveys the R package pec which provides a set of functions for efficient computation of prediction error curves. The software implements inverse probability of censoring weights to deal with right censored data and several variants of cross-validation to deal with the apparent error problem. In principle, all kinds of prediction models can be assessed, and the package readily supports most traditional regression modeling strategies, like Cox regression or additive hazard regression, as well as state of the art machine learning methods such as random forests, a nonparametric method which provides promising alternatives to traditional strategies in low and high-dimensional settings. We show how the functionality of pec can be extended to yet unsupported prediction models. As an example, we implement support for random forest prediction models based on the R-packages randomSurvivalForest and party. Using data of the Copenhagen Stroke Study we use pec to compare random forests to a Cox regression model derived from stepwise variable selection. Reproducible results on the user level are given for publicly available data from the German breast cancer study group.},
  pmcid = {PMC4194196},
  file = {/home/skynet3/Zotero/storage/VA9BN6CG/Mogensen et al. - 2012 - Evaluating Random Forests for Survival Analysis us.pdf}
}

@misc{mohamedMonteCarloGradient2020,
  title = {Monte {{Carlo Gradient Estimation}} in {{Machine Learning}}},
  author = {Mohamed, Shakir and Rosca, Mihaela and Figurnov, Michael and Mnih, Andriy},
  date = {2020-09-29},
  number = {arXiv:1906.10652},
  eprint = {1906.10652},
  eprinttype = {arxiv},
  primaryclass = {cs, math, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1906.10652},
  url = {http://arxiv.org/abs/1906.10652},
  urldate = {2022-11-11},
  abstract = {This paper is a broad and accessible survey of the methods we have at our disposal for Monte Carlo gradient estimation in machine learning and across the statistical sciences: the problem of computing the gradient of an expectation of a function with respect to parameters defining the distribution that is integrated; the problem of sensitivity analysis. In machine learning research, this gradient problem lies at the core of many learning problems, in supervised, unsupervised and reinforcement learning. We will generally seek to rewrite such gradients in a form that allows for Monte Carlo estimation, allowing them to be easily and efficiently used and analysed. We explore three strategies--the pathwise, score function, and measure-valued gradient estimators--exploring their historical development, derivation, and underlying assumptions. We describe their use in other fields, show how they are related and can be combined, and expand on their possible generalisations. Wherever Monte Carlo gradient estimators have been derived and deployed in the past, important advances have followed. A deeper and more widely-held understanding of this problem will lead to further advances, and it is these advances that we wish to support.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Mathematics - Optimization and Control,Statistics - Machine Learning},
  file = {/home/skynet3/Zotero/storage/YW3VTFLA/Mohamed et al. - 2020 - Monte Carlo Gradient Estimation in Machine Learnin.pdf;/home/skynet3/Zotero/storage/UTMTKH83/1906.html}
}

@misc{mohammadPreprocessingTextReally2018,
  title = {Is Preprocessing of Text Really Worth Your Time for Online Comment Classification?},
  author = {Mohammad, Fahim},
  date = {2018-08-29},
  number = {arXiv:1806.02908},
  eprint = {1806.02908},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1806.02908},
  url = {http://arxiv.org/abs/1806.02908},
  urldate = {2022-11-11},
  abstract = {A large proportion of online comments present on public domains are constructive, however a significant proportion are toxic in nature. The comments contain lot of typos which increases the number of features manifold, making the ML model difficult to train. Considering the fact that the data scientists spend approximately 80\% of their time in collecting, cleaning and organizing their data [1], we explored how much effort should we invest in the preprocessing (transformation) of raw comments before feeding it to the state-of-the-art classification models. With the help of four models on Jigsaw toxic comment classification data, we demonstrated that the training of model without any transformation produce relatively decent model. Applying even basic transformations, in some cases, lead to worse performance and should be applied with caution.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/home/skynet3/Zotero/storage/RF8HFEU3/Mohammad - 2018 - Is preprocessing of text really worth your time fo.pdf;/home/skynet3/Zotero/storage/8TSAYWI8/1806.html}
}

@book{molnarInterpretableMachineLearning,
  title = {Interpretable {{Machine Learning}}},
  author = {Molnar, Christoph},
  url = {https://christophm.github.io/interpretable-ml-book/},
  urldate = {2022-11-11},
  abstract = {Machine learning algorithms usually operate as black boxes and it is unclear how they derived a certain decision. This book is a guide for practitioners to make machine learning decisions interpretable.}
}

@online{MonitoringDataQuality2020,
  title = {Monitoring {{Data Quality}} at {{Scale}} with {{Statistical Modeling}}},
  date = {2020-05-07T09:00:29+00:00},
  url = {https://www.uber.com/blog/monitoring-data-quality-at-scale/},
  urldate = {2022-11-11},
  abstract = {Uber employs statistical modeling to find anomalies in data and continually monitor data quality.},
  organization = {{Uber Blog}},
  file = {/home/skynet3/Zotero/storage/NSDYFYLB/monitoring-data-quality-at-scale.html}
}

@misc{mooijDistinguishingCauseEffect2015,
  ids = {mooijDistinguishingCauseEffect2015a},
  title = {Distinguishing Cause from Effect Using Observational Data: Methods and Benchmarks},
  shorttitle = {Distinguishing Cause from Effect Using Observational Data},
  author = {Mooij, Joris M. and Peters, Jonas and Janzing, Dominik and Zscheischler, Jakob and Schölkopf, Bernhard},
  date = {2015-12-24},
  number = {arXiv:1412.3773},
  eprint = {1412.3773},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1412.3773},
  url = {http://arxiv.org/abs/1412.3773},
  urldate = {2022-11-11},
  abstract = {The discovery of causal relationships from purely observational data is a fundamental problem in science. The most elementary form of such a causal discovery problem is to decide whether X causes Y or, alternatively, Y causes X, given joint observations of two variables X, Y. An example is to decide whether altitude causes temperature, or vice versa, given only joint measurements of both variables. Even under the simplifying assumptions of no confounding, no feedback loops, and no selection bias, such bivariate causal discovery problems are challenging. Nevertheless, several approaches for addressing those problems have been proposed in recent years. We review two families of such methods: Additive Noise Methods (ANM) and Information Geometric Causal Inference (IGCI). We present the benchmark CauseEffectPairs that consists of data for 100 different cause-effect pairs selected from 37 datasets from various domains (e.g., meteorology, biology, medicine, engineering, economy, etc.) and motivate our decisions regarding the "ground truth" causal directions of all pairs. We evaluate the performance of several bivariate causal discovery methods on these real-world benchmark data and in addition on artificially simulated data. Our empirical results on real-world data indicate that certain methods are indeed able to distinguish cause from effect using only purely observational data, although more benchmark data would be needed to obtain statistically significant conclusions. One of the best performing methods overall is the additive-noise method originally proposed by Hoyer et al. (2009), which obtains an accuracy of 63+-10 \% and an AUC of 0.74+-0.05 on the real-world benchmark. As the main theoretical contribution of this work we prove the consistency of that method.},
  archiveprefix = {arXiv},
  version = {3},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning,Statistics - Other Statistics},
  file = {/home/skynet3/Zotero/storage/UPHH8GPW/Mooij et al. - 2015 - Distinguishing cause from effect using observation.pdf;/home/skynet3/Zotero/storage/5DY4JMMU/1412.html}
}

@misc{moritzVisualizingMillionTime2018,
  ids = {moritzVisualizingMillionTime2018a},
  title = {Visualizing a {{Million Time Series}} with the {{Density Line Chart}}},
  author = {Moritz, Dominik and Fisher, Danyel},
  date = {2018-09-06},
  number = {arXiv:1808.06019},
  eprint = {1808.06019},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/1808.06019},
  urldate = {2022-11-11},
  abstract = {Data analysts often need to work with multiple series of data—conventionally shown as line charts—at once. Few visual representations allow analysts to view many lines simultaneously without becoming overwhelming or cluttered. In this paper, we introduce the DenseLines technique to calculate a discrete density representation of time series. DenseLines normalizes time series by the arc length to compute accurate densities. The derived density visualization allows users both to see the aggregate trends of multiple series and to identify anomalous extrema.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Human-Computer Interaction},
  file = {/home/skynet3/Zotero/storage/WT4QH7KP/Moritz and Fisher - 2018 - Visualizing a Million Time Series with the Density.pdf}
}

@article{moscovichCrossvalidationBiasDue2022,
  ids = {moscovichCrossvalidationBiasDue2022a},
  title = {On the Cross-Validation Bias Due to Unsupervised Pre-Processing},
  author = {Moscovich, Amit and Rosset, Saharon},
  date = {2022-09},
  journaltitle = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  shortjournal = {J R Stat Soc Series B},
  volume = {84},
  number = {4},
  eprint = {1901.08974},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  pages = {1474--1502},
  issn = {1369-7412, 1467-9868},
  doi = {10.1111/rssb.12537},
  url = {http://arxiv.org/abs/1901.08974},
  urldate = {2022-11-11},
  abstract = {Cross-validation is the de facto standard for predictive model evaluation and selection. In proper use, it provides an unbiased estimate of a model's predictive performance. However, data sets often undergo various forms of data-dependent preprocessing, such as mean-centering, rescaling, dimensionality reduction, and outlier removal. It is often believed that such preprocessing stages, if done in an unsupervised manner (that does not incorporate the class labels or response values) are generally safe to do prior to cross-validation. In this paper, we study three commonly-practiced preprocessing procedures prior to a regression analysis: (i) variance-based feature selection; (ii) grouping of rare categorical features; and (iii) feature rescaling. We demonstrate that unsupervised preprocessing can, in fact, introduce a substantial bias into cross-validation estimates and potentially hurt model selection. This bias may be either positive or negative and its exact magnitude depends on all the parameters of the problem in an intricate manner. Further research is needed to understand the real-world impact of this bias across different application domains, particularly when dealing with small sample sizes and high-dimensional data.},
  archiveprefix = {arXiv},
  keywords = {62-07,Computer Science - Machine Learning,G.3,Statistics - Machine Learning,Statistics - Methodology},
  file = {/home/skynet3/Zotero/storage/9FAD8K4J/Moscovich and Rosset - 2022 - On the cross-validation bias due to unsupervised p.pdf;/home/skynet3/Zotero/storage/D9BAVRMI/1901.html}
}

@online{moultonLogsTailsLong2013,
  title = {Logs, {{Tails}}, {{Long Tails}}},
  author = {Moulton, Ryan},
  date = {2013-08-09T22:49:34+00:00},
  url = {https://moultano.wordpress.com/2013/08/09/logs-tails-long-tails/},
  urldate = {2022-11-11},
  abstract = {Learn the shape of the logs of probability distributions, and you’ll be able to guess how they behave.},
  langid = {english},
  organization = {{Ryan Moulton's Articles}},
  file = {/home/skynet3/Zotero/storage/SQAA6TQ6/logs-tails-long-tails.html}
}

@article{muggeNationalAccountingParadox2021,
  title = {The National Accounting Paradox: How Statistical Norms Corrode International Economic Data},
  shorttitle = {The National Accounting Paradox},
  author = {Mügge, Daniel and Linsi, Lukas},
  date = {2021-06-01},
  journaltitle = {European Journal of International Relations},
  volume = {27},
  number = {2},
  pages = {403--427},
  publisher = {{SAGE Publications Ltd}},
  issn = {1354-0661},
  doi = {10.1177/1354066120936339},
  url = {https://doi.org/10.1177/1354066120936339},
  urldate = {2022-11-11},
  abstract = {The transnationalization and digitization of economic activity has undermined the quality of official economic statistics, which still center on national territories and material production. Why do we not witness more vigorous efforts to bring statistical standards in line with present-day economic realities, or admissions that precision in economic data has become increasingly illusive? The paradoxical answer, we argue, lies in the norms underpinning global statistical practice. Users expect statistics to draw on unambiguous sources, to allow for comparison over time and across countries, and they prize coherence?both internally and with holistic macroeconomic models. Yet as we show, the ambition of the transnational statistical community to meet these norms has in fact undermined the ability of economic data to represent economic life more faithfully. We base our findings on interviews with two dozen leading statisticians at international economic organizations, archival research at the International Monetary Fund and a thorough review of debates among statistical experts.},
  langid = {english},
  file = {/home/skynet3/Zotero/storage/DX2JNRLY/Mügge and Linsi - 2021 - The national accounting paradox how statistical n.pdf}
}

@article{mulderMatrixFPriorEstimating2018,
  title = {The {{Matrix-F Prior}} for {{Estimating}} and {{Testing Covariance Matrices}}},
  author = {Mulder, Joris and Pericchi, Luis Raúl},
  date = {2018-12},
  journaltitle = {Bayesian Analysis},
  volume = {13},
  number = {4},
  pages = {1193--1214},
  publisher = {{International Society for Bayesian Analysis}},
  issn = {1936-0975, 1931-6690},
  doi = {10.1214/17-BA1092},
  url = {https://projecteuclid.org/journals/bayesian-analysis/volume-13/issue-4/The-Matrix-F-Prior-for-Estimating-and-Testing-Covariance-Matrices/10.1214/17-BA1092.full},
  urldate = {2022-11-11},
  abstract = {The matrix-F distribution is presented as prior for covariance matrices as an alternative to the conjugate inverted Wishart distribution. A special case of the univariate F distribution for a variance parameter is equivalent to a half-t distribution for a standard deviation, which is becoming increasingly popular in the Bayesian literature. The matrix-F distribution can be conveniently modeled as a Wishart mixture of Wishart or inverse Wishart distributions, which allows straightforward implementation in a Gibbs sampler. By mixing the covariance matrix of a multivariate normal distribution with a matrix-F distribution, a multivariate horseshoe type prior is obtained which is useful for modeling sparse signals. Furthermore, it is shown that the intrinsic prior for testing covariance matrices in non-hierarchical models has a matrix-F distribution. This intrinsic prior is also useful for testing inequality constrained hypotheses on variances. Finally through simulation it is shown that the matrix-variate F distribution has good frequentist properties as prior for the random effects covariance matrix in generalized linear mixed models.},
  keywords = {hierarchical models,horsehoe prior,intrinsic prior,matrix-variate F distribution,testing inequality constraints},
  file = {/home/skynet3/Zotero/storage/L87FFAXI/Mulder and Pericchi - 2018 - The Matrix-F Prior for Estimating and Testing Cova.pdf;/home/skynet3/Zotero/storage/Z6Y77F5I/17-BA1092.html}
}

@misc{mulgraveBayesianPosteriorInterval2021,
  ids = {mulgraveBayesianPosteriorInterval2021a},
  title = {Bayesian {{Posterior Interval Calibration}} to {{Improve}} the {{Interpretability}} of {{Observational Studies}}},
  author = {Mulgrave, Jami J. and Madigan, David and Hripcsak, George},
  date = {2021-02-15},
  number = {arXiv:2003.06002},
  eprint = {2003.06002},
  eprinttype = {arxiv},
  primaryclass = {stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2003.06002},
  url = {http://arxiv.org/abs/2003.06002},
  urldate = {2022-11-11},
  abstract = {Observational healthcare data offer the potential to estimate causal effects of medical products on a large scale. However, the confidence intervals and p-values produced by observational studies only account for random error and fail to account for systematic error. As a consequence, operating characteristics such as confidence interval coverage and Type I error rates often deviate sharply from their nominal values and render interpretation impossible. While there is longstanding awareness of systematic error in observational studies, analytic approaches to empirically account for systematic error are relatively new. Several authors have proposed approaches using negative controls (also known as "falsification hypotheses") and positive controls. The basic idea is to adjust confidence intervals and p-values in light of the bias (if any) detected in the analyses of the negative and positive control. In this work, we propose a Bayesian statistical procedure for posterior interval calibration that uses negative and positive controls. We show that the posterior interval calibration procedure restores nominal characteristics, such as 95\% coverage of the true effect size by the 95\% posterior interval.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Applications},
  file = {/home/skynet3/Zotero/storage/BTTR88Z5/Mulgrave et al. - 2021 - Bayesian Posterior Interval Calibration to Improve.pdf;/home/skynet3/Zotero/storage/G39DL6NP/2003.html}
}

@inproceedings{mullerIdentifyingMislabeledInstances2019,
  title = {Identifying {{Mislabeled Instances}} in {{Classification Datasets}}},
  booktitle = {2019 {{International Joint Conference}} on {{Neural Networks}} ({{IJCNN}})},
  author = {Müller, Nicolas Michael and Markert, Karla},
  date = {2019-07},
  eprint = {1912.05283},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  pages = {1--8},
  doi = {10.1109/IJCNN.2019.8851920},
  url = {http://arxiv.org/abs/1912.05283},
  urldate = {2022-11-11},
  abstract = {A key requirement for supervised machine learning is labeled training data, which is created by annotating unlabeled data with the appropriate class. Because this process can in many cases not be done by machines, labeling needs to be performed by human domain experts. This process tends to be expensive both in time and money, and is prone to errors. Additionally, reviewing an entire labeled dataset manually is often prohibitively costly, so many real world datasets contain mislabeled instances. To address this issue, we present in this paper a non-parametric end-to-end pipeline to find mislabeled instances in numerical, image and natural language datasets. We evaluate our system quantitatively by adding a small number of label noise to 29 datasets, and show that we find mislabeled instances with an average precision of more than 0.84 when reviewing our system's top 1\textbackslash\% recommendation. We then apply our system to publicly available datasets and find mislabeled instances in CIFAR-100, Fashion-MNIST, and others. Finally, we publish the code and an applicable implementation of our approach.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/skynet3/Zotero/storage/CJM8RMGY/Müller and Markert - 2019 - Identifying Mislabeled Instances in Classification.pdf;/home/skynet3/Zotero/storage/2SQHXWQ8/1912.html}
}

@article{mullinixGeneralizabilitySurveyExperiments2015,
  title = {The {{Generalizability}} of {{Survey Experiments}}*},
  author = {Mullinix, Kevin J. and Leeper, Thomas J. and Druckman, James N. and Freese, Jeremy},
  year = {2015/ed},
  journaltitle = {Journal of Experimental Political Science},
  volume = {2},
  number = {2},
  pages = {109--138},
  publisher = {{Cambridge University Press}},
  issn = {2052-2630, 2052-2649},
  doi = {10.1017/XPS.2015.19},
  url = {https://www.cambridge.org/core/journals/journal-of-experimental-political-science/article/abs/generalizability-of-survey-experiments/72D4E3DB90569AD7F2D469E9DF3A94CB},
  urldate = {2022-11-11},
  abstract = {Survey experiments have become a central methodology across the social sciences. Researchers can combine experiments’ causal power with the generalizability of population-based samples. Yet, due to the expense of population-based samples, much research relies on convenience samples (e.g. students, online opt-in samples). The emergence of affordable, but non-representative online samples has reinvigorated debates about the external validity of experiments. We conduct two studies of how experimental treatment effects obtained from convenience samples compare to effects produced by population samples. In Study 1, we compare effect estimates from four different types of convenience samples and a population-based sample. In Study 2, we analyze treatment effects obtained from 20 experiments implemented on a population-based sample and Amazon's Mechanical Turk (MTurk). The results reveal considerable similarity between many treatment effects obtained from convenience and nationally representative population-based samples. While the results thus bolster confidence in the utility of convenience samples, we conclude with guidance for the use of a multitude of samples for advancing scientific knowledge.},
  langid = {english},
  keywords = {causal inference,sampling,Survey experiments},
  file = {/home/skynet3/Zotero/storage/LN6Z8VQG/Mullinix et al. - 2015 - The Generalizability of Survey Experiments.pdf;/home/skynet3/Zotero/storage/PHERDT52/72D4E3DB90569AD7F2D469E9DF3A94CB.html}
}

@online{MultiprocessingVsThreading2019,
  title = {Multiprocessing vs. {{Threading}} in {{Python}}: {{What Every Data Scientist Needs}} to {{Know}}},
  shorttitle = {Multiprocessing vs. {{Threading}} in {{Python}}},
  date = {2019-09-07T13:44:45},
  url = {https://blog.floydhub.com/multiprocessing-vs-threading-in-python-what-every-data-scientist-needs-to-know/},
  urldate = {2022-11-11},
  abstract = {This deep dive on Python parallelization libraries - multiprocessing and threading - will explain which to use when for different data scientist problem sets.},
  langid = {english},
  organization = {{FloydHub Blog}},
  file = {/home/skynet3/Zotero/storage/UGVHHA84/multiprocessing-vs-threading-in-python-what-every-data-scientist-needs-to-know.html}
}

@article{mummoloDemandEffectsSurvey2019,
  title = {Demand {{Effects}} in {{Survey Experiments}}: {{An Empirical Assessment}}},
  shorttitle = {Demand {{Effects}} in {{Survey Experiments}}},
  author = {Mummolo, Jonathan and Peterson, Erik},
  date = {2019-05},
  journaltitle = {American Political Science Review},
  volume = {113},
  number = {2},
  pages = {517--529},
  publisher = {{Cambridge University Press}},
  issn = {0003-0554, 1537-5943},
  doi = {10.1017/S0003055418000837},
  url = {https://www.cambridge.org/core/journals/american-political-science-review/article/abs/demand-effects-in-survey-experiments-an-empirical-assessment/043386DC63A69098E859414EF9932EBC},
  urldate = {2022-11-11},
  abstract = {Survey experiments are ubiquitous in social science. A frequent critique is that positive results in these studies stem from experimenter demand effects (EDEs)—bias that occurs when participants infer the purpose of an experiment and respond so as to help confirm a researcher’s hypothesis. We argue that online survey experiments have several features that make them robust to EDEs, and test for their presence in studies that involve over 12,000 participants and replicate five experimental designs touching on all empirical political science subfields. We randomly assign participants information about experimenter intent and show that providing this information does not alter the treatment effects in these experiments. Even financial incentives to respond in line with researcher expectations fail to consistently induce demand effects. Research participants exhibit a limited ability to adjust their behavior to align with researcher expectations, a finding with important implications for the design and interpretation of survey experiments.},
  langid = {english},
  file = {/home/skynet3/Zotero/storage/V8NHPPPF/043386DC63A69098E859414EF9932EBC.html}
}

@article{muschelliROCAUCBinary2020,
  title = {{{ROC}} and {{AUC}} with a {{Binary Predictor}}: A {{Potentially Misleading Metric}}},
  shorttitle = {{{ROC}} and {{AUC}} with a {{Binary Predictor}}},
  author = {Muschelli, John},
  date = {2020-10},
  journaltitle = {Journal of Classification},
  shortjournal = {J Classif},
  volume = {37},
  number = {3},
  eprint = {1903.04881},
  eprinttype = {arxiv},
  primaryclass = {stat},
  pages = {696--708},
  issn = {0176-4268, 1432-1343},
  doi = {10.1007/s00357-019-09345-1},
  url = {http://arxiv.org/abs/1903.04881},
  urldate = {2022-11-11},
  abstract = {In analysis of binary outcomes, the receiver operator characteristic (ROC) curve is heavily used to show the performance of a model or algorithm. The ROC curve is informative about the performance over a series of thresholds and can be summarized by the area under the curve (AUC), a single number. When a predictor is categorical, the ROC curve has one less than number of categories as potential thresholds; when the predictor is binary there is only one threshold. As the AUC may be used in decision-making processes on determining the best model, it important to discuss how it agrees with the intuition from the ROC curve. We discuss how the interpolation of the curve between thresholds with binary predictors can largely change the AUC. Overall, we show using a linear interpolation from the ROC curve with binary predictors corresponds to the estimated AUC, which is most commonly done in software, which we believe can lead to misleading results. We compare R, Python, Stata, and SAS software implementations. We recommend using reporting the interpolation used and discuss the merit of using the step function interpolator, also referred to as the "pessimistic" approach by Fawcett (2006).},
  archiveprefix = {arXiv},
  keywords = {Statistics - Applications,Statistics - Computation},
  file = {/home/skynet3/Zotero/storage/RBS2ZLCU/Muschelli - 2020 - ROC and AUC with a Binary Predictor a Potentially.pdf;/home/skynet3/Zotero/storage/9BXV54MV/1903.html}
}

@video{mutualinformationReinforcementLearningSix2022,
  title = {Reinforcement {{Learning}}: {{A Six Part Series}}},
  shorttitle = {Reinforcement {{Learning}}},
  editor = {{Mutual Information}},
  date = {2022-10-24},
  url = {https://www.youtube.com/watch?v=NFo9v_yKQXA},
  urldate = {2022-11-12},
  editortype = {director}
}

@article{mutzPerilsBalanceTesting2019,
  title = {The {{Perils}} of {{Balance Testing}} in {{Experimental Design}}: {{Messy Analyses}} of {{Clean Data}}},
  shorttitle = {The {{Perils}} of {{Balance Testing}} in {{Experimental Design}}},
  author = {Mutz, Diana C. and Pemantle, Robin and Pham, Philip},
  date = {2019-01-02},
  journaltitle = {The American Statistician},
  shortjournal = {The American Statistician},
  volume = {73},
  number = {1},
  pages = {32--42},
  issn = {0003-1305, 1537-2731},
  doi = {10.1080/00031305.2017.1322143},
  url = {https://www.tandfonline.com/doi/full/10.1080/00031305.2017.1322143},
  urldate = {2022-11-11},
  langid = {english},
  file = {/home/skynet3/Zotero/storage/JP6XYW2H/Mutz et al. - 2019 - The Perils of Balance Testing in Experimental Desi.pdf}
}

@online{myrianthousArgsKwargsPython2022,
  title = {*args and **kwargs in {{Python}}},
  author = {Myrianthous, Giorgos},
  date = {2022-06-22T15:23:13},
  url = {https://towardsdatascience.com/args-kwargs-python-d9c71b220970},
  urldate = {2022-11-11},
  abstract = {Discussing the difference between positional and keyword arguments and how to use *args and **kwargs in Python},
  langid = {english},
  organization = {{Medium}},
  file = {/home/skynet3/Zotero/storage/FZLF63A8/args-kwargs-python-d9c71b220970.html}
}

@software{nafaTakingUncertaintySeriously2022,
  title = {Taking {{Uncertainty Seriously}}: {{Bayesian Marginal Structural Models}} for {{Causal Inference}} in {{Political Science}}},
  shorttitle = {Taking {{Uncertainty Seriously}}},
  author = {Nafa, A. Jordan},
  date = {2022-10-20T11:48:19Z},
  origdate = {2021-12-27T19:02:03Z},
  url = {https://github.com/ajnafa/Latent-Bayesian-MSM},
  urldate = {2022-11-11},
  abstract = {Taking Uncertainty Seriously: Bayesian Marginal Structural Models for Causal Inference in Political Science},
  keywords = {bayesian,causal-inference,inverse-probability-weights,political-science,stan}
}

@misc{naitzatTopologyDeepNeural2020,
  ids = {naitzatTopologyDeepNeural2020a},
  title = {Topology of Deep Neural Networks},
  author = {Naitzat, Gregory and Zhitnikov, Andrey and Lim, Lek-Heng},
  date = {2020-04-13},
  number = {arXiv:2004.06093},
  eprint = {2004.06093},
  eprinttype = {arxiv},
  primaryclass = {cs, math, stat},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2004.06093},
  urldate = {2022-11-11},
  abstract = {We study how the topology of a data set \$M = M\_a \textbackslash cup M\_b \textbackslash subseteq \textbackslash mathbb\{R\}\^d\$, representing two classes \$a\$ and \$b\$ in a binary classification problem, changes as it passes through the layers of a well-trained neural network, i.e., with perfect accuracy on training set and near-zero generalization error (\$\textbackslash approx 0.01\textbackslash\%\$). The goal is to shed light on two mysteries in deep neural networks: (i) a nonsmooth activation function like ReLU outperforms a smooth one like hyperbolic tangent; (ii) successful neural network architectures rely on having many layers, even though a shallow network can approximate any function arbitrary well. We performed extensive experiments on the persistent homology of a wide range of point cloud data sets, both real and simulated. The results consistently demonstrate the following: (1) Neural networks operate by changing topology, transforming a topologically complicated data set into a topologically simple one as it passes through the layers. No matter how complicated the topology of \$M\$ we begin with, when passed through a well-trained neural network \$f : \textbackslash mathbb\{R\}\^d \textbackslash to \textbackslash mathbb\{R\}\^p\$, there is a vast reduction in the Betti numbers of both components \$M\_a\$ and \$M\_b\$; in fact they nearly always reduce to their lowest possible values: \$\textbackslash beta\_k\textbackslash bigl(f(M\_i)\textbackslash bigr) = 0\$ for \$k \textbackslash ge 1\$ and \$\textbackslash beta\_0\textbackslash bigl(f(M\_i)\textbackslash bigr) = 1\$, \$i =a, b\$. Furthermore, (2) the reduction in Betti numbers is significantly faster for ReLU activation than hyperbolic tangent activation as the former defines nonhomeomorphic maps that change topology, whereas the latter defines homeomorphic maps that preserve topology. Lastly, (3) shallow and deep networks transform data sets differently -- a shallow network operates mainly through changing geometry and changes topology only in its final layers, a deep one spreads topological changes more evenly across all layers.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,I.2.6,Mathematics - Algebraic Topology,Statistics - Machine Learning},
  file = {/home/skynet3/Zotero/storage/FNQYKJJL/Naitzat et al. - 2020 - Topology of deep neural networks.pdf;/home/skynet3/Zotero/storage/7GEVUTWR/2004.html}
}

@software{NameMatch2022,
  title = {Name {{Match}}},
  date = {2022-11-11T15:19:19Z},
  origdate = {2021-02-26T01:26:17Z},
  url = {https://github.com/urban-labs/namematch},
  urldate = {2022-11-11},
  abstract = {Tool for probabilistically linking the records of individual entities (e.g. people) within and across datasets},
  organization = {{University of Chicago Urban Labs}}
}

@article{nattinoPolymatchingAlgorithmObservational2022,
  title = {Polymatching Algorithm in Observational Studies with Multiple Treatment Groups},
  author = {Nattino, Giovanni and Song, Chi and Lu, Bo},
  date = {2022-03-01},
  journaltitle = {Computational Statistics \& Data Analysis},
  shortjournal = {Computational Statistics \& Data Analysis},
  volume = {167},
  pages = {107364},
  issn = {0167-9473},
  doi = {10.1016/j.csda.2021.107364},
  url = {https://www.sciencedirect.com/science/article/pii/S0167947321001985},
  urldate = {2022-11-11},
  abstract = {Matched designs are commonly used in non-randomized studies to evaluate causal effects for dichotomous treatment. Optimal matching algorithms have been devised to form matched pairs or sets between treatment and control groups in various designs, including 1-k matching and full matching. With multiple treatment arms, however, the optimal matching problem cannot be solved in polynomial-time. This is a major challenge for implementing matched designs with multiple arms, which are important for evaluating causal effects with different dose levels or constructing evidence factors with multiple control groups. A polymatching framework for generating matched sets among multiple groups is proposed. An iterative multi-way algorithm for implementation is developed, which takes advantage of the existing optimal two-group matching algorithm repeatedly. An upper bound for the total distance attained by our algorithm is provided to show that the distance result is close to the optimal solution. Simulation studies are conducted to compare the proposed algorithm with the nearest neighbor algorithm under different scenarios. The algorithm is also used to construct a difference-in-difference matched design among four groups, to examine the impact of Medicaid expansion on the health status of Ohioans.},
  langid = {english},
  keywords = {Causal inference,Difference-in-difference,Multiple treatment groups,Polymatching,Polynomial-time algorithm},
  file = {/home/skynet3/Zotero/storage/CY3M6VDV/S0167947321001985.html}
}

@online{nealBiasVarianceTradeoffTextbooks2020,
  title = {On the {{Bias-Variance Tradeoff}}: {{Textbooks Need}} an {{Update}} ({{Blog Post}})},
  shorttitle = {On the {{Bias-Variance Tradeoff}}},
  author = {Neal, Brady},
  date = {2020-01-05},
  url = {https://www.bradyneal.com/bias-variance-tradeoff-textbooks-update},
  urldate = {2022-11-15},
  abstract = {The concept of the bias-variance tradeoff is not universal. In this post, we cover a modern, evidence-based perspective on the bias-variance tradeoff.},
  langid = {english},
  file = {/home/skynet3/Zotero/storage/SAYRPWR7/bias-variance-tradeoff-textbooks-update.html}
}

@article{nealGraphTerminologyBayesian,
  title = {Graph Terminology {{Bayesian}} Networks and Causal Graphs {{The}} Basic Building Blocks of Graphs {{The}} Flow of Association and Causation},
  author = {Neal, Brady},
  pages = {141},
  langid = {english},
  file = {/home/skynet3/Zotero/storage/TRV8ZUZ9/Neal - Graph terminology Bayesian networks and causal gra.pdf}
}

@online{nealIntroductionCausalInference,
  title = {Introduction to {{Causal Inference}}},
  author = {Neal, Brady},
  url = {https://www.bradyneal.com/causal-inference-course},
  urldate = {2022-11-11},
  abstract = {Introduction to Causal Inference. A free online course on causal inference from a machine learning perspective.},
  langid = {english}
}

@article{nealIntroductionCausalInference2020,
  title = {Introduction to Causal Inference from a Machine Learning Perspective},
  author = {Neal, Brady},
  date = {2020},
  journaltitle = {Course Lecture Notes (draft)},
  url = {https://www.bradyneal.com/Introduction_to_Causal_Inference-Dec17_2020-Neal.pdf}
}

@article{nealWhatArePotential,
  title = {What Are Potential Outcomes? {{The}} Fundamental Problem of Causal Inference {{Getting}} around the Fundamental Problem of Causal Inference {{A}} Complete Example with Estimation},
  author = {Neal, Brady},
  pages = {198},
  langid = {english},
  file = {/home/skynet3/Zotero/storage/WUVFB9U5/Neal - What are potential outcomes The fundamental probl.pdf}
}

@online{nealWhichCausalInference2019,
  title = {Which Causal Inference Book You Should Read},
  author = {Neal, Brady},
  date = {2019-11-23},
  url = {https://www.bradyneal.com/which-causal-inference-book},
  urldate = {2022-11-15},
  abstract = {A flowchart to help you choose the best causal inference book to read. Also, a few short causal inference book reviews and pointers to other good books.},
  langid = {english},
  file = {/home/skynet3/Zotero/storage/RDTQ8DSJ/which-causal-inference-book.html}
}

@online{NeetCodeIo,
  title = {{{NeetCode}}.Io},
  url = {https://neetcode.io/},
  urldate = {2022-11-11},
  file = {/home/skynet3/Zotero/storage/MAPK6KWX/neetcode.io.html}
}

@article{negiRevisitingRegressionAdjustment2021,
  title = {Revisiting Regression Adjustment in Experiments with Heterogeneous Treatment Effects},
  author = {Negi, Akanksha and Wooldridge, Jeffrey M.},
  date = {2021-05-28},
  journaltitle = {Econometric Reviews},
  shortjournal = {Econometric Reviews},
  volume = {40},
  number = {5},
  pages = {504--534},
  issn = {0747-4938, 1532-4168},
  doi = {10.1080/07474938.2020.1824732},
  url = {https://www.tandfonline.com/doi/full/10.1080/07474938.2020.1824732},
  urldate = {2022-11-11},
  langid = {english}
}

@misc{neiswangerAsymptoticallyExactEmbarrassingly2014,
  title = {Asymptotically {{Exact}}, {{Embarrassingly Parallel MCMC}}},
  author = {Neiswanger, Willie and Wang, Chong and Xing, Eric},
  date = {2014-03-21},
  number = {arXiv:1311.4780},
  eprint = {1311.4780},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1311.4780},
  url = {http://arxiv.org/abs/1311.4780},
  urldate = {2022-11-11},
  abstract = {Communication costs, resulting from synchronization requirements during learning, can greatly slow down many parallel machine learning algorithms. In this paper, we present a parallel Markov chain Monte Carlo (MCMC) algorithm in which subsets of data are processed independently, with very little communication. First, we arbitrarily partition data onto multiple machines. Then, on each machine, any classical MCMC method (e.g., Gibbs sampling) may be used to draw samples from a posterior distribution given the data subset. Finally, the samples from each machine are combined to form samples from the full posterior. This embarrassingly parallel algorithm allows each machine to act independently on a subset of the data (without communication) until the final combination stage. We prove that our algorithm generates asymptotically exact samples and empirically demonstrate its ability to parallelize burn-in and sampling in several models.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Distributed; Parallel; and Cluster Computing,Computer Science - Machine Learning,Statistics - Computation,Statistics - Machine Learning},
  file = {/home/skynet3/Zotero/storage/5GMGC2JP/Neiswanger et al. - 2014 - Asymptotically Exact, Embarrassingly Parallel MCMC.pdf;/home/skynet3/Zotero/storage/6H4JC2ZF/1311.html}
}

@online{NeulabNn4nlpconceptsRepository,
  title = {Neulab/Nn4nlp-Concepts: {{A}} Repository of Concepts Related to Neural Networks for {{NLP}}},
  shorttitle = {Neulab/Nn4nlp-Concepts},
  url = {https://github.com/neulab/nn4nlp-concepts},
  urldate = {2022-11-11},
  abstract = {A repository of concepts related to neural networks for NLP - neulab/nn4nlp-concepts: A repository of concepts related to neural networks for NLP},
  langid = {english},
  organization = {{GitHub}},
  file = {/home/skynet3/Zotero/storage/R8Y2AUGY/nn4nlp-concepts.html}
}

@article{neunhoefferHowCrossValidationCan2019,
  ids = {neunhoefferHowCrossValidationCan2019a},
  title = {How {{Cross-Validation Can Go Wrong}} and {{What}} to {{Do About It}}},
  author = {Neunhoeffer, Marcel and Sternberg, Sebastian},
  date = {2019-01},
  journaltitle = {Political Analysis},
  shortjournal = {Polit. Anal.},
  volume = {27},
  number = {1},
  pages = {101--106},
  issn = {1047-1987, 1476-4989},
  doi = {10.1017/pan.2018.39},
  url = {https://www.cambridge.org/core/product/identifier/S1047198718000396/type/journal_article},
  urldate = {2022-11-11},
  langid = {english},
  file = {/home/skynet3/Zotero/storage/GYFVSE4J/Neunhoeffer and Sternberg - 2019 - How Cross-Validation Can Go Wrong and What to Do A.pdf}
}

@article{nicholsonExaminingLinguisticShifts2022,
  title = {Examining Linguistic Shifts between Preprints and Publications},
  author = {Nicholson, David N. and Rubinetti, Vincent and Hu, Dongbo and Thielk, Marvin and Hunter, Lawrence E. and Greene, Casey S.},
  date = {2022-02-01},
  journaltitle = {PLOS Biology},
  shortjournal = {PLOS Biology},
  volume = {20},
  number = {2},
  pages = {e3001470},
  publisher = {{Public Library of Science}},
  issn = {1545-7885},
  doi = {10.1371/journal.pbio.3001470},
  url = {https://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.3001470},
  urldate = {2022-11-11},
  abstract = {Preprints allow researchers to make their findings available to the scientific community before they have undergone peer review. Studies on preprints within bioRxiv have been largely focused on article metadata and how often these preprints are downloaded, cited, published, and discussed online. A missing element that has yet to be examined is the language contained within the bioRxiv preprint repository. We sought to compare and contrast linguistic features within bioRxiv preprints to published biomedical text as a whole as this is an excellent opportunity to examine how peer review changes these documents. The most prevalent features that changed appear to be associated with typesetting and mentions of supporting information sections or additional files. In addition to text comparison, we created document embeddings derived from a preprint-trained word2vec model. We found that these embeddings are able to parse out different scientific approaches and concepts, link unannotated preprint–peer-reviewed article pairs, and identify journals that publish linguistically similar papers to a given preprint. We also used these embeddings to examine factors associated with the time elapsed between the posting of a first preprint and the appearance of a peer-reviewed publication. We found that preprints with more versions posted and more textual changes took longer to publish. Lastly, we constructed a web application (https://greenelab.github.io/preprint-similarity-search/) that allows users to identify which journals and articles that are most linguistically similar to a bioRxiv or medRxiv preprint as well as observe where the preprint would be positioned within a published article landscape.},
  langid = {english},
  keywords = {Bioinformatics,Genomics,Metadata,Neuroscience,Peer review,Principal component analysis,Scientific publishing,Scientists},
  file = {/home/skynet3/Zotero/storage/HQYI8IS8/Nicholson et al. - 2022 - Examining linguistic shifts between preprints and .pdf;/home/skynet3/Zotero/storage/YC47JPYZ/article.html}
}

@article{nikuEfficientEstimationGeneralized2019,
  title = {Efficient Estimation of Generalized Linear Latent Variable Models},
  author = {Niku, Jenni and Brooks, Wesley and Herliansyah, Riki and Hui, Francis K. C. and Taskinen, Sara and Warton, David I.},
  date = {2019-05-01},
  journaltitle = {PLOS ONE},
  shortjournal = {PLOS ONE},
  volume = {14},
  number = {5},
  pages = {e0216129},
  publisher = {{Public Library of Science}},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0216129},
  url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0216129},
  urldate = {2022-11-11},
  abstract = {Generalized linear latent variable models (GLLVM) are popular tools for modeling multivariate, correlated responses. Such data are often encountered, for instance, in ecological studies, where presence-absences, counts, or biomass of interacting species are collected from a set of sites. Until very recently, the main challenge in fitting GLLVMs has been the lack of computationally efficient estimation methods. For likelihood based estimation, several closed form approximations for the marginal likelihood of GLLVMs have been proposed, but their efficient implementations have been lacking in the literature. To fill this gap, we show in this paper how to obtain computationally convenient estimation algorithms based on a combination of either the Laplace approximation method or variational approximation method, and automatic optimization techniques implemented in R software. An extensive set of simulation studies is used to assess the performances of different methods, from which it is shown that the variational approximation method used in conjunction with automatic optimization offers a powerful tool for estimation.},
  langid = {english},
  keywords = {Algorithms,Amoebas,Approximation methods,Binomials,Birds,Optimization,Simulation and modeling,Statistical models},
  file = {/home/skynet3/Zotero/storage/E4JFC8RQ/Niku et al. - 2019 - Efficient estimation of generalized linear latent .pdf;/home/skynet3/Zotero/storage/AG9GN4WI/article.html}
}

@article{nilesWhyWePublish2020,
  title = {Why We Publish Where We Do: {{Faculty}} Publishing Values and Their Relationship to Review, Promotion and Tenure Expectations},
  shorttitle = {Why We Publish Where We Do},
  author = {Niles, Meredith T. and Schimanski, Lesley A. and McKiernan, Erin C. and Alperin, Juan Pablo},
  date = {2020-03-11},
  journaltitle = {PLOS ONE},
  shortjournal = {PLOS ONE},
  volume = {15},
  number = {3},
  pages = {e0228914},
  publisher = {{Public Library of Science}},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0228914},
  url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0228914},
  urldate = {2022-11-11},
  abstract = {Using an online survey of academics at 55 randomly selected institutions across the US and Canada, we explore priorities for publishing decisions and their perceived importance within review, promotion, and tenure (RPT). We find that respondents most value journal readership, while they believe their peers most value prestige and related metrics such as impact factor when submitting their work for publication. Respondents indicated that total number of publications, number of publications per year, and journal name recognition were the most valued factors in RPT. Older and tenured respondents (most likely to serve on RPT committees) were less likely to value journal prestige and metrics for publishing, while untenured respondents were more likely to value these factors. These results suggest disconnects between what academics value versus what they think their peers value, and between the importance of journal prestige and metrics for tenured versus untenured faculty in publishing and RPT perceptions.},
  langid = {english},
  keywords = {Bibliometrics,Careers,Citation analysis,Open access publishing,Peer review,Psychological attitudes,Scientific publishing,Surveys},
  file = {/home/skynet3/Zotero/storage/Q2CHMBCE/Niles et al. - 2020 - Why we publish where we do Faculty publishing val.pdf;/home/skynet3/Zotero/storage/LVQ5KNXR/article.html}
}

@article{NineCirclesScientific2012,
  title = {The {{Nine Circles}} of {{Scientific Hell}}},
  date = {2012-11-01},
  journaltitle = {Perspectives on Psychological Science},
  shortjournal = {Perspect Psychol Sci},
  volume = {7},
  number = {6},
  pages = {643--644},
  publisher = {{SAGE Publications Inc}},
  issn = {1745-6916},
  doi = {10.1177/1745691612459519},
  url = {https://doi.org/10.1177/1745691612459519},
  urldate = {2022-11-11},
  langid = {english},
  file = {/home/skynet3/Zotero/storage/PM83EQ2N/2012 - The Nine Circles of Scientific Hell.pdf}
}

@misc{nixonMeasuringCalibrationDeep2020,
  ids = {nixonMeasuringCalibrationDeep2020a},
  title = {Measuring {{Calibration}} in {{Deep Learning}}},
  author = {Nixon, Jeremy and Dusenberry, Mike and Jerfel, Ghassen and Nguyen, Timothy and Liu, Jeremiah and Zhang, Linchuan and Tran, Dustin},
  date = {2020-08-07},
  number = {arXiv:1904.01685},
  eprint = {1904.01685},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1904.01685},
  url = {http://arxiv.org/abs/1904.01685},
  urldate = {2022-11-11},
  abstract = {Overconfidence and underconfidence in machine learning classifiers is measured by calibration: the degree to which the probabilities predicted for each class match the accuracy of the classifier on that prediction. How one measures calibration remains a challenge: expected calibration error, the most popular metric, has numerous flaws which we outline, and there is no clear empirical understanding of how its choices affect conclusions in practice, and what recommendations there are to counteract its flaws. In this paper, we perform a comprehensive empirical study of choices in calibration measures including measuring all probabilities rather than just the maximum prediction, thresholding probability values, class conditionality, number of bins, bins that are adaptive to the datapoint density, and the norm used to compare accuracies to confidences. To analyze the sensitivity of calibration measures, we study the impact of optimizing directly for each variant with recalibration techniques. Across MNIST, Fashion MNIST, CIFAR-10/100, and ImageNet, we find that conclusions on the rank ordering of recalibration methods is drastically impacted by the choice of calibration measure. We find that conditioning on the class leads to more effective calibration evaluations, and that using the L2 norm rather than the L1 norm improves both optimization for calibration metrics and the rank correlation measuring metric consistency. Adaptive binning schemes lead to more stablity of metric rank ordering when the number of bins vary, and is also recommended. We open source a library for the use of our calibration measures.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/skynet3/Zotero/storage/HK7AZMPL/Nixon et al. - 2020 - Measuring Calibration in Deep Learning.pdf;/home/skynet3/Zotero/storage/BJC7GD57/1904.html}
}

@online{NLPHighlights,
  title = {{{NLP Highlights}}},
  url = {https://open.spotify.com/show/4tGHzmicSHIVU3ksf5iYv8},
  urldate = {2022-11-11},
  abstract = {Listen to NLP Highlights on Spotify.},
  langid = {english},
  organization = {{Spotify}},
  file = {/home/skynet3/Zotero/storage/DBI9QNHW/4tGHzmicSHIVU3ksf5iYv8.html}
}

@online{NonparametricBootstrapBayesian,
  title = {The {{Non-parametric Bootstrap}} as a {{Bayesian Model}} - {{Publishable Stuff}}},
  url = {https://www.sumsar.net/blog/2015/04/the-non-parametric-bootstrap-as-a-bayesian-model/},
  urldate = {2022-11-11}
}

@misc{northcuttPervasiveLabelErrors2021,
  title = {Pervasive {{Label Errors}} in {{Test Sets Destabilize Machine Learning Benchmarks}}},
  author = {Northcutt, Curtis G. and Athalye, Anish and Mueller, Jonas},
  date = {2021-11-07},
  number = {arXiv:2103.14749},
  eprint = {2103.14749},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2103.14749},
  url = {http://arxiv.org/abs/2103.14749},
  urldate = {2022-11-11},
  abstract = {We identify label errors in the test sets of 10 of the most commonly-used computer vision, natural language, and audio datasets, and subsequently study the potential for these label errors to affect benchmark results. Errors in test sets are numerous and widespread: we estimate an average of at least 3.3\% errors across the 10 datasets, where for example label errors comprise at least 6\% of the ImageNet validation set. Putative label errors are identified using confident learning algorithms and then human-validated via crowdsourcing (51\% of the algorithmically-flagged candidates are indeed erroneously labeled, on average across the datasets). Traditionally, machine learning practitioners choose which model to deploy based on test accuracy - our findings advise caution here, proposing that judging models over correctly labeled test sets may be more useful, especially for noisy real-world datasets. Surprisingly, we find that lower capacity models may be practically more useful than higher capacity models in real-world datasets with high proportions of erroneously labeled data. For example, on ImageNet with corrected labels: ResNet-18 outperforms ResNet-50 if the prevalence of originally mislabeled test examples increases by just 6\%. On CIFAR-10 with corrected labels: VGG-11 outperforms VGG-19 if the prevalence of originally mislabeled test examples increases by just 5\%. Test set errors across the 10 datasets can be viewed at https://labelerrors.com and all label errors can be reproduced by https://github.com/cleanlab/label-errors.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/skynet3/Zotero/storage/EA9NL3W5/Northcutt et al. - 2021 - Pervasive Label Errors in Test Sets Destabilize Ma.pdf;/home/skynet3/Zotero/storage/QV4KCBY9/2103.html}
}

@book{norvangUsingObservationalStudy2021,
  ids = {norvangUsingObservationalStudy2021a},
  title = {Using {{Observational Study Data}} as an {{External Control Group}} for a {{Clinical Trial}}: An {{Empirical Comparison}} of {{Methods}} to {{Account}} for {{Longitudinal Missing Data}}},
  shorttitle = {Using {{Observational Study Data}} as an {{External Control Group}} for a {{Clinical Trial}}},
  author = {Norvang, Vibeke and Haavardsholm, Espen and Tedeschi, Sara and Lyu, Houchen and Sexton, Joseph and Mjaavatten, Maria and Kvien, Tore and Solomon, Daniel and Yoshida, Kazuki},
  date = {2021-12-23},
  doi = {10.21203/rs.3.rs-1200218/v1},
  abstract = {Background: Observational data are increasingly being used to conduct external comparisons to clinical trials. In this study, we empirically examined whether different methodological approaches to longitudinal missing data affected study conclusions in this setting. Methods: We used data from one clinical trial and one prospective observational study, both Norwegian multicenter studies including patients with recently diagnosed rheumatoid arthritis and implementing similar treatment strategies, but with different stringency. A binary disease remission status was defined at 6, 12, and 24 months in both studies. After identifying patterns of longitudinal missing outcome data, we evaluated the following five approaches to handle missingness: analyses of patients with complete follow-up data, multiple imputation (MI), inverse probability of censoring weighting (IPCW), and two combinations of MI and IPCW. Results: We found a complex non-monotone missing data pattern in the observational study (N=328), while missing data in the trial (N=188) was monotone due to drop-out. In the observational study, only 39.0\% of patients had complete outcome data, compared to 89.9\% in the trial. All approaches to missing data indicated favorable outcomes of the treatment strategy in the trial and resulted in similar study conclusions. Variations in results across approaches were mainly due to variations in estimated outcomes for the observational data. Conclusions: Five different approaches to handle longitudinal missing data resulted in similar conclusions in our example. However, the extent and complexity of missing observational data affected estimated comparative outcomes across approaches, highlighting the need for careful consideration of methods to account for missingness when using observational data as external controls to trial data.},
  file = {/home/skynet3/Zotero/storage/J53DH5WL/Norvang et al. - 2021 - Using Observational Study Data as an External Cont.pdf;/home/skynet3/Zotero/storage/SCV3ME9V/Norvang et al. - 2021 - Using Observational Study Data as an External Cont.pdf}
}

@software{norvigPytudes2022,
  ids = {norvigPytudes2022a},
  title = {Pytudes},
  author = {Norvig, Peter},
  date = {2022-11-11T14:07:52Z},
  origdate = {2017-03-01T05:43:35Z},
  url = {https://github.com/norvig/pytudes/blob/4bcea911eea4df8163c11b760e1270a974e4b8dd/ipynb/Probability.ipynb},
  urldate = {2022-11-11},
  abstract = {Python programs, usually short, of considerable difficulty, to perfect particular skills.}
}

@online{NotebookCommunity,
  title = {| Notebook.Community},
  url = {https://notebook.community/ethen8181/machine-learning/ab_tests/quantile_regression/quantile_regression},
  urldate = {2022-11-11},
  file = {/home/skynet3/Zotero/storage/SD6DC5F5/quantile_regression.html}
}

@online{NotesChangingRmarkdown,
  title = {Notes on {{Changing}} from {{Rmarkdown}}/{{Bookdown}} to {{Quarto}} | {{Credibly Curious}}},
  url = {https://www.njtierney.com/post/2022/04/11/rmd-to-qmd/},
  urldate = {2022-11-11},
  file = {/home/skynet3/Zotero/storage/KB4Y4FS3/rmd-to-qmd.html}
}

@online{NumPy,
  title = {{{NumPy}}},
  url = {https://numpy.org/},
  urldate = {2022-11-11},
  file = {/home/skynet3/Zotero/storage/XYT4R8HI/numpy.org.html}
}

@online{NumpyMeanNumPy,
  title = {Numpy.Mean — {{NumPy}} v1.23 {{Manual}}},
  url = {https://numpy.org/doc/stable/reference/generated/numpy.mean.html},
  urldate = {2022-11-11}
}

@software{NumPyro2022,
  title = {{{NumPyro}}},
  date = {2022-11-11T00:56:34Z},
  origdate = {2019-02-13T21:13:59Z},
  url = {https://github.com/pyro-ppl/numpyro},
  urldate = {2022-11-11},
  abstract = {Probabilistic programming with NumPy powered by JAX for autograd and JIT compilation to GPU/TPU/CPU.},
  organization = {{pyro-ppl}},
  keywords = {bayesian-inference,hmc,inference-algorithms,jax,mcmc,numpy,probabilistic-programming,pyro}
}

@misc{ogburnCounterexamplesBlessingsMultiple2020,
  title = {Counterexamples to "{{The Blessings}} of {{Multiple Causes}}" by {{Wang}} and {{Blei}}},
  author = {Ogburn, Elizabeth L. and Shpitser, Ilya and Tchetgen, Eric J. Tchetgen},
  date = {2020-04-24},
  number = {arXiv:2001.06555},
  eprint = {2001.06555},
  eprinttype = {arxiv},
  primaryclass = {stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2001.06555},
  url = {http://arxiv.org/abs/2001.06555},
  urldate = {2022-11-11},
  abstract = {This note has been updated (April, 2020) to respond to "Towards Clarifying the Theory of the Deconfounder" by Yixin Wang, David M. Blei (arXiv:2003.04948). This original note, posted in January, 2020, is meant to complement our previous comment on "The Blessings of Multiple Causes" by Wang and Blei (2019). We provide a more succinct and transparent explanation of the fact that the deconfounder does not control for multi-cause confounding. The argument given in Wang and Blei (2019) makes two mistakes: (1) attempting to infer independence conditional on one variable from independence conditional on a different, unrelated variable, and (2) attempting to infer joint independence from pairwise independence. We give two simple counterexamples to the deconfounder claim.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Machine Learning,Statistics - Methodology},
  file = {/home/skynet3/Zotero/storage/2H76PZN2/Ogburn et al. - 2020 - Counterexamples to The Blessings of Multiple Caus.pdf;/home/skynet3/Zotero/storage/UA85LWTG/2001.html}
}

@misc{olandBeCarefulWhat2017,
  title = {Be {{Careful What You Backpropagate}}: {{A Case For Linear Output Activations}} \& {{Gradient Boosting}}},
  shorttitle = {Be {{Careful What You Backpropagate}}},
  author = {Oland, Anders and Bansal, Aayush and Dannenberg, Roger B. and Raj, Bhiksha},
  date = {2017-07-13},
  number = {arXiv:1707.04199},
  eprint = {1707.04199},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1707.04199},
  url = {http://arxiv.org/abs/1707.04199},
  urldate = {2022-11-11},
  abstract = {In this work, we show that saturating output activation functions, such as the softmax, impede learning on a number of standard classification tasks. Moreover, we present results showing that the utility of softmax does not stem from the normalization, as some have speculated. In fact, the normalization makes things worse. Rather, the advantage is in the exponentiation of error gradients. This exponential gradient boosting is shown to speed up convergence and improve generalization. To this end, we demonstrate faster convergence and better performance on diverse classification tasks: image classification using CIFAR-10 and ImageNet, and semantic segmentation using PASCAL VOC 2012. In the latter case, using the state-of-the-art neural network architecture, the model converged 33\% faster with our method (roughly two days of training less) than with the standard softmax activation, and with a slightly better performance to boot.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/home/skynet3/Zotero/storage/6WB98BVB/Oland et al. - 2017 - Be Careful What You Backpropagate A Case For Line.pdf;/home/skynet3/Zotero/storage/S3FTHU4D/1707.html}
}

@software{olsenGroupdata22022,
  title = {Groupdata2},
  author = {Olsen, Ludvig Renbo},
  date = {2022-04-15T23:46:32Z},
  origdate = {2016-10-30T19:39:22Z},
  url = {https://github.com/LudvigOlsen/groupdata2},
  urldate = {2022-11-11},
  abstract = {R-package: Methods for dividing data into groups. Create balanced partitions and cross-validation folds. Perform time series windowing and general grouping and splitting of data. Balance existing groups with up- and downsampling or collapse them to fewer groups.},
  keywords = {balance,cross-validation,data,data-frame,fold,group-factor,groups,participants,partition,rstats,split,staircase}
}

@online{OpenSyllabusExplorer,
  title = {Open {{Syllabus}}: {{Explorer}}},
  shorttitle = {Open {{Syllabus}}},
  url = {https://opensyllabus.org/},
  urldate = {2022-11-11},
  abstract = {Mapping the college curriculum across 7,292,573 syllabi.},
  langid = {english},
  organization = {{Open Syllabus}},
  file = {/home/skynet3/Zotero/storage/LJX6DQE9/opensyllabus.org.html}
}

@online{ORCID,
  ids = {ORCIDa},
  title = {{{ORCID}}},
  url = {https://orcid.org/signin},
  urldate = {2022-11-11},
  file = {/home/skynet3/Zotero/storage/CCC3JTPE/signin.html}
}

@online{OrdinalRegression,
  title = {Ordinal {{Regression}}},
  url = {https://betanalpha.github.io/assets/case_studies/ordinal_regression.html},
  urldate = {2022-11-11},
  file = {/home/skynet3/Zotero/storage/CYQQFFFH/ordinal_regression.html}
}

@online{oskolkovHowExactlyUMAP2021,
  title = {How {{Exactly UMAP Works}}},
  author = {Oskolkov, Nikolay},
  date = {2021-03-10T20:32:25},
  url = {https://towardsdatascience.com/how-exactly-umap-works-13e3040e1668},
  urldate = {2022-11-11},
  abstract = {And why exactly it is better than tSNE},
  langid = {english},
  organization = {{Medium}},
  file = {/home/skynet3/Zotero/storage/NAEFNZFQ/how-exactly-umap-works-13e3040e1668.html}
}

@inreference{Outlier2022,
  ids = {Outlier2022a},
  title = {Outlier},
  booktitle = {Wikipedia},
  date = {2022-07-17T03:32:08Z},
  url = {https://en.wikipedia.org/w/index.php?title=Outlier&oldid=1098704735},
  urldate = {2022-11-11},
  abstract = {In statistics, an outlier is a data point that differs significantly from other observations. An outlier may be due to variability in the measurement or it may indicate experimental error; the latter are sometimes excluded from the data set.  An outlier can cause serious problems in statistical analyses. Outliers can occur by chance in any distribution, but they often indicate either measurement error or that the population has a heavy-tailed distribution. In the former case one wishes to discard them or use statistics that are robust to outliers, while in the latter case they indicate that the distribution has high skewness and that one should be very cautious in using tools or intuitions that assume a normal distribution. A frequent cause of outliers is a mixture of two distributions, which may be two distinct sub-populations, or may indicate 'correct trial' versus 'measurement error'; this is modeled by a mixture model. In most larger samplings of data, some data points will be further away from the sample mean than what is deemed reasonable. This can be due to incidental systematic error or flaws in the theory that generated an assumed family of probability distributions, or it may be that some observations are far from the center of the data. Outlier points can therefore indicate faulty data, erroneous procedures, or areas where a certain theory might not be valid. However, in large samples, a small number of outliers is to be expected (and not due to any anomalous condition). Outliers, being the most extreme observations, may include the sample maximum or sample minimum, or both, depending on whether they are extremely high or low. However, the sample maximum and minimum are not always outliers because they may not be unusually far from other observations. Naive interpretation of statistics derived from data sets that include outliers may be misleading. For example, if one is calculating the average temperature of 10 objects in a room, and nine of them are between 20 and 25 degrees Celsius, but an oven is at 175 °C, the median of the data will be between 20 and 25 °C but the mean temperature will be between 35.5 and 40 °C. In this case, the median better reflects the temperature of a randomly sampled object (but not the temperature in the room) than the mean; naively interpreting the mean as "a typical sample", equivalent to the median, is incorrect. As illustrated in this case, outliers may indicate data points that belong to a different population than the rest of the sample set. Estimators capable of coping with outliers are said to be robust: the median is a robust statistic of central tendency, while the mean is not. However, the mean is generally a more precise estimator.},
  langid = {english},
  annotation = {Page Version ID: 1098704735},
  file = {/home/skynet3/Zotero/storage/5T5UHCJL/Outlier.html}
}

@online{Overview,
  title = {Overview},
  url = {https://stanfordnlp.github.io/stanza/},
  urldate = {2022-11-11},
  abstract = {A Python NLP Library for Many Human Languages},
  langid = {american},
  organization = {{Stanza}},
  file = {/home/skynet3/Zotero/storage/ZB39AHXG/stanza.html}
}

@video{OverviewGoogleWork,
  ids = {OverviewGoogleWorka},
  title = {An {{Overview}} of {{Google}}'s {{Work}} on {{AutoML}} and {{Future Directions}}},
  url = {https://slideslive.com/38917526/an-overview-of-googles-work-on-automl-and-future-directions?ref=og-meta-tags},
  urldate = {2022-11-11},
  abstract = {Machine learning has achieved considerable successes in recent years, but this success often relies on human experts, who construct appropriate features, design learning architectures, set their...},
  langid = {american},
  file = {/home/skynet3/Zotero/storage/C4VZRPSZ/an-overview-of-googles-work-on-automl-and-future-directions.html}
}

@online{OverviewGradientDescent2016,
  title = {An Overview of Gradient Descent Optimization Algorithms},
  date = {2016-01-19T14:20:00},
  url = {https://ruder.io/optimizing-gradient-descent/},
  urldate = {2022-11-11},
  abstract = {Gradient descent is the preferred way to optimize neural networks and many other machine learning algorithms but is often used as a black box. This post explores how many of the most popular gradient-based optimization algorithms such as Momentum, Adagrad, and Adam actually work.},
  langid = {english},
  organization = {{Sebastian Ruder}}
}

@article{owenConditionalNaturePublication2021,
  title = {The Conditional Nature of Publication Bias: A Meta-Regression Analysis},
  shorttitle = {The Conditional Nature of Publication Bias},
  author = {Owen, Erica and Li, Quan},
  date = {2021-10},
  journaltitle = {Political Science Research and Methods},
  volume = {9},
  number = {4},
  pages = {867--877},
  publisher = {{Cambridge University Press}},
  issn = {2049-8470, 2049-8489},
  doi = {10.1017/psrm.2020.15},
  url = {https://www.cambridge.org/core/journals/political-science-research-and-methods/article/abs/conditional-nature-of-publication-bias-a-metaregression-analysis/40C0A166F3ED1516A051C5ED270D1650},
  urldate = {2022-11-11},
  abstract = {Publication bias is pervasive in social and behavioral sciences because journals and scholars tend to reward and be rewarded for statistically significant findings. However, the determinants of the severity of publication bias are less well understood. We argue that publication bias depends on whether an independent variable is a key variable or statistical control in traditional regression modeling. The bias should be severe only for the key variable that relates to a central question and hypothesis in a study. We offer an empirical strategy to detect the conditional nature of publication bias. As an illustration, we perform a meta-regression of 229 model estimates from 36 articles in the democracy-foreign direct investment literature. We find that publication bias is most severe when democracy is a key variable, but appears weak when democracy is a control. Our research demonstrates that empirical estimates for key and control variables follow different data generation processes and makes a novel contribution to the study of publication bias that affects many research areas.},
  langid = {english},
  keywords = {Quantitative methods},
  file = {/home/skynet3/Zotero/storage/GGWPTQZX/40C0A166F3ED1516A051C5ED270D1650.html}
}

@online{Packages2e,
  title = {R {{Packages}} (2e)},
  url = {https://r-pkgs.org/},
  urldate = {2022-11-11},
  file = {/home/skynet3/Zotero/storage/XRUFNESJ/r-pkgs.org.html}
}

@online{PageNotFound,
  title = {Page Not Found | {{IJCAI}}},
  url = {https://www.ijcai.org/proceedings-2017/0289.pdf},
  urldate = {2022-11-11}
}

@online{PageNotFounda,
  ids = {PageNotFoundb},
  title = {Page Not Found · {{GitHub Pages}}},
  url = {https://fulmicoton.com/posts/bayesian_rating/);},
  urldate = {2022-11-11},
  file = {/home/skynet3/Zotero/storage/FEWAKP36/)\;.html}
}

@software{PalBands,
  title = {Pal.Bands()},
  url = {https://cran.r-project.org/web/packages/pals/vignettes/pals_examples.html},
  urldate = {2022-11-11},
  abstract = {The purpose of the pals package is twofold:}
}

@online{PandasDataFramePandas,
  title = {Pandas.{{DataFrame}} — Pandas 1.5.1 Documentation},
  url = {https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html},
  urldate = {2022-11-11},
  file = {/home/skynet3/Zotero/storage/QZ29ZR6Y/pandas.DataFrame.html}
}

@online{PandasDocumentationPandas,
  title = {Pandas Documentation — Pandas 1.5.1 Documentation},
  url = {https://pandas.pydata.org/docs/},
  urldate = {2022-11-11}
}

@online{PaperLists,
  title = {Paper {{Lists}}},
  url = {https://papers.labml.ai/lists},
  urldate = {2022-11-11},
  file = {/home/skynet3/Zotero/storage/8FS6XGIX/lists.html}
}

@online{PapersCodeOverview,
  title = {Papers with {{Code}} - {{An Overview}} of {{Activation Functions}}},
  url = {https://paperswithcode.com/methods/category/activation-functions},
  urldate = {2022-11-11},
  abstract = {Activation functions are functions that we apply in neural networks after (typically) applying an affine transformation combining weights and input features. They are typically non-linear functions. The rectified linear unit, or ReLU, has been the most popular in the past decade, although the choice is architecture dependent and many alternatives have emerged in recent years. In this section, you will find a constantly updating list of activation functions.},
  langid = {english},
  file = {/home/skynet3/Zotero/storage/HBFGYSMG/activation-functions.html}
}

@online{ParallelizingNeuralNetworks,
  title = {Parallelizing Neural Networks on One {{GPU}} with {{JAX}} | {{Will Whitney}}},
  url = {http://willwhitney.com/parallel-training-jax.html},
  urldate = {2022-11-11},
  file = {/home/skynet3/Zotero/storage/46HH6BZR/parallel-training-jax.html}
}

@article{parvandehConsensusFeaturesNested2020,
  ids = {parvandehConsensusFeaturesNested2020a},
  title = {Consensus Features Nested Cross-Validation},
  author = {Parvandeh, Saeid and Yeh, Hung-Wen and Paulus, Martin P and McKinney, Brett A},
  date = {2020-01-27},
  journaltitle = {Bioinformatics},
  shortjournal = {Bioinformatics},
  volume = {36},
  number = {10},
  eprint = {31985777},
  eprinttype = {pmid},
  pages = {3093--3098},
  issn = {1367-4803},
  doi = {10.1093/bioinformatics/btaa046},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7776094/},
  urldate = {2022-11-11},
  abstract = {Summary Feature selection can improve the accuracy of machine-learning models, but appropriate steps must be taken to avoid overfitting. Nested cross-validation (nCV) is a common approach that chooses the classification model and features to represent a given outer fold based on features that give the maximum inner-fold accuracy. Differential privacy is a related technique to avoid overfitting that uses a privacy-preserving noise mechanism to identify features that are stable between training and holdout sets., We develop consensus nested cross-validation (cnCV) that combines the idea of feature stability from differential privacy with nCV. Feature selection is applied in each inner fold and the consensus of top features across folds is used as a measure of feature stability or reliability instead of classification accuracy, which is used in standard nCV. We use simulated data with main effects, correlation and interactions to compare the classification accuracy and feature selection performance of the new cnCV with standard nCV, Elastic Net optimized by cross-validation, differential privacy and private evaporative cooling (pEC). We also compare these methods using real RNA-seq data from a study of major depressive disorder., The cnCV method has similar training and validation accuracy to nCV, but cnCV has much shorter run times because it does not construct classifiers in the inner folds. The cnCV method chooses a more parsimonious set of features with fewer false positives than nCV. The cnCV method has similar accuracy to pEC and cnCV selects stable features between folds without the need to specify a privacy threshold. We show that cnCV is an effective and efficient approach for combining feature selection with classification. Availability and implementation Code available at https://github.com/insilico/cncv. Supplementary information  are available at Bioinformatics online.},
  pmcid = {PMC7776094},
  file = {/home/skynet3/Zotero/storage/RJ4NQA5R/Parvandeh et al. - 2020 - Consensus features nested cross-validation.pdf;/home/skynet3/Zotero/storage/9HFLELNX/5716331.html}
}

@article{patienceIntellectualContributionsMeriting2019,
  title = {Intellectual Contributions Meriting Authorship: {{Survey}} Results from the Top Cited Authors across All Science Categories},
  shorttitle = {Intellectual Contributions Meriting Authorship},
  author = {Patience, Gregory S. and Galli, Federico and Patience, Paul A. and Boffito, Daria C.},
  date = {2019-01-16},
  journaltitle = {PLOS ONE},
  shortjournal = {PLOS ONE},
  volume = {14},
  number = {1},
  pages = {e0198117},
  publisher = {{Public Library of Science}},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0198117},
  url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0198117},
  urldate = {2022-11-11},
  abstract = {Authorship is the currency of an academic career for which the number of papers researchers publish demonstrates creativity, productivity, and impact. To discourage coercive authorship practices and inflated publication records, journals require authors to affirm and detail their intellectual contributions but this strategy has been unsuccessful as authorship lists continue to grow. Here, we surveyed close to 6000 of the top cited authors in all science categories with a list of 25 research activities that we adapted from the National Institutes of Health (NIH) authorship guidelines. Responses varied widely from individuals in the same discipline, same level of experience, and same geographic region. Most researchers agreed with the NIH criteria and grant authorship to individuals who draft the manuscript, analyze and interpret data, and propose ideas. However, thousands of the researchers also value supervision and contributing comments to the manuscript, whereas the NIH recommends discounting these activities when attributing authorship. People value the minutiae of research beyond writing and data reduction: researchers in the humanities value it less than those in pure and applied sciences; individuals from Far East Asia and Middle East and Northern Africa value these activities more than anglophones and northern Europeans. While developing national and international collaborations, researchers must recognize differences in peoples values while assigning authorship.},
  langid = {english},
  keywords = {Africa,Asia,Computer applications,Europe,Experimental design,Iran,Medical journals,Surveys},
  file = {/home/skynet3/Zotero/storage/SIPB6MGR/Patience et al. - 2019 - Intellectual contributions meriting authorship Su.pdf;/home/skynet3/Zotero/storage/23V4PA9P/article.html}
}

@misc{pawleySpatialAutocorrelationBane2018,
  ids = {pawleySpatialAutocorrelationBane2018a},
  title = {Spatial Autocorrelation: Bane or Bonus?},
  shorttitle = {Spatial Autocorrelation},
  author = {Pawley, Matt D. M. and McArdle, Brian H.},
  date = {2018-08-06},
  pages = {385526},
  publisher = {{bioRxiv}},
  doi = {10.1101/385526},
  url = {https://www.biorxiv.org/content/10.1101/385526v1},
  urldate = {2022-11-11},
  abstract = {Spatial autocorrelation is a general phenomenon within biogeographical studies. However, considerable confusion exists about how to analyse spatially autocorrelated data collected using classical sampling methods (e.g. simple random sampling). We show that the two common discordant views about autocorrelated data depend upon the desired scale of inference. Although inferential statistics seeking to generalise to different unsampled spatial areas need to be adjusted for autocorrelation, if the inference is restricted to the area from which samples have been taken, then standard tests are applicable. In the latter case, incorporating autocorrelation into the model may actually improve the precision and power of the analysis. We found that the scale of spatial inference is rarely discussed, despite being of central importance to any spatial analysis. We suggest that spatial inference should rarely be formally generalised to unsampled areas, since it typically requires some assumption of stationarity, and is thus vulnerable to accusations of “pseudo-replication”.},
  langid = {english},
  file = {/home/skynet3/Zotero/storage/VWM8DXU4/Pawley and McArdle - 2018 - Spatial autocorrelation bane or bonus.pdf;/home/skynet3/Zotero/storage/CR6XPJL2/385526v1.html}
}

@misc{pazShouldWeTrust2019,
  type = {Working Paper},
  title = {Should {{We Trust Clustered Standard Errors}}? {{A Comparison}} with {{Randomization-Based Methods}}},
  shorttitle = {Should {{We Trust Clustered Standard Errors}}?},
  author = {Paz, Lourenço S. and West, James E.},
  date = {2019-06},
  series = {Working {{Paper Series}}},
  number = {25926},
  publisher = {{National Bureau of Economic Research}},
  doi = {10.3386/w25926},
  url = {https://www.nber.org/papers/w25926},
  urldate = {2022-11-11},
  abstract = {We compare the precision of critical values obtained under conventional sampling-based methods with those obtained using sample order statics computed through draws from a randomized counterfactual based on the null hypothesis. When based on a small number of draws (200), critical values in the extreme left and right tail (0.005 and 0.995) contain a small bias toward failing to reject the null hypothesis which quickly dissipates with additional draws. The precision of randomization-based critical values compares favorably with conventional sampling-based critical values when the number of draws is approximately 7 times the sample size for a basic OLS model using homoskedastic data, but considerably less in models based on clustered standard errors, or the classic Differences-in-Differences. Randomization-based methods dramatically outperform conventional methods for treatment effects in Differences-in-Differences specifications with unbalanced panels and a small number of treated groups.},
  file = {/home/skynet3/Zotero/storage/E8DRYRXQ/Paz and West - 2019 - Should We Trust Clustered Standard Errors A Compa.pdf}
}

@online{PCATidyVerse,
  title = {{{PCA}} in a Tidy(Verse) Framework - {{goonR}} Blog},
  url = {https://tbradley1013.github.io/2018/02/01/pca-in-a-tidy-verse-framework/?utm_content=bufferfaf31&utm_medium=social&utm_source=twitter.com&utm_campaign=buffer},
  urldate = {2022-11-11},
  abstract = {Principal Component Analysis in the tidyverse},
  langid = {english},
  file = {/home/skynet3/Zotero/storage/NYBFE6QW/pca-in-a-tidy-verse-framework.html}
}

@online{Pcurve,
  title = {P-Curve},
  url = {http://www.p-curve.com/},
  urldate = {2022-11-11},
  file = {/home/skynet3/Zotero/storage/ZQQI6FC4/www.p-curve.com.html}
}

@book{pearlBookWhyNew2018,
  title = {The {{Book}} of {{Why}}: {{The New Science}} of {{Cause}} and {{Effect}}},
  shorttitle = {The {{Book}} of {{Why}}},
  author = {Pearl, Judea and Mackenzie, Dana},
  date = {2018-05-15},
  eprint = {EmY8DwAAQBAJ},
  eprinttype = {googlebooks},
  publisher = {{Penguin Books Limited}},
  abstract = {The hugely influential book on how the understanding of causality revolutionized science and the world, by the pioneer of artificial intelligence'Wonderful ... illuminating and fun to read' Daniel Kahneman, Nobel Prize-winner and author of Thinking, Fast and Slow'Correlation does not imply causation.' For decades, this mantra was invoked by scientists in order to avoid taking positions as to whether one thing caused another, such as smoking and cancer, or carbon dioxide and global warming. But today, that taboo is dead. The causal revolution, sparked by world-renowned computer scientist Judea Pearl and his colleagues, has cut through a century of confusion and placed cause and effect on a firm scientific basis. Now, Pearl and science journalist Dana Mackenzie explain causal thinking to general readers for the first time, showing how it allows us to explore the world that is and the worlds that could have been. It is the essence of human and artificial intelligence. And just as Pearl's discoveries have enabled machines to think better, The Book of Why explains how we too can think better.'Pearl's accomplishments over the last 30 years have provided the theoretical basis for progress in artificial intelligence and have redefined the term "thinking machine"' Vint Cerf},
  isbn = {978-0-241-24264-3},
  langid = {english},
  pagetotal = {453},
  keywords = {Computers / Artificial Intelligence / General,Mathematics / Probability & Statistics / General,Philosophy / Mind & Body,Science / Cognitive Science,Science / General}
}

@article{pearlDoesObesityShorten2018,
  ids = {pearlDoesObesityShorten2018a},
  title = {Does {{Obesity Shorten Life}}? {{Or}} Is It the {{Soda}}? {{On Non-manipulable Causes}}},
  shorttitle = {Does {{Obesity Shorten Life}}?},
  author = {Pearl, Judea},
  date = {2018-09-25},
  journaltitle = {Journal of Causal Inference},
  volume = {6},
  number = {2},
  pages = {20182001},
  issn = {2193-3685, 2193-3677},
  doi = {10.1515/jci-2018-2001},
  url = {https://www.degruyter.com/document/doi/10.1515/jci-2018-2001/html},
  urldate = {2022-11-11},
  abstract = {Non-manipulable factors, such as gender or race have posed conceptual and practical challenges to causal analysts. On the one hand these factors do have consequences, and on the other hand, they do not fit into the experimentalist conception of causation. This paper addresses this challenge in the context of public debates over the health cost of obesity, and offers a new perspective, based on the theory of Structural Causal Models (SCM).},
  langid = {english},
  file = {/home/skynet3/Zotero/storage/3MZY7A3E/Pearl - 2018 - Does Obesity Shorten Life Or is it the Soda On N.pdf}
}

@article{pearlInterpretation2019,
  title = {On the {{Interpretation}} of Do(x)},
  author = {Pearl, Judea},
  date = {2019-03-01},
  journaltitle = {Journal of Causal Inference},
  volume = {7},
  number = {1},
  publisher = {{De Gruyter}},
  issn = {2193-3685},
  doi = {10.1515/jci-2019-2002},
  url = {https://www.degruyter.com/document/doi/10.1515/jci-2019-2002/html},
  urldate = {2022-11-15},
  abstract = {This paper provides empirical interpretation of the do(x)do(x) operator when applied to non-manipulable variables such as race, obesity, or cholesterol level. We view do(x)do(x) as an ideal intervention that provides valuable information on the effects of manipulable variables and is thus empirically testable. We draw parallels between this interpretation and ways of enabling machines to learn effects of untried actions from those tried. We end with the conclusion that researchers need not distinguish manipulable from non-manipulable variables; both types are equally eligible to receive the do(x)do(x) operator and to produce useful information for decision makers.},
  langid = {english},
  keywords = {causal effects,interventions,Manipulability,reinforcement learning,testability},
  file = {/home/skynet3/Zotero/storage/74RGZLB7/Pearl - 2019 - On the Interpretation of do(x).pdf}
}

@article{pearlTRYGVEHAAVELMOEMERGENCE2015,
  title = {{{TRYGVE HAAVELMO AND THE EMERGENCE OF CAUSAL CALCULUS}}},
  author = {Pearl, Judea},
  date = {2015-02},
  journaltitle = {Econometric Theory},
  shortjournal = {Econom. Theory},
  volume = {31},
  number = {1},
  pages = {152--179},
  issn = {0266-4666, 1469-4360},
  doi = {10.1017/S0266466614000231},
  url = {https://www.cambridge.org/core/product/identifier/S0266466614000231/type/journal_article},
  urldate = {2022-11-15},
  abstract = {Haavelmo was the first to recognize the capacity of economic models to guide policies. This paper describes some of the barriers that Haavelmo’s ideas have had (and still have) to overcome, and lays out a logical framework that has evolved from Haavelmo’s insight and matured into a coherent and comprehensive account of the relationships between theory, data and policy questions. The mathematical tools that emerge from this framework now enable investigators to answer complex policy and counterfactual questions using simple routines, some by mere inspection of the model’s structure. Several such problems are illustrated by examples, including misspecification tests, nonparametric identification, mediation analysis, and introspection. Finally, we observe that economists are largely unaware of the benefits that Haavelmo’s ideas bestow upon them and, to close this gap, we identify concrete recent advances in causal analysis that economists can utilize in research and education.},
  langid = {english},
  file = {/home/skynet3/Zotero/storage/DWLMFNLQ/Pearl - 2015 - TRYGVE HAAVELMO AND THE EMERGENCE OF CAUSAL CALCUL.pdf}
}

@online{Pedemonte96CausaleffectPython,
  title = {Pedemonte96/Causaleffect: {{Python}} Package to Compute Conditional and Non-Conditional Causal Effects.},
  shorttitle = {Pedemonte96/Causaleffect},
  url = {https://github.com/pedemonte96/causaleffect},
  urldate = {2022-11-11},
  abstract = {Python package to compute conditional and non-conditional causal effects. - pedemonte96/causaleffect: Python package to compute conditional and non-conditional causal effects.},
  langid = {english},
  organization = {{GitHub}},
  file = {/home/skynet3/Zotero/storage/SUFKVYCY/causaleffect.html}
}

@misc{peguerolesStructureLearningTime2018,
  title = {Structure {{Learning}} from {{Time Series}} with {{False Discovery Control}}},
  author = {Pegueroles, Bernat Guillen and Vinzamuri, Bhanukiran and Shanmugam, Karthikeyan and Hedden, Steve and Moyer, Jonathan D. and Varshney, Kush R.},
  date = {2018-05-24},
  number = {arXiv:1805.09909},
  eprint = {1805.09909},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1805.09909},
  url = {http://arxiv.org/abs/1805.09909},
  urldate = {2022-11-11},
  abstract = {We consider the Granger causal structure learning problem from time series data. Granger causal algorithms predict a 'Granger causal effect' between two variables by testing if prediction error of one decreases significantly in the absence of the other variable among the predictor covariates. Almost all existing Granger causal algorithms condition on a large number of variables (all but two variables) to test for effects between a pair of variables. We propose a new structure learning algorithm called MMPC-p inspired by the well known MMHC algorithm for non-time series data. We show that under some assumptions, the algorithm provides false discovery rate control. The algorithm is sound and complete when given access to perfect directed information testing oracles. We also outline a novel tester for the linear Gaussian case. We show through our extensive experiments that the MMPC-p algorithm scales to larger problems and has improved statistical power compared to existing state of the art for large sparse graphs. We also apply our algorithm on a global development dataset and validate our findings with subject matter experts.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/skynet3/Zotero/storage/994PD54I/Pegueroles et al. - 2018 - Structure Learning from Time Series with False Dis.pdf;/home/skynet3/Zotero/storage/4H5H68CQ/1805.html}
}

@software{phamMinimumViableStudy2022,
  title = {Minimum {{Viable Study Plan}} for {{Machine Learning Interviews}}},
  author = {Pham, Khang},
  date = {2022-11-11T17:54:18Z},
  origdate = {2020-08-11T06:26:32Z},
  url = {https://github.com/khangich/machine-learning-interview},
  urldate = {2022-11-11},
  abstract = {Machine Learning Interviews from FAANG, Snapchat, LinkedIn. I have offers from Snapchat, Coupang, Stitchfix etc. Blog: mlengineer.io.},
  keywords = {deep-learning,interivew,interview-preparation,interview-questions,leetcode,machine-learning,mvp,system-design}
}

@article{phamProblemsOpportunitiesTraining2020,
  title = {Problems and {{Opportunities}} in {{Training Deep Learning Software Systems}}: {{An Analysis}} of {{Variance}}},
  author = {Pham, Hung Viet and Qian, Shangshu and Wang, Jiannan and Lutellier, Thibaud and Rosenthal, Jonathan and Tan, Lin and Yu, Yaoliang and Nagappan, Nachiappan},
  date = {2020},
  pages = {13},
  abstract = {Deep learning (DL) training algorithms utilize nondeterminism to improve models’ accuracy and training efficiency. Hence, multiple identical training runs (e.g., identical training data, algorithm, and network) produce different models with different accuracies and training times. In addition to these algorithmic factors, DL libraries (e.g., TensorFlow and cuDNN) introduce additional variance (referred to as implementation-level variance) due to parallelism, optimization, and floating-point computation.},
  langid = {english},
  file = {/home/skynet3/Zotero/storage/A65JBSIY/Pham et al. - 2020 - Problems and Opportunities in Training Deep Learni.pdf}
}

@online{phdConcernsBotsMechanical2018,
  title = {Concerns {{About Bots}} on {{Mechanical Turk}}: {{Problems}} and {{Solutions}}},
  shorttitle = {Concerns {{About Bots}} on {{Mechanical Turk}}},
  author = {PhD, Aaron Moss},
  date = {2018-08-10T22:46:00+00:00},
  url = {https://www.cloudresearch.com/resources/blog/concerns-about-bots-on-mechanical-turk-problems-and-solutions/},
  urldate = {2022-11-11},
  abstract = {When researchers collect data online, it’s natural to be concerned about data quality. Participants aren’t in the lab, so researchers can’t see who is taking their survey, what those participants are doing while answering questions, or whether participants are who they say they are. Not knowing is unsettling.},
  langid = {american},
  organization = {{CloudResearch}}
}

@online{phdHowMeasureStatistical2019,
  title = {How to {{Measure Statistical Causality}}: {{A Transfer Entropy Approach}} with {{Applications}} to {{Finance}}},
  shorttitle = {How to {{Measure Statistical Causality}}},
  author = {PhD, Thársis Souza},
  date = {2019-08-25T02:21:12},
  url = {https://towardsdatascience.com/causality-931372313a1c},
  urldate = {2022-11-11},
  abstract = {With Open Source code and applications to get you started.},
  langid = {english},
  organization = {{Medium}},
  file = {/home/skynet3/Zotero/storage/IGVM9BNS/causality-931372313a1c.html}
}

@online{phdNoYouCan2020,
  type = {Substack newsletter},
  title = {No, You Can't Explain What a p-Value Is with One Sentence ({{Parts I}}, {{II}})},
  author = {PhD, Darren Dahly},
  date = {2020-09-06},
  url = {https://statsepi.substack.com/p/no-you-cant-explain-what-a-p-value},
  urldate = {2022-11-11},
  abstract = {But I'm going to try and do it in as few as possible. \#fml},
  organization = {{My 2 cents}},
  file = {/home/skynet3/Zotero/storage/J64XUK6A/no-you-cant-explain-what-a-p-value.html}
}

@misc{philippExplodingGradientProblem2018,
  title = {The Exploding Gradient Problem Demystified - Definition, Prevalence, Impact, Origin, Tradeoffs, and Solutions},
  author = {Philipp, George and Song, Dawn and Carbonell, Jaime G.},
  date = {2018-04-06},
  number = {arXiv:1712.05577},
  eprint = {1712.05577},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1712.05577},
  url = {http://arxiv.org/abs/1712.05577},
  urldate = {2022-11-11},
  abstract = {Whereas it is believed that techniques such as Adam, batch normalization and, more recently, SeLU nonlinearities "solve" the exploding gradient problem, we show that this is not the case in general and that in a range of popular MLP architectures, exploding gradients exist and that they limit the depth to which networks can be effectively trained, both in theory and in practice. We explain why exploding gradients occur and highlight the *collapsing domain problem*, which can arise in architectures that avoid exploding gradients. ResNets have significantly lower gradients and thus can circumvent the exploding gradient problem, enabling the effective training of much deeper networks. We show this is a direct consequence of the Pythagorean equation. By noticing that *any neural network is a residual network*, we devise the *residual trick*, which reveals that introducing skip connections simplifies the network mathematically, and that this simplicity may be the major cause for their success.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/home/skynet3/Zotero/storage/XSQ83FE3/Philipp et al. - 2018 - The exploding gradient problem demystified - defin.pdf;/home/skynet3/Zotero/storage/L7WFC46V/1712.html}
}

@misc{phuongFormalAlgorithmsTransformers2022,
  ids = {phuongFormalAlgorithmsTransformers2022a},
  title = {Formal {{Algorithms}} for {{Transformers}}},
  author = {Phuong, Mary and Hutter, Marcus},
  date = {2022-07-19},
  number = {arXiv:2207.09238},
  eprint = {2207.09238},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2207.09238},
  url = {http://arxiv.org/abs/2207.09238},
  urldate = {2022-11-11},
  abstract = {This document aims to be a self-contained, mathematically precise overview of transformer architectures and algorithms (*not* results). It covers what transformers are, how they are trained, what they are used for, their key architectural components, and a preview of the most prominent models. The reader is assumed to be familiar with basic ML terminology and simpler neural network architectures such as MLPs.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/home/skynet3/Zotero/storage/Z47FIYHB/Phuong and Hutter - 2022 - Formal Algorithms for Transformers.pdf;/home/skynet3/Zotero/storage/K5YY3FMC/2207.html}
}

@article{picardTorchManualSeed,
  ids = {picardTorchManualSeeda},
  title = {Torch.Manual Seed(3407) Is All You Need: {{On}} the Influence of Random Seeds in Deep Learning Architecture for Computer Vision},
  author = {Picard, David},
  pages = {9},
  langid = {english},
  file = {/home/skynet3/Zotero/storage/Q9X4T5W7/Picard - torch.manual seed(3407) is all you need On the in.pdf}
}

@software{pinderPackageSupport2022,
  title = {Package Support},
  author = {Pinder, Thomas},
  date = {2022-11-09T21:20:33Z},
  origdate = {2020-09-27T20:40:59Z},
  url = {https://github.com/thomaspinder/GPJax},
  urldate = {2022-11-11},
  abstract = {A didactic Gaussian process package for researchers in Jax.},
  keywords = {bayesian-inference,gaussian-processes,jax,machine-learning,probabilistic-programming}
}

@misc{piponiJointDistributionsTensorFlow2020,
  ids = {piponiJointDistributionsTensorFlow2020a},
  title = {Joint {{Distributions}} for {{TensorFlow Probability}}},
  author = {Piponi, Dan and Moore, Dave and Dillon, Joshua V.},
  date = {2020-01-21},
  number = {arXiv:2001.11819},
  eprint = {2001.11819},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2001.11819},
  urldate = {2022-11-11},
  abstract = {A central tenet of probabilistic programming is that a model is specified exactly once in a canonical representation which is usable by inference algorithms. We describe JointDistributions, a family of declarative representations of directed graphical models in TensorFlow Probability.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Programming Languages,Statistics - Computation,Statistics - Machine Learning},
  file = {/home/skynet3/Zotero/storage/99J75992/Piponi et al. - 2020 - Joint Distributions for TensorFlow Probability.pdf;/home/skynet3/Zotero/storage/IKQ5IDWA/2001.html}
}

@book{PlainPersonGuide,
  title = {The {{Plain Person}}’s {{Guide}} to {{Plain Text Social Science}}},
  url = {https://plain-text.co/},
  urldate = {2022-11-11},
  abstract = {The Plain Person’s Guide to Plain Text Social Science},
  file = {/home/skynet3/Zotero/storage/ZQF7HG3N/plain-text.co.html}
}

@misc{pleissIdentifyingMislabeledData2020,
  title = {Identifying {{Mislabeled Data}} Using the {{Area Under}} the {{Margin Ranking}}},
  author = {Pleiss, Geoff and Zhang, Tianyi and Elenberg, Ethan R. and Weinberger, Kilian Q.},
  date = {2020-12-23},
  number = {arXiv:2001.10528},
  eprint = {2001.10528},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2001.10528},
  url = {http://arxiv.org/abs/2001.10528},
  urldate = {2022-11-11},
  abstract = {Not all data in a typical training set help with generalization; some samples can be overly ambiguous or outrightly mislabeled. This paper introduces a new method to identify such samples and mitigate their impact when training neural networks. At the heart of our algorithm is the Area Under the Margin (AUM) statistic, which exploits differences in the training dynamics of clean and mislabeled samples. A simple procedure - adding an extra class populated with purposefully mislabeled threshold samples - learns a AUM upper bound that isolates mislabeled data. This approach consistently improves upon prior work on synthetic and real-world datasets. On the WebVision50 classification task our method removes 17\% of training data, yielding a 1.6\% (absolute) improvement in test error. On CIFAR100 removing 13\% of the data leads to a 1.2\% drop in error.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/skynet3/Zotero/storage/76SY53T6/Pleiss et al. - 2020 - Identifying Mislabeled Data using the Area Under t.pdf;/home/skynet3/Zotero/storage/D3PQHIMX/2001.html}
}

@online{plieningerRegressionModelingProportion,
  title = {Regression {{Modeling With Proportion Data}} ({{Part}} 1)},
  author = {Plieninger, Hansjörg},
  url = {https://hansjoerg.me/2019/05/10/regression-modeling-with-proportion-data-part-1/},
  urldate = {2022-11-11},
  abstract = {Tutorial on modeling proportions/ratios in R using data from the German Handball-Bundesliga. Includes tidyverse, plots, residuals, model comparisons, holdout.},
  langid = {english},
  organization = {{It was simple}},
  file = {/home/skynet3/Zotero/storage/8LSE8KKQ/regression-modeling-with-proportion-data-part-1.html}
}

@article{plischkeComputingShapleyEffects2021,
  ids = {plischkeComputingShapleyEffects2021a},
  title = {Computing {{Shapley Effects}} for {{Sensitivity Analysis}}},
  author = {Plischke, Elmar and Rabitti, Giovanni and Borgonovo, Emanuele},
  date = {2021-01},
  journaltitle = {SIAM/ASA Journal on Uncertainty Quantification},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  volume = {9},
  number = {4},
  eprint = {2002.12024},
  eprinttype = {arxiv},
  primaryclass = {stat},
  pages = {1411--1437},
  issn = {2166-2525},
  doi = {10.1137/19M1304738},
  url = {http://arxiv.org/abs/2002.12024},
  urldate = {2022-11-11},
  abstract = {Shapley effects are attracting increasing attention as sensitivity measures. When the value function is the conditional variance, they account for the individual and higher order effects of a model input. They are also well defined under model input dependence. However, one of the issues associated with their use is computational cost. We present a new algorithm that offers major improvements for the computation of Shapley effects, reducing computational burden by several orders of magnitude (from \$k!\textbackslash cdot k\$ to \$2\^k\$, where \$k\$ is the number of inputs) with respect to currently available implementations. The algorithm works in the presence of input dependencies. The algorithm also makes it possible to estimate all generalized (Shapley-Owen) effects for interactions.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Computation,Statistics - Methodology},
  file = {/home/skynet3/Zotero/storage/C2KBHWMS/Plischke et al. - 2021 - Computing Shapley Effects for Sensitivity Analysis.pdf;/home/skynet3/Zotero/storage/3TWDFHMK/2002.html}
}

@online{PlottingBackgroundData,
  title = {Plotting Background Data for Groups with Ggplot2 • {{blogR}}},
  url = {https://drsimonj.svbtle.com/plotting-background-data-for-groups-with-ggplot2},
  urldate = {2022-11-11},
  abstract = {This tweet by mikefc alerted me to a mind-blowingly simple but amazing trick using the ggplot2 package: to visualise data for different groups in a facetted plot with all of the data plotted in the background. Here’s an example that we’ll learn to... | blogR | Walkthroughs and projects using R for data science.},
  langid = {english},
  organization = {{blogR on Svbtle}},
  file = {/home/skynet3/Zotero/storage/XQ4BSCP7/plotting-background-data-for-groups-with-ggplot2.html}
}

@online{PodiaNotFound,
  title = {Podia: {{Not}} Found},
  url = {https://store.metasnake.com/effective-pandas-book](https://store.metasnake.com/effective-pandas-book},
  urldate = {2022-11-11},
  file = {/home/skynet3/Zotero/storage/6WXUTYHC/effective-pandas-book.html}
}

@misc{poggioTheoryDeepLearning2018,
  title = {Theory of {{Deep Learning III}}: Explaining the Non-Overfitting Puzzle},
  shorttitle = {Theory of {{Deep Learning III}}},
  author = {Poggio, Tomaso and Kawaguchi, Kenji and Liao, Qianli and Miranda, Brando and Rosasco, Lorenzo and Boix, Xavier and Hidary, Jack and Mhaskar, Hrushikesh},
  date = {2018-01-16},
  number = {arXiv:1801.00173},
  eprint = {1801.00173},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1801.00173},
  url = {http://arxiv.org/abs/1801.00173},
  urldate = {2022-11-11},
  abstract = {A main puzzle of deep networks revolves around the absence of overfitting despite large overparametrization and despite the large capacity demonstrated by zero training error on randomly labeled data. In this note, we show that the dynamics associated to gradient descent minimization of nonlinear networks is topologically equivalent, near the asymptotically stable minima of the empirical error, to linear gradient system in a quadratic potential with a degenerate (for square loss) or almost degenerate (for logistic or crossentropy loss) Hessian. The proposition depends on the qualitative theory of dynamical systems and is supported by numerical results. Our main propositions extend to deep nonlinear networks two properties of gradient descent for linear networks, that have been recently established (1) to be key to their generalization properties: 1. Gradient descent enforces a form of implicit regularization controlled by the number of iterations, and asymptotically converges to the minimum norm solution for appropriate initial conditions of gradient descent. This implies that there is usually an optimum early stopping that avoids overfitting of the loss. This property, valid for the square loss and many other loss functions, is relevant especially for regression. 2. For classification, the asymptotic convergence to the minimum norm solution implies convergence to the maximum margin solution which guarantees good classification error for "low noise" datasets. This property holds for loss functions such as the logistic and cross-entropy loss independently of the initial conditions. The robustness to overparametrization has suggestive implications for the robustness of the architecture of deep convolutional networks with respect to the curse of dimensionality.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/home/skynet3/Zotero/storage/NN28LGYY/Poggio et al. - 2018 - Theory of Deep Learning III explaining the non-ov.pdf;/home/skynet3/Zotero/storage/LL87MKMG/1801.html}
}

@article{pooleInferenceDeterministicSimulation,
  ids = {pooleInferenceDeterministicSimulationa},
  title = {Inference for Deterministic Simulation Models: {{The Bayesian}} Melding Approach},
  author = {Poole, David and Raftery, Adrian E},
  pages = {12},
  langid = {english},
  file = {/home/skynet3/Zotero/storage/JIE69DXY/Poole and Raftery - Inference for deterministic simulation models The.pdf}
}

@online{PostgreSQLAVGFunction,
  ids = {PostgreSQLAVGFunctiona},
  title = {{{PostgreSQL AVG}} Function},
  url = {https://www.postgresqltutorial.com/postgresql-avg-function/},
  urldate = {2022-11-11},
  abstract = {In this tutorial, you will learn how to use PostgreSQL AVG function to calculate average value of a numeric column.},
  langid = {american},
  file = {/home/skynet3/Zotero/storage/42G9JKP6/postgresql-avg-function.html}
}

@online{PostgreSQLDocumentation,
  title = {{{PostgreSQL}}: {{Documentation}}},
  url = {https://www.postgresql.org/docs/},
  urldate = {2022-11-11},
  file = {/home/skynet3/Zotero/storage/FZQ2WDUZ/docs.html}
}

@misc{powerGrokkingGeneralizationOverfitting2022,
  ids = {powerGrokkingGeneralizationOverfitting2022a},
  title = {Grokking: {{Generalization Beyond Overfitting}} on {{Small Algorithmic Datasets}}},
  shorttitle = {Grokking},
  author = {Power, Alethea and Burda, Yuri and Edwards, Harri and Babuschkin, Igor and Misra, Vedant},
  date = {2022-01-06},
  number = {arXiv:2201.02177},
  eprint = {2201.02177},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2201.02177},
  url = {http://arxiv.org/abs/2201.02177},
  urldate = {2022-11-11},
  abstract = {In this paper we propose to study generalization of neural networks on small algorithmically generated datasets. In this setting, questions about data efficiency, memorization, generalization, and speed of learning can be studied in great detail. In some situations we show that neural networks learn through a process of "grokking" a pattern in the data, improving generalization performance from random chance level to perfect generalization, and that this improvement in generalization can happen well past the point of overfitting. We also study generalization as a function of dataset size and find that smaller datasets require increasing amounts of optimization for generalization. We argue that these datasets provide a fertile ground for studying a poorly understood aspect of deep learning: generalization of overparametrized neural networks beyond memorization of the finite training dataset.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/home/skynet3/Zotero/storage/XRBYSQEQ/Power et al. - 2022 - Grokking Generalization Beyond Overfitting on Sma.pdf;/home/skynet3/Zotero/storage/LEKJRNI3/2201.html}
}

@online{PPLAPIs,
  title = {{{PPL APIs}}},
  url = {https://colcarroll.github.io/ppl-api/},
  urldate = {2022-11-11}
}

@online{PredictiveIntelligenceLabJAXBO,
  title = {{{PredictiveIntelligenceLab}}/{{JAX-BO}}},
  url = {https://github.com/PredictiveIntelligenceLab/JAX-BO},
  urldate = {2022-11-11},
  abstract = {Contribute to PredictiveIntelligenceLab/JAX-BO development by creating an account on GitHub.},
  langid = {english},
  organization = {{GitHub}},
  file = {/home/skynet3/Zotero/storage/NYTB3U37/JAX-BO.html}
}

@online{PrincetonLIPSNumpyhilbertcurveNumpy,
  title = {{{PrincetonLIPS}}/Numpy-Hilbert-Curve: {{Numpy}} Implementation of {{Hilbert}} Curves in Arbitrary Dimensions},
  shorttitle = {{{PrincetonLIPS}}/Numpy-Hilbert-Curve},
  url = {https://github.com/PrincetonLIPS/numpy-hilbert-curve},
  urldate = {2022-11-11},
  abstract = {Numpy implementation of Hilbert curves in arbitrary dimensions - PrincetonLIPS/numpy-hilbert-curve: Numpy implementation of Hilbert curves in arbitrary dimensions},
  langid = {english},
  organization = {{GitHub}},
  file = {/home/skynet3/Zotero/storage/5TUTACTD/numpy-hilbert-curve.html}
}

@online{PrincipledBayesianWorkflow,
  title = {Towards {{A Principled Bayesian Workflow}}},
  url = {https://betanalpha.github.io/assets/case_studies/principled_bayesian_workflow.html},
  urldate = {2022-11-11},
  file = {/home/skynet3/Zotero/storage/2KYN4TCG/principled_bayesian_workflow.html}
}

@online{PrincipledMachineLearning,
  title = {Principled {{Machine Learning}}: {{Practices}} and {{Tools}} for {{Efficient Collaboration}}},
  shorttitle = {Principled {{Machine Learning}}},
  url = {https://dev.to/robogeek/principled-machine-learning-4eho},
  urldate = {2022-11-11},
  abstract = {Machine learning projects are often harder than they should be. The code to train an ML model is just software, and we should be able to rerun that software any time we like. But the state of tools to manage machine learning processes is inadequate. ML teams can easily get into a situation of not remembering how a model was trained, or the training data might be lost or overwritten. Sharing a project with colleagues might be clumsy.},
  langid = {english},
  organization = {{DEV Community 👩‍💻👨‍💻}},
  file = {/home/skynet3/Zotero/storage/A5APALJN/principled-machine-learning-4eho.html}
}

@online{PriorChoiceRecommendations,
  ids = {PriorChoiceRecommendationsa},
  title = {Prior {{Choice Recommendations}} · Stan-Dev/Stan {{Wiki}}},
  url = {https://github.com/stan-dev/stan},
  urldate = {2022-11-11},
  abstract = {Stan development repository. The master branch contains the current release. The develop branch contains the latest stable development.  See the Developer Process Wiki for details.   - Prior Choice...},
  langid = {english},
  organization = {{GitHub}},
  file = {/home/skynet3/Zotero/storage/ENGQY27Z/Prior-Choice-Recommendations.html}
}

@online{PriorModeling,
  title = {Prior {{Modeling}}},
  url = {https://betanalpha.github.io/assets/case_studies/prior_modeling.html},
  urldate = {2022-11-11},
  file = {/home/skynet3/Zotero/storage/3R68D8JW/prior_modeling.html}
}

@online{PriorPredictiveChecks,
  title = {{$<$}p{$>$} {$<$}h2 Style="font-Family: '{{Bahnschrift}}'; Font-Size = 80\%; Font-Weight: 100; Color: \#000000;"{$>$}{{Prior}} Predictive Checks for {{Bayesian}} Regression.{$<$}/H2{$>$} {$<$}/P{$>$}},
  url = {https://engineeringdecisionanalysis.shinyapps.io/Priors/?_ga=2.230023708.1800474280.1612547156-513452691.1612547156},
  urldate = {2022-11-11},
  file = {/home/skynet3/Zotero/storage/T9G45LFT/Priors.html}
}

@online{ProbabilisticProgrammingandBayesianMethodsforHackersCh1Introduction,
  ids = {ProbabilisticProgrammingandBayesianMethodsforHackersCh1Introductiona},
  title = {Probabilistic-{{Programming-and-Bayesian-Methods-for-Hackers}}/{{Ch1}}\_{{Introduction}}\_{{TFP}}.Ipynb at Master · {{CamDavidsonPilon}}/{{Probabilistic-Programming-and-Bayesian-Methods-for-Hackers}}},
  url = {https://github.com/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers},
  urldate = {2022-11-11},
  abstract = {aka \&quot;Bayesian Methods for Hackers\&quot;: An introduction to Bayesian methods + probabilistic programming with a computation/understanding-first, mathematics-second point of view. All in pure P...},
  langid = {english},
  organization = {{GitHub}},
  file = {/home/skynet3/Zotero/storage/VRRVVBDM/Ch1_Introduction_TFP.html}
}

@online{ProbabilisticProgrammingVariational,
  title = {Probabilistic {{Programming}} with {{Variational Inference}}: {{Under}} the {{Hood}} | {{Will Crichton}}},
  url = {https://willcrichton.net/notes/probabilistic-programming-under-the-hood/},
  urldate = {2022-11-11},
  file = {/home/skynet3/Zotero/storage/XLP9U8NH/probabilistic-programming-under-the-hood.html}
}

@online{ProbabilityCheatsheet,
  title = {Probability {{Cheatsheet}}},
  url = {http://www.wzchen.com/probability-cheatsheet},
  urldate = {2022-11-11},
  langid = {american},
  organization = {{Hi. I'm William Chen.}},
  file = {/home/skynet3/Zotero/storage/K55KIMDA/probability-cheatsheet.html}
}

@online{ProbmlDynamaxState,
  title = {Probml/Dynamax: {{State Space Models}} Library in {{JAX}}},
  shorttitle = {Probml/Dynamax},
  url = {https://github.com/probml/dynamax},
  urldate = {2022-11-11},
  abstract = {State Space Models library in JAX. Contribute to probml/dynamax development by creating an account on GitHub.},
  langid = {english},
  organization = {{GitHub}},
  file = {/home/skynet3/Zotero/storage/B7F9XGV9/dynamax.html}
}

@article{probstHyperparametersTuningStrategies2019,
  title = {Hyperparameters and Tuning Strategies for Random Forest},
  author = {Probst, Philipp and Wright, Marvin N. and Boulesteix, Anne-Laure},
  date = {2019},
  journaltitle = {WIREs Data Mining and Knowledge Discovery},
  volume = {9},
  number = {3},
  pages = {e1301},
  issn = {1942-4795},
  doi = {10.1002/widm.1301},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/widm.1301},
  urldate = {2022-11-11},
  abstract = {The random forest (RF) algorithm has several hyperparameters that have to be set by the user, for example, the number of observations drawn randomly for each tree and whether they are drawn with or without replacement, the number of variables drawn randomly for each split, the splitting rule, the minimum number of samples that a node must contain, and the number of trees. In this paper, we first provide a literature review on the parameters' influence on the prediction performance and on variable importance measures. It is well known that in most cases RF works reasonably well with the default values of the hyperparameters specified in software packages. Nevertheless, tuning the hyperparameters can improve the performance of RF. In the second part of this paper, after a presenting brief overview of tuning strategies, we demonstrate the application of one of the most established tuning strategies, model-based optimization (MBO). To make it easier to use, we provide the tuneRanger R package that tunes RF with MBO automatically. In a benchmark study on several datasets, we compare the prediction performance and runtime of tuneRanger with other tuning implementations in R and RF with default hyperparameters. This article is categorized under: Algorithmic Development {$>$} Biological Data Mining Algorithmic Development {$>$} Statistics Algorithmic Development {$>$} Hierarchies and Trees Technologies {$>$} Machine Learning},
  langid = {english},
  keywords = {ensemble,literature review,out-of-bag,performance evaluation,ranger,sequential model-based optimization,tuning parameter},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/widm.1301},
  file = {/home/skynet3/Zotero/storage/T4CPKY3R/Probst et al. - 2019 - Hyperparameters and tuning strategies for random f.pdf;/home/skynet3/Zotero/storage/6S3CMFY7/widm.html}
}

@online{ProgramEvaluation,
  ids = {ProgramEvaluationa},
  title = {Program {{Evaluation}}},
  url = {https://www.lecy.info/program-evaluation},
  urldate = {2022-11-11},
  langid = {american},
  organization = {{Jesse D. Lecy}},
  file = {/home/skynet3/Zotero/storage/LL8X29IM/program-evaluation.html}
}

@misc{prollochsReinforcementLearning2018,
  ids = {prollochsReinforcementLearning2018a},
  title = {Reinforcement {{Learning}} in {{R}}},
  author = {Pröllochs, Nicolas and Feuerriegel, Stefan},
  date = {2018-09-29},
  number = {arXiv:1810.00240},
  eprint = {1810.00240},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1810.00240},
  url = {http://arxiv.org/abs/1810.00240},
  urldate = {2022-11-11},
  abstract = {Reinforcement learning refers to a group of methods from artificial intelligence where an agent performs learning through trial and error. It differs from supervised learning, since reinforcement learning requires no explicit labels; instead, the agent interacts continuously with its environment. That is, the agent starts in a specific state and then performs an action, based on which it transitions to a new state and, depending on the outcome, receives a reward. Different strategies (e.g. Q-learning) have been proposed to maximize the overall reward, resulting in a so-called policy, which defines the best possible action in each state. Mathematically, this process can be formalized by a Markov decision process and it has been implemented by packages in R; however, there is currently no package available for reinforcement learning. As a remedy, this paper demonstrates how to perform reinforcement learning in R and, for this purpose, introduces the ReinforcementLearning package. The package provides a remarkably flexible framework and is easily applied to a wide range of different problems. We demonstrate its use by drawing upon common examples from the literature (e.g. finding optimal game strategies).},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/skynet3/Zotero/storage/9D9BY6LI/Pröllochs and Feuerriegel - 2018 - Reinforcement Learning in R.pdf;/home/skynet3/Zotero/storage/6XZ3KGXF/1810.html}
}

@online{PrototypeIntroductionNamed,
  title = {(Prototype) {{Introduction}} to {{Named Tensors}} in {{PyTorch}} — {{PyTorch Tutorials}} 1.11.0+cu102 Documentation},
  url = {https://pytorch.org/tutorials/intermediate/named_tensor_tutorial.html},
  urldate = {2022-11-11},
  file = {/home/skynet3/Zotero/storage/LM7ANRAG/named_tensor_tutorial.html}
}

@misc{puliOutofdistributionGeneralizationPresence2022,
  title = {Out-of-Distribution {{Generalization}} in the {{Presence}} of {{Nuisance-Induced Spurious Correlations}}},
  author = {Puli, Aahlad and Zhang, Lily H. and Oermann, Eric K. and Ranganath, Rajesh},
  date = {2022-03-17},
  number = {arXiv:2107.00520},
  eprint = {2107.00520},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2107.00520},
  url = {http://arxiv.org/abs/2107.00520},
  urldate = {2022-11-11},
  abstract = {In many prediction problems, spurious correlations are induced by a changing relationship between the label and a nuisance variable that is also correlated with the covariates. For example, in classifying animals in natural images, the background, which is a nuisance, can predict the type of animal. This nuisance-label relationship does not always hold, and the performance of a model trained under one such relationship may be poor on data with a different nuisance-label relationship. To build predictive models that perform well regardless of the nuisance-label relationship, we develop Nuisance-Randomized Distillation (NURD). We introduce the nuisance-randomized distribution, a distribution where the nuisance and the label are independent. Under this distribution, we define the set of representations such that conditioning on any member, the nuisance and the label remain independent. We prove that the representations in this set always perform better than chance, while representations outside of this set may not. NURD finds a representation from this set that is most informative of the label under the nuisance-randomized distribution, and we prove that this representation achieves the highest performance regardless of the nuisance-label relationship. We evaluate NURD on several tasks including chest X-ray classification where, using non-lung patches as the nuisance, NURD produces models that predict pneumonia under strong spurious correlations.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/skynet3/Zotero/storage/MNMQFC8I/Puli et al. - 2022 - Out-of-distribution Generalization in the Presence.pdf;/home/skynet3/Zotero/storage/VFVJ3D2L/2107.html}
}

@misc{puspitaningrumSurveyRecentAbstract2021,
  title = {A {{Survey}} of {{Recent Abstract Summarization Techniques}}},
  author = {Puspitaningrum, Diyah},
  date = {2021-04-15},
  number = {arXiv:2105.00824},
  eprint = {2105.00824},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2105.00824},
  url = {http://arxiv.org/abs/2105.00824},
  urldate = {2022-11-11},
  abstract = {This paper surveys several recent abstract summarization methods: T5, Pegasus, and ProphetNet. We implement the systems in two languages: English and Indonesian languages. We investigate the impact of pre-training models (one T5, three Pegasuses, three ProphetNets) on several Wikipedia datasets in English and Indonesian language and compare the results to the Wikipedia systems' summaries. The T5-Large, the Pegasus-XSum, and the ProphetNet-CNNDM provide the best summarization. The most significant factors that influence ROUGE performance are coverage, density, and compression. The higher the scores, the better the summary. Other factors that influence the ROUGE scores are the pre-training goal, the dataset's characteristics, the dataset used for testing the pre-trained model, and the cross-lingual function. Several suggestions to improve this paper's limitation are: 1) assure that the dataset used for the pre-training model must sufficiently large, contains adequate instances for handling cross-lingual purpose; 2) Advanced process (finetuning) shall be reasonable. We recommend using the large dataset consists of comprehensive coverage of topics from many languages before implementing advanced processes such as the train-infer-train procedure to the zero-shot translation in the training stage of the pre-training model.},
  archiveprefix = {arXiv},
  keywords = {68P20 (Primary) 68T07; 68T50 (Secondary),Computer Science - Computation and Language,Computer Science - Information Retrieval,Computer Science - Machine Learning,H.3.1,I.2.7},
  file = {/home/skynet3/Zotero/storage/BP6URC7C/Puspitaningrum - 2021 - A Survey of Recent Abstract Summarization Techniqu.pdf;/home/skynet3/Zotero/storage/VWT8EIPY/2105.html}
}

@online{PutProd,
  title = {Put {{R}} in Prod},
  url = {https://putrinprod.com/},
  urldate = {2022-11-11},
  abstract = {Tools and guides for putting R in production}
}

@online{Pyro,
  title = {Pyro},
  url = {https://pyro.ai/},
  urldate = {2022-11-11},
  file = {/home/skynet3/Zotero/storage/9C2XGMW6/pyro.ai.html}
}

@online{PyTorchBabySteps2018,
  title = {{{PyTorch With Baby Steps}}: {{From}} Y=x {{To Training A Convnet}}},
  shorttitle = {{{PyTorch With Baby Steps}}},
  date = {2018-02-08T00:00:00},
  url = {https://lelon.io/blog/pytorch-baby-steps},
  urldate = {2022-11-11},
  abstract = {An example-driven, incremental tutorial introduction to PyTorch.},
  langid = {english},
  file = {/home/skynet3/Zotero/storage/AY6VZUYR/pytorch-baby-steps.html}
}

@online{PyTorchImperativeStyle2019,
  title = {{{PyTorch}}: {{An Imperative Style}}, {{High-Performance Deep Learning Library}}},
  shorttitle = {{{PyTorch}}},
  date = {2019-12-03T22:06:05+00:00},
  url = {https://deepai.org/publication/pytorch-an-imperative-style-high-performance-deep-learning-library},
  urldate = {2022-11-11},
  abstract = {12/03/19 - Deep learning frameworks have often focused on either usability or speed, but not both. PyTorch is a machine learning library that...},
  organization = {{DeepAI}},
  file = {/home/skynet3/Zotero/storage/7FABET8C/pytorch-an-imperative-style-high-performance-deep-learning-library.html}
}

@online{PytorchTorcharrowCommon,
  title = {Pytorch/Torcharrow: {{Common}} and Composable Data Structures Built on {{PyTorch Tensor}} for Efficient Batch Data Representation and Processing in {{PyTorch}} Model Authoring},
  shorttitle = {Pytorch/Torcharrow},
  url = {https://github.com/pytorch/torcharrow},
  urldate = {2022-11-11},
  abstract = {Common and composable data structures built on PyTorch Tensor for efficient batch data representation and processing in PyTorch model authoring - pytorch/torcharrow: Common and composable data stru...},
  langid = {english},
  organization = {{GitHub}},
  file = {/home/skynet3/Zotero/storage/9RHA5Q23/torcharrow.html}
}

@online{Q36161Wikidata,
  title = {Q36161) - {{Wikidata}}},
  url = {https://www.wikidata.org/wiki/Q36161);},
  urldate = {2022-11-11},
  file = {/home/skynet3/Zotero/storage/VQPTG6Y6/Q36161)\;.html}
}

@article{qiuPretrainedModelsNatural2020,
  ids = {qiuPretrainedModelsNatural2020a},
  title = {Pre-Trained {{Models}} for {{Natural Language Processing}}: {{A Survey}}},
  shorttitle = {Pre-Trained {{Models}} for {{Natural Language Processing}}},
  author = {Qiu, Xipeng and Sun, Tianxiang and Xu, Yige and Shao, Yunfan and Dai, Ning and Huang, Xuanjing},
  date = {2020-10},
  journaltitle = {Science China Technological Sciences},
  shortjournal = {Sci. China Technol. Sci.},
  volume = {63},
  number = {10},
  eprint = {2003.08271},
  eprinttype = {arxiv},
  primaryclass = {cs},
  pages = {1872--1897},
  issn = {1674-7321, 1869-1900},
  doi = {10.1007/s11431-020-1647-3},
  url = {http://arxiv.org/abs/2003.08271},
  urldate = {2022-11-11},
  abstract = {Recently, the emergence of pre-trained models (PTMs) has brought natural language processing (NLP) to a new era. In this survey, we provide a comprehensive review of PTMs for NLP. We first briefly introduce language representation learning and its research progress. Then we systematically categorize existing PTMs based on a taxonomy with four perspectives. Next, we describe how to adapt the knowledge of PTMs to the downstream tasks. Finally, we outline some potential directions of PTMs for future research. This survey is purposed to be a hands-on guide for understanding, using, and developing PTMs for various NLP tasks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/skynet3/Zotero/storage/D7Z38T8L/Qiu et al. - 2020 - Pre-trained Models for Natural Language Processing.pdf;/home/skynet3/Zotero/storage/VPHQTY4R/2003.html}
}

@online{QuantitativeEvaluationsCounterfactuals2021,
  title = {On {{Quantitative Evaluations}} of {{Counterfactuals}}},
  date = {2021-10-30T05:00:36+00:00},
  url = {https://deepai.org/publication/on-quantitative-evaluations-of-counterfactuals},
  urldate = {2022-11-11},
  abstract = {10/30/21 - As counterfactual examples become increasingly popular for explaining decisions of deep learning models, it is essential to unders...},
  organization = {{DeepAI}},
  file = {/home/skynet3/Zotero/storage/ISEADHYX/on-quantitative-evaluations-of-counterfactuals.html}
}

@article{rabinowiczCrossValidationCorrelatedData2022,
  title = {Cross-{{Validation}} for {{Correlated Data}}},
  author = {Rabinowicz, Assaf and Rosset, Saharon},
  date = {2022-04-03},
  journaltitle = {Journal of the American Statistical Association},
  volume = {117},
  number = {538},
  pages = {718--731},
  publisher = {{Taylor \& Francis}},
  issn = {0162-1459},
  doi = {10.1080/01621459.2020.1801451},
  url = {https://amstat.tandfonline.com/doi/abs/10.1080/01621459.2020.1801451},
  urldate = {2022-11-11},
  abstract = {–K-fold cross-validation (CV) with squared error loss is widely used for evaluating predictive models, especially when strong distributional assumptions cannot be taken. However, CV with squared error loss is not free from distributional assumptions, in particular in cases involving non-iid data. This article analyzes CV for correlated data. We present a criterion for suitability of standard CV in presence of correlations. When this criterion does not hold, we introduce a bias corrected CV estimator which we term  CV c , CVc,  that yields an unbiased estimate of prediction error in many settings where standard CV is invalid. We also demonstrate our results numerically, and find that introducing our correction substantially improves both, model evaluation and model selection in simulations and real data studies. Supplementary materials for this article are available online.},
  keywords = {Dependent data,Gaussian process regression,Linear mixed model,Model selection,Prediction error estimation},
  file = {/home/skynet3/Zotero/storage/5VPWHW7K/Rabinowicz and Rosset - 2022 - Cross-Validation for Correlated Data.pdf}
}

@misc{rahmanExploratoryCharacterizationBugs2020,
  title = {An {{Exploratory Characterization}} of {{Bugs}} in {{COVID-19 Software Projects}}},
  author = {Rahman, Akond and Farhana, Effat},
  date = {2020-05-31},
  number = {arXiv:2006.00586},
  eprint = {2006.00586},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2006.00586},
  url = {http://arxiv.org/abs/2006.00586},
  urldate = {2022-11-11},
  abstract = {Context: The dire consequences of the COVID-19 pandemic has influenced development of COVID-19 software i.e., software used for analysis and mitigation of COVID-19. Bugs in COVID-19 software can be consequential, as COVID-19 software projects can impact public health policy and user data privacy. Objective: The goal of this paper is to help practitioners and researchers improve the quality of COVID-19 software through an empirical study of open source software projects related to COVID-19. Methodology: We use 129 open source COVID-19 software projects hosted on GitHub to conduct our empirical study. Next, we apply qualitative analysis on 550 bug reports from the collected projects to identify bug categories. Findings: We identify 8 bug categories, which include data bugs i.e., bugs that occur during mining and storage of COVID-19 data. The identified bug categories appear for 7 categories of software projects including (i) projects that use statistical modeling to perform predictions related to COVID-19, and (ii) medical equipment software that are used to design and implement medical equipment, such as ventilators. Conclusion: Based on our findings, we advocate for robust statistical model construction through better synergies between data science practitioners and public health experts. Existence of security bugs in user tracking software necessitates development of tools that will detect data privacy violations and security weaknesses.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Software Engineering,D.2.9,J.3},
  file = {/home/skynet3/Zotero/storage/GDWEQ9WW/Rahman and Farhana - 2020 - An Exploratory Characterization of Bugs in COVID-1.pdf;/home/skynet3/Zotero/storage/8ZYVU7RD/2006.html}
}

@article{rajaramanHighDimensionalConvexGeometry,
  title = {High-{{Dimensional Convex Geometry}}},
  author = {Rajaraman, Amit},
  pages = {106},
  langid = {english},
  file = {/home/skynet3/Zotero/storage/FFF7W8UP/Rajaraman - High-Dimensional Convex Geometry.pdf}
}

@misc{rammalLeaveOneOutConditionalMutual2022,
  ids = {rammalLeaveOneOutConditionalMutual2022a},
  title = {On {{Leave-One-Out Conditional Mutual Information For Generalization}}},
  author = {Rammal, Mohamad Rida and Achille, Alessandro and Golatkar, Aditya and Diggavi, Suhas and Soatto, Stefano},
  date = {2022-07-01},
  number = {arXiv:2207.00581},
  eprint = {2207.00581},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2207.00581},
  url = {http://arxiv.org/abs/2207.00581},
  urldate = {2022-11-11},
  abstract = {We derive information theoretic generalization bounds for supervised learning algorithms based on a new measure of leave-one-out conditional mutual information (loo-CMI). Contrary to other CMI bounds, which are black-box bounds that do not exploit the structure of the problem and may be hard to evaluate in practice, our loo-CMI bounds can be computed easily and can be interpreted in connection to other notions such as classical leave-one-out cross-validation, stability of the optimization algorithm, and the geometry of the loss-landscape. It applies both to the output of training algorithms as well as their predictions. We empirically validate the quality of the bound by evaluating its predicted generalization gap in scenarios for deep learning. In particular, our bounds are non-vacuous on large-scale image-classification tasks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/home/skynet3/Zotero/storage/3ALWINPC/Rammal et al. - 2022 - On Leave-One-Out Conditional Mutual Information Fo.pdf;/home/skynet3/Zotero/storage/9EDR6KIB/2207.html}
}

@article{raoCategoricalPerceptionPValues2022,
  title = {Categorical {{Perception}} of P-{{Values}}},
  author = {Rao, V. N. Vimal and Bye, Jeffrey K. and Varma, Sashank},
  date = {2022},
  journaltitle = {Topics in Cognitive Science},
  volume = {14},
  number = {2},
  pages = {414--425},
  issn = {1756-8765},
  doi = {10.1111/tops.12589},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/tops.12589},
  urldate = {2022-11-11},
  abstract = {Traditional statistics instruction emphasizes a .05 significance level for hypothesis tests. Here, we investigate the consequences of this training for researchers’ mental representations of probabilities — whether .05 becomes a boundary, that is, a discontinuity of the mental number line, and alters their reasoning about p-values. Graduate students with statistical training (n = 25) viewed pairs of p-values and judged whether they were “similar” or “different.” After controlling for several covariates, participants were more likely and faster to judge p-values as “different” when they crossed the .05 boundary (e.g., .046 vs. .052) compared to when they did not (e.g., .026 vs. .032). This result suggests a categorical perception-like effect for the processing of p-values. It may be a consequence of traditional statistical instruction creating a psychologically real divide between so-called statistical “significance” and “nonsignificance.” Such a distortion is undesirable given modern approaches to statistical reasoning that de-emphasize dichotomizing the p-value continuum.},
  langid = {english},
  keywords = {Categorical perception,Probabilistic reasoning,Rational number processing,Statistical significance,Statistics education},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/tops.12589},
  file = {/home/skynet3/Zotero/storage/YJAPPLA4/Rao et al. - 2022 - Categorical Perception of p-Values.pdf;/home/skynet3/Zotero/storage/7RKW3DAB/tops.html}
}

@article{ratnerSnorkelRapidTraining2017,
  ids = {ratnerSnorkelRapidTraining2017a},
  title = {Snorkel: {{Rapid Training Data Creation}} with {{Weak Supervision}}},
  shorttitle = {Snorkel},
  author = {Ratner, Alexander and Bach, Stephen H. and Ehrenberg, Henry and Fries, Jason and Wu, Sen and Ré, Christopher},
  date = {2017-11},
  journaltitle = {Proceedings of the VLDB Endowment},
  shortjournal = {Proc. VLDB Endow.},
  volume = {11},
  number = {3},
  eprint = {1711.10160},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  pages = {269--282},
  issn = {2150-8097},
  doi = {10.14778/3157794.3157797},
  url = {http://arxiv.org/abs/1711.10160},
  urldate = {2022-11-11},
  abstract = {Labeling training data is increasingly the largest bottleneck in deploying machine learning systems. We present Snorkel, a first-of-its-kind system that enables users to train state-of-the-art models without hand labeling any training data. Instead, users write labeling functions that express arbitrary heuristics, which can have unknown accuracies and correlations. Snorkel denoises their outputs without access to ground truth by incorporating the first end-to-end implementation of our recently proposed machine learning paradigm, data programming. We present a flexible interface layer for writing labeling functions based on our experience over the past year collaborating with companies, agencies, and research labs. In a user study, subject matter experts build models 2.8x faster and increase predictive performance an average 45.5\% versus seven hours of hand labeling. We study the modeling tradeoffs in this new setting and propose an optimizer for automating tradeoff decisions that gives up to 1.8x speedup per pipeline execution. In two collaborations, with the U.S. Department of Veterans Affairs and the U.S. Food and Drug Administration, and on four open-source text and image data sets representative of other deployments, Snorkel provides 132\% average improvements to predictive performance over prior heuristic approaches and comes within an average 3.60\% of the predictive performance of large hand-curated training sets.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/skynet3/Zotero/storage/EE42DUEM/Ratner et al. - 2017 - Snorkel Rapid Training Data Creation with Weak Su.pdf;/home/skynet3/Zotero/storage/N2NZRHAM/1711.html}
}

@misc{ravivWhatValueData2020,
  title = {What Is the {{Value}} of {{Data}}? {{On Mathematical Methods}} for {{Data Quality Estimation}}},
  shorttitle = {What Is the {{Value}} of {{Data}}?},
  author = {Raviv, Netanel and Jain, Siddharth and Bruck, Jehoshua},
  date = {2020-05-12},
  number = {arXiv:2001.03464},
  eprint = {2001.03464},
  eprinttype = {arxiv},
  primaryclass = {cs, math, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2001.03464},
  url = {http://arxiv.org/abs/2001.03464},
  urldate = {2022-11-11},
  abstract = {Data is one of the most important assets of the information age, and its societal impact is undisputed. Yet, rigorous methods of assessing the quality of data are lacking. In this paper, we propose a formal definition for the quality of a given dataset. We assess a dataset's quality by a quantity we call the expected diameter, which measures the expected disagreement between two randomly chosen hypotheses that explain it, and has recently found applications in active learning. We focus on Boolean hyperplanes, and utilize a collection of Fourier analytic, algebraic, and probabilistic methods to come up with theoretical guarantees and practical solutions for the computation of the expected diameter. We also study the behaviour of the expected diameter on algebraically structured datasets, conduct experiments that validate this notion of quality, and demonstrate the feasibility of our techniques.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Information Theory,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/skynet3/Zotero/storage/FW4I2YDI/Raviv et al. - 2020 - What is the Value of Data On Mathematical Methods.pdf;/home/skynet3/Zotero/storage/G4YW88QL/2001.html}
}

@online{ReadingsLecturesVideos,
  title = {Readings, Lectures, and Videos},
  url = {https://datavizs21.classes.andrewheiss.com/content/},
  urldate = {2022-11-11},
  abstract = {Communicate science and statistics with beautiful graphics made with R and ggplot2},
  langid = {american},
  organization = {{Data Visualization}},
  file = {/home/skynet3/Zotero/storage/4UYXCRJB/content.html}
}

@online{ReasonsWhyYou,
  ids = {ReasonsWhyYoua},
  title = {3 Reasons Why You Can’t Always Use Predictive Performance to Choose among Models | {{Statistical Modeling}}, {{Causal Inference}}, and {{Social Science}}},
  url = {https://statmodeling.stat.columbia.edu/2015/10/23/26857/},
  urldate = {2022-11-11}
}

@online{RecognizingVariablesTheir2019,
  title = {Recognizing {{Variables}} from Their {{Data}} via {{Deep Embeddings}} of {{Distributions}}},
  date = {2019-09-11T04:10:48+00:00},
  url = {https://deepai.org/publication/recognizing-variables-from-their-data-via-deep-embeddings-of-distributions},
  urldate = {2022-11-11},
  abstract = {09/11/19 - A key obstacle in automated analytics and meta-learning is the inability to recognize when different datasets contain measurements...},
  organization = {{DeepAI}},
  file = {/home/skynet3/Zotero/storage/CPWC2MRH/recognizing-variables-from-their-data-via-deep-embeddings-of-distributions.html}
}

@online{Rectangling,
  title = {Rectangling},
  url = {https://tidyr.tidyverse.org/articles/rectangle.html},
  urldate = {2022-11-11},
  abstract = {Rectangling is the art and craft of taking a deeply nested list (often  sourced from wild caught JSON or XML) and taming it into a tidy data set of  rows and columns. This vignette introduces you to the main rectangling tools provided by tidyr: `unnest\_longer()`, `unnest\_wider()`, `unnest\_auto()`, and `hoist()`.},
  langid = {english},
  file = {/home/skynet3/Zotero/storage/JXEPNSQ6/rectangle.html}
}

@misc{redellShapleyDecompositionRSquared2019,
  title = {Shapley {{Decomposition}} of {{R-Squared}} in {{Machine Learning Models}}},
  author = {Redell, Nickalus},
  date = {2019-08-26},
  number = {arXiv:1908.09718},
  eprint = {1908.09718},
  eprinttype = {arxiv},
  primaryclass = {stat},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/1908.09718},
  urldate = {2022-11-11},
  abstract = {In this paper we introduce a metric aimed at helping machine learning practitioners quickly summarize and communicate the overall importance of each feature in any black-box machine learning prediction model. Our proposed metric, based on a Shapley-value variance decomposition of the familiar \$R\^2\$ from classical statistics, is a model-agnostic approach for assessing feature importance that fairly allocates the proportion of model-explained variability in the data to each model feature. This metric has several desirable properties including boundedness at 0 and 1 and a feature-level variance decomposition summing to the overall model \$R\^2\$. In contrast to related methods for computing feature-level \$R\^2\$ variance decompositions with linear models, our method makes use of pre-computed Shapley values which effectively shifts the computational burden from iteratively fitting many models to the Shapley values themselves. And with recent advancements in Shapley value calculations for gradient boosted decision trees and neural networks, computing our proposed metric after model training can come with minimal computational overhead. Our implementation is available in the R package shapFlex.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Methodology},
  file = {/home/skynet3/Zotero/storage/ZFNM44N5/Redell - 2019 - Shapley Decomposition of R-Squared in Machine Lear.pdf;/home/skynet3/Zotero/storage/FSBH65VL/1908.html}
}

@article{reedPracticeLaggingVariables2015,
  ids = {reedPracticeLaggingVariables2015a},
  title = {On the {{Practice}} of {{Lagging Variables}} to {{Avoid Simultaneity}}},
  author = {Reed, William Robert},
  date = {2015},
  journaltitle = {Oxford Bulletin of Economics and Statistics},
  volume = {77},
  number = {6},
  pages = {897--905},
  issn = {1468-0084},
  doi = {10.1111/obes.12088},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/obes.12088},
  urldate = {2022-11-11},
  abstract = {A common practice in applied economics research consists of replacing a suspected simultaneously determined explanatory variable with its lagged value. This note demonstrates that this practice does not enable one to avoid simultaneity bias. The associated estimates are still inconsistent, and hypothesis testing is invalid. An alternative is to use lagged values of the endogenous variable in instrumental variable estimation. However, this is only an effective estimation strategy if the lagged values do not themselves belong in the respective estimating equation, and if they are sufficiently correlated with the simultaneously determined explanatory variable.},
  langid = {english},
  keywords = {Lagged variables,Reverse causality,Simultaneity},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/obes.12088},
  file = {/home/skynet3/Zotero/storage/HPZL5MC9/Reed - 2015 - On the Practice of Lagging Variables to Avoid Simu.pdf;/home/skynet3/Zotero/storage/JF9S5WVL/obes.html}
}

@online{RegExplain,
  title = {🕵️‍♂️ {{RegExplain}}},
  url = {https://www.garrickadenbuie.com/project/regexplain/},
  urldate = {2022-11-11},
  abstract = {An RStudio addin slash utility belt for regular expressions},
  langid = {american},
  organization = {{Garrick Aden‑Buie}},
  file = {/home/skynet3/Zotero/storage/ZA5EJ8F4/regexplain.html}
}

@online{RegressionOtherStories,
  title = {Regression and {{Other Stories}} - {{Examples}}},
  url = {https://avehtari.github.io/ROS-Examples/examples.html},
  urldate = {2022-11-11}
}

@article{reinhartExpandingScopeStatistical2021,
  title = {Expanding the Scope of Statistical Computing: {{Training}} Statisticians to Be Software Engineers},
  shorttitle = {Expanding the Scope of Statistical Computing},
  author = {Reinhart, Alex and Genovese, Christopher R.},
  date = {2021-01-01},
  journaltitle = {Journal of Statistics and Data Science Education},
  shortjournal = {Journal of Statistics and Data Science Education},
  volume = {29},
  eprint = {1912.13076},
  eprinttype = {arxiv},
  primaryclass = {stat},
  pages = {S7-S15},
  issn = {2693-9169},
  doi = {10.1080/10691898.2020.1845109},
  url = {http://arxiv.org/abs/1912.13076},
  urldate = {2022-11-11},
  abstract = {Traditionally, statistical computing courses have taught the syntax of a particular programming language or specific statistical computation methods. Since the publication of Nolan and Temple Lang (2010), we have seen a greater emphasis on data wrangling, reproducible research, and visualization. This shift better prepares students for careers working with complex datasets and producing analyses for multiple audiences. But, we argue, statisticians are now often called upon to develop statistical software, not just analyses, such as R packages implementing new analysis methods or machine learning systems integrated into commercial products. This demands different skills. We describe a graduate course that we developed to meet this need by focusing on four themes: programming practices; software design; important algorithms and data structures; and essential tools and methods. Through code review and revision, and a semester-long software project, students practice all the skills of software engineering. The course allows students to expand their understanding of computing as applied to statistical problems while building expertise in the kind of software development that is increasingly the province of the working statistician. We see this as a model for the future evolution of the computing curriculum in statistics and data science.},
  archiveprefix = {arXiv},
  issue = {sup1},
  keywords = {Statistics - Computation,Statistics - Other Statistics},
  file = {/home/skynet3/Zotero/storage/PC8D9Z7E/Reinhart and Genovese - 2021 - Expanding the scope of statistical computing Trai.pdf;/home/skynet3/Zotero/storage/9KDF5Z72/1912.html}
}

@misc{reinkeCommonLimitationsImage2022,
  title = {Common {{Limitations}} of {{Image Processing Metrics}}: {{A Picture Story}}},
  shorttitle = {Common {{Limitations}} of {{Image Processing Metrics}}},
  author = {Reinke, Annika and Tizabi, Minu D. and Sudre, Carole H. and Eisenmann, Matthias and Rädsch, Tim and Baumgartner, Michael and Acion, Laura and Antonelli, Michela and Arbel, Tal and Bakas, Spyridon and Bankhead, Peter and Benis, Arriel and Cardoso, M. Jorge and Cheplygina, Veronika and Christodoulou, Evangelia and Cimini, Beth and Collins, Gary S. and Farahani, Keyvan and van Ginneken, Bram and Glocker, Ben and Godau, Patrick and Hamprecht, Fred and Hashimoto, Daniel A. and Heckmann-Nötzel, Doreen and Hoffman, Michael M. and Huisman, Merel and Isensee, Fabian and Jannin, Pierre and Kahn, Charles E. and Karargyris, Alexandros and Karthikesalingam, Alan and Kainz, Bernhard and Kavur, Emre and Kenngott, Hannes and Kleesiek, Jens and Kooi, Thijs and Kozubek, Michal and Kreshuk, Anna and Kurc, Tahsin and Landman, Bennett A. and Litjens, Geert and Madani, Amin and Maier-Hein, Klaus and Martel, Anne L. and Mattson, Peter and Meijering, Erik and Menze, Bjoern and Moher, David and Moons, Karel G. M. and Müller, Henning and Nichyporuk, Brennan and Nickel, Felix and Noyan, M. Alican and Petersen, Jens and Polat, Gorkem and Rajpoot, Nasir and Reyes, Mauricio and Rieke, Nicola and Riegler, Michael and Rivaz, Hassan and Saez-Rodriguez, Julio and Gutierrez, Clarisa Sanchez and Schroeter, Julien and Saha, Anindo and Shetty, Shravya and van Smeden, Maarten and Stieltjes, Bram and Summers, Ronald M. and Taha, Abdel A. and Tsaftaris, Sotirios A. and Van Calster, Ben and Varoquaux, Gaël and Wiesenfarth, Manuel and Yaniv, Ziv R. and Kopp-Schneider, Annette and Jäger, Paul and Maier-Hein, Lena},
  options = {useprefix=true},
  date = {2022-07-07},
  number = {arXiv:2104.05642},
  eprint = {2104.05642},
  eprinttype = {arxiv},
  primaryclass = {cs, eess},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2104.05642},
  url = {http://arxiv.org/abs/2104.05642},
  urldate = {2022-11-11},
  abstract = {While the importance of automatic image analysis is continuously increasing, recent meta-research revealed major flaws with respect to algorithm validation. Performance metrics are particularly key for meaningful, objective, and transparent performance assessment and validation of the used automatic algorithms, but relatively little attention has been given to the practical pitfalls when using specific metrics for a given image analysis task. These are typically related to (1) the disregard of inherent metric properties, such as the behaviour in the presence of class imbalance or small target structures, (2) the disregard of inherent data set properties, such as the non-independence of the test cases, and (3) the disregard of the actual biomedical domain interest that the metrics should reflect. This living dynamically document has the purpose to illustrate important limitations of performance metrics commonly applied in the field of image analysis. In this context, it focuses on biomedical image analysis problems that can be phrased as image-level classification, semantic segmentation, instance segmentation, or object detection task. The current version is based on a Delphi process on metrics conducted by an international consortium of image analysis experts from more than 60 institutions worldwide.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Electrical Engineering and Systems Science - Image and Video Processing},
  file = {/home/skynet3/Zotero/storage/C7Q763XK/Reinke et al. - 2022 - Common Limitations of Image Processing Metrics A .pdf;/home/skynet3/Zotero/storage/TKQNFKAA/2104.html}
}

@misc{reisQuantifyingOnlyPositive2021,
  ids = {reisQuantifyingOnlyPositive2021a},
  title = {Quantifying {{With Only Positive Training Data}}},
  author = {dos Reis, Denis and de Souto, Marcílio and de Sousa, Elaine and Batista, Gustavo},
  options = {useprefix=true},
  date = {2021-10-12},
  number = {arXiv:2004.10356},
  eprint = {2004.10356},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2004.10356},
  url = {http://arxiv.org/abs/2004.10356},
  urldate = {2022-11-11},
  abstract = {Quantification is the research field that studies methods for counting the number of data points that belong to each class in an unlabeled sample. Traditionally, researchers in this field assume the availability of labelled observations for all classes to induce a quantification model. However, we often face situations where the number of classes is large or even unknown, or we have reliable data for a single class. When inducing a multi-class quantifier is infeasible, we are often concerned with estimates for a specific class of interest. In this context, we have proposed a novel setting known as One-class Quantification (OCQ). In contrast, Positive and Unlabeled Learning (PUL), another branch of Machine Learning, has offered solutions to OCQ, despite quantification not being the focal point of PUL. This article closes the gap between PUL and OCQ and brings both areas together under a unified view. We compare our method, Passive Aggressive Threshold (PAT), against PUL methods and show that PAT generally is the fastest and most accurate algorithm. PAT induces quantification models that can be reused to quantify different samples of data. We additionally introduce Exhaustive TIcE (ExTIcE), an improved version of the PUL algorithm Tree Induction for c Estimation (TIcE). We show that ExTIcE quantifies more accurately than PAT and the other assessed algorithms in scenarios where several negative observations are identical to the positive ones.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/skynet3/Zotero/storage/N5W8RN8C/Reis et al. - 2021 - Quantifying With Only Positive Training Data.pdf;/home/skynet3/Zotero/storage/IMBZ2F87/2004.html}
}

@online{RelaxedSoftmaxLearning2019,
  title = {Relaxed {{Softmax}} for Learning from {{Positive}} and {{Unlabeled}} Data},
  date = {2019-09-17T20:29:57+00:00},
  url = {https://deepai.org/publication/relaxed-softmax-for-learning-from-positive-and-unlabeled-data},
  urldate = {2022-11-11},
  abstract = {09/17/19 - In recent years, the softmax model and its fast approximations have become the de-facto loss functions for deep neural networks wh...},
  organization = {{DeepAI}},
  file = {/home/skynet3/Zotero/storage/YA8LG84S/relaxed-softmax-for-learning-from-positive-and-unlabeled-data.html}
}

@online{ReproducibleResearchErrata,
  title = {Reproducible {{Research Errata}}},
  url = {http://christophergandrud.github.io/RepResR-RStudio/},
  urldate = {2022-11-11}
}

@online{RequestedPageNot,
  title = {Requested Page Not Found | {{The Rachel}} and {{Selim Benin School}} of {{Computer Science}} and {{Engineering}} | {{The Hebrew University}}},
  url = {https://www.cs.huji.ac.il/w},
  urldate = {2022-11-11},
  file = {/home/skynet3/Zotero/storage/YZNXCY6K/w.html}
}

@online{ResearcherDegreesFreedom,
  title = {Researcher {{Degrees}} of {{Freedom Analysis}}},
  url = {https://joachim-gassen.github.io/rdfanalysis/},
  urldate = {2022-11-11},
  abstract = {This experimental in-development package provides a set of functions      to develop data analysis code that systematically documents researcher degrees      of freedom when conducting analyses on observational data.      The resulting code base is self-documenting, supports unit testing and power     simulations based on simulated data. The documented researcher degrees of      freedom can be exhausted to generate a distribution of outcome estimates.},
  langid = {english},
  file = {/home/skynet3/Zotero/storage/X7V8MQG3/rdfanalysis.html}
}

@article{reuningExploringDynamicsLatent2019,
  ids = {reuningExploringDynamicsLatent2019a},
  title = {Exploring the {{Dynamics}} of {{Latent Variable Models}}},
  author = {Reuning, Kevin and Kenwick, Michael R. and Fariss, Christopher J.},
  date = {2019-10},
  journaltitle = {Political Analysis},
  volume = {27},
  number = {4},
  pages = {503--517},
  publisher = {{Cambridge University Press}},
  issn = {1047-1987, 1476-4989},
  doi = {10.1017/pan.2019.1},
  url = {https://www.cambridge.org/core/journals/political-analysis/article/abs/exploring-the-dynamics-of-latent-variable-models/CBE116F37900DAE957B2D7EB53DB0907},
  urldate = {2022-11-11},
  abstract = {Researchers face a tradeoff when applying latent variable models to  time-series, cross-sectional data. Static models minimize bias but assume data are temporally independent, resulting in a loss of efficiency. Dynamic models explicitly model temporal data structures, but smooth estimates of the latent trait across time, resulting in bias when the latent trait changes rapidly. We address this tradeoff by investigating a new approach for modeling and evaluating latent variable estimates: a robust dynamic model. The robust model is capable of minimizing bias and accommodating volatile changes in the latent trait. Simulations demonstrate that the robust model outperforms other models when the underlying latent trait is subject to rapid change, and is equivalent to the dynamic model in the absence of volatility. We reproduce latent estimates from studies of judicial ideology and democracy. For judicial ideology, the robust model uncovers shocks in judicial voting patterns that were not previously identified in the dynamic model. For democracy, the robust model provides more precise estimates of sudden institutional changes such as the imposition of martial law in the Philippines (1972–1981) and the short-lived Saur Revolution in Afghanistan (1978). Overall, the robust model is a useful alternative to the standard dynamic model for modeling latent traits that change rapidly over time.},
  langid = {english},
  keywords = {Bayesian analysis,dynamic modeling,latent variables}
}

@dataset{reuningReplicationDataExploring2018,
  ids = {reuningReplicationDataExploring2018a},
  title = {Replication {{Data}} for: {{Exploring}} the {{Dynamics}} of {{Latent Variable Models}}},
  shorttitle = {Replication {{Data}} For},
  author = {Reuning, Kevin and Kenwick, Michael R. and Fariss, Christopher J.},
  date = {2018},
  publisher = {{Harvard Dataverse}},
  doi = {10.7910/DVN/SSLCFF},
  url = {https://dataverse.harvard.edu/citation?persistentId=doi:10.7910/DVN/SSLCFF},
  urldate = {2022-11-11},
  abstract = {Researchers face a tradeoff when applying latent variable models to time-series, cross-section*al data. Static models minimize bias but assume data are temporally independent, resulting in a loss of efficiency. Dynamic models explicitly model temporal data structures, but smooth estimates of the latent trait across time, resulting in bias when the latent trait changes rapidly. We address this tradeoff by investigating a new approach for modeling and evaluating latent variable estimates: a robust dynamic model. The robust model is capable of minimizing bias and accommodating volatile changes in the latent trait. Simulations demonstrate that the robust model outperforms other models when the underlying latent trait is subject to rapid change, and is equivalent to the dynamic model in the absence of volatility. We reproduce latent estimates from studies of judicial ideology and democracy. For judicial ideology, the robust model uncovers shocks in judicial voting patterns that were not previously identified in the dynamic model. For democracy, the robust model provides more precise estimates of sudden institutional changes such as the imposition of martial law in the Philippines (1972-1981) and the short-lived Saur Revolution in Afghanistan (1978). Overall, the robust model is a useful alternative to the standard dynamic model for modeling latent traits that change rapidly over time.},
  editora = {Reuning, Kevin and Political Analysis},
  editoratype = {collaborator},
  langid = {english},
  file = {/home/skynet3/Zotero/storage/YZDKPMXF/Reuning et al. - 2018 - Replication Data for Exploring the Dynamics of La.pdf}
}

@online{ReusableHoldoutPreserving,
  title = {The Reusable Holdout: {{Preserving}} Validity in Adaptive Data Analysis},
  shorttitle = {The Reusable Holdout},
  url = {https://ai.googleblog.com/2015/08/the-reusable-holdout-preserving.html},
  urldate = {2022-11-11},
  langid = {english},
  file = {/home/skynet3/Zotero/storage/I2VZKT4Y/the-reusable-holdout-preserving.html}
}

@online{reynoldsOtherKindMachine2022,
  title = {The Other Kind of Machine Learning Regression — Unmeasured Method Performance},
  author = {Reynolds, Stuart},
  date = {2022-01-25T22:41:22},
  url = {https://stuart-reynolds.medium.com/the-other-kind-of-machine-learning-regression-unmeasured-method-performance-81b7eb00efda},
  urldate = {2022-11-11},
  abstract = {A beats B (p{$<$}0.000001, n=500k). B beats A (p{$<$}0.000001, n=500k).},
  langid = {english},
  organization = {{Medium}},
  file = {/home/skynet3/Zotero/storage/5J97S9TV/the-other-kind-of-machine-learning-regression-unmeasured-method-performance-81b7eb00efda.html}
}

@misc{ribeiroWhyShouldTrust2016,
  title = {"{{Why Should I Trust You}}?": {{Explaining}} the {{Predictions}} of {{Any Classifier}}},
  shorttitle = {"{{Why Should I Trust You}}?},
  author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
  date = {2016-08-09},
  number = {arXiv:1602.04938},
  eprint = {1602.04938},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/1602.04938},
  urldate = {2022-11-11},
  abstract = {Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one. In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally around the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the flexibility of these methods by explaining different models for text (e.g. random forests) and image classification (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and identifying why a classifier should not be trusted.},
  archiveprefix = {arXiv},
  version = {3},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/skynet3/Zotero/storage/Z5KKXY64/Ribeiro et al. - 2016 - Why Should I Trust You Explaining the Predicti.pdf;/home/skynet3/Zotero/storage/MEK3ACWT/1602.html}
}

@article{riceExpressingRegretUnified2022,
  ids = {riceExpressingRegretUnified2022a},
  title = {Expressing {{Regret}}: {{A Unified View}} of {{Credible Intervals}}},
  shorttitle = {Expressing {{Regret}}},
  author = {Rice, Kenneth and Ye, Lingbo},
  date = {2022-07-03},
  journaltitle = {The American Statistician},
  volume = {76},
  number = {3},
  pages = {248--256},
  publisher = {{Taylor \& Francis}},
  issn = {0003-1305},
  doi = {10.1080/00031305.2022.2039764},
  url = {https://doi.org/10.1080/00031305.2022.2039764},
  urldate = {2022-11-11},
  abstract = {Posterior uncertainty is typically summarized as a credible interval, an interval in the parameter space that contains a fixed proportion—usually 95\%—of the posterior’s support. For multivariate parameters, credible sets perform the same role. There are of course many potential 95\% intervals from which to choose, yet even standard choices are rarely justified in any formal way. In this article we give a general method, focusing on the loss function that motivates an estimate—the Bayes rule—around which we construct a credible set. The set contains all points which, as estimates, would have minimally-worse expected loss than the Bayes rule: we call this excess expected loss “regret.” The approach can be used for any model and prior, and we show how it justifies all widely used choices of credible interval/set. Further examples show how it provides insights into more complex estimation problems. Supplementary materials for this article are available online.},
  keywords = {Bayesian methods,Estimation,Inference},
  annotation = {\_eprint: https://doi.org/10.1080/00031305.2022.2039764}
}

@article{richardsonSingleWorldIntervention2013,
  title = {Single World Intervention Graphs ({{SWIGs}}): {{A}} Unification of the Counterfactual and Graphical Approaches to Causality},
  shorttitle = {Single World Intervention Graphs ({{SWIGs}})},
  author = {Richardson, Thomas S. and Robins, James M.},
  date = {2013},
  journaltitle = {Center for the Statistics and the Social Sciences, University of Washington Series. Working Paper},
  volume = {128},
  number = {30},
  pages = {2013},
  file = {/home/skynet3/Zotero/storage/SQ3G5CHX/Richardson and Robins - 2013 - Single world intervention graphs (SWIGs) A unific.pdf}
}

@book{riedererMarkdownCookbook,
  title = {R {{Markdown Cookbook}}},
  author = {Riederer, Christophe Dervieux, Emily, Yihui Xie},
  url = {https://bookdown.org/yihui/rmarkdown-cookbook/},
  urldate = {2022-11-11},
  abstract = {This book showcases short, practical examples of lesser-known tips and tricks to helps users get the most out of these tools. After reading this book, you will understand how R Markdown documents are transformed from plain text and how you may customize nearly every step of this processing. For example, you will learn how to dynamically create content from R code, reference code in other documents or chunks, control the formatting with customer templates, fine-tune how your code is processed, and incorporate multiple languages into your analysis.}
}

@online{RmcelreathStatRethinking,
  title = {Rmcelreath/Stat\_rethinking\_2022: {{Statistical Rethinking}} Course Winter 2022},
  shorttitle = {Rmcelreath/Stat\_rethinking\_2022},
  url = {https://github.com/rmcelreath/stat_rethinking_2022},
  urldate = {2022-11-11},
  abstract = {Statistical Rethinking course winter 2022. Contribute to rmcelreath/stat\_rethinking\_2022 development by creating an account on GitHub.},
  langid = {english},
  organization = {{GitHub}},
  file = {/home/skynet3/Zotero/storage/CBNZDFRX/stat_rethinking_2022.html}
}

@misc{robertMarkovChainMonte2020,
  ids = {robertMarkovChainMonte2020a,robertMarkovChainMonte2020b},
  title = {Markov {{Chain Monte Carlo Methods}}, a Survey with Some Frequent Misunderstandings},
  author = {Robert, Christian P. and Changye, Wu},
  date = {2020-01-17},
  number = {arXiv:2001.06249},
  eprint = {2001.06249},
  eprinttype = {arxiv},
  primaryclass = {stat},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2001.06249},
  urldate = {2022-11-11},
  abstract = {In this chapter, we review some of the most standard MCMC tools used in Bayesian computation, along with vignettes on standard misunderstandings of these approaches taken from Q \textbackslash\&\textasciitilde A's on the forum Cross-validated answered by the first author.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Computation},
  file = {/home/skynet3/Zotero/storage/SCYBMX7I/Robert and Changye - 2020 - Markov Chain Monte Carlo Methods, a survey with so.pdf;/home/skynet3/Zotero/storage/V3TGRXQN/2001.html;/home/skynet3/Zotero/storage/WXSV6A8W/2001.html}
}

@article{robertsCommonPitfallsRecommendations2021,
  title = {Common Pitfalls and Recommendations for Using Machine Learning to Detect and Prognosticate for {{COVID-19}} Using Chest Radiographs and {{CT}} Scans},
  author = {Roberts, Michael and Driggs, Derek and Thorpe, Matthew and Gilbey, Julian and Yeung, Michael and Ursprung, Stephan and Aviles-Rivero, Angelica I. and Etmann, Christian and McCague, Cathal and Beer, Lucian and Weir-McCall, Jonathan R. and Teng, Zhongzhao and Gkrania-Klotsas, Effrossyni and Rudd, James H. F. and Sala, Evis and Schönlieb, Carola-Bibiane},
  date = {2021-03},
  journaltitle = {Nature Machine Intelligence},
  shortjournal = {Nat Mach Intell},
  volume = {3},
  number = {3},
  pages = {199--217},
  publisher = {{Nature Publishing Group}},
  issn = {2522-5839},
  doi = {10.1038/s42256-021-00307-0},
  url = {https://www.nature.com/articles/s42256-021-00307-0},
  urldate = {2022-11-11},
  abstract = {Machine learning methods offer great promise for fast and accurate detection and prognostication of coronavirus disease 2019 (COVID-19) from standard-of-care chest radiographs (CXR) and chest computed tomography (CT) images. Many articles have been published in 2020 describing new machine learning-based models for both of these tasks, but it is unclear which are of potential clinical utility. In this systematic review, we consider all published papers and preprints, for the period from 1 January 2020 to 3 October 2020, which describe new machine learning models for the diagnosis or prognosis of COVID-19 from CXR or CT images. All manuscripts uploaded to bioRxiv, medRxiv and arXiv along with all entries in EMBASE and MEDLINE in this timeframe are considered. Our search identified 2,212 studies, of which 415 were included after initial screening and, after quality screening, 62 studies were included in this systematic review. Our review finds that none of the models identified are of potential clinical use due to methodological flaws and/or underlying biases. This is a major weakness, given the urgency with which validated COVID-19 models are needed. To address this, we give many recommendations which, if followed, will solve these issues and lead to higher-quality model development and well-documented manuscripts.},
  issue = {3},
  langid = {english},
  keywords = {Computational science,Diagnostic markers,Prognostic markers,SARS-CoV-2},
  file = {/home/skynet3/Zotero/storage/JIKF89WQ/Roberts et al. - 2021 - Common pitfalls and recommendations for using mach.pdf;/home/skynet3/Zotero/storage/JGBNLMUS/s42256-021-00307-0.html}
}

@article{robertsCrossvalidationStrategiesData2017,
  ids = {robertsCrossvalidationStrategiesData2017a},
  title = {Cross-Validation Strategies for Data with Temporal, Spatial, Hierarchical, or Phylogenetic Structure},
  author = {Roberts, David R. and Bahn, Volker and Ciuti, Simone and Boyce, Mark S. and Elith, Jane and Guillera-Arroita, Gurutzeta and Hauenstein, Severin and Lahoz-Monfort, José J. and Schröder, Boris and Thuiller, Wilfried and Warton, David I. and Wintle, Brendan A. and Hartig, Florian and Dormann, Carsten F.},
  date = {2017-08},
  journaltitle = {Ecography},
  shortjournal = {Ecography},
  volume = {40},
  number = {8},
  pages = {913--929},
  issn = {09067590},
  doi = {10.1111/ecog.02881},
  url = {https://onlinelibrary.wiley.com/doi/10.1111/ecog.02881},
  urldate = {2022-11-11},
  langid = {english},
  file = {/home/skynet3/Zotero/storage/HI2WUYMG/Roberts et al. - 2017 - Cross-validation strategies for data with temporal.pdf;/home/skynet3/Zotero/storage/EQBC3UVY/ecog.html}
}

@misc{robinsonTappedOutBarely2019,
  ids = {robinsonTappedOutBarely2019a},
  title = {Tapped {{Out}} or {{Barely Tapped}}? {{Recommendations}} for {{How}} to {{Harness}} the {{Vast}} and {{Largely Unused Potential}} of the {{Mechanical Turk Participant Pool}}},
  shorttitle = {Tapped {{Out}} or {{Barely Tapped}}?},
  author = {Robinson, Jonathan and Rosenzweig, Cheskie and Moss, Aaron J. and LItman, Leib},
  date = {2019-06-07T13:17:09},
  publisher = {{PsyArXiv}},
  doi = {10.31234/osf.io/jq589},
  url = {https://psyarxiv.com/jq589/},
  urldate = {2022-11-11},
  abstract = {Mechanical Turk (MTurk) is a common source of research participants within the academic community. Despite MTurk’s utility and benefits over traditional subject pools some researchers have questioned whether it is sustainable. Specifically, some have asked whether MTurk workers are too familiar with manipulations and measures common in the social sciences, the result of many researchers relying on the same small participant pool. Here, we show that concerns about non-naivete on MTurk are due less to the MTurk platform itself and more to the way researchers use the platform. Specifically, we find that there are at least 250,000 MTurk workers worldwide and that a large majority of US workers are new to the platform each year and therefore relatively inexperienced as research participants. We describe how inexperienced workers are excluded from studies, in part, because of the worker reputation qualifications researchers commonly use. Then, we propose and evaluate an alternative approach to sampling on MTurk that allows researchers to access inexperienced participants without sacrificing data quality. We recommend that in some cases researchers should limit the number of highly experienced workers allowed in their study by excluding these workers or by stratifying sample recruitment based on worker experience levels. We discuss the trade-offs of different sampling practices on MTurk and describe how the above sampling strategies can help researchers harness the vast and largely untapped potential of the Mechanical Turk participant pool.},
  langid = {american},
  keywords = {other,Psychology,Social and Behavioral Sciences,Social and Personality Psychology},
  file = {/home/skynet3/Zotero/storage/T92FR8YB/Robinson et al. - 2019 - Tapped Out or Barely Tapped Recommendations for H.pdf}
}

@online{RobustnessChecksAre,
  title = {Robustness Checks Are a Joke | {{Statistical Modeling}}, {{Causal Inference}}, and {{Social Science}}},
  url = {https://statmodeling.stat.columbia.edu/2018/11/14/robustness-checks-joke/},
  urldate = {2022-11-11},
  file = {/home/skynet3/Zotero/storage/NP2NX93B/robustness-checks-joke.html}
}

@misc{roederLinearIdentifiabilityLearned2020,
  ids = {roederLinearIdentifiabilityLearned2020a},
  title = {On {{Linear Identifiability}} of {{Learned Representations}}},
  author = {Roeder, Geoffrey and Metz, Luke and Kingma, Diederik P.},
  date = {2020-07-07},
  number = {arXiv:2007.00810},
  eprint = {2007.00810},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2007.00810},
  url = {http://arxiv.org/abs/2007.00810},
  urldate = {2022-11-11},
  abstract = {Identifiability is a desirable property of a statistical model: it implies that the true model parameters may be estimated to any desired precision, given sufficient computational resources and data. We study identifiability in the context of representation learning: discovering nonlinear data representations that are optimal with respect to some downstream task. When parameterized as deep neural networks, such representation functions typically lack identifiability in parameter space, because they are overparameterized by design. In this paper, building on recent advances in nonlinear ICA, we aim to rehabilitate identifiability by showing that a large family of discriminative models are in fact identifiable in function space, up to a linear indeterminacy. Many models for representation learning in a wide variety of domains have been identifiable in this sense, including text, images and audio, state-of-the-art at time of publication. We derive sufficient conditions for linear identifiability and provide empirical support for the result on both simulated and real-world data.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/skynet3/Zotero/storage/DH8ZQXC7/Roeder et al. - 2020 - On Linear Identifiability of Learned Representatio.pdf;/home/skynet3/Zotero/storage/QGZMP7BX/2007.html}
}

@misc{rohrerThatLotProcess2021,
  title = {That’s a Lot to {{Process}}! {{Pitfalls}} of {{Popular Path Models}}},
  author = {Rohrer, Julia M. and Hünermund, Paul and Arslan, Ruben C. and Elson, Malte},
  date = {2021-02-01T15:40:11},
  publisher = {{PsyArXiv}},
  doi = {10.31234/osf.io/paeb7},
  url = {https://psyarxiv.com/paeb7/},
  urldate = {2022-11-11},
  abstract = {Path models to test claims about mediation and moderation are a staple of psychology. But applied researchers may sometimes not understand the underlying causal inference problems and thus endorse conclusions that rest on unrealistic assumptions. In this article, we aim to provide a clear explanation for the limited conditions under which standard procedures for mediation and moderation analysis can succeed. We discuss why reversing arrows or comparing model fit indices cannot tell us which model is the right one, and how tests of conditional independence can at least tell us where our model goes wrong. Causal modeling practices in psychology are far from optimal but may be kept alive by domain norms which demand that every article makes some novel claim about processes and boundary conditions. We end with a vision for a different research culture in which causal inference is pursued in a much slower, more deliberate, and collaborative manner.},
  langid = {american},
  keywords = {causal infernce,mediation,moderation,PROCESS,Quantitative Methods,Social and Behavioral Sciences},
  file = {/home/skynet3/Zotero/storage/L43B3ZVA/Rohrer et al. - 2021 - That’s a lot to Process! Pitfalls of Popular Path .pdf}
}

@article{romanoHypothesisTestingEconometrics2010,
  ids = {romanoHypothesisTestingEconometrics2010a},
  title = {Hypothesis {{Testing}} in {{Econometrics}}},
  author = {Romano, Joseph P. and Shaikh, Azeem M. and Wolf, Michael},
  date = {2010-09-04},
  journaltitle = {Annual Review of Economics},
  shortjournal = {Annu. Rev. Econ.},
  volume = {2},
  number = {1},
  pages = {75--104},
  issn = {1941-1383, 1941-1391},
  doi = {10.1146/annurev.economics.102308.124342},
  url = {https://www.annualreviews.org/doi/10.1146/annurev.economics.102308.124342},
  urldate = {2022-11-11},
  abstract = {This article reviews important concepts and methods that are useful for hypothesis testing. First, we discuss the Neyman-Pearson framework. Various approaches to optimality are presented, including finite-sample and large-sample optimality. Then, we summarize some of the most important methods, as well as resampling methodology, which is useful to set critical values. Finally, we consider the problem of multiple testing, which has witnessed a burgeoning literature in recent years. Along the way, we incorporate some examples that are current in the econometrics literature. While many problems with well-known successful solutions are included, we also address open problems that are not easily handled with current technology, stemming from such issues as lack of optimality or poor asymptotic approximations.},
  langid = {english},
  file = {/home/skynet3/Zotero/storage/J8TCUWGI/Romano et al. - 2010 - Hypothesis Testing in Econometrics.pdf}
}

@online{RopensciAssertrAssertive,
  title = {Ropensci/Assertr: {{Assertive}} Programming for {{R}} Analysis Pipelines},
  shorttitle = {Ropensci/Assertr},
  url = {https://github.com/ropensci/assertr},
  urldate = {2022-11-11},
  abstract = {Assertive programming for R analysis pipelines. Contribute to ropensci/assertr development by creating an account on GitHub.},
  langid = {english},
  organization = {{GitHub}},
  file = {/home/skynet3/Zotero/storage/4UZYU8CU/assertr.html}
}

@online{RopensciCharlatanCreate,
  title = {Ropensci/Charlatan: {{Create}} Fake Data in {{R}}},
  shorttitle = {Ropensci/Charlatan},
  url = {https://github.com/ropensci/charlatan},
  urldate = {2022-11-11},
  abstract = {Create fake data in R. Contribute to ropensci/charlatan development by creating an account on GitHub.},
  langid = {english},
  organization = {{GitHub}},
  file = {/home/skynet3/Zotero/storage/NB9X8TX2/charlatan.html}
}

@online{RopensciCld3Bindings,
  title = {Ropensci/Cld3: {{Bindings}} to {{Google}}'s {{Compact Language Detector}} 3},
  shorttitle = {Ropensci/Cld3},
  url = {https://github.com/ropensci/cld3},
  urldate = {2022-11-11},
  abstract = {Bindings to Google's Compact Language Detector 3 . Contribute to ropensci/cld3 development by creating an account on GitHub.},
  langid = {english},
  organization = {{GitHub}},
  file = {/home/skynet3/Zotero/storage/QDCRQXFR/cld3.html}
}

@online{RopensciSkimrFrictionless,
  title = {Ropensci/Skimr: {{A}} Frictionless, Pipeable Approach to Dealing with Summary Statistics},
  shorttitle = {Ropensci/Skimr},
  url = {https://github.com/ropensci/skimr},
  urldate = {2022-11-11},
  abstract = {A frictionless, pipeable approach to dealing with summary statistics - ropensci/skimr: A frictionless, pipeable approach to dealing with summary statistics},
  langid = {english},
  organization = {{GitHub}},
  file = {/home/skynet3/Zotero/storage/K6VDSLTJ/skimr.html}
}

@online{roqueLevelsDifficultyBayesian2020,
  title = {5 {{Levels}} of {{Difficulty}} — {{Bayesian Gaussian Random Walk}} with {{PyMC3}} and {{Theano}}},
  author = {Roque, Luís},
  date = {2020-12-15T22:02:25},
  url = {https://towardsdatascience.com/5-levels-of-difficulty-bayesian-gaussian-random-walk-with-pymc3-and-theano-34343911c7d2},
  urldate = {2022-11-11},
  abstract = {State-Space Models in Bayesian Time Series Analysis with PyMC3},
  langid = {english},
  organization = {{Medium}},
  file = {/home/skynet3/Zotero/storage/T4CL84MU/5-levels-of-difficulty-bayesian-gaussian-random-walk-with-pymc3-and-theano-34343911c7d2.html}
}

@misc{rosenmanCombiningObservationalExperimental2020,
  ids = {rosenmanCombiningObservationalExperimental2020a},
  title = {Combining {{Observational}} and {{Experimental Datasets Using Shrinkage Estimators}}},
  author = {Rosenman, Evan and Basse, Guillaume and Owen, Art and Baiocchi, Michael},
  date = {2020-05-18},
  number = {arXiv:2002.06708},
  eprint = {2002.06708},
  eprinttype = {arxiv},
  primaryclass = {math, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2002.06708},
  url = {http://arxiv.org/abs/2002.06708},
  urldate = {2022-11-11},
  abstract = {We consider the problem of combining data from observational and experimental sources to make causal conclusions. This problem is increasingly relevant, as the modern era has yielded passive collection of massive observational datasets in areas such as e-commerce and electronic health. These data may be used to supplement experimental data, which is frequently expensive to obtain. In Rosenman et al. (2018), we considered this problem under the assumption that all confounders were measured. Here, we relax the assumption of unconfoundedness. To derive combined estimators with desirable properties, we make use of results from the Stein Shrinkage literature. Our contributions are threefold. First, we propose a generic procedure for deriving shrinkage estimators in this setting, making use of a generalized unbiased risk estimate. Second, we develop two new estimators, prove finite sample conditions under which they have lower risk than an estimator using only experimental data, and show that each achieves a notion of asymptotic optimality. Third, we draw connections between our approach and results in sensitivity analysis, including proposing a method for evaluating the feasibility of our estimators.},
  archiveprefix = {arXiv},
  keywords = {Mathematics - Statistics Theory,Statistics - Methodology},
  file = {/home/skynet3/Zotero/storage/2HWFNRM6/Rosenman et al. - 2020 - Combining Observational and Experimental Datasets .pdf;/home/skynet3/Zotero/storage/GKWM2KEN/2002.html}
}

@book{rossIntroductionProbabilitySimulation2022,
  title = {An {{Introduction}} to {{Probability}} and {{Simulation}}},
  author = {Ross, Kevin},
  date = {2022},
  url = {https://bookdown.org/kevin_davisross/probsim-book/},
  urldate = {2022-11-15},
  abstract = {This textbook presents a simulation-based approach to probability, using the Symbulate package.},
  file = {/home/skynet3/Zotero/storage/H2EPJ9ZJ/probsim-book.html}
}

@article{rotellaProbabilityLogoddsOdds,
  ids = {rotellaProbabilityLogoddsOddsa},
  title = {Probability, Log-Odds, and Odds},
  author = {Rotella, Jay},
  pages = {3},
  langid = {english},
  file = {/home/skynet3/Zotero/storage/C5KRTTK3/Rotella - Probability, log-odds, and odds.pdf}
}

@article{rothWhenParallelTrends,
  title = {When {{Is Parallel Trends Sensitive}} to {{Functional Form}}?},
  author = {Roth, Jonathan and Sant’Anna, Pedro H C},
  pages = {13},
  abstract = {This paper assesses when the validity of difference-in-differences depends on functional form. We provide a novel characterization: the parallel trends assumption holds under all strictly monotonic transformations of the outcome if and only if a stronger “parallel trends”-type condition holds for the cumulative distribution function of untreated potential outcomes. This condition for parallel trends to be insensitive to functional form is satisfied if and essentially only if the population can be partitioned into a subgroup for which treatment is effectively randomly assigned and a remaining subgroup for which the distribution of untreated potential outcomes is stable over time. These conditions have testable implications, and we introduce falsification tests for the null that parallel trends is insensitive to functional form.},
  langid = {english},
  file = {/home/skynet3/Zotero/storage/2BCTI352/Roth and Sant’Anna - When Is Parallel Trends Sensitive to Functional Fo.pdf}
}

@misc{rothWhenParallelTrends2022,
  title = {When {{Is Parallel Trends Sensitive}} to {{Functional Form}}?},
  author = {Roth, Jonathan and Sant'Anna, Pedro H. C.},
  date = {2022-09-19},
  number = {arXiv:2010.04814},
  eprint = {2010.04814},
  eprinttype = {arxiv},
  primaryclass = {econ, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2010.04814},
  url = {http://arxiv.org/abs/2010.04814},
  urldate = {2022-11-11},
  abstract = {This paper assesses when the validity of difference-in-differences depends on functional form. We provide a novel characterization: the parallel trends assumption holds under all strictly monotonic transformations of the outcome if and only if a stronger ``parallel trends''-type condition holds for the cumulative distribution function of untreated potential outcomes. This condition for parallel trends to be insensitive to functional form is satisfied if and essentially only if the population can be partitioned into a subgroup for which treatment is effectively randomly assigned and a remaining subgroup for which the distribution of untreated potential outcomes is stable over time. These conditions have testable implications, and we introduce falsification tests for the null that parallel trends is insensitive to functional form.},
  archiveprefix = {arXiv},
  keywords = {Economics - Econometrics,Statistics - Methodology},
  file = {/home/skynet3/Zotero/storage/7ICYGZB9/Roth and Sant'Anna - 2022 - When Is Parallel Trends Sensitive to Functional Fo.pdf;/home/skynet3/Zotero/storage/NG22XV6A/2010.html}
}

@misc{rothWhenParallelTrends2022a,
  title = {When {{Is Parallel Trends Sensitive}} to {{Functional Form}}?},
  author = {Roth, Jonathan and Sant'Anna, Pedro H. C.},
  date = {2022-09-19},
  number = {arXiv:2010.04814},
  eprint = {2010.04814},
  eprinttype = {arxiv},
  primaryclass = {econ, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2010.04814},
  url = {http://arxiv.org/abs/2010.04814},
  urldate = {2022-11-11},
  abstract = {This paper assesses when the validity of difference-in-differences depends on functional form. We provide a novel characterization: the parallel trends assumption holds under all strictly monotonic transformations of the outcome if and only if a stronger ``parallel trends''-type condition holds for the cumulative distribution function of untreated potential outcomes. This condition for parallel trends to be insensitive to functional form is satisfied if and essentially only if the population can be partitioned into a subgroup for which treatment is effectively randomly assigned and a remaining subgroup for which the distribution of untreated potential outcomes is stable over time. These conditions have testable implications, and we introduce falsification tests for the null that parallel trends is insensitive to functional form.},
  archiveprefix = {arXiv},
  keywords = {Economics - Econometrics,Statistics - Methodology},
  file = {/home/skynet3/Zotero/storage/WWPJIVDC/Roth and Sant'Anna - 2022 - When Is Parallel Trends Sensitive to Functional Fo.pdf;/home/skynet3/Zotero/storage/7DP5QDTC/2010.html}
}

@article{rothWhenParallelTrendsa,
  title = {When {{Is Parallel Trends Sensitive}} to {{Functional Form}}?},
  author = {Roth, Jonathan and Sant’Anna, Pedro H C},
  pages = {13},
  abstract = {This paper assesses when the validity of difference-in-differences depends on functional form. We provide a novel characterization: the parallel trends assumption holds under all strictly monotonic transformations of the outcome if and only if a stronger “parallel trends”-type condition holds for the cumulative distribution function of untreated potential outcomes. This condition for parallel trends to be insensitive to functional form is satisfied if and essentially only if the population can be partitioned into a subgroup for which treatment is effectively randomly assigned and a remaining subgroup for which the distribution of untreated potential outcomes is stable over time. These conditions have testable implications, and we introduce falsification tests for the null that parallel trends is insensitive to functional form.},
  langid = {english},
  file = {/home/skynet3/Zotero/storage/3Q3NCXRX/Roth and Sant’Anna - When Is Parallel Trends Sensitive to Functional Fo.pdf}
}

@misc{rougierExactFormOckham2020,
  title = {The Exact Form of the '{{Ockham}} Factor' in Model Selection},
  author = {Rougier, Jonathan and Priebe, Carey},
  date = {2020-04-15},
  number = {arXiv:1906.11592},
  eprint = {1906.11592},
  eprinttype = {arxiv},
  primaryclass = {math, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1906.11592},
  url = {http://arxiv.org/abs/1906.11592},
  urldate = {2022-11-11},
  abstract = {We explore the arguments for maximizing the `evidence' as an algorithm for model selection. We show, using a new definition of model complexity which we term `flexibility', that maximizing the evidence should appeal to both Bayesian and Frequentist statisticians. This is due to flexibility's unique position in the exact decomposition of log-evidence into log-fit minus flexibility. In the Gaussian linear model, flexibility is asymptotically equal to the Bayesian Information Criterion (BIC) penalty, but we caution against using BIC in place of flexibility for model selection.},
  archiveprefix = {arXiv},
  keywords = {Mathematics - Statistics Theory,Statistics - Methodology},
  file = {/home/skynet3/Zotero/storage/7LHQQFVE/Rougier and Priebe - 2020 - The exact form of the 'Ockham factor' in model sel.pdf;/home/skynet3/Zotero/storage/QK5AJF66/1906.html}
}

@software{rozemberczkiAwesomeGraphClassification2022,
  title = {Awesome {{Graph Classification}}},
  author = {Rozemberczki, Benedek},
  date = {2022-11-09T22:39:29Z},
  origdate = {2018-07-14T12:13:44Z},
  url = {https://github.com/benedekrozemberczki/awesome-graph-classification},
  urldate = {2022-11-11},
  abstract = {A collection of important graph embedding, classification and representation learning papers with implementations.},
  keywords = {attention-mechanism,classification-algorithm,deep-graph-kernels,deepwalk,graph-attention-model,graph-attention-networks,graph-classification,graph-convolutional-networks,graph-embedding,graph-kernel,graph-kernels,graph-representation-learning,graph2vec,kernel-methods,netlsd,network-embedding,node-embedding,node2vec,structural-attention,weisfeiler-lehman}
}

@online{RPubsSamplingWeird,
  title = {{{RPubs}} - {{Sampling}} from Weird Probability Distributions},
  url = {https://rpubs.com/a_pear_9/weird_distributions},
  urldate = {2022-11-11},
  file = {/home/skynet3/Zotero/storage/57KQQYKB/weird_distributions.html}
}

@online{RspatialTerraPackage,
  title = {Rspatial/Terra: {{R}} Package for Spatial Data Handling {{https://rspatial.github.io/terra/reference/terra-package.html}}},
  shorttitle = {Rspatial/Terra},
  url = {https://github.com/rspatial/terra},
  urldate = {2022-11-11},
  abstract = {R package for spatial data handling https://rspatial.github.io/terra/reference/terra-package.html - rspatial/terra: R package for spatial data handling https://rspatial.github.io/terra/reference/te...},
  langid = {english},
  organization = {{GitHub}},
  file = {/home/skynet3/Zotero/storage/WDZMH6K9/terra.html}
}

@online{RspeerPythonftfyFixes,
  title = {Rspeer/Python-Ftfy: {{Fixes}} Mojibake and Other Glitches in {{Unicode}} Text, after the Fact.},
  shorttitle = {Rspeer/Python-Ftfy},
  url = {https://github.com/rspeer/python-ftfy},
  urldate = {2022-11-11},
  abstract = {Fixes mojibake and other glitches in Unicode text, after the fact. - rspeer/python-ftfy: Fixes mojibake and other glitches in Unicode text, after the fact.},
  langid = {english},
  organization = {{GitHub}},
  file = {/home/skynet3/Zotero/storage/YMIVNQIG/python-ftfy.html}
}

@online{RulesMachineLearning,
  title = {Rules of {{Machine Learning}}:},
  shorttitle = {Rules of {{Machine Learning}}},
  url = {https://developers.google.com/machine-learning/guides/rules-of-ml},
  urldate = {2022-11-11},
  langid = {english},
  organization = {{Google Developers}},
  file = {/home/skynet3/Zotero/storage/BJB4FI9A/rules-of-ml.html}
}

@article{rungeInferringCausationTime2019,
  title = {Inferring Causation from Time Series in {{Earth}} System Sciences},
  author = {Runge, Jakob and Bathiany, Sebastian and Bollt, Erik and Camps-Valls, Gustau and Coumou, Dim and Deyle, Ethan and Glymour, Clark and Kretschmer, Marlene and Mahecha, Miguel D. and Muñoz-Marí, Jordi and van Nes, Egbert H. and Peters, Jonas and Quax, Rick and Reichstein, Markus and Scheffer, Marten and Schölkopf, Bernhard and Spirtes, Peter and Sugihara, George and Sun, Jie and Zhang, Kun and Zscheischler, Jakob},
  options = {useprefix=true},
  date = {2019-06-14},
  journaltitle = {Nature Communications},
  shortjournal = {Nat Commun},
  volume = {10},
  number = {1},
  pages = {2553},
  publisher = {{Nature Publishing Group}},
  issn = {2041-1723},
  doi = {10.1038/s41467-019-10105-3},
  url = {https://www.nature.com/articles/s41467-019-10105-3},
  urldate = {2022-11-15},
  abstract = {The heart of the scientific enterprise is a rational effort to understand the causes behind the phenomena we observe. In large-scale complex dynamical systems such as the Earth system, real experiments are rarely feasible. However, a rapidly increasing amount of observational and simulated data opens up the use of novel data-driven causal methods beyond the commonly adopted correlation techniques. Here, we give an overview of causal inference frameworks and identify promising generic application cases common in Earth system sciences and beyond. We discuss challenges and initiate the benchmark platform causeme.netto close the gap between method users and developers.},
  issue = {1},
  langid = {english},
  keywords = {Climate sciences,Computational science,Databases,Environmental sciences,Statistical physics,thermodynamics and nonlinear dynamics},
  file = {/home/skynet3/Zotero/storage/6RGTHQFE/Runge et al. - 2019 - Inferring causation from time series in Earth syst.pdf;/home/skynet3/Zotero/storage/HMUXSD2R/s41467-019-10105-3.html}
}

@online{RunningScriptsSchedule,
  ids = {RunningScriptsSchedulea},
  title = {Running {{R Scripts}} on a {{Schedule}} with {{GitHub Actions}}},
  url = {https://simonpcouch.com/blog/r-github-actions-commit/},
  urldate = {2022-11-11},
  abstract = {Blogging on statistical software and other assorted tomfoolery.},
  langid = {english},
  organization = {{Simon P. Couch}},
  file = {/home/skynet3/Zotero/storage/IIJT68VM/r-github-actions-commit.html}
}

@online{rustandiBayesianChangepointDetection09:00:00-04:00,
  title = {Bayesian {{Changepoint Detection}} in ({{Num}}){{Pyro}}},
  author = {Rustandi, Indrayana},
  year = {09:00:00-04:00},
  url = {https://irustandi.github.io/bayesian-changepoint-detection-in-numpyro.html},
  urldate = {2022-11-11},
  abstract = {Chad Scherrer has a blog post about how to do Bayesian changepoint detection in PyMC3, in the context of detecting changepoint associated with the yearly number of coal mining disasters. Here we will see how to implement the same model in Pyro, a probabilistic programming language and environment using PyTorch …},
  langid = {english},
  organization = {{Indra's Technical Musings}},
  file = {/home/skynet3/Zotero/storage/PAQLFKNI/bayesian-changepoint-detection-in-numpyro.html}
}

@online{RyantibsConformalTools,
  title = {Ryantibs/Conformal: {{Tools}} for Conformal Inference in Regression},
  shorttitle = {Ryantibs/Conformal},
  url = {https://github.com/ryantibs/conformal},
  urldate = {2022-11-11},
  abstract = {Tools for conformal inference in regression. Contribute to ryantibs/conformal development by creating an account on GitHub.},
  langid = {english},
  organization = {{GitHub}},
  file = {/home/skynet3/Zotero/storage/QCWXRQYS/conformal.html}
}

@inproceedings{saezAutomaticallyGeneratingWikipedia2018,
  title = {Automatically {{Generating Wikipedia Info-boxes}} from {{Wikidata}}},
  booktitle = {Companion of the {{The Web Conference}} 2018 on {{The Web Conference}} 2018  - {{WWW}} '18},
  author = {Sáez, Tomás and Hogan, Aidan},
  date = {2018},
  pages = {1823--1830},
  publisher = {{ACM Press}},
  location = {{Lyon, France}},
  doi = {10.1145/3184558.3191647},
  url = {http://dl.acm.org/citation.cfm?doid=3184558.3191647},
  urldate = {2022-11-11},
  abstract = {Info-boxes provide a summary of the most important meta-data relating to a particular entity described by a Wikipedia article. However, many articles have no info-box or have info-boxes with only minimal information; furthermore, there is a huge disparity between the level of detail available for info-boxes in English articles and those for other languages. Wikidata has been proposed as a central repository of facts to try to address such disparities, and has been used as a source of information to generate info-boxes. However, current processes still rely on human intervention either to create generic templates for entities of a given type or to create a specific info-box for a specific article in a specific language. As such, there are still many articles of Wikipedia without info-boxes but where relevant data are provided by Wikidata. In this paper, we investigate fully automatic methods to generate info-boxes for Wikipedia from the Wikidata knowledge graph. The primary challenge is to create ranking mechanisms that provide an intuitive prioritisation of the facts associated with an entity. We discuss this challenge, propose several straightforward metrics to prioritise information in info-boxes, and present an initial user evaluation to compare the quality of info-boxes generated by various metrics.},
  eventtitle = {Companion of the {{The Web Conference}} 2018},
  isbn = {978-1-4503-5640-4},
  langid = {english},
  file = {/home/skynet3/Zotero/storage/3PWBQ9R6/Sáez and Hogan - 2018 - Automatically Generating Wikipedia Info-boxes from.pdf}
}

@article{sahaRandomForestsSpatially2021,
  title = {Random {{Forests}} for {{Spatially Dependent Data}}},
  author = {Saha, Arkajyoti and Basu, Sumanta and Datta, Abhirup},
  date = {2021-07-06},
  journaltitle = {Journal of the American Statistical Association},
  volume = {0},
  number = {0},
  pages = {1--19},
  publisher = {{Taylor \& Francis}},
  issn = {0162-1459},
  doi = {10.1080/01621459.2021.1950003},
  url = {https://doi.org/10.1080/01621459.2021.1950003},
  urldate = {2022-11-11},
  abstract = {Spatial linear mixed-models, consisting of a linear covariate effect and a Gaussian process (GP) distributed spatial random effect, are widely used for analyses of geospatial data. We consider the setting where the covariate effect is nonlinear. Random forests (RF) are popular for estimating nonlinear functions but applications of RF for spatial data have often ignored the spatial correlation. We show that this impacts the performance of RF adversely. We propose RF-GLS, a novel and well-principled extension of RF, for estimating nonlinear covariate effects in spatial mixed models where the spatial correlation is modeled using GP. RF-GLS extends RF in the same way generalized least squares (GLS) fundamentally extends ordinary least squares (OLS) to accommodate for dependence in linear models. RF becomes a special case of RF-GLS, and is substantially outperformed by RF-GLS for both estimation and prediction across extensive numerical experiments with spatially correlated data. RF-GLS can be used for functional estimation in other types of dependent data like time series. We prove consistency of RF-GLS for β-mixing dependent error processes that include the popular spatial Matérn GP. As a byproduct, we also establish, to our knowledge, the first consistency result for RF under dependence. We establish results of independent importance, including a general consistency result of GLS optimizers of data-driven function classes, and a uniform law of large number under β-mixing dependence with weaker assumptions. These new tools can be potentially useful for asymptotic analysis of other GLS-style estimators in nonparametric regression with dependent data.},
  keywords = {Gaussian processes,Generalized least squares,Random forests,Spatial},
  annotation = {\_eprint: https://doi.org/10.1080/01621459.2021.1950003},
  file = {/home/skynet3/Zotero/storage/R3WTUH2Y/Saha et al. - 2021 - Random Forests for Spatially Dependent Data.pdf}
}

@article{saitoPrecisionRecallPlotMore2015,
  ids = {saitoPrecisionRecallPlotMore2015a},
  title = {The {{Precision-Recall Plot Is More Informative}} than the {{ROC Plot When Evaluating Binary Classifiers}} on {{Imbalanced Datasets}}},
  author = {Saito, Takaya and Rehmsmeier, Marc},
  date = {2015-03-04},
  journaltitle = {PLoS ONE},
  shortjournal = {PLoS One},
  volume = {10},
  number = {3},
  eprint = {25738806},
  eprinttype = {pmid},
  pages = {e0118432},
  publisher = {{Public Library of Science}},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0118432},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4349800/},
  urldate = {2022-11-11},
  abstract = {Binary classifiers are routinely evaluated with performance measures such as sensitivity and specificity, and performance is frequently illustrated with Receiver Operating Characteristics (ROC) plots. Alternative measures such as positive predictive value (PPV) and the associated Precision/Recall (PRC) plots are used less frequently. Many bioinformatics studies develop and evaluate classifiers that are to be applied to strongly imbalanced datasets in which the number of negatives outweighs the number of positives significantly. While ROC plots are visually appealing and provide an overview of a classifier's performance across a wide range of specificities, one can ask whether ROC plots could be misleading when applied in imbalanced classification scenarios. We show here that the visual interpretability of ROC plots in the context of imbalanced datasets can be deceptive with respect to conclusions about the reliability of classification performance, owing to an intuitive but wrong interpretation of specificity. PRC plots, on the other hand, can provide the viewer with an accurate prediction of future classification performance due to the fact that they evaluate the fraction of true positives among positive predictions. Our findings have potential implications for the interpretation of a large number of studies that use ROC plots on imbalanced datasets.},
  pmcid = {PMC4349800},
  keywords = {Bioinformatics,Caenorhabditis elegans,Exponential functions,Genome-wide association studies,Interpolation,Measurement,MicroRNAs,Support vector machines},
  file = {/home/skynet3/Zotero/storage/WHRZR8GD/Saito and Rehmsmeier - 2015 - The Precision-Recall Plot Is More Informative than.pdf;/home/skynet3/Zotero/storage/KS7VJBEE/article.html}
}

@misc{salmeronMultiCollPackageDetect2019,
  title = {"{{multiColl}}": {{An R}} Package to Detect Multicollinearity},
  shorttitle = {"{{multiColl}}"},
  author = {Salmerón, Román and García, Catalina and García, José},
  date = {2019-10-31},
  number = {arXiv:1910.14590},
  eprint = {1910.14590},
  eprinttype = {arxiv},
  primaryclass = {stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1910.14590},
  url = {http://arxiv.org/abs/1910.14590},
  urldate = {2022-11-11},
  abstract = {This work presents a guide for the use of some of the functions of the R package "multiColl" for the detection of near multicollinearity. The main contribution, in comparison to other existing packages in R or other econometric software, is the treatment of qualitative independent variables and the intercept in the simple/multiple linear regression model.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Computation,Statistics - Methodology},
  file = {/home/skynet3/Zotero/storage/2H2MVQGZ/Salmerón et al. - 2019 - multiColl An R package to detect multicollinear.pdf;/home/skynet3/Zotero/storage/YHV8J4EI/1910.html}
}

@misc{santurkarHowDoesBatch2019,
  title = {How {{Does Batch Normalization Help Optimization}}?},
  author = {Santurkar, Shibani and Tsipras, Dimitris and Ilyas, Andrew and Madry, Aleksander},
  date = {2019-04-14},
  number = {arXiv:1805.11604},
  eprint = {1805.11604},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1805.11604},
  url = {http://arxiv.org/abs/1805.11604},
  urldate = {2022-11-11},
  abstract = {Batch Normalization (BatchNorm) is a widely adopted technique that enables faster and more stable training of deep neural networks (DNNs). Despite its pervasiveness, the exact reasons for BatchNorm's effectiveness are still poorly understood. The popular belief is that this effectiveness stems from controlling the change of the layers' input distributions during training to reduce the so-called "internal covariate shift". In this work, we demonstrate that such distributional stability of layer inputs has little to do with the success of BatchNorm. Instead, we uncover a more fundamental impact of BatchNorm on the training process: it makes the optimization landscape significantly smoother. This smoothness induces a more predictive and stable behavior of the gradients, allowing for faster training.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/home/skynet3/Zotero/storage/E29RVRY2/Santurkar et al. - 2019 - How Does Batch Normalization Help Optimization.pdf;/home/skynet3/Zotero/storage/MVLLDVQV/1805.html}
}

@article{sariyarRecordLinkagePackageDetecting2010,
  ids = {sariyarRecordLinkagePackageDetecting2010a},
  title = {The {{RecordLinkage Package}}: {{Detecting Errors}} in {{Data}}},
  shorttitle = {The {{RecordLinkage Package}}},
  author = {Sariyar, Murat and Borg, Andreas},
  date = {2010},
  journaltitle = {The R Journal},
  shortjournal = {The R Journal},
  volume = {2},
  number = {2},
  pages = {61},
  issn = {2073-4859},
  doi = {10.32614/RJ-2010-017},
  url = {https://journal.r-project.org/archive/2010/RJ-2010-017/index.html},
  urldate = {2022-11-11},
  abstract = {Record linkage deals with detecting homonyms and mainly synonyms in data. The package RecordLinkage provides means to perform and evaluate different record linkage methods. A stochastic framework is implemented which calculates weights through an EM algorithm. The determination of the necessary thresholds in this model can be achieved by tools of extreme value theory. Furthermore, machine learning methods are utilized, including decision trees (rpart), bootstrap aggregating (bagging), ada boost (ada), neural nets (nnet) and support vector machines (svm). The generation of record pairs and comparison patterns from single data items are provided as well. Comparison patterns can be chosen to be binary or based on some string metrics. In order to reduce computation time and memory usage, blocking can be used. Future development will concentrate on additional and refined methods, performance improvements and input/output facilities needed for real-world application.},
  langid = {english},
  file = {/home/skynet3/Zotero/storage/3XIK294L/Sariyar and Borg - 2010 - The RecordLinkage Package Detecting Errors in Dat.pdf}
}

@inproceedings{sarmaPriorSettingPractice2020,
  ids = {sarmaPriorSettingPractice2020a,sarmaPriorSettingPractice2020b},
  title = {Prior {{Setting}} in {{Practice}}: {{Strategies}} and {{Rationales Used}} in {{Choosing Prior Distributions}} for {{Bayesian Analysis}}},
  shorttitle = {Prior {{Setting}} in {{Practice}}},
  booktitle = {Proceedings of the 2020 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Sarma, Abhraneel and Kay, Matthew},
  date = {2020-04-23},
  series = {{{CHI}} '20},
  pages = {1--12},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/3313831.3376377},
  url = {https://doi.org/10.1145/3313831.3376377},
  urldate = {2022-11-11},
  abstract = {Bayesian statistical analysis is steadily growing in popularity and use. Choosing priors is an integral part of Bayesian inference. While there exist extensive normative recommendations for prior setting, little is known about how priors are chosen in practice. We conducted a survey (N = 50) and interviews (N = 9) where we used interactive visualizations to elicit prior distributions from researchers experienced withBayesian statistics and asked them for rationales for those priors. We found that participants' experience and philosophy influence how much and what information they are willing to incorporate into their priors, manifesting as different levels of informativeness and skepticism. We also identified three broad strategies participants use to set their priors: centrality matching, interval matching, and visual mass allocation. We discovered that participants' understanding of the notion of 'weakly informative priors"-a commonly-recommended normative approach to prior setting-manifests very differently across participants. Our results have implications both for how to develop prior setting recommendations and how to design tools to elicit priors in Bayesian analysis.},
  isbn = {978-1-4503-6708-0},
  keywords = {bayesian inference,descriptive analysis,prior distributions},
  file = {/home/skynet3/Zotero/storage/CCQ863AJ/Sarma and Kay - 2020 - Prior Setting in Practice Strategies and Rational.pdf}
}

@online{SaudiwinIdealstanIdealstan,
  title = {Saudiwin/Idealstan: Idealstan Offers Item-Response Theory ({{IRT}}) Ideal-Point Estimation for Binary, Ordinal, Counts and Continuous Responses with Time-Varying and Missing-Data Inference. {{Latent}} Space Model Also Included. {{Full}} and Approximate {{Bayesian}} Sampling with '{{Stan}}' (Www.Mc-Stan.Org).},
  shorttitle = {Saudiwin/Idealstan},
  url = {https://github.com/saudiwin/idealstan},
  urldate = {2022-11-11},
  abstract = {idealstan offers item-response theory (IRT) ideal-point estimation for binary, ordinal, counts and continuous responses with time-varying and missing-data inference. Latent space model also include...},
  langid = {english},
  organization = {{GitHub}},
  file = {/home/skynet3/Zotero/storage/2RV5TVGH/idealstan.html}
}

@article{savjeGeneralizedFullMatching2021,
  title = {Generalized {{Full Matching}}},
  author = {Sävje, Fredrik and Higgins, Michael J. and Sekhon, Jasjeet S.},
  date = {2021-10},
  journaltitle = {Political Analysis},
  volume = {29},
  number = {4},
  pages = {423--447},
  publisher = {{Cambridge University Press}},
  issn = {1047-1987, 1476-4989},
  doi = {10.1017/pan.2020.32},
  url = {https://www.cambridge.org/core/journals/political-analysis/article/abs/generalized-full-matching/3DA71D8BEDA6F02B5D36457E114C79B6?utm_source=hootsuite&utm_medium=twitter&utm_campaign=PAN_Nov20},
  urldate = {2022-11-11},
  abstract = {Matching is a conceptually straightforward method to make groups of units comparable on observed characteristics. The method is, however, limited to settings where the study design is simple and the sample is moderately sized. We illustrate these limitations by asking what the causal effects would have been if a large-scale voter mobilization experiment that took place in Michigan for the 2006 election were scaled up to the full population of registered voters. Matching could help us answer this question, but no existing matching method can accommodate the six treatment arms and the 6,762,701 observations involved in the study. To offer a solution for this and similar empirical problems, we introduce a generalization of the full matching method that can be used with any number of treatment conditions and complex compositional constraints. The associated algorithm produces near-optimal matchings; the worst-case maximum within-group dissimilarity is guaranteed to be no more than four times greater than the optimal solution, and simulation results indicate that it comes considerably closer to the optimal solution on average. The algorithm’s ability to balance the treatment groups does not sacrifice speed, and it uses little memory, terminating in linearithmic time using linear space. This enables investigators to construct well-performing matchings within minutes even in complex studies with samples of several million units.},
  langid = {english},
  keywords = {causal inference,matching methods,treatment effects}
}

@software{schepGglabeller2022,
  title = {Gglabeller},
  author = {Schep, Alicia},
  date = {2022-10-07T16:55:49Z},
  origdate = {2017-02-21T20:44:52Z},
  url = {https://github.com/AliciaSchep/gglabeller},
  urldate = {2022-11-11},
  abstract = {Shiny gadget for labeling points on ggplot},
  keywords = {ggplot2,labeling-points,plot,r,shiny}
}

@misc{schlosserBiasesHumanMobility2021,
  ids = {schlosserBiasesHumanMobility2021a},
  title = {Biases in Human Mobility Data Impact Epidemic Modeling},
  author = {Schlosser, Frank and Sekara, Vedran and Brockmann, Dirk and Garcia-Herranz, Manuel},
  date = {2021-12-23},
  number = {arXiv:2112.12521},
  eprint = {2112.12521},
  eprinttype = {arxiv},
  primaryclass = {physics, q-bio},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2112.12521},
  url = {http://arxiv.org/abs/2112.12521},
  urldate = {2022-11-11},
  abstract = {Large-scale human mobility data is a key resource in data-driven policy making and across many scientific fields. Most recently, mobility data was extensively used during the COVID-19 pandemic to study the effects of governmental policies and to inform epidemic models. Large-scale mobility is often measured using digital tools such as mobile phones. However, it remains an open question how truthfully these digital proxies represent the actual travel behavior of the general population. Here, we examine mobility datasets from multiple countries and identify two fundamentally different types of bias caused by unequal access to, and unequal usage of mobile phones. We introduce the concept of data generation bias, a previously overlooked type of bias, which is present when the amount of data that an individual produces influences their representation in the dataset. We find evidence for data generation bias in all examined datasets in that high-wealth individuals are overrepresented, with the richest 20\% contributing over 50\% of all recorded trips, substantially skewing the datasets. This inequality is consequential, as we find mobility patterns of different wealth groups to be structurally different, where the mobility networks of high-wealth users are denser and contain more long-range connections. To mitigate the skew, we present a framework to debias data and show how simple techniques can be used to increase representativeness. Using our approach we show how biases can severely impact outcomes of dynamic processes such as epidemic simulations, where biased data incorrectly estimates the severity and speed of disease transmission. Overall, we show that a failure to account for biases can have detrimental effects on the results of studies and urge researchers and practitioners to account for data-fairness in all future studies of human mobility.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computers and Society,Physics - Physics and Society,Quantitative Biology - Populations and Evolution},
  file = {/home/skynet3/Zotero/storage/85TLPB6Y/Schlosser et al. - 2021 - Biases in human mobility data impact epidemic mode.pdf;/home/skynet3/Zotero/storage/CP7VPEAY/2112.html}
}

@misc{schmidtDescendingCrowdedValley2021,
  ids = {schmidtDescendingCrowdedValley2021a},
  title = {Descending through a {{Crowded Valley}} - {{Benchmarking Deep Learning Optimizers}}},
  author = {Schmidt, Robin M. and Schneider, Frank and Hennig, Philipp},
  date = {2021-08-10},
  number = {arXiv:2007.01547},
  eprint = {2007.01547},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2007.01547},
  urldate = {2022-11-11},
  abstract = {Choosing the optimizer is considered to be among the most crucial design decisions in deep learning, and it is not an easy one. The growing literature now lists hundreds of optimization methods. In the absence of clear theoretical guidance and conclusive empirical evidence, the decision is often made based on anecdotes. In this work, we aim to replace these anecdotes, if not with a conclusive ranking, then at least with evidence-backed heuristics. To do so, we perform an extensive, standardized benchmark of fifteen particularly popular deep learning optimizers while giving a concise overview of the wide range of possible choices. Analyzing more than \$50,000\$ individual runs, we contribute the following three points: (i) Optimizer performance varies greatly across tasks. (ii) We observe that evaluating multiple optimizers with default parameters works approximately as well as tuning the hyperparameters of a single, fixed optimizer. (iii) While we cannot discern an optimization method clearly dominating across all tested tasks, we identify a significantly reduced subset of specific optimizers and parameter choices that generally lead to competitive results in our experiments: Adam remains a strong contender, with newer methods failing to significantly and consistently outperform it. Our open-sourced results are available as challenging and well-tuned baselines for more meaningful evaluations of novel optimization methods without requiring any further computational efforts.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/skynet3/Zotero/storage/JJRKRK5A/Schmidt et al. - 2021 - Descending through a Crowded Valley - Benchmarking.pdf;/home/skynet3/Zotero/storage/QD6EXX3W/2007.html}
}

@misc{schomakerRegressionCausality2021,
  title = {Regression and {{Causality}}},
  author = {Schomaker, Michael},
  date = {2021-03-04},
  number = {arXiv:2006.11754},
  eprint = {2006.11754},
  eprinttype = {arxiv},
  primaryclass = {math, stat},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2006.11754},
  urldate = {2022-11-11},
  abstract = {The causal effect of an intervention (treatment/exposure) on an outcome can be estimated by: i) specifying knowledge about the data-generating process; ii) assessing under what assumptions a target quantity, such as for example a causal odds ratio, can be identified given the specified knowledge (and given the measured data); and then, iii) using appropriate statistical estimation techniques to estimate the desired parameter of interest. As regression is the cornerstone of statistical analysis, it seems obvious to ask: is it appropriate to use estimated regression parameters for causal effect estimation? It turns out that using regression for effect estimation is possible, but typically requires more assumptions than competing methods. This manuscript provides a comprehensive summary of the assumptions needed to identify and estimate a causal parameter using regression and, equally important, discusses the resulting implications for statistical practice.},
  archiveprefix = {arXiv},
  keywords = {Mathematics - Statistics Theory,Statistics - Methodology},
  file = {/home/skynet3/Zotero/storage/RN7M48SE/Schomaker - 2021 - Regression and Causality.pdf;/home/skynet3/Zotero/storage/7B3RYMT5/2006.html}
}

@article{schoutenDanceMechanismsHow2021,
  title = {The {{Dance}} of the {{Mechanisms}}: {{How Observed Information Influences}} the {{Validity}} of {{Missingness Assumptions}}},
  shorttitle = {The {{Dance}} of the {{Mechanisms}}},
  author = {Schouten, Rianne Margaretha and Vink, Gerko},
  date = {2021-08-01},
  journaltitle = {Sociological Methods \& Research},
  volume = {50},
  number = {3},
  pages = {1243--1258},
  publisher = {{SAGE Publications Inc}},
  issn = {0049-1241},
  doi = {10.1177/0049124118799376},
  url = {https://doi.org/10.1177/0049124118799376},
  urldate = {2022-11-11},
  abstract = {Missing data in scientific research go hand in hand with assumptions about the nature of the missingness. When dealing with missing values, a set of beliefs has to be formulated about the extent to which the observed data may also hold for the missing parts of the data. It is vital that the validity of these missingness assumptions is verified, tested, and that assumptions are adjusted when necessary. In this article, we demonstrate how observed data structures could a priori indicate whether it is likely that our beliefs about the missingness can be trusted. To this end, we simulate complete data and generate missing values according several types of MCAR, MAR, and MNAR mechanisms. We demonstrate that in scenarios where the data correlations are either low or very substantial, strictly different mechanisms yield equivalent statistical inferences. In addition, we show that the choice of quantity of scientific interest together with the distribution of the nonresponse govern the validity of the missingness assumptions.},
  langid = {english},
  file = {/home/skynet3/Zotero/storage/NX4GJ8HF/Schouten and Vink - 2021 - The Dance of the Mechanisms How Observed Informat.pdf}
}

@article{schultzTemperatureExogenousImpact2019,
  title = {Is {{Temperature Exogenous}}? {{The Impact}} of {{Civil Conflict}} on the {{Instrumental Climate Record}} in {{Sub-Saharan Africa}}},
  shorttitle = {Is {{Temperature Exogenous}}?},
  author = {Schultz, Kenneth A. and Mankin, Justin S.},
  date = {2019},
  journaltitle = {American Journal of Political Science},
  volume = {63},
  number = {4},
  pages = {723--739},
  issn = {1540-5907},
  doi = {10.1111/ajps.12425},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/ajps.12425},
  urldate = {2022-11-11},
  abstract = {Research into the effects of climate on political and economic outcomes assumes that short-term variation in weather is exogenous to the phenomena being studied. However, weather data are derived from stations operated by national governments, whose political capacity and stability affect the quality and continuity of coverage. We show that civil conflict risk in sub-Saharan Africa is negatively correlated with the number and density of weather stations contributing to a country's temperature record. This effect is both cross-sectional—countries with higher average conflict risk tend to have poorer coverage—and cross-temporal—civil conflict leads to loss of weather stations. Poor coverage induces a small downward bias in one widely used temperature data set, due to its interpolation method, and increases measurement error, potentially attenuating estimates of the temperature–conflict relationship. Combining multiple observational data sets to reduce measurement error almost doubles the estimated effect of temperature anomalies on civil conflict risk.},
  langid = {english},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/ajps.12425},
  file = {/home/skynet3/Zotero/storage/22AMAQJC/ajps.html}
}

@misc{schwemmerMethodologicalDivideSociology2018,
  title = {The {{Methodological Divide}} of {{Sociology}} - {{Evidence From Two Decades}} of {{Journal Publications}}},
  author = {Schwemmer, Carsten and Wieczorek, Oliver},
  date = {2018-09-09T14:45:33},
  publisher = {{SocArXiv}},
  doi = {10.31235/osf.io/s59bp},
  url = {https://osf.io/preprints/socarxiv/s59bp/},
  urldate = {2022-11-11},
  abstract = {Past research indicates that Sociology is a low-consensus discipline, where different schools of thought have distinct expectations about suitable scientific practices. This division of Sociology into different subfields is to a large extent related to methodology and choices between qualitative or quantitative research methods. Relying on theoretical constructs of the academic prestige economy, boundary demarcation, and taste for research, we examine the methodological divide in generalist Sociology journals. Using automated text analysis for 8,737 abstracts of papers published between 1995 and 2017, we discover evidence of this divide, but also of an entanglement between methodological choices and different research topics. Moreover, our results suggest a marginally increasing time trend for the publication of quantitative research in generalist journals. We discuss how this consolidation of methodological practices could enforce the entrenchment of different schools of thought, which ultimately reduces the potential for innovative and effective sociological research.},
  langid = {american},
  keywords = {and Technology,Knowledge,research methodology,Science,scientometrics,Social and Behavioral Sciences,Sociology,sociology of science,symbolic boundaries},
  file = {/home/skynet3/Zotero/storage/3SFK65W4/Schwemmer and Wieczorek - 2018 - The Methodological Divide of Sociology - Evidence .pdf}
}

@online{ScikitsurvivalScikitsurvival19,
  title = {Scikit-Survival — Scikit-Survival 0.19.0},
  url = {https://scikit-survival.readthedocs.io/en/latest/index.html},
  urldate = {2022-11-11},
  file = {/home/skynet3/Zotero/storage/2KADJINF/index.html}
}

@misc{scogginsTrustUsOpen2022,
  title = {‘{{Trust Us}}’: {{Open Data}} and {{Preregistration}} in {{Political Science}} and {{International Relations}}},
  shorttitle = {‘{{Trust Us}}’},
  author = {Scoggins, Bermond and Robertson, Matthew Peter},
  date = {2022-01-16T08:38:22},
  publisher = {{MetaArXiv}},
  doi = {10.31222/osf.io/8h2bp},
  url = {https://osf.io/preprints/metaarxiv/8h2bp/},
  urldate = {2022-11-11},
  abstract = {The scientific method is predicated on transparency -- yet the pace at which transparent research practices are being adopted by the scientific community is slow. The replication crisis in psychology showed that published findings employing statistical inference are threatened by undetected errors, data manipulation, and data falsification. To mitigate these problems and bolster research credibility, open data and preregistration have increasingly been adopted in the natural and social sciences. While many political science and international relations journals have committed to implementing these reforms, the extent of open science practices is unknown. We bring large-scale text analysis and machine learning classifiers to bear on the question. Using population-level data -- 93,931 articles across the top 160 political science and IR journals between 2010 and 2021 -- we find that approximately 21\% of all statistical inference papers have open data, and 5\% of all experiments are preregistered. Despite this shortfall, the example of leading journals in the field shows that change is feasible and can be effected quickly.},
  langid = {american},
  keywords = {experiment,metascience,Models and Methods,open data,open science,political science,Political Science,preregistration,replication,reproducibility,research design,Social and Behavioral Sciences,statistical inference},
  file = {/home/skynet3/Zotero/storage/6CKGPCR5/Scoggins and Robertson - 2022 - ‘Trust Us’ Open Data and Preregistration in Polit.pdf}
}

@article{scottBayesBigData2016,
  ids = {scottBayesBigData2016a},
  title = {Bayes and Big Data: The Consensus {{Monte Carlo}} Algorithm},
  shorttitle = {Bayes and Big Data},
  author = {Scott, Steven L. and Blocker, Alexander W. and Bonassi, Fernando V. and Chipman, Hugh A. and George, Edward I. and McCulloch, Robert E.},
  date = {2016-04-02},
  journaltitle = {International Journal of Management Science and Engineering Management},
  volume = {11},
  number = {2},
  pages = {78--88},
  publisher = {{Taylor \& Francis}},
  issn = {1750-9653},
  doi = {10.1080/17509653.2016.1142191},
  url = {https://orsociety.tandfonline.com/doi/full/10.1080/17509653.2016.1142191},
  urldate = {2022-11-11},
  abstract = {A useful definition of ‘big data’ is data that is too big to process comfortably on a single machine, either because of processor, memory, or disk bottlenecks. Graphics processing units can alleviate the processor bottleneck, but memory or disk bottlenecks can only be eliminated by splitting data across multiple machines. Communication between large numbers of machines is expensive (regardless of the amount of data being communicated), so there is a need for algorithms that perform distributed approximate Bayesian analyses with minimal communication. Consensus Monte Carlo operates by running a separate Monte Carlo algorithm on each machine, and then averaging individual Monte Carlo draws across machines. Depending on the model, the resulting draws can be nearly indistinguishable from the draws that would have been obtained by running a single-machine algorithm for a very long time. Examples of consensus Monte Carlo are shown for simple models where single-machine solutions are available, for large single-layer hierarchical models, and for Bayesian additive regression trees (BART).},
  keywords = {Bayesian inference,big data,C1,C4,C6,C8,distributed computing,embarrassingly parallel,Markov chain Monte Carlo},
  file = {/home/skynet3/Zotero/storage/MZAUWB7K/Scott et al. - 2016 - Bayes and big data the consensus Monte Carlo algo.pdf}
}

@article{sculleyPACEPROGRESSEMPIRICAL2018,
  ids = {sculleyPACEPROGRESSEMPIRICAL2018a},
  title = {{{ON PACE}}, {{PROGRESS}}, {{AND EMPIRICAL RIGOR}}},
  author = {Sculley, D and Snoek, Jasper and Rahimi, Ali and Wiltschko, Alex},
  date = {2018},
  pages = {4},
  abstract = {The field of ML is distinguished both by rapid innovation and rapid dissemination of results. While the pace of progress has been extraordinary by any measure, in this paper we explore potential issues that we believe to be arising as a result. In particular, we observe that the rate of empirical advancement may not have been matched by consistent increase in the level of empirical rigor across the field as a whole. This short position paper highlights examples where progress has actually been slowed as a result, offers thoughts on incentive structures currently at play, and gives suggestions as seeds for discussions on productive change.},
  langid = {english},
  file = {/home/skynet3/Zotero/storage/6D4FND74/Sculley et al. - 2018 - ON PACE, PROGRESS, AND EMPIRICAL RIGOR.pdf}
}

@online{SebastianruderNLPprogressRepository,
  title = {Sebastianruder/{{NLP-progress}}: {{Repository}} to Track the Progress in {{Natural Language Processing}} ({{NLP}}), Including the Datasets and the Current State-of-the-Art for the Most Common {{NLP}} Tasks.},
  shorttitle = {Sebastianruder/{{NLP-progress}}},
  url = {https://github.com/sebastianruder/NLP-progress},
  urldate = {2022-11-11},
  abstract = {Repository to track the progress in Natural Language Processing (NLP), including the datasets and the current state-of-the-art for the most common NLP tasks. - sebastianruder/NLP-progress: Reposito...},
  langid = {english},
  organization = {{GitHub}},
  file = {/home/skynet3/Zotero/storage/24TG7S7R/NLP-progress.html}
}

@online{SebKrantzCollapseAdvanced,
  title = {{{SebKrantz}}/Collapse: {{Advanced}} and {{Fast Data Transformation}} in {{R}}},
  shorttitle = {{{SebKrantz}}/Collapse},
  url = {https://github.com/SebKrantz/collapse},
  urldate = {2022-11-11},
  abstract = {Advanced and Fast Data Transformation in R. Contribute to SebKrantz/collapse development by creating an account on GitHub.},
  langid = {english},
  organization = {{GitHub}},
  file = {/home/skynet3/Zotero/storage/JRZWU4JN/collapse.html}
}

@article{seiboldComputationalReproducibilityStudy2021,
  title = {A Computational Reproducibility Study of {{PLOS ONE}} Articles Featuring Longitudinal Data Analyses},
  author = {Seibold, Heidi and Czerny, Severin and Decke, Siona and Dieterle, Roman and Eder, Thomas and Fohr, Steffen and Hahn, Nico and Hartmann, Rabea and Heindl, Christoph and Kopper, Philipp and Lepke, Dario and Loidl, Verena and Mandl, Maximilian and Musiol, Sarah and Peter, Jessica and Piehler, Alexander and Rojas, Elio and Schmid, Stefanie and Schmidt, Hannah and Schmoll, Melissa and Schneider, Lennart and To, Xiao-Yin and Tran, Viet and Völker, Antje and Wagner, Moritz and Wagner, Joshua and Waize, Maria and Wecker, Hannah and Yang, Rui and Zellner, Simone and Nalenz, Malte},
  date = {2021-06-21},
  journaltitle = {PLOS ONE},
  shortjournal = {PLOS ONE},
  volume = {16},
  number = {6},
  pages = {e0251194},
  publisher = {{Public Library of Science}},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0251194},
  url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0251194},
  urldate = {2022-11-11},
  abstract = {Computational reproducibility is a corner stone for sound and credible research. Especially in complex statistical analyses—such as the analysis of longitudinal data—reproducing results is far from simple, especially if no source code is available. In this work we aimed to reproduce analyses of longitudinal data of 11 articles published in PLOS ONE. Inclusion criteria were the availability of data and author consent. We investigated the types of methods and software used and whether we were able to reproduce the data analysis using open source software. Most articles provided overview tables and simple visualisations. Generalised Estimating Equations (GEEs) were the most popular statistical models among the selected articles. Only one article used open source software and only one published part of the analysis code. Replication was difficult in most cases and required reverse engineering of results or contacting the authors. For three articles we were not able to reproduce the results, for another two only parts of them. For all but two articles we had to contact the authors to be able to reproduce the results. Our main learning is that reproducing papers is difficult if no code is supplied and leads to a high burden for those conducting the reproductions. Open data policies in journals are good, but to truly boost reproducibility we suggest adding open code policies.},
  langid = {english},
  keywords = {Alcohol consumption,Computer software,Marijuana,Open source software,Preprocessing,Reproducibility,Source code,Statistical models},
  file = {/home/skynet3/Zotero/storage/9W7MHNDB/Seibold et al. - 2021 - A computational reproducibility study of PLOS ONE .pdf}
}

@article{sendorCoreConceptsPharmacoepidemiology2022,
  title = {Core Concepts in Pharmacoepidemiology: {{Confounding}} by Indication and the Role of Active Comparators},
  shorttitle = {Core Concepts in Pharmacoepidemiology},
  author = {Sendor, Rachel and Stürmer, Til},
  date = {2022},
  journaltitle = {Pharmacoepidemiology and Drug Safety},
  volume = {31},
  number = {3},
  pages = {261--269},
  issn = {1099-1557},
  doi = {10.1002/pds.5407},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/pds.5407},
  urldate = {2022-11-11},
  abstract = {Confounding by indication poses a significant threat to the validity of nonexperimental studies assessing effectiveness and safety of medical interventions. While no different from other forms of confounding in theory, confounding by indication often requires specific methods to address the bias it creates in addition to common epidemiological adjustment or restriction methods. Clinical indication influencing treatment prescription is patient-specific and complex, making it challenging to measure within nonexperimental research. Restriction of the study population to patients with the indication for treatment would effectively mitigate confounding by indication and bring about comparability between exposure and comparator populations with respect to probability of the exposure. Active comparators are often an effective practical solution to restrict the study population in this manner when indication cannot be measured accurately. This article discusses various forms of confounding by indication, the utility of active comparators for nonexperimental studies of treatment effects, and the active comparator, new user (ACNU) study design to implicitly condition on indication. Considerations for selecting active comparators and conducting an ACNU study design are discussed to enable increased adoption of these methods, improve quality of nonexperimental studies, and ultimately strengthen our evidence base for intended and unintended treatment effects in relevant target populations.},
  langid = {english},
  keywords = {bias,confounding,epidemiology,nonexperimental studies,pharmacoepidemiology,study design,therapy},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/pds.5407},
  file = {/home/skynet3/Zotero/storage/B93Y7Q48/pds.html}
}

@online{september24th2019TensorflowPitfallsAILab,
  title = {Tensorflow 2.0 {{Pitfalls}} - {{AILab Blog}}},
  author = {September 24th 2019, JENS JOHANNSMEIER AILab OVGU},
  url = {http://blog.ai.ovgu.de/posts/jens/2019/001_tf20_pitfalls/index.html},
  urldate = {2022-11-11},
  langid = {english}
}

@online{Sequence,
  title = {Sequence},
  url = {https://planetmath.org/sequence},
  urldate = {2022-11-11}
}

@inreference{Sequence2022,
  title = {Sequence},
  booktitle = {Wikipedia},
  date = {2022-11-03T14:55:53Z},
  url = {https://en.wikipedia.org/w/index.php?title=Sequence&oldid=1119810149},
  urldate = {2022-11-11},
  abstract = {In mathematics, a sequence is an enumerated collection of objects in which repetitions are allowed and order matters. Like a set, it contains members (also called elements, or terms). The number of elements (possibly infinite) is called the length of the sequence. Unlike a set, the same elements can appear multiple times at different positions in a sequence, and unlike a set, the order does matter. Formally, a sequence can be defined as a function from natural numbers (the positions of elements in the sequence) to the elements at each position. The notion of a sequence can be generalized to an indexed family, defined as a function from an arbitrary index set. For example, (M, A, R, Y) is a sequence of letters with the letter 'M' first and 'Y' last. This sequence differs from (A, R, M, Y). Also, the sequence (1, 1, 2, 3, 5, 8), which contains the number 1 at two different positions, is a valid sequence. Sequences can be finite, as in these examples, or infinite, such as the sequence of all even positive integers (2, 4, 6, ...). The position of an element in a sequence is its rank or index; it is the natural number for which the element is the image. The first element has index 0 or 1, depending on the context or a specific convention. In mathematical analysis, a sequence is often denoted by letters in the form of                                    a                        n                                     \{\textbackslash displaystyle a\_\{n\}\}   ,                                    b                        n                                     \{\textbackslash displaystyle b\_\{n\}\}    and                                    c                        n                                     \{\textbackslash displaystyle c\_\{n\}\}   , where the subscript n refers to the nth element of the sequence; for example, the nth element of the Fibonacci sequence                         F                 \{\textbackslash displaystyle F\}    is generally denoted as                                    F                        n                                     \{\textbackslash displaystyle F\_\{n\}\}   . In computing and computer science, finite sequences are sometimes called strings, words or lists, the different names commonly corresponding to different ways to represent them in computer memory; infinite sequences are called streams. The empty sequence ( ) is included in most notions of sequence, but may be excluded depending on the context.},
  langid = {english},
  annotation = {Page Version ID: 1119810149}
}

@inreference{Sequence2022a,
  title = {Sequence},
  booktitle = {Wikipedia},
  date = {2022-11-03T14:55:53Z},
  url = {https://en.wikipedia.org/w/index.php?title=Sequence&oldid=1119810149},
  urldate = {2022-11-11},
  abstract = {In mathematics, a sequence is an enumerated collection of objects in which repetitions are allowed and order matters. Like a set, it contains members (also called elements, or terms). The number of elements (possibly infinite) is called the length of the sequence. Unlike a set, the same elements can appear multiple times at different positions in a sequence, and unlike a set, the order does matter. Formally, a sequence can be defined as a function from natural numbers (the positions of elements in the sequence) to the elements at each position. The notion of a sequence can be generalized to an indexed family, defined as a function from an arbitrary index set. For example, (M, A, R, Y) is a sequence of letters with the letter 'M' first and 'Y' last. This sequence differs from (A, R, M, Y). Also, the sequence (1, 1, 2, 3, 5, 8), which contains the number 1 at two different positions, is a valid sequence. Sequences can be finite, as in these examples, or infinite, such as the sequence of all even positive integers (2, 4, 6, ...). The position of an element in a sequence is its rank or index; it is the natural number for which the element is the image. The first element has index 0 or 1, depending on the context or a specific convention. In mathematical analysis, a sequence is often denoted by letters in the form of                                    a                        n                                     \{\textbackslash displaystyle a\_\{n\}\}   ,                                    b                        n                                     \{\textbackslash displaystyle b\_\{n\}\}    and                                    c                        n                                     \{\textbackslash displaystyle c\_\{n\}\}   , where the subscript n refers to the nth element of the sequence; for example, the nth element of the Fibonacci sequence                         F                 \{\textbackslash displaystyle F\}    is generally denoted as                                    F                        n                                     \{\textbackslash displaystyle F\_\{n\}\}   . In computing and computer science, finite sequences are sometimes called strings, words or lists, the different names commonly corresponding to different ways to represent them in computer memory; infinite sequences are called streams. The empty sequence ( ) is included in most notions of sequence, but may be excluded depending on the context.},
  langid = {english},
  annotation = {Page Version ID: 1119810149},
  file = {/home/skynet3/Zotero/storage/F4CSZV2Z/Sequence.html}
}

@online{SequenceEncyclopediaMathematics,
  ids = {SequenceEncyclopediaMathematicsa},
  title = {Sequence - {{Encyclopedia}} of {{Mathematics}}},
  url = {https://encyclopediaofmath.org/index.php?title=Sequence&oldid=48671},
  urldate = {2022-11-11},
  file = {/home/skynet3/Zotero/storage/8QGKS2HA/index.html}
}

@online{Set,
  ids = {Seta},
  title = {Set},
  url = {https://www.wikidata.org/wiki/Q36161},
  urldate = {2022-11-11},
  abstract = {well-defined mathematical collection of distinct objects},
  langid = {english},
  file = {/home/skynet3/Zotero/storage/L5GKTWCQ/set.html}
}

@online{SetMathematicsWikipedia,
  ids = {SetMathematicsWikipediaa},
  title = {Set (Mathematics)) - {{Wikipedia}}},
  url = {https://en.wikipedia.org/wiki/Set_(mathematics))},
  urldate = {2022-11-11},
  langid = {english},
  file = {/home/skynet3/Zotero/storage/YDQ33PRL/Set_(mathematics))\;.html}
}

@online{SfirkeJanitorSimple,
  title = {Sfirke/Janitor: Simple Tools for Data Cleaning in {{R}}},
  shorttitle = {Sfirke/Janitor},
  url = {https://github.com/sfirke/janitor},
  urldate = {2022-11-11},
  abstract = {simple tools for data cleaning in R. Contribute to sfirke/janitor development by creating an account on GitHub.},
  langid = {english},
  organization = {{GitHub}},
  file = {/home/skynet3/Zotero/storage/4ESD8XW2/janitor.html}
}

@article{shahzadQualityReportingRandomised2022,
  ids = {shahzadQualityReportingRandomised2022a},
  title = {Quality of Reporting of Randomised Controlled Trials of Artificial Intelligence in Healthcare: A Systematic Review},
  shorttitle = {Quality of Reporting of Randomised Controlled Trials of Artificial Intelligence in Healthcare},
  author = {Shahzad, Rida and Ayub, Bushra and Siddiqui, M. A. Rehman},
  date = {2022-09-01},
  journaltitle = {BMJ Open},
  volume = {12},
  number = {9},
  pages = {e061519},
  publisher = {{British Medical Journal Publishing Group}},
  issn = {2044-6055, 2044-6055},
  doi = {10.1136/bmjopen-2022-061519},
  url = {https://bmjopen.bmj.com/content/12/9/e061519},
  urldate = {2022-11-11},
  abstract = {Objectives The aim of this study was to evaluate the quality of reporting of randomised controlled trials (RCTs) of artificial intelligence (AI) in healthcare against Consolidated Standards of Reporting Trials—AI (CONSORT-AI) guidelines. Design Systematic review. Data sources We searched PubMed and EMBASE databases for studies reported from January 2015 to December 2021. Eligibility criteria We included RCTs reported in English that used AI as the intervention. Protocols, conference abstracts, studies on robotics and studies related to medical education were excluded. Data extraction The included studies were graded using the CONSORT-AI checklist, comprising 43 items, by two independent graders. The results were tabulated and descriptive statistics were reported. Results We screened 1501 potential abstracts, of which 112 full-text articles were reviewed for eligibility. A total of 42 studies were included. The number of participants ranged from 22 to 2352. Only two items of the CONSORT-AI items were fully reported in all studies. Five items were not applicable in more than 85\% of the studies. Nineteen per cent (8/42) of the studies did not report more than 50\% (21/43) of the CONSORT-AI checklist items. Conclusions The quality of reporting of RCTs in AI is suboptimal. As reporting is variable in existing RCTs, caution should be exercised in interpreting the findings of some studies.},
  langid = {english},
  keywords = {clinical trials,health informatics,statistics & research methods},
  file = {/home/skynet3/Zotero/storage/V5GAM9C8/Shahzad et al. - 2022 - Quality of reporting of randomised controlled tria.pdf;/home/skynet3/Zotero/storage/RUXPCQ7H/e061519.html}
}

@online{SHAPLIMEPython,
  title = {{{SHAP}} and {{LIME Python Libraries}} - {{Using SHAP}} \& {{LIME}} with {{XGBoost}}},
  url = {https://www.dominodatalab.com/blog/shap-lime-python-libraries-part-2-using-shap-lime},
  urldate = {2022-11-11},
  abstract = {We provides insights on how to use the SHAP and LIME Python libraries, how to interpret their output, and how to prepare for producing model explanations.},
  langid = {english}
}

@article{shiffrinScientificProgressIrreproducibility2018,
  title = {Scientific Progress despite Irreproducibility: {{A}} Seeming Paradox},
  shorttitle = {Scientific Progress despite Irreproducibility},
  author = {Shiffrin, Richard M. and Borner, Katy and Stigler, Stephen M.},
  date = {2018-03-13},
  journaltitle = {Proceedings of the National Academy of Sciences},
  shortjournal = {Proc. Natl. Acad. Sci. U.S.A.},
  volume = {115},
  number = {11},
  eprint = {1710.01946},
  eprinttype = {arxiv},
  primaryclass = {stat},
  pages = {2632--2639},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1711786114},
  url = {http://arxiv.org/abs/1710.01946},
  urldate = {2022-11-11},
  abstract = {It appears paradoxical that science is producing outstanding new results and theories at a rapid rate at the same time that researchers are identifying serious problems in the practice of science that cause many reports to be irreproducible and invalid. Certainly the practice of science needs to be improved and scientists are now pursuing this goal. However, in this perspective we argue that this seeming paradox is not new, has always been part of the way science works, and likely will remain so. We first introduce the paradox. We then review a wide range of challenges that appear to make scientific success difficult. Next, we describe the factors that make science work-in the past, present, and presumably also in the future. We then suggest that remedies for the present practice of science need to be applied selectively so as not to slow progress, and illustrate with a few examples. We conclude with arguments that communication of science needs to emphasize not just problems but the enormous successes and benefits that science has brought and is now bringing to all elements of modern society.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Other Statistics},
  file = {/home/skynet3/Zotero/storage/AGK2H9GG/Shiffrin et al. - 2018 - Scientific progress despite irreproducibility A s.pdf;/home/skynet3/Zotero/storage/3S6996UI/1710.html}
}

@article{shiSelectiveReviewNegative2020,
  title = {A {{Selective Review}} of {{Negative Control Methods}} in {{Epidemiology}}},
  author = {Shi, Xu and Miao, Wang and Tchetgen, Eric Tchetgen},
  date = {2020-12},
  journaltitle = {Current epidemiology reports},
  shortjournal = {Curr Epidemiol Rep},
  volume = {7},
  number = {4},
  eprint = {33996381},
  eprinttype = {pmid},
  pages = {190--202},
  issn = {2196-2995},
  doi = {10.1007/s40471-020-00243-4},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8118596/},
  urldate = {2022-11-11},
  abstract = {Purpose of Review Negative controls are a powerful tool to detect and adjust for bias in epidemiological research. This paper introduces negative controls to a broader audience and provides guidance on principled design and causal analysis based on a formal negative control framework. Recent Findings We review and summarize causal and statistical assumptions, practical strategies, and validation criteria that can be combined with subject-matter knowledge to perform negative control analyses. We also review existing statistical methodologies for the detection, reduction, and correction of confounding bias, and briefly discuss recent advances towards nonparametric identification of causal effects in a double-negative control design. Summary There is great potential for valid and accurate causal inference leveraging contemporary healthcare data in which negative controls are routinely available. Design and analysis of observational data leveraging negative controls is an area of growing interest in health and social sciences. Despite these developments, further effort is needed to disseminate these novel methods to ensure they are adopted by practicing epidemiologists.},
  pmcid = {PMC8118596},
  file = {/home/skynet3/Zotero/storage/ASQW3HZ5/Shi et al. - 2020 - A Selective Review of Negative Control Methods in .pdf}
}

@article{shouCdfquantregPackageCDFQuantile2019,
  title = {Cdfquantreg: {{An R Package}} for {{CDF-Quantile Regression}}},
  shorttitle = {Cdfquantreg},
  author = {Shou, Yiyun and Smithson, Michael},
  date = {2019-01-29},
  journaltitle = {Journal of Statistical Software},
  volume = {88},
  pages = {1--30},
  issn = {1548-7660},
  doi = {10.18637/jss.v088.i01},
  url = {https://doi.org/10.18637/jss.v088.i01},
  urldate = {2022-11-11},
  abstract = {The CDF-quantile family of two-parameter distributions with support (0, 1) described in Smithson and Merkle (2014) and recently elaborated by Smithson and Shou (2017), considerably expands the variety of distributions available for modeling random variables on the unit interval. This family is especially useful for modeling quantiles, and also sometimes out-performs the other distributions. The distributions are very tractable, with a location and dispersion parameter, explicit probability distribution functions, cumulative distribution functions, and quantiles. They enable a wide variety of quantile regression models with predictors for the location and dispersion parameters, and simple interpretations of those parameters. The R package cdfquantreg (Shou and Smithson 2019) (at least R 3.2.0) presented in this paper includes 36 distributions from the CDF-quantile family. Separate submodels may be specified for the location and for the dispersion parameters, with different or overlapping sets of predictors in each. The package offers maximum likelihood, Bayesian MCMC, and bootstrap estimation methods. Model diagnostics, including the gradient, three types of residuals, and the dfbeta influence measures, are available for evaluating models. The package also provides pseudo-random generators for all of its distributions. Many of its functions and their usage have forms familiar to R users, and the documentation is extensive. We also present a SAS macro for general linear models using the CDF-quantile family that includes many of the same capabilities as the cdfquantreg package. The paper provides examples of applications to real data-sets.},
  langid = {english},
  keywords = {distribution,quantile regression,R,unit interval},
  file = {/home/skynet3/Zotero/storage/Q5YIK2LY/Shou and Smithson - 2019 - cdfquantreg An R Package for CDF-Quantile Regress.pdf}
}

@article{shwedTemporalStructureScientific2010,
  ids = {shwedTemporalStructureScientific2010a},
  title = {The {{Temporal Structure}} of {{Scientific Consensus Formation}}},
  author = {Shwed, Uri and Bearman, Peter S.},
  date = {2010-12-01},
  journaltitle = {American Sociological Review},
  shortjournal = {Am Sociol Rev},
  volume = {75},
  number = {6},
  pages = {817--840},
  publisher = {{SAGE Publications Inc}},
  issn = {0003-1224},
  doi = {10.1177/0003122410388488},
  url = {https://doi.org/10.1177/0003122410388488},
  urldate = {2022-11-11},
  abstract = {This article engages with problems that are usually opaque: What trajectories do scientific debates assume, when does a scientific community consider a proposition to be a fact, and how can we know that? We develop a strategy for evaluating the state of scientific contestation on issues. The analysis builds from Latour?s black box imagery, which we observe in scientific citation networks. We show that as consensus forms, the importance of internal divisions to the overall network structure declines. We consider substantive cases that are now considered facts, such as the carcinogenicity of smoking and the non-carcinogenicity of coffee. We then employ the same analysis to currently contested cases: the suspected carcinogenicity of cellular phones, and the relationship between vaccines and autism. Extracting meaning from the internal structure of scientific knowledge carves a niche for renewed sociological commentary on science, revealing a typology of trajectories that scientific propositions may experience en route to consensus.},
  langid = {english},
  file = {/home/skynet3/Zotero/storage/RBKKXZIK/Shwed and Bearman - 2010 - The Temporal Structure of Scientific Consensus For.pdf}
}

@online{SignYourAccount,
  ids = {SignYourAccounta},
  title = {Sign in to Your Account},
  url = {https://login.microsoftonline.com/f1502c4c-ee2e-411c-9715-c855f6753b84/oauth2/authorize?client%5Fid=00000003%2D0000%2D0ff1%2Dce00%2D000000000000&response%5Fmode=form%5Fpost&response%5Ftype=code%20id%5Ftoken&resource=00000003%2D0000%2D0ff1%2Dce00%2D000000000000&scope=openid&nonce=9E2FBADBE675A3E14452C17886497C2A037358B847D5FC7A%2D48AE2448E086CAE93A297457F214563569E779ABAA136593D10263B9C5EF6F17&redirect%5Furi=https%3A%2F%2Ftechnionmail%2Dmy%2Esharepoint%2Ecom%2F%5Fforms%2Fdefault%2Easpx&state=OD0w&claims=%7B%22id%5Ftoken%22%3A%7B%22xms%5Fcc%22%3A%7B%22values%22%3A%5B%22CP1%22%5D%7D%7D%7D&wsucxt=1&cobrandid=11bd8083%2D87e0%2D41b5%2Dbb78%2D0bc43c8a8e8a&client%2Drequest%2Did=80b077a0%2D10c0%2D5000%2D77ee%2D602f1f73e816},
  urldate = {2022-11-11},
  file = {/home/skynet3/Zotero/storage/3Y3P2PNV/authorize.html}
}

@article{silberzahnManyAnalystsOne2018,
  ids = {silberzahnManyAnalystsOne2018a},
  title = {Many {{Analysts}}, {{One Data Set}}: {{Making Transparent How Variations}} in {{Analytic Choices Affect Results}}},
  shorttitle = {Many {{Analysts}}, {{One Data Set}}},
  author = {Silberzahn, R. and Uhlmann, E. L. and Martin, D. P. and Anselmi, P. and Aust, F. and Awtrey, E. and Bahník, Š. and Bai, F. and Bannard, C. and Bonnier, E. and Carlsson, R. and Cheung, F. and Christensen, G. and Clay, R. and Craig, M. A. and Dalla Rosa, A. and Dam, L. and Evans, M. H. and Flores Cervantes, I. and Fong, N. and Gamez-Djokic, M. and Glenz, A. and Gordon-McKeon, S. and Heaton, T. J. and Hederos, K. and Heene, M. and Hofelich Mohr, A. J. and Högden, F. and Hui, K. and Johannesson, M. and Kalodimos, J. and Kaszubowski, E. and Kennedy, D. M. and Lei, R. and Lindsay, T. A. and Liverani, S. and Madan, C. R. and Molden, D. and Molleman, E. and Morey, R. D. and Mulder, L. B. and Nijstad, B. R. and Pope, N. G. and Pope, B. and Prenoveau, J. M. and Rink, F. and Robusto, E. and Roderique, H. and Sandberg, A. and Schlüter, E. and Schönbrodt, F. D. and Sherman, M. F. and Sommer, S. A. and Sotak, K. and Spain, S. and Spörlein, C. and Stafford, T. and Stefanutti, L. and Tauber, S. and Ullrich, J. and Vianello, M. and Wagenmakers, E.-J. and Witkowiak, M. and Yoon, S. and Nosek, B. A.},
  date = {2018-09-01},
  journaltitle = {Advances in Methods and Practices in Psychological Science},
  volume = {1},
  number = {3},
  pages = {337--356},
  publisher = {{SAGE Publications Inc}},
  issn = {2515-2459},
  doi = {10.1177/2515245917747646},
  url = {https://doi.org/10.1177/2515245917747646},
  urldate = {2022-11-11},
  abstract = {Twenty-nine teams involving 61 analysts used the same data set to address the same research question: whether soccer referees are more likely to give red cards to dark-skin-toned players than to light-skin-toned players. Analytic approaches varied widely across the teams, and the estimated effect sizes ranged from 0.89 to 2.93 (Mdn = 1.31) in odds-ratio units. Twenty teams (69\%) found a statistically significant positive effect, and 9 teams (31\%) did not observe a significant relationship. Overall, the 29 different analyses used 21 unique combinations of covariates. Neither analysts? prior beliefs about the effect of interest nor their level of expertise readily explained the variation in the outcomes of the analyses. Peer ratings of the quality of the analyses also did not account for the variability. These findings suggest that significant variation in the results of analyses of complex data may be difficult to avoid, even by experts with honest intentions. Crowdsourcing data analysis, a strategy in which numerous research teams are recruited to simultaneously investigate the same research question, makes transparent how defensible, yet subjective, analytic choices influence research results.},
  langid = {english},
  file = {/home/skynet3/Zotero/storage/RCIKVM6G/Silberzahn et al. - 2018 - Many Analysts, One Data Set Making Transparent Ho.pdf}
}

@online{sileoUnderstandingBERTTransformer2019,
  title = {Understanding {{BERT Transformer}}: {{Attention}} Isn’t All You Need},
  shorttitle = {Understanding {{BERT Transformer}}},
  author = {Sileo, Damien},
  date = {2019-03-04T08:11:25},
  url = {https://medium.com/synapse-dev/understanding-bert-transformer-attention-isnt-all-you-need-5839ebd396db},
  urldate = {2022-11-11},
  abstract = {A parsing/composition framework for understanding Transformers},
  langid = {english},
  organization = {{synapse\_dev}}
}

@article{siliniFastEffectivePseudo2021,
  title = {Fast and Effective Pseudo Transfer Entropy for Bivariate Data-Driven Causal Inference},
  author = {Silini, Riccardo and Masoller, Cristina},
  date = {2021-04-19},
  journaltitle = {Scientific Reports},
  shortjournal = {Sci Rep},
  volume = {11},
  number = {1},
  pages = {8423},
  publisher = {{Nature Publishing Group}},
  issn = {2045-2322},
  doi = {10.1038/s41598-021-87818-3},
  url = {https://www.nature.com/articles/s41598-021-87818-3},
  urldate = {2022-11-11},
  abstract = {Identifying, from time series analysis, reliable indicators of causal relationships is essential for many disciplines. Main challenges are distinguishing correlation from causality and discriminating between direct and indirect interactions. Over the years many methods for data-driven causal inference have been proposed; however, their success largely depends on the characteristics of the system under investigation. Often, their data requirements, computational cost or number of parameters limit their applicability. Here we propose a computationally efficient measure for causality testing, which we refer to as pseudo transfer entropy (pTE), that we derive from the standard definition of transfer entropy (TE) by using a Gaussian approximation. We demonstrate the power of the pTE measure on simulated and on real-world data. In all cases we find that pTE returns results that are very similar to those returned by Granger causality (GC). Importantly, for short time series, pTE combined with time-shifted (T-S) surrogates for significance testing strongly reduces the computational cost with respect to the widely used iterative amplitude adjusted Fourier transform (IAAFT) surrogate testing. For example, for time series of 100 data points, pTE and T-S reduce the computational time by \$\$82\textbackslash\%\$\$with respect to GC and IAAFT. We also show that pTE is robust against observational noise. Therefore, we argue that the causal inference approach proposed here will be extremely valuable when causality networks need to be inferred from the analysis of a large number of short time series.},
  issue = {1},
  langid = {english},
  keywords = {Information theory and computation,Statistical physics,thermodynamics and nonlinear dynamics},
  file = {/home/skynet3/Zotero/storage/ZPQ9NXGN/Silini and Masoller - 2021 - Fast and effective pseudo transfer entropy for biv.pdf;/home/skynet3/Zotero/storage/A5TNDX6S/s41598-021-87818-3.html}
}

@article{simmonsFalsePositivePsychologyUndisclosed2011,
  ids = {simmonsFalsePositivePsychologyUndisclosed2011a},
  title = {False-{{Positive Psychology}}: {{Undisclosed Flexibility}} in {{Data Collection}} and {{Analysis Allows Presenting Anything}} as {{Significant}}},
  shorttitle = {False-{{Positive Psychology}}},
  author = {Simmons, Joseph P. and Nelson, Leif D. and Simonsohn, Uri},
  date = {2011-11-01},
  journaltitle = {Psychological Science},
  shortjournal = {Psychol Sci},
  volume = {22},
  number = {11},
  pages = {1359--1366},
  publisher = {{SAGE Publications Inc}},
  issn = {0956-7976},
  doi = {10.1177/0956797611417632},
  url = {https://doi.org/10.1177/0956797611417632},
  urldate = {2022-11-11},
  abstract = {In this article, we accomplish two things. First, we show that despite empirical psychologists? nominal endorsement of a low rate of false-positive findings (≤ .05), flexibility in data collection, analysis, and reporting dramatically increases actual false-positive rates. In many cases, a researcher is more likely to falsely find evidence that an effect exists than to correctly find evidence that it does not. We present computer simulations and a pair of actual experiments that demonstrate how unacceptably easy it is to accumulate (and report) statistically significant evidence for a false hypothesis. Second, we suggest a simple, low-cost, and straightforwardly effective disclosure-based solution to this problem. The solution involves six concrete requirements for authors and four guidelines for reviewers, all of which impose a minimal burden on the publication process.},
  langid = {english},
  file = {/home/skynet3/Zotero/storage/BJUWJFIJ/Simmons et al. - 2011 - False-Positive Psychology Undisclosed Flexibility.pdf}
}

@online{simonsohn103MediationAnalysis2022,
  title = {[103] {{Mediation Analysis}} Is {{Counterintuitively Invalid}}},
  author = {Simonsohn, Uri},
  date = {2022-09-26T11:00:02+00:00},
  url = {http://datacolada.org/103},
  urldate = {2022-11-11},
  abstract = {Mediation analysis is very common in behavioral science despite suffering from many invalidating shortcomings. While most of the shortcomings are intuitive [1], this post focuses on a counterintuitive one. It is one of those quirky statistical things that can be fun to think about, so it would merit a blog post even if it were...},
  langid = {american},
  organization = {{Data Colada}}
}

@online{simonsohn91PhackingFast2020,
  title = {[91] p-Hacking Fast and Slow: {{Evaluating}} a Forthcoming {{AER}} Paper Deeming Some Econ Literatures Less Trustworthy},
  shorttitle = {[91] p-Hacking Fast and Slow},
  author = {Simonsohn, Uri},
  date = {2020-09-15T11:00:22+00:00},
  url = {http://datacolada.org/91},
  urldate = {2022-11-11},
  abstract = {The authors of a forthcoming AER article (.pdf), "Methods Matter: P-Hacking and Publication Bias in Causal Analysis in Economics", painstakingly harvested thousands of test results from 25 economics journals to answer an interesting question: Are studies that use some research designs more trustworthy than others? In this post I will explain why I think their...},
  langid = {american},
  organization = {{Data Colada}}
}

@online{simonsohn95GroundhogAddressing2021,
  title = {[95] {{Groundhog}}: {{Addressing The Threat That R Poses To Reproducible Research}}},
  shorttitle = {[95] {{Groundhog}}},
  author = {Simonsohn, Uri},
  date = {2021-01-05T12:01:37+00:00},
  url = {http://datacolada.org/95},
  urldate = {2022-11-11},
  abstract = {R, the free and open source program for statistical computing, poses a substantial threat to the reproducibility of published research. This post explains the problem and introduces a solution. The Problem: Packages R itself has some reproducibility problems (see example in this footnote [1]), but the big problem is its packages: the addon scripts that...},
  langid = {american},
  organization = {{Data Colada}}
}

@article{simonsohnJustPostIt2013,
  ids = {simonsohnJustPostIt2013a},
  title = {Just {{Post It}}: {{The Lesson From Two Cases}} of {{Fabricated Data Detected}} by {{Statistics Alone}}},
  shorttitle = {Just {{Post It}}},
  author = {Simonsohn, Uri},
  date = {2013-10-01},
  journaltitle = {Psychological Science},
  shortjournal = {Psychol Sci},
  volume = {24},
  number = {10},
  pages = {1875--1888},
  publisher = {{SAGE Publications Inc}},
  issn = {0956-7976},
  doi = {10.1177/0956797613480366},
  url = {https://doi.org/10.1177/0956797613480366},
  urldate = {2022-11-11},
  abstract = {I argue that requiring authors to post the raw data supporting their published results has the benefit, among many others, of making fraud much less likely to go undetected. I illustrate this point by describing two cases of suspected fraud I identified exclusively through statistical analysis of reported means and standard deviations. Analyses of the raw data behind these published results provided invaluable confirmation of the initial suspicions, ruling out benign explanations (e.g., reporting errors, unusual distributions), identifying additional signs of fabrication, and also ruling out one of the suspected fraud?s explanations for his anomalous results. If journals, granting agencies, universities, or other entities overseeing research promoted or required data posting, it seems inevitable that fraud would be reduced.},
  langid = {english}
}

@misc{simonsohnSpecificationCurveDescriptive2019,
  type = {SSRN Scholarly Paper},
  ids = {simonsohnSpecificationCurveDescriptive2019a},
  title = {Specification {{Curve}}: {{Descriptive}} and {{Inferential Statistics}} on {{All Reasonable Specifications}}},
  shorttitle = {Specification {{Curve}}},
  author = {Simonsohn, Uri and Simmons, Joseph P. and Nelson, Leif D.},
  date = {2019-10-29},
  number = {2694998},
  location = {{Rochester, NY}},
  doi = {10.2139/ssrn.2694998},
  url = {https://papers.ssrn.com/abstract=2694998},
  urldate = {2022-11-11},
  abstract = {Empirical results often hinge on data analytic decisions that are simultaneously defensible, arbitrary, and motivated. To mitigate this problem we introduce Specification-Curve Analysis, which consists of three steps: (i) identifying the set of theoretically justified, statistically valid, and non-redundant analytic specifications, (ii) displaying alternative results graphically, allowing the identification of decisions producing different results, and (iii) conducting statistical tests to determine whether as a whole results are inconsistent with the null hypothesis. We illustrate its use by applying it to three published findings. One proves robust, one weak, one not robust at all.},
  langid = {english},
  keywords = {p-hacking,Specification Curve},
  file = {/home/skynet3/Zotero/storage/RNFYIYT8/Simonsohn et al. - 2019 - Specification Curve Descriptive and Inferential S.pdf;/home/skynet3/Zotero/storage/UVVB3XDE/papers.html}
}

@article{sjolanderFrequentistBayesianApproaches2019,
  title = {Frequentist versus {{Bayesian}} Approaches to Multiple Testing},
  author = {Sjölander, Arvid and Vansteelandt, Stijn},
  date = {2019-09-01},
  journaltitle = {European Journal of Epidemiology},
  shortjournal = {Eur J Epidemiol},
  volume = {34},
  number = {9},
  pages = {809--821},
  issn = {1573-7284},
  doi = {10.1007/s10654-019-00517-2},
  url = {https://doi.org/10.1007/s10654-019-00517-2},
  urldate = {2022-11-11},
  abstract = {Multiple tests arise frequently in epidemiologic research.However, the issue of multiplicity adjustment is surrounded by confusion and controversy, and there is no uniform agreement on whether or when adjustment is warranted. In this paper we compare frequentist and Bayesian frameworks for multiple testing. We argue that the frequentist framework leads to logical difficulties, and is unable to distinguish between relevant and irrelevant multiplicity adjustments.We further argue that these logical difficulties resolve within the Bayesian framework, and that the Bayesian framework makes a clear and coherent distinction between relevant and irrelevant adjustments.We use Directed Acyclic Graphs to illustrate the differences between the two frameworks, and to motivate our arguments.},
  langid = {english},
  keywords = {Bonferroni correction,Data fishing,Multiple comparisons,Multiple tests,p-value,Posterior distribution},
  file = {/home/skynet3/Zotero/storage/PKM3S4RK/Sjölander and Vansteelandt - 2019 - Frequentist versus Bayesian approaches to multiple.pdf}
}

@misc{skalseDefiningCharacterizingReward2022,
  ids = {skalseDefiningCharacterizingReward2022a},
  title = {Defining and {{Characterizing Reward Hacking}}},
  author = {Skalse, Joar and Howe, Nikolaus H. R. and Krasheninnikov, Dmitrii and Krueger, David},
  date = {2022-09-26},
  number = {arXiv:2209.13085},
  eprint = {2209.13085},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2209.13085},
  url = {http://arxiv.org/abs/2209.13085},
  urldate = {2022-11-11},
  abstract = {We provide the first formal definition of reward hacking, a phenomenon where optimizing an imperfect proxy reward function, \$\textbackslash mathcal\{\textbackslash tilde\{R\}\}\$, leads to poor performance according to the true reward function, \$\textbackslash mathcal\{R\}\$. We say that a proxy is unhackable if increasing the expected proxy return can never decrease the expected true return. Intuitively, it might be possible to create an unhackable proxy by leaving some terms out of the reward function (making it "narrower") or overlooking fine-grained distinctions between roughly equivalent outcomes, but we show this is usually not the case. A key insight is that the linearity of reward (in state-action visit counts) makes unhackability a very strong condition. In particular, for the set of all stochastic policies, two reward functions can only be unhackable if one of them is constant. We thus turn our attention to deterministic policies and finite sets of stochastic policies, where non-trivial unhackable pairs always exist, and establish necessary and sufficient conditions for the existence of simplifications, an important special case of unhackability. Our results reveal a tension between using reward functions to specify narrow tasks and aligning AI systems with human values.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/skynet3/Zotero/storage/QAMEDTMD/Skalse et al. - 2022 - Defining and Characterizing Reward Hacking.pdf;/home/skynet3/Zotero/storage/AFEXABJM/2209.html}
}

@online{slezEndogeneityHistoricalData2020,
  title = {The {{Endogeneity}} of {{Historical Data}}},
  author = {Slez, Adam},
  date = {2020-08-28T12:00:00+00:00},
  url = {https://broadstreet.blog/2020/08/28/the-endogeneity-of-historical-data/},
  urldate = {2022-11-11},
  abstract = {I am a historical sociologist by training. ~While contemporary historical sociology is undoubtedly inspired by the work of classical sociologists including Karl Marx, Max Weber, and W. E. B. D…},
  langid = {american},
  organization = {{Broadstreet}}
}

@online{SlidingWindowFunctions,
  title = {Sliding {{Window Functions}}},
  url = {https://davisvaughan.github.io/slider/index.html},
  urldate = {2022-11-11},
  abstract = {Provides type-stable rolling window functions over any R data     type. Cumulative and expanding windows are also supported. For more     advanced usage, an index can be used as a secondary vector that     defines how sliding windows are to be created.},
  langid = {english},
  file = {/home/skynet3/Zotero/storage/FKLH252M/index.html}
}

@article{sloughPhantomCounterfactuals,
  title = {Phantom {{Counterfactuals}}},
  author = {Slough, Tara},
  journaltitle = {American Journal of Political Science},
  volume = {n/a},
  number = {n/a},
  issn = {1540-5907},
  doi = {10.1111/ajps.12715},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/ajps.12715},
  urldate = {2022-11-11},
  abstract = {Researchers often seek to identify the effects of a treatment on a sequence of behaviors, such as whether citizens register to vote and whether they then cast ballots. I show that average treatment effects (ATEs) are only identified until the first behavior (registering to vote) that affects the set of possible subsequent actions (voting). When one action changes the set of possible subsequent actions, it creates ‘phantom counterfactuals,’ or undefined potential outcomes, which render ATEs unidentified. I show that applied theory allows researchers to diagnose phantom counterfactuals, which helps to recognize unidentified ATEs and focus instead on other estimands that are identified. I illustrate this approach using a stylized model of crime reporting, showing how different theories generate different sets of identified estimands while holding constant an experimental design. I thereby establish the necessity of applied theory for causal identification in empirical research with sequential behavioral outcomes.},
  langid = {english},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/ajps.12715},
  file = {/home/skynet3/Zotero/storage/LV3PCSZD/ajps.html}
}

@misc{smaldinoHowTranslateVerbal2020,
  title = {How to Translate a Verbal Theory into a Formal Model},
  author = {Smaldino, Paul E.},
  date = {2020-05-26T18:41:19},
  publisher = {{MetaArXiv}},
  doi = {10.31222/osf.io/n7qsh},
  url = {https://osf.io/preprints/metaarxiv/n7qsh/},
  urldate = {2022-11-11},
  abstract = {Turning verbal theories into formal models is an essential business of a mature science. Here I elaborate on taxonomies of models, provide ten lessons for translating a verbal theory into a formal model, and discuss the specific challenges involved in collaborations between modelers and non-modelers. It's a start.},
  langid = {american},
  keywords = {agent-based models,flatten the curve,formal models,mathematical models,Medicine and Health Sciences,Physical Sciences and Mathematics,SIR,Social and Behavioral Sciences,theory},
  file = {/home/skynet3/Zotero/storage/ISFCXGQY/Smaldino - 2020 - How to translate a verbal theory into a formal mod.pdf}
}

@article{smithEvidenceBasedMedicineOral2014,
  ids = {smithEvidenceBasedMedicineOral2014a},
  title = {Evidence-{{Based Medicine}}—{{An Oral History}}},
  author = {Smith, Richard and Rennie, Drummond},
  date = {2014-01-22},
  journaltitle = {JAMA},
  shortjournal = {JAMA},
  volume = {311},
  number = {4},
  pages = {365--367},
  issn = {0098-7484},
  doi = {10.1001/jama.2013.286182},
  url = {https://doi.org/10.1001/jama.2013.286182},
  urldate = {2022-11-11},
  abstract = {The phrase evidence-based medicine (EBM) was coined by Gordon Guyatt and then appeared in an article in The Rational Clinical Examination series in JAMA in 1992, but the roots of EBM go much further back. The personal stories of the origins of EBM were recently explored in a filmed oral history of some of the individuals most strongly associated with the birth of the movement (see Video, Evidence-Based Medicine: An Oral History).},
  file = {/home/skynet3/Zotero/storage/K5JXF68V/Smith and Rennie - 2014 - Evidence-Based Medicine—An Oral History.pdf;/home/skynet3/Zotero/storage/FIGXCUFM/1817042.html}
}

@article{smithParachuteUsePrevent2003,
  title = {Parachute Use to Prevent Death and Major Trauma Related to Gravitational Challenge: Systematic Review of Randomised Controlled Trials},
  shorttitle = {Parachute Use to Prevent Death and Major Trauma Related to Gravitational Challenge},
  author = {Smith, Gordon C S and Pell, Jill P},
  date = {2003-12-20},
  journaltitle = {BMJ : British Medical Journal},
  shortjournal = {BMJ},
  volume = {327},
  number = {7429},
  eprint = {14684649},
  eprinttype = {pmid},
  pages = {1459--1461},
  issn = {0959-8138},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC300808/},
  urldate = {2022-11-11},
  abstract = {Objectives To determine whether parachutes are effective in preventing major trauma related to gravitational challenge., Design Systematic review of randomised controlled trials., Data sources: Medline, Web of Science, Embase, and the Cochrane Library databases; appropriate internet sites and citation lists., Study selection: Studies showing the effects of using a parachute during free fall., Main outcome measure Death or major trauma, defined as an injury severity score {$>$} 15., Results We were unable to identify any randomised controlled trials of parachute intervention., Conclusions As with many interventions intended to prevent ill health, the effectiveness of parachutes has not been subjected to rigorous evaluation by using randomised controlled trials. Advocates of evidence based medicine have criticised the adoption of interventions evaluated by using only observational data. We think that everyone might benefit if the most radical protagonists of evidence based medicine organised and participated in a double blind, randomised, placebo controlled, crossover trial of the parachute.},
  pmcid = {PMC300808},
  file = {/home/skynet3/Zotero/storage/AW63CZ4L/Smith and Pell - 2003 - Parachute use to prevent death and major trauma re.pdf}
}

@article{smithSoftwareCitationPrinciples2016,
  ids = {smithSoftwareCitationPrinciples2016a},
  title = {Software Citation Principles},
  author = {Smith, Arfon M. and Katz, Daniel S. and Niemeyer, Kyle E.},
  date = {2016-09-19},
  journaltitle = {PeerJ Computer Science},
  shortjournal = {PeerJ Comput. Sci.},
  volume = {2},
  pages = {e86},
  publisher = {{PeerJ Inc.}},
  issn = {2376-5992},
  doi = {10.7717/peerj-cs.86},
  url = {https://peerj.com/articles/cs-86},
  urldate = {2022-11-11},
  abstract = {Software is a critical part of modern research and yet there is little support across the scholarly ecosystem for its acknowledgement and citation. Inspired by the activities of the FORCE11 working group focused on data citation, this document summarizes the recommendations of the FORCE11 Software Citation Working Group and its activities between June 2015 and April 2016. Based on a review of existing community practices, the goal of the working group was to produce a consolidated set of citation principles that may encourage broad adoption of a consistent policy for software citation across disciplines and venues. Our work is presented here as a set of software citation principles, a discussion of the motivations for developing the principles, reviews of existing community practice, and a discussion of the requirements these principles would place upon different stakeholders. Working examples and possible technical solutions for how these principles can be implemented will be discussed in a separate paper.},
  langid = {english},
  file = {/home/skynet3/Zotero/storage/VNRUXYJM/Smith et al. - 2016 - Software citation principles.pdf;/home/skynet3/Zotero/storage/DX5YIMXN/cs-86.html}
}

@online{SocialScienceComputing,
  ids = {SocialScienceComputinga},
  title = {Social {{Science Computing Cooperative}} – {{UW}}–{{Madison}}},
  url = {https://www.sscc.wisc.edu/},
  urldate = {2022-11-11},
  abstract = {SSCC serves the following member agencies:~ Center for Demography and Ecology~•~The Center for Demography of Health and Aging~•~The Center on Wisconsin Strategy~•~Economics~•~Institute on Aging~•~Institute for Research on Poverty~•~School of Human Ecology~•~School of Pharmacy~•~Sociology~•~University of Wisconsin Survey Center~•},
  langid = {american},
  organization = {{Social Science Computing Cooperative}},
  file = {/home/skynet3/Zotero/storage/BIBF6WRB/www.sscc.wisc.edu.html}
}

@online{SoftmaxFunctionIts,
  title = {The {{Softmax}} Function and Its Derivative - {{Eli Bendersky}}'s Website},
  url = {https://eli.thegreenplace.net/2016/the-softmax-function-and-its-derivative/},
  urldate = {2022-11-11},
  file = {/home/skynet3/Zotero/storage/5TMX65GF/the-softmax-function-and-its-derivative.html}
}

@article{sohilIntroductionStatisticalLearning2022,
  ids = {sohilIntroductionStatisticalLearning2022a},
  title = {An Introduction to Statistical Learning with Applications in {{R}}: By {{Gareth James}}, {{Daniela Witten}}, {{Trevor Hastie}}, and {{Robert Tibshirani}}, {{New York}}, {{Springer Science}} and {{Business Media}}, 2013, \$41.98, {{eISBN}}: 978-1-4614-7137-7},
  shorttitle = {An Introduction to Statistical Learning with Applications in {{R}}},
  author = {Sohil, Fariha and Sohali, Muhammad Umair and Shabbir, Javid},
  date = {2022-01-02},
  journaltitle = {Statistical Theory and Related Fields},
  shortjournal = {Statistical Theory and Related Fields},
  volume = {6},
  number = {1},
  pages = {87--87},
  issn = {2475-4269, 2475-4277},
  doi = {10.1080/24754269.2021.1980261},
  url = {https://www.tandfonline.com/doi/full/10.1080/24754269.2021.1980261},
  urldate = {2022-11-11},
  langid = {english},
  file = {/home/skynet3/Zotero/storage/JNFNWHCU/Sohil et al. - 2022 - An introduction to statistical learning with appli.pdf;/home/skynet3/Zotero/storage/XBLSYZ8D/Sohil et al. - 2022 - An introduction to statistical learning with appli.pdf}
}

@online{SolveSQLCode,
  title = {Solve {{SQL Code Challenges}}},
  url = {https://www.hackerrank.com/domains/sql},
  urldate = {2022-11-11},
  abstract = {A special-purpose language designed for managing data held in a relational database.},
  langid = {american},
  organization = {{HackerRank}}
}

@online{SophiaDawkins0000000226090820,
  title = {Sophia {{Dawkins}} (0000-0002-2609-0820)},
  url = {https://orcid.org/0000-0002-2609-0820},
  urldate = {2022-11-11},
  abstract = {ORCID record for Sophia Dawkins. ORCID provides an identifier for individuals to use with their name as they engage in research, scholarship, and innovation activities.},
  langid = {english},
  organization = {{ORCID}},
  file = {/home/skynet3/Zotero/storage/KG5M436P/0000-0002-2609-0820.html}
}

@online{Sparklyr,
  ids = {Sparklyra},
  title = {Sparklyr},
  url = {https://spark.rstudio.com/},
  urldate = {2022-11-11}
}

@online{SparsityBlues,
  title = {Sparsity {{Blues}}},
  url = {https://betanalpha.github.io/assets/case_studies/modeling_sparsity.html},
  urldate = {2022-11-11}
}

@software{spieringAwesomeDeepLearning2022,
  title = {Awesome {{Deep Learning}} for {{Natural Language Processing}} ({{NLP}})},
  author = {Spiering, Brian},
  date = {2022-11-09T17:15:23Z},
  origdate = {2016-06-04T13:38:28Z},
  url = {https://github.com/brianspiering/awesome-dl4nlp},
  urldate = {2022-11-11},
  abstract = {A curated list of awesome Deep Learning for Natural Language Processing resources}
}

@online{SplinesStanSplines,
  title = {Splines\_in\_{{Stan}}/Splines\_in\_stan.Pdf at Master · Milkha/{{Splines}}\_in\_{{Stan}}},
  url = {https://github.com/milkha/Splines_in_Stan},
  urldate = {2022-11-11},
  abstract = {Implementation of B-Splines in Stan. Contribute to milkha/Splines\_in\_Stan development by creating an account on GitHub.},
  langid = {english},
  organization = {{GitHub}},
  file = {/home/skynet3/Zotero/storage/CQIDQDA9/splines_in_stan.html}
}

@online{srivastavaEverythingFuckedSyllabus2016,
  title = {Everything Is Fucked: {{The}} Syllabus},
  shorttitle = {Everything Is Fucked},
  author = {Srivastava, Sanjay},
  date = {2016-08-11T19:09:45+00:00},
  url = {https://thehardestscience.com/2016/08/11/everything-is-fucked-the-syllabus/},
  urldate = {2022-11-11},
  abstract = {PSY 607: Everything is Fucked Prof. Sanjay Srivastava Class meetings: Mondays 9:00 – 10:50 in 257 Straub Office hours: Held on Twitter at your convenience (@hardsci) In a much-discussed artic…},
  langid = {english},
  organization = {{The Hardest Science}},
  file = {/home/skynet3/Zotero/storage/FLBIBUX2/everything-is-fucked-the-syllabus.html}
}

@online{StanfordTACREDHomepage,
  title = {Stanford {{TACRED Homepage}}},
  url = {https://nlp.stanford.edu/projects/tacred/},
  urldate = {2022-11-11},
  file = {/home/skynet3/Zotero/storage/A56R5MRT/tacred.html}
}

@online{StanfordUniversityCS224d,
  title = {Stanford {{University CS224d}}: {{Deep Learning}} for {{Natural Language Processing}}},
  url = {http://cs224d.stanford.edu/syllabus.html},
  urldate = {2022-11-11}
}

@software{StarSpace2022,
  title = {{{StarSpace}}},
  date = {2022-11-10T03:13:39Z},
  origdate = {2017-06-28T17:50:18Z},
  url = {https://github.com/facebookresearch/StarSpace},
  urldate = {2022-11-11},
  abstract = {Learning embeddings for classification, retrieval and ranking.},
  organization = {{Meta Research}}
}

@online{StatisticalRethinking2022,
  title = {Statistical {{Rethinking}} 2022 - {{YouTube}}},
  url = {https://www.youtube.com/playlist?list=PLDcUM9US4XdMROZ57-OIRtIK0aOynbgZN},
  urldate = {2022-11-11}
}

@online{StatisticalRethinkingRichard,
  title = {Statistical {{Rethinking}} | {{Richard McElreath}}},
  url = {https://xcelab.net/rm/statistical-rethinking/},
  urldate = {2022-11-11},
  langid = {american}
}

@video{statquestwithjoshstarmerGradientBoostPart2019,
  title = {Gradient {{Boost Part}} 1 (of 4): {{Regression Main Ideas}}},
  shorttitle = {Gradient {{Boost Part}} 1 (of 4)},
  editor = {{StatQuest with Josh Starmer}},
  date = {2019-03-25},
  url = {https://www.youtube.com/watch?v=3CC4N4z3GJc},
  urldate = {2022-11-11},
  editortype = {director}
}

@video{statquestwithjoshstarmerStatQuestIntroductionPyTorch2022,
  title = {The {{StatQuest Introduction}} to {{PyTorch}}},
  editor = {{StatQuest with Josh Starmer}},
  date = {2022-04-24},
  url = {https://www.youtube.com/watch?v=FHdlXe1bSe4},
  urldate = {2022-11-11},
  editortype = {director}
}

@video{statquestwithjoshstarmerXGBoostPartRegression2019,
  title = {{{XGBoost Part}} 1 (of 4): {{Regression}}},
  shorttitle = {{{XGBoost Part}} 1 (of 4)},
  editor = {{StatQuest with Josh Starmer}},
  date = {2019-12-16},
  url = {https://www.youtube.com/watch?v=OtD8wVaFm6E},
  urldate = {2022-11-11},
  editortype = {director}
}

@online{StefanomeschiariLatex2expUse,
  title = {Stefano-Meschiari/Latex2exp: {{Use LaTeX}} in {{R}} Graphics.},
  shorttitle = {Stefano-Meschiari/Latex2exp},
  url = {https://github.com/stefano-meschiari/latex2exp},
  urldate = {2022-11-11},
  abstract = {Use LaTeX in R graphics. Contribute to stefano-meschiari/latex2exp development by creating an account on GitHub.},
  langid = {english},
  organization = {{GitHub}},
  file = {/home/skynet3/Zotero/storage/H48GSN2M/latex2exp.html}
}

@misc{stehrLearningCausalitySynthesis2019,
  title = {Learning {{Causality}}: {{Synthesis}} of {{Large-Scale Causal Networks}} from {{High-Dimensional Time Series Data}}},
  shorttitle = {Learning {{Causality}}},
  author = {Stehr, Mark-Oliver and Avar, Peter and Korte, Andrew R. and Parvin, Lida and Sahab, Ziad J. and Bunin, Deborah I. and Knapp, Merrill and Nishita, Denise and Poggio, Andrew and Talcott, Carolyn L. and Davis, Brian M. and Morton, Christine A. and Sevinsky, Christopher J. and Zavodszky, Maria I. and Vertes, Akos},
  date = {2019-05-06},
  number = {arXiv:1905.02291},
  eprint = {1905.02291},
  eprinttype = {arxiv},
  primaryclass = {cs, q-bio, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1905.02291},
  url = {http://arxiv.org/abs/1905.02291},
  urldate = {2022-11-11},
  abstract = {There is an abundance of complex dynamic systems that are critical to our daily lives and our society but that are hardly understood, and even with today's possibilities to sense and collect large amounts of experimental data, they are so complex and continuously evolving that it is unlikely that their dynamics will ever be understood in full detail. Nevertheless, through computational tools we can try to make the best possible use of the current technologies and available data. We believe that the most useful models will have to take into account the imbalance between system complexity and available data in the context of limited knowledge or multiple hypotheses. The complex system of biological cells is a prime example of such a system that is studied in systems biology and has motivated the methods presented in this paper. They were developed as part of the DARPA Rapid Threat Assessment (RTA) program, which is concerned with understanding of the mechanism of action (MoA) of toxins or drugs affecting human cells. Using a combination of Gaussian processes and abstract network modeling, we present three fundamentally different machine-learning-based approaches to learn causal relations and synthesize causal networks from high-dimensional time series data. While other types of data are available and have been analyzed and integrated in our RTA work, we focus on transcriptomics (that is gene expression) data obtained from high-throughput microarray experiments in this paper to illustrate capabilities and limitations of our algorithms. Our algorithms make different but overall relatively few biological assumptions, so that they are applicable to other types of biological data and potentially even to other complex systems that exhibit high dimensionality but are not of biological nature.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Quantitative Biology - Cell Behavior,Statistics - Machine Learning},
  file = {/home/skynet3/Zotero/storage/T7XKBFVB/Stehr et al. - 2019 - Learning Causality Synthesis of Large-Scale Causa.pdf;/home/skynet3/Zotero/storage/T8M7YBB5/1905.html}
}

@misc{stommesReliabilityPublishedFindings2021,
  ids = {stommesReliabilityPublishedFindings2021a,stommesReliabilityPublishedFindings2021b},
  title = {On the Reliability of Published Findings Using the Regression Discontinuity Design in Political Science},
  author = {Stommes, Drew and Aronow, P. M. and Sävje, Fredrik},
  date = {2021-09-29},
  number = {arXiv:2109.14526},
  eprint = {2109.14526},
  eprinttype = {arxiv},
  primaryclass = {stat},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2109.14526},
  urldate = {2022-11-11},
  abstract = {The regression discontinuity (RD) design offers identification of causal effects under weak assumptions, earning it the position as a standard method in modern political science research. But identification does not necessarily imply that the causal effects can be estimated accurately with limited data. In this paper, we highlight that estimation is particularly challenging with the RD design and investigate how these challenges manifest themselves in the empirical literature. We collect all RD-based findings published in top political science journals from 2009--2018. The findings exhibit pathological features; estimates tend to bunch just above the conventional level of statistical significance. A reanalysis of all studies with available data suggests that researcher's discretion is not a major driver of these pathological features, but researchers tend to use inappropriate methods for inference, rendering standard errors artificially small. A retrospective power analysis reveals that most of these studies were underpowered to detect all but large effects. The issues we uncover, combined with well-documented selection pressures in academic publishing, cause concern that many published findings using the RD design are exaggerated, if not entirely spurious.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Methodology},
  file = {/home/skynet3/Zotero/storage/J2IIW5CP/Stommes et al. - 2021 - On the reliability of published findings using the.pdf;/home/skynet3/Zotero/storage/J8UD3V2M/2109.html;/home/skynet3/Zotero/storage/XQ25A823/2109.html}
}

@misc{storksRecentAdvancesNatural2020,
  title = {Recent {{Advances}} in {{Natural Language Inference}}: {{A Survey}} of {{Benchmarks}}, {{Resources}}, and {{Approaches}}},
  shorttitle = {Recent {{Advances}} in {{Natural Language Inference}}},
  author = {Storks, Shane and Gao, Qiaozi and Chai, Joyce Y.},
  date = {2020-02-26},
  number = {arXiv:1904.01172},
  eprint = {1904.01172},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1904.01172},
  url = {http://arxiv.org/abs/1904.01172},
  urldate = {2022-11-11},
  abstract = {In the NLP community, recent years have seen a surge of research activities that address machines' ability to perform deep language understanding which goes beyond what is explicitly stated in text, rather relying on reasoning and knowledge of the world. Many benchmark tasks and datasets have been created to support the development and evaluation of such natural language inference ability. As these benchmarks become instrumental and a driving force for the NLP research community, this paper aims to provide an overview of recent benchmarks, relevant knowledge resources, and state-of-the-art learning and inference approaches in order to support a better understanding of this growing field.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/home/skynet3/Zotero/storage/45PW3WGW/Storks et al. - 2020 - Recent Advances in Natural Language Inference A S.pdf;/home/skynet3/Zotero/storage/XH62E5V3/1904.html}
}

@online{StudyUncertaintyQuantification2022,
  title = {A Study of Uncertainty Quantification in Overparametrized High-Dimensional Models},
  date = {2022-10-23T16:01:08+00:00},
  url = {https://deepai.org/publication/a-study-of-uncertainty-quantification-in-overparametrized-high-dimensional-models},
  urldate = {2022-11-11},
  abstract = {10/23/22 - Uncertainty quantification is a central challenge in reliable and trustworthy machine learning. Naive measures such as last-layer ...},
  organization = {{DeepAI}},
  file = {/home/skynet3/Zotero/storage/MP6QLQVL/a-study-of-uncertainty-quantification-in-overparametrized-high-dimensional-models.html}
}

@online{SubsetFunctionRDocumentation,
  title = {Subset Function - {{RDocumentation}}},
  url = {https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/subset},
  urldate = {2022-11-11}
}

@online{SubsetRowsUsing,
  title = {Subset Rows Using Column Values — Filter},
  url = {https://dplyr.tidyverse.org/reference/filter.html},
  urldate = {2022-11-11},
  abstract = {The filter() function is used to subset a data frame, retaining all rows that satisfy your conditions. To be retained, the row must produce a value of TRUE for all conditions. Note that when a condition evaluates to NA the row will be dropped, unlike base subsetting with [.},
  langid = {english}
}

@online{Subtraction,
  title = {Subtraction},
  url = {https://www.wikidata.org/wiki/Q40754},
  urldate = {2022-11-11},
  abstract = {process of subtracting a number from another (minuend−subtrahend=difference); mathematical operation that represents the operation of removing objects from a collection},
  langid = {english}
}

@inreference{Subtraction2022,
  title = {Subtraction},
  booktitle = {Wikipedia},
  date = {2022-09-21T21:56:42Z},
  url = {https://en.wikipedia.org/w/index.php?title=Subtraction&oldid=1111602119},
  urldate = {2022-11-11},
  abstract = {Subtraction is an arithmetic operation that represents the operation of removing objects from a collection. Subtraction is signified by the minus sign, −. For example, in the adjacent picture, there are 5 − 2 peaches—meaning 5 peaches with 2 taken away, resulting in a total of 3 peaches. Therefore, the difference of 5 and 2 is 3; that is, 5 − 2 = 3. While primarily associated with natural numbers in arithmetic, subtraction can also represent removing or decreasing physical and abstract quantities using different kinds of objects including negative numbers, fractions, irrational numbers, vectors, decimals, functions, and matrices.Subtraction follows several important patterns. It is anticommutative, meaning that changing the order changes the sign of the answer. It is also not associative, meaning that when one subtracts more than two numbers, the order in which subtraction is performed matters. Because 0 is the additive identity, subtraction of it does not change a number. Subtraction also obeys predictable rules concerning related operations, such as addition and multiplication. All of these rules can be proven, starting with the subtraction of integers and generalizing up through the real numbers and beyond. General binary operations that follow these patterns are studied in abstract algebra. Performing subtraction on natural numbers is one of the simplest numerical tasks. Subtraction of very small numbers is accessible to young children. In primary education for instance, students are taught to subtract numbers in the decimal system, starting with single digits and progressively tackling more difficult problems. In advanced algebra and in computer algebra, an expression involving subtraction like A − B is generally treated as a shorthand notation for the addition A + (−B). Thus, A − B contains two terms, namely A and −B. This allows an easier use of associativity and commutativity.},
  langid = {english},
  annotation = {Page Version ID: 1111602119}
}

@inreference{Subtraction2022a,
  title = {Subtraction},
  booktitle = {Wikipedia},
  date = {2022-09-21T21:56:42Z},
  url = {https://en.wikipedia.org/w/index.php?title=Subtraction&oldid=1111602119},
  urldate = {2022-11-11},
  abstract = {Subtraction is an arithmetic operation that represents the operation of removing objects from a collection. Subtraction is signified by the minus sign, −. For example, in the adjacent picture, there are 5 − 2 peaches—meaning 5 peaches with 2 taken away, resulting in a total of 3 peaches. Therefore, the difference of 5 and 2 is 3; that is, 5 − 2 = 3. While primarily associated with natural numbers in arithmetic, subtraction can also represent removing or decreasing physical and abstract quantities using different kinds of objects including negative numbers, fractions, irrational numbers, vectors, decimals, functions, and matrices.Subtraction follows several important patterns. It is anticommutative, meaning that changing the order changes the sign of the answer. It is also not associative, meaning that when one subtracts more than two numbers, the order in which subtraction is performed matters. Because 0 is the additive identity, subtraction of it does not change a number. Subtraction also obeys predictable rules concerning related operations, such as addition and multiplication. All of these rules can be proven, starting with the subtraction of integers and generalizing up through the real numbers and beyond. General binary operations that follow these patterns are studied in abstract algebra. Performing subtraction on natural numbers is one of the simplest numerical tasks. Subtraction of very small numbers is accessible to young children. In primary education for instance, students are taught to subtract numbers in the decimal system, starting with single digits and progressively tackling more difficult problems. In advanced algebra and in computer algebra, an expression involving subtraction like A − B is generally treated as a shorthand notation for the addition A + (−B). Thus, A − B contains two terms, namely A and −B. This allows an easier use of associativity and commutativity.},
  langid = {english},
  annotation = {Page Version ID: 1111602119},
  file = {/home/skynet3/Zotero/storage/FNU93KEI/Subtraction.html}
}

@misc{sunSparsesoftmaxSimplerFaster2021,
  title = {Sparse-Softmax: {{A Simpler}} and {{Faster Alternative Softmax Transformation}}},
  shorttitle = {Sparse-Softmax},
  author = {Sun, Shaoshi and Zhang, Zhenyuan and Huang, BoCheng and Lei, Pengbin and Su, Jianlin and Pan, Shengfeng and Cao, Jiarun},
  date = {2021-12-23},
  number = {arXiv:2112.12433},
  eprint = {2112.12433},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2112.12433},
  url = {http://arxiv.org/abs/2112.12433},
  urldate = {2022-11-11},
  abstract = {The softmax function is widely used in artificial neural networks for the multiclass classification problems, where the softmax transformation enforces the output to be positive and sum to one, and the corresponding loss function allows to use maximum likelihood principle to optimize the model. However, softmax leaves a large margin for loss function to conduct optimizing operation when it comes to high-dimensional classification, which results in low-performance to some extent. In this paper, we provide an empirical study on a simple and concise softmax variant, namely sparse-softmax, to alleviate the problem that occurred in traditional softmax in terms of high-dimensional classification problems. We evaluate our approach in several interdisciplinary tasks, the experimental results show that sparse-softmax is simpler, faster, and produces better results than the baseline models.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/skynet3/Zotero/storage/8H42WVZE/Sun et al. - 2021 - Sparse-softmax A Simpler and Faster Alternative S.pdf;/home/skynet3/Zotero/storage/RY45BZ8G/2112.html}
}

@online{SupervisedClusteringHow2022,
  title = {Supervised {{Clustering}}: {{How}} to {{Use SHAP Values}} for {{Better Cluster Analysis}}},
  shorttitle = {Supervised {{Clustering}}},
  date = {2022-05-16T07:49:38},
  url = {https://www.aidancooper.co.uk/supervised-clustering-shap-values/},
  urldate = {2022-11-11},
  abstract = {Cluster analysis is a popular method for identifying subgroups within a population, but the results are often challenging to interpret and action. Supervised clustering leverages SHAP values to identify better-separated clusters using a more structured representation of the data.},
  langid = {english},
  organization = {{Aidan Cooper}},
  file = {/home/skynet3/Zotero/storage/QKGE7K62/supervised-clustering-shap-values.html}
}

@online{SurveyParametersAssociated2022,
  title = {A {{Survey}} of {{Parameters Associated}} with the {{Quality}} of {{Benchmarks}} in {{NLP}}},
  date = {2022-10-14T06:44:14+00:00},
  url = {https://deepai.org/publication/a-survey-of-parameters-associated-with-the-quality-of-benchmarks-in-nlp},
  urldate = {2022-11-11},
  abstract = {10/14/22 - Several benchmarks have been built with heavy investment in resources to track our progress in NLP. Thousands of papers published ...},
  organization = {{DeepAI}},
  file = {/home/skynet3/Zotero/storage/J7PP25PJ/a-survey-of-parameters-associated-with-the-quality-of-benchmarks-in-nlp.html}
}

@misc{swayamdiptaDatasetCartographyMapping2020,
  ids = {swayamdiptaDatasetCartographyMapping2020a},
  title = {Dataset {{Cartography}}: {{Mapping}} and {{Diagnosing Datasets}} with {{Training Dynamics}}},
  shorttitle = {Dataset {{Cartography}}},
  author = {Swayamdipta, Swabha and Schwartz, Roy and Lourie, Nicholas and Wang, Yizhong and Hajishirzi, Hannaneh and Smith, Noah A. and Choi, Yejin},
  date = {2020-10-15},
  number = {arXiv:2009.10795},
  eprint = {2009.10795},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2009.10795},
  url = {http://arxiv.org/abs/2009.10795},
  urldate = {2022-11-11},
  abstract = {Large datasets have become commonplace in NLP research. However, the increased emphasis on data quantity has made it challenging to assess the quality of data. We introduce Data Maps---a model-based tool to characterize and diagnose datasets. We leverage a largely ignored source of information: the behavior of the model on individual instances during training (training dynamics) for building data maps. This yields two intuitive measures for each example---the model's confidence in the true class, and the variability of this confidence across epochs---obtained in a single run of training. Experiments across four datasets show that these model-dependent measures reveal three distinct regions in the data map, each with pronounced characteristics. First, our data maps show the presence of "ambiguous" regions with respect to the model, which contribute the most towards out-of-distribution generalization. Second, the most populous regions in the data are "easy to learn" for the model, and play an important role in model optimization. Finally, data maps uncover a region with instances that the model finds "hard to learn"; these often correspond to labeling errors. Our results indicate that a shift in focus from quantity to quality of data could lead to robust models and improved out-of-distribution generalization.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/home/skynet3/Zotero/storage/GVHQCVJP/Swayamdipta et al. - 2020 - Dataset Cartography Mapping and Diagnosing Datase.pdf;/home/skynet3/Zotero/storage/2VWLFDHR/2009.html}
}

@online{SyntheticControlUsing,
  title = {Synthetic {{Control Using Lasso}} ({{SCUL}})},
  url = {https://hollina.github.io/scul/},
  urldate = {2022-11-11},
  abstract = {Synthetic control methods are a popular strategy for estimating counterfactual outcomes using weighted averages of untreated groups. We use lasso regressions to construct synthetic control weights, allowing for a high-dimensional donor pool and for negatively correlated donors to contribute to the synthetic prediction; neither of which is possible using traditional methods. This package provides code to run the synthetic control using lasso (SCUL) estimator that is outlined in Hollingsworth and Wing (2020) "Tactics for design and inference in synthetic control studies: An applied example using high-dimensional data." https://doi.org/10.31235/osf.io/fc9xt},
  langid = {english},
  file = {/home/skynet3/Zotero/storage/DWSJCB95/scul.html}
}

@misc{szymanskiEffectChoiceNeural2018,
  title = {The Effect of the Choice of Neural Network Depth and Breadth on the Size of Its Hypothesis Space},
  author = {Szymanski, Lech and McCane, Brendan and Albert, Michael},
  date = {2018-06-06},
  number = {arXiv:1806.02460},
  eprint = {1806.02460},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1806.02460},
  url = {http://arxiv.org/abs/1806.02460},
  urldate = {2022-11-11},
  abstract = {We show that the number of unique function mappings in a neural network hypothesis space is inversely proportional to \$\textbackslash prod\_lU\_l!\$, where \$U\_\{l\}\$ is the number of neurons in the hidden layer \$l\$.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/skynet3/Zotero/storage/3DKGRKU8/Szymanski et al. - 2018 - The effect of the choice of neural network depth a.pdf;/home/skynet3/Zotero/storage/J7EJ375H/1806.html}
}

@inreference{TableDatabase2021,
  ids = {TableDatabase2021a},
  title = {Table (Database)},
  booktitle = {Wikipedia},
  date = {2021-12-23T13:40:41Z},
  url = {https://en.wikipedia.org/w/index.php?title=Table_(database)&oldid=1061718298},
  urldate = {2022-11-11},
  abstract = {A table is a collection of related data held in a table format within a database. It consists of columns and rows. In relational databases, and flat file databases, a table is a set of data elements (values) using a model of vertical columns (identifiable by name) and horizontal rows, the cell being the unit where a row and column intersect. A table has a specified number of columns, but can have any number of rows. Each row is identified by one or more values appearing in a particular column subset. A specific choice of columns which uniquely identify rows is called the primary key. "Table" is another term for "relation"; although there is the difference in that a table is usually a multiset (bag) of rows where a relation is a set and does not allow duplicates. Besides the actual data rows, tables generally have associated with them some metadata, such as constraints on the table or on the values within particular columns.The data in a table does not have to be physically stored in the database. Views also function as relational tables, but their data are calculated at query time. External tables (in Informix or Oracle, for example) can also be thought of as views. In many systems for computational statistics, such as R and Python's pandas, a data frame or data table is a data type supporting the table abstraction. Conceptually, it is a list of records or observations all containing the same fields or columns. The implementation consists of a list of arrays or vectors, each with a name.},
  langid = {english},
  annotation = {Page Version ID: 1061718298},
  file = {/home/skynet3/Zotero/storage/G2V8XXJ2/Table_(database).html}
}

@inreference{TableInformation2022,
  ids = {TableInformation2022a},
  title = {Table (Information)},
  booktitle = {Wikipedia},
  date = {2022-08-06T17:38:26Z},
  url = {https://en.wikipedia.org/w/index.php?title=Table_(information)&oldid=1102746749},
  urldate = {2022-11-11},
  abstract = {A table is an arrangement of information or data, typically in rows and columns, or possibly in a more complex structure. Tables are widely used in communication, research, and data analysis. Tables appear in print media, handwritten notes, computer software, architectural ornamentation, traffic signs, and many other places. The precise conventions and terminology for describing tables vary depending on the context. Further, tables differ significantly in variety, structure, flexibility, notation, representation and use. Information or data conveyed in table form is said to be in tabular format (adjective). In books and technical articles, tables are typically presented apart from the main text in numbered and captioned floating blocks.},
  langid = {english},
  annotation = {Page Version ID: 1102746749},
  file = {/home/skynet3/Zotero/storage/V2AB6ITW/Table_(information).html}
}

@misc{tajeunaModelingRegimeShifts2022,
  ids = {tajeunaModelingRegimeShifts2022a},
  title = {Modeling {{Regime Shifts}} in {{Multiple Time Series}}},
  author = {Tajeuna, Etienne Gael and Bouguessa, Mohamed and Wang, Shengrui},
  date = {2022-05-13},
  number = {arXiv:2109.09692},
  eprint = {2109.09692},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2109.09692},
  url = {http://arxiv.org/abs/2109.09692},
  urldate = {2022-11-11},
  abstract = {We investigate the problem of discovering and modeling regime shifts in an ecosystem comprising multiple time series known as co-evolving time series. Regime shifts refer to the changing behaviors exhibited by series at different time intervals. Learning these changing behaviors is a key step toward time series forecasting. While advances have been made, existing methods suffer from one or more of the following shortcomings: (1) failure to take relationships between time series into consideration for discovering regimes in multiple time series; (2) lack of an effective approach that models time-dependent behaviors exhibited by series; (3) difficulties in handling data discontinuities which may be informative. Most of the existing methods are unable to handle all of these three issues in a unified framework. This, therefore, motivates our effort to devise a principled approach for modeling interactions and time-dependency in co-evolving time series. Specifically, we model an ecosystem of multiple time series by summarizing the heavy ensemble of time series into a lighter and more meaningful structure called a \textbackslash textit\{mapping grid\}. By using the mapping grid, our model first learns time series behavioral dependencies through a dynamic network representation, then learns the regime transition mechanism via a full time-dependent Cox regression model. The originality of our approach lies in modeling interactions between time series in regime identification and in modeling time-dependent regime transition probabilities, usually assumed to be static in existing work.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/skynet3/Zotero/storage/UP787PSI/Tajeuna et al. - 2022 - Modeling Regime Shifts in Multiple Time Series.pdf;/home/skynet3/Zotero/storage/6ZT48GKN/2109.html}
}

@article{tamerPartialIdentificationEconometrics2010,
  ids = {tamerPartialIdentificationEconometrics2010a},
  title = {Partial {{Identification}} in {{Econometrics}}},
  author = {Tamer, Elie},
  date = {2010-09-04},
  journaltitle = {Annual Review of Economics},
  shortjournal = {Annu. Rev. Econ.},
  volume = {2},
  number = {1},
  pages = {167--195},
  issn = {1941-1383, 1941-1391},
  doi = {10.1146/annurev.economics.050708.143401},
  url = {https://www.annualreviews.org/doi/10.1146/annurev.economics.050708.143401},
  urldate = {2022-11-11},
  abstract = {Identification in econometric models maps prior assumptions and the data to information about a parameter of interest. The partial identification approach to inference recognizes that this process should not result in a binary answer that consists of whether the parameter is point identified. Rather, given the data, the partial identification approach characterizes the informational content of various assumptions by providing a menu of estimates, each based on different sets of assumptions, some of which are plausible and some of which are not. Of course, more assumptions beget more information, so stronger conclusions can be made at the expense of more assumptions. The partial identification approach advocates a more fluid view of identification and hence provides the empirical researcher with methods to help study the spectrum of information that we can harness about a parameter of interest using a menu of assumptions. This approach links conclusions drawn from various empirical models to sets of assumptions made in a transparent way. It allows researchers to examine the informational content of their assumptions and their impacts on the inferences made. Naturally, with finite sample sizes, this approach leads to statistical complications, as one needs to deal with characterizing sampling uncertainty in models that do not point identify a parameter. Therefore, new methods for inference are developed. These methods construct confidence sets for partially identified parameters, and confidence regions for sets of parameters, or identifiable sets.},
  langid = {english},
  file = {/home/skynet3/Zotero/storage/IF4F8EPJ/Tamer - 2010 - Partial Identification in Econometrics.pdf}
}

@misc{tanayNewAngleL22018,
  title = {A {{New Angle}} on {{L2 Regularization}}},
  author = {Tanay, Thomas and Griffin, Lewis D.},
  date = {2018-06-28},
  number = {arXiv:1806.11186},
  eprint = {1806.11186},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1806.11186},
  url = {http://arxiv.org/abs/1806.11186},
  urldate = {2022-11-11},
  abstract = {Imagine two high-dimensional clusters and a hyperplane separating them. Consider in particular the angle between: the direction joining the two clusters' centroids and the normal to the hyperplane. In linear classification, this angle depends on the level of L2 regularization used. Can you explain why?},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/home/skynet3/Zotero/storage/HNGD57DN/Tanay and Griffin - 2018 - A New Angle on L2 Regularization.pdf;/home/skynet3/Zotero/storage/E49BTN8Q/1806.html}
}

@online{TargetsPackageUser,
  title = {The \{targets\} {{R}} Package User Manual},
  url = {https://books.ropensci.org/targets/},
  urldate = {2022-11-11},
  file = {/home/skynet3/Zotero/storage/72CHN3DC/targets.html}
}

@online{TaxonomyBenchmarksGraph2022,
  title = {Taxonomy of {{Benchmarks}} in {{Graph Representation Learning}}},
  date = {2022-06-15T18:01:10+00:00},
  url = {https://deepai.org/publication/taxonomy-of-benchmarks-in-graph-representation-learning},
  urldate = {2022-11-11},
  abstract = {06/15/22 - Graph Neural Networks (GNNs) extend the success of neural networks to graph-structured data by accounting for their intrinsic geom...},
  organization = {{DeepAI}},
  file = {/home/skynet3/Zotero/storage/CIIX5KYA/taxonomy-of-benchmarks-in-graph-representation-learning.html}
}

@misc{taylorMultiWayCorrelationCoefficient2020,
  title = {A {{Multi-Way Correlation Coefficient}}},
  author = {Taylor, Benjamin M.},
  date = {2020-03-05},
  number = {arXiv:2003.02561},
  eprint = {2003.02561},
  eprinttype = {arxiv},
  primaryclass = {stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2003.02561},
  url = {http://arxiv.org/abs/2003.02561},
  urldate = {2022-11-11},
  abstract = {Pearson's correlation is an important summary measure of the amount of dependence between two variables. It is natural to want to generalise the concept of correlation as a single number that measures the inter-relatedness of three or more variables e.g. how `correlated' are a collection of variables in which non are specifically to be treated as an `outcome'? In this short article, we introduce such a measure, and show that it reduces to the modulus of Pearson's \$r\$ in the two dimensional case.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Methodology},
  file = {/home/skynet3/Zotero/storage/AH2E2NW9/Taylor - 2020 - A Multi-Way Correlation Coefficient.pdf;/home/skynet3/Zotero/storage/DQU9QVZH/2003.html}
}

@misc{tchetgenIntroductionProximalCausal2020,
  title = {An {{Introduction}} to {{Proximal Causal Learning}}},
  author = {Tchetgen, Eric J. Tchetgen and Ying, Andrew and Cui, Yifan and Shi, Xu and Miao, Wang},
  date = {2020-09-23},
  number = {arXiv:2009.10982},
  eprint = {2009.10982},
  eprinttype = {arxiv},
  primaryclass = {stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2009.10982},
  url = {http://arxiv.org/abs/2009.10982},
  urldate = {2022-11-11},
  abstract = {A standard assumption for causal inference from observational data is that one has measured a sufficiently rich set of covariates to ensure that within covariate strata, subjects are exchangeable across observed treatment values. Skepticism about the exchangeability assumption in observational studies is often warranted because it hinges on investigators' ability to accurately measure covariates capturing all potential sources of confounding. Realistically, confounding mechanisms can rarely if ever, be learned with certainty from measured covariates. One can therefore only ever hope that covariate measurements are at best proxies of true underlying confounding mechanisms operating in an observational study, thus invalidating causal claims made on basis of standard exchangeability conditions. Causal learning from proxies is a challenging inverse problem which has to date remained unresolved. In this paper, we introduce a formal potential outcome framework for proximal causal learning, which while explicitly acknowledging covariate measurements as imperfect proxies of confounding mechanisms, offers an opportunity to learn about causal effects in settings where exchangeability on the basis of measured covariates fails. Sufficient conditions for nonparametric identification are given, leading to the proximal g-formula and corresponding proximal g-computation algorithm for estimation. These may be viewed as generalizations of Robins' foundational g-formula and g-computation algorithm, which account explicitly for bias due to unmeasured confounding. Both point treatment and time-varying treatment settings are considered, and an application of proximal g-computation of causal effects is given for illustration.},
  archiveprefix = {arXiv},
  keywords = {62A01,Statistics - Methodology},
  file = {/home/skynet3/Zotero/storage/ELH5NQ7C/Tchetgen et al. - 2020 - An Introduction to Proximal Causal Learning.pdf;/home/skynet3/Zotero/storage/4WPU3HK4/2009.html}
}

@online{teamManifoldUberStack,
  ids = {teamManifoldUberStacka},
  title = {Inside {{Manifold}}: {{Uber}}’s {{Stack}} for {{Debugging Machine Learning Models}} – {{Towards AI}}},
  shorttitle = {Inside {{Manifold}}},
  author = {Team, Towards AI},
  url = {https://towardsai.net/p/l/inside-manifold-ubers-stack-for-debugging-machine-learning-models, https://towardsai.net/p/l/inside-manifold-ubers-stack-for-debugging-machine-learning-models},
  urldate = {2022-11-11},
  abstract = {Originally published on Towards AI the World's Leading AI and Technology News and Media Company. If you are building an AI-related product or service, we invite you to consider...},
  langid = {american},
  file = {/home/skynet3/Zotero/storage/64344EZI/inside-manifold-ubers-stack-for-debugging-machine-learning-models.html}
}

@book{teamStanUserGuide,
  title = {Stan {{User}}’s {{Guide}}},
  author = {Team, Stan Development},
  url = {https://mc-stan.org/docs/stan-users-guide/index.html},
  urldate = {2022-11-11},
  abstract = {Stan user’s guide with examples and programming techniques.},
  file = {/home/skynet3/Zotero/storage/S9Q8DYKA/index.html}
}

@inreference{Tensor2022,
  ids = {Tensor2022a},
  title = {Tensor},
  booktitle = {Wikipedia},
  date = {2022-09-13T23:34:46Z},
  url = {https://en.wikipedia.org/w/index.php?title=Tensor&oldid=1110161319},
  urldate = {2022-11-11},
  abstract = {In mathematics, a tensor is an algebraic object that describes a multilinear relationship between sets of algebraic objects related to a vector space. Objects that tensors may map between include vectors and scalars, and even other tensors. There are many types of tensors, including scalars and vectors (which are the simplest tensors), dual vectors, multilinear maps between vector spaces, and even some operations such as the dot product. Tensors are defined independent of any basis, although they are often referred to by their components in a basis related to a particular coordinate system. Tensors have become important in physics because they provide a concise mathematical framework for formulating and solving physics problems in areas such as mechanics (stress, elasticity, fluid mechanics, moment of inertia, ...), electrodynamics (electromagnetic tensor, Maxwell tensor, permittivity, magnetic susceptibility, ...), general relativity (stress–energy tensor, curvature tensor, ...) and others. In applications, it is common to study situations in which a different tensor can occur at each point of an object; for example the stress within an object may vary from one location to another. This leads to the concept of a tensor field. In some areas, tensor fields are so ubiquitous that they are often simply called "tensors". Tullio Levi-Civita and Gregorio Ricci-Curbastro popularised tensors in 1900 – continuing the earlier work of Bernhard Riemann and Elwin Bruno Christoffel and others – as part of the absolute differential calculus. The concept enabled an alternative formulation of the intrinsic differential geometry of a manifold in the form of the Riemann curvature tensor.},
  langid = {english},
  annotation = {Page Version ID: 1110161319},
  file = {/home/skynet3/Zotero/storage/2R8NY9QZ/Tensor.html}
}

@online{TensorFlow,
  title = {{{TensorFlow}}},
  url = {https://www.tensorflow.org/},
  urldate = {2022-11-11},
  abstract = {An end-to-end open source machine learning platform for everyone. Discover TensorFlow's flexible ecosystem of tools, libraries and community resources.},
  langid = {english},
  organization = {{TensorFlow}},
  file = {/home/skynet3/Zotero/storage/QQANMK3G/www.tensorflow.org.html}
}

@article{therneauUsingTimeDependent,
  title = {Using {{Time Dependent Covariates}} and {{Time Dependent Coefficients}} in the {{Cox Model}}},
  author = {Therneau, Terry and Crowson, Cynthia and Atkinson, Elizabeth},
  pages = {31},
  langid = {english},
  file = {/home/skynet3/Zotero/storage/UQ4G28WY/Therneau et al. - Using Time Dependent Covariates and Time Dependent.pdf}
}

@online{thewhitetieWhyItThat2016,
  type = {Forum post},
  title = {Why Is It That Natural Log Changes Are Percentage Changes? {{What}} Is about Logs That Makes This So?},
  shorttitle = {Why Is It That Natural Log Changes Are Percentage Changes?},
  author = {{thewhitetie}},
  date = {2016-11-04},
  url = {https://stats.stackexchange.com/q/244199},
  urldate = {2022-11-11},
  organization = {{Cross Validated}},
  file = {/home/skynet3/Zotero/storage/IKKAUH2R/why-is-it-that-natural-log-changes-are-percentage-changes-what-is-about-logs-th.html}
}

@misc{thieleCutpointrImprovedEstimation2020,
  ids = {thieleCutpointrImprovedEstimation2020a},
  title = {Cutpointr: {{Improved Estimation}} and {{Validation}} of {{Optimal Cutpoints}} in {{R}}},
  shorttitle = {Cutpointr},
  author = {Thiele, Christian and Hirschfeld, Gerrit},
  date = {2020-02-21},
  number = {arXiv:2002.09209},
  eprint = {2002.09209},
  eprinttype = {arxiv},
  primaryclass = {stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2002.09209},
  url = {http://arxiv.org/abs/2002.09209},
  urldate = {2022-11-11},
  abstract = {'Optimal cutpoints' for binary classification tasks are often established by testing which cutpoint yields the best discrimination, for example the Youden index, in a specific sample. This results in 'optimal' cutpoints that are highly variable and systematically overestimate the out-of-sample performance. To address these concerns, the cutpointr package offers robust methods for estimating optimal cutpoints and the out-of-sample performance. The robust methods include bootstrapping and smoothing based on kernel estimation, generalized additive models, smoothing splines, and local regression. These methods can be applied to a wide range of binary-classification and cost-based metrics. cutpointr also provides mechanisms to utilize user-defined metrics and estimation methods. The package has capabilities for parallelization of the bootstrapping, including reproducible random number generation. Furthermore, it is pipe-friendly, for example for compatibility with functions from tidyverse. Various functions for plotting receiver operating characteristic curves, precision recall graphs, bootstrap results and other representations of the data are included. The package contains example data from a study on psychological characteristics and suicide attempts suitable for applying binary classification algorithms.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Computation},
  file = {/home/skynet3/Zotero/storage/7Y6HXAV4/Thiele and Hirschfeld - 2020 - cutpointr Improved Estimation and Validation of O.pdf;/home/skynet3/Zotero/storage/5Z8A74WY/2002.html}
}

@online{ThisBookPostGIS,
  title = {About This {{Book}} · {{PostGIS}} in {{Action}}, {{Second Edition}}},
  url = {https://livebook.manning.com/book/postgis-in-action-second-edition/about-this-book/},
  urldate = {2022-11-11},
  abstract = {liveBooks are enhanced books. They add narration, interactive exercises, code execution, and other features to eBooks.},
  langid = {american},
  file = {/home/skynet3/Zotero/storage/BLHSAYWY/about-this-book.html}
}

@article{thoemmesGraphicalRepresentationMissing2015,
  ids = {thoemmesGraphicalRepresentationMissing2015a},
  title = {Graphical {{Representation}} of {{Missing Data Problems}}},
  author = {Thoemmes, Felix and Mohan, Karthika},
  date = {2015-10-02},
  journaltitle = {Structural Equation Modeling: A Multidisciplinary Journal},
  shortjournal = {Structural Equation Modeling: A Multidisciplinary Journal},
  volume = {22},
  number = {4},
  pages = {631--642},
  issn = {1070-5511, 1532-8007},
  doi = {10.1080/10705511.2014.937378},
  url = {http://www.tandfonline.com/doi/full/10.1080/10705511.2014.937378},
  urldate = {2022-11-11},
  abstract = {Rubin’s classic missingness mechanisms are central to handling missing data and minimizing biases that can arise due to missingness. However, the formulaic expressions that posit certain independencies among missing and observed data are difficult to grasp. As a result, applied researchers often rely on informal translations of these assumptions. We present a graphical representation of missing data mechanism, formalized in Mohan, Pearl, and Tian (2013). We show that graphical models provide a tool for comprehending, encoding, and communicating assumptions about the missingness process. Furthermore, we demonstrate on several examples how graph-theoretical criteria can determine if biases due to missing data might emerge in some estimates of interests and which auxiliary variables are needed to control for such biases, given assumptions about the missingness process.},
  langid = {english},
  file = {/home/skynet3/Zotero/storage/RS4GI9R3/Thoemmes and Mohan - 2015 - Graphical Representation of Missing Data Problems.pdf}
}

@online{thomasFastAiHow2017,
  title = {Fast.Ai - {{How}} (and Why) to Create a Good Validation Set},
  author = {Thomas, Rachel},
  date = {2017-11-13},
  url = {https://www.fast.ai/posts/2017-11-13-validation-sets.html},
  urldate = {2022-11-11},
  abstract = {An all-too-common scenario: a seemingly impressive machine learning model is a complete failure when implemented in production.},
  langid = {english},
  file = {/home/skynet3/Zotero/storage/9RA8KDPM/2017-11-13-validation-sets.html}
}

@online{tiaoImplementingVariationalAutoencoders2017,
  title = {Implementing {{Variational Autoencoders}} in {{Keras}}: {{Beyond}} the {{Quickstart}}},
  shorttitle = {Implementing {{Variational Autoencoders}} in {{Keras}}},
  author = {Tiao, Louis},
  date = {2017-10-23T01:19:59+11:00},
  url = {http://louistiao.me/posts/implementing-variational-autoencoders-in-keras-beyond-the-quickstart-tutorial/},
  urldate = {2022-11-11},
  abstract = {Keras is awesome. It is a very well-designed library that clearly abides by its guiding principles of modularity and extensibility, enabling us to easily assemble powerful, complex models from primiti},
  langid = {english},
  organization = {{Louis Tiao}}
}

@online{tiaoPrimerPolyagammaRandom2021,
  title = {A {{Primer}} on {{Pólya-gamma Random Variables}} - {{Part II}}: {{Bayesian Logistic Regression}}},
  shorttitle = {A {{Primer}} on {{Pólya-gamma Random Variables}} - {{Part II}}},
  author = {Tiao, Louis},
  date = {2021-04-20T17:20:53+01:00},
  url = {https://tiao.io/post/polya-gamma-bayesian-logistic-regression/},
  urldate = {2022-11-11},
  abstract = {One weird trick to make exact inference in Bayesian logistic regression tractable.},
  langid = {american},
  organization = {{Louis Tiao}},
  file = {/home/skynet3/Zotero/storage/KJK6LET6/polya-gamma-bayesian-logistic-regression.html}
}

@online{TidyCharacterizationsModel,
  title = {Tidy {{Characterizations}} of {{Model Performance}}},
  url = {https://yardstick.tidymodels.org/index.html},
  urldate = {2022-11-11},
  abstract = {Tidy tools for quantifying how well model fits to a data set     such as confusion matrices, class probability curve summaries, and     regression metrics (e.g., RMSE).},
  langid = {english},
  file = {/home/skynet3/Zotero/storage/GIB7ZYJ2/index.html}
}

@online{TidyDataGeoms,
  title = {Tidy {{Data}} and {{Geoms}} for {{Bayesian Models}}},
  url = {http://mjskay.github.io/tidybayes/},
  urldate = {2022-11-11},
  abstract = {Compose data for and extract, manipulate, and visualize posterior draws from Bayesian models     (JAGS, Stan, rstanarm, brms, MCMCglmm, coda, ...) in a tidy data format. Functions are provided     to help extract tidy data frames of draws from Bayesian models and that generate point     summaries and intervals in a tidy format. In addition, ggplot2 geoms and stats are provided for     common visualization primitives like points with multiple uncertainty intervals, eye plots (intervals plus     densities), and fit curves with multiple, arbitrary uncertainty bands.},
  langid = {english}
}

@online{TidymodelsPackage,
  title = {The Tidymodels {{Package}}},
  url = {https://www.tidyverse.org/blog/2018/08/tidymodels-0-0-1/},
  urldate = {2022-11-11},
  abstract = {tidymodels 0.0.1 is on CRAN.},
  langid = {american},
  file = {/home/skynet3/Zotero/storage/UDEFIUQY/tidymodels-0-0-1.html}
}

@online{TidymodelsTidyposteriorBayesian,
  title = {Tidymodels/Tidyposterior: {{Bayesian}} Comparisons of Models Using Resampled Statistics},
  shorttitle = {Tidymodels/Tidyposterior},
  url = {https://github.com/tidymodels/tidyposterior},
  urldate = {2022-11-11},
  abstract = {Bayesian comparisons of models using resampled statistics - tidymodels/tidyposterior: Bayesian comparisons of models using resampled statistics},
  langid = {english},
  organization = {{GitHub}},
  file = {/home/skynet3/Zotero/storage/A9RRY9P5/tidyposterior.html}
}

@online{Tidyverse,
  title = {Tidyverse},
  url = {https://www.tidyverse.org/},
  urldate = {2022-11-11},
  abstract = {The tidyverse is an integrated collection of R packages designed to make data science fast, fluid, and fun.},
  langid = {american},
  file = {/home/skynet3/Zotero/storage/WAG65GSJ/www.tidyverse.org.html}
}

@online{TidyverseDtplyrData,
  title = {Tidyverse/Dtplyr: {{Data}} Table Backend for Dplyr},
  shorttitle = {Tidyverse/Dtplyr},
  url = {https://github.com/tidyverse/dtplyr},
  urldate = {2022-11-11},
  abstract = {Data table backend for dplyr. Contribute to tidyverse/dtplyr development by creating an account on GitHub.},
  langid = {english},
  organization = {{GitHub}},
  file = {/home/skynet3/Zotero/storage/PMWKCFJD/dtplyr.html}
}

@article{tikkaIdentifyingCausalEffects2017,
  title = {Identifying {{Causal Effects}} with the {{R Package}} Causaleffect},
  author = {Tikka, Santtu and Karvanen, Juha},
  date = {2017},
  journaltitle = {Journal of Statistical Software},
  shortjournal = {J. Stat. Soft.},
  volume = {76},
  number = {12},
  eprint = {1806.07161},
  eprinttype = {arxiv},
  primaryclass = {stat},
  issn = {1548-7660},
  doi = {10.18637/jss.v076.i12},
  url = {http://arxiv.org/abs/1806.07161},
  urldate = {2022-11-11},
  abstract = {Do-calculus is concerned with estimating the interventional distribution of an action from the observed joint probability distribution of the variables in a given causal structure. All identifiable causal effects can be derived using the rules of do-calculus, but the rules themselves do not give any direct indication whether the effect in question is identifiable or not. Shpitser and Pearl constructed an algorithm for identifying joint interventional distributions in causal models, which contain unobserved variables and induce directed acyclic graphs. This algorithm can be seen as a repeated application of the rules of do-calculus and known properties of probabilities, and it ultimately either derives an expression for the causal distribution, or fails to identify the effect, in which case the effect is non-identifiable. In this paper, the R package causaleffect is presented, which provides an implementation of this algorithm. Functionality of causaleffect is also demonstrated through examples.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Methodology},
  file = {/home/skynet3/Zotero/storage/5RUHNWQL/Tikka and Karvanen - 2017 - Identifying Causal Effects with the R Package caus.pdf;/home/skynet3/Zotero/storage/57RVFWQZ/1806.html}
}

@online{TimevisPackageCreating,
  title = {Timevis - {{An R}} Package for Creating Timeline Visualizations},
  url = {https://daattali.com/shiny/timevis-demo/},
  urldate = {2022-11-11},
  abstract = {Create rich and fully interactive timeline visualizations. Timelines can be included in Shiny apps and R markdown documents, or viewed from the R console and RStudio Viewer.}
}

@misc{tirumalaMemorizationOverfittingAnalyzing2022,
  ids = {tirumalaMemorizationOverfittingAnalyzing2022a},
  title = {Memorization {{Without Overfitting}}: {{Analyzing}} the {{Training Dynamics}} of {{Large Language Models}}},
  shorttitle = {Memorization {{Without Overfitting}}},
  author = {Tirumala, Kushal and Markosyan, Aram H. and Zettlemoyer, Luke and Aghajanyan, Armen},
  date = {2022-11-02},
  number = {arXiv:2205.10770},
  eprint = {2205.10770},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2205.10770},
  url = {http://arxiv.org/abs/2205.10770},
  urldate = {2022-11-11},
  abstract = {Despite their wide adoption, the underlying training and memorization dynamics of very large language models is not well understood. We empirically study exact memorization in causal and masked language modeling, across model sizes and throughout the training process. We measure the effects of dataset size, learning rate, and model size on memorization, finding that larger language models memorize training data faster across all settings. Surprisingly, we show that larger models can memorize a larger portion of the data before over-fitting and tend to forget less throughout the training process. We also analyze the memorization dynamics of different parts of speech and find that models memorize nouns and numbers first; we hypothesize and provide empirical evidence that nouns and numbers act as a unique identifier for memorizing individual training examples. Together, these findings present another piece of the broader puzzle of trying to understand what actually improves as models get bigger.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/home/skynet3/Zotero/storage/2KQ2NTQW/Tirumala et al. - 2022 - Memorization Without Overfitting Analyzing the Tr.pdf;/home/skynet3/Zotero/storage/52PMIIMR/2205.html}
}

@online{toorAvoidingTechnicalDebt2019,
  title = {Avoiding Technical Debt in Social Science Research},
  author = {Toor, Skye},
  date = {2019-11-21T22:40:53},
  url = {https://medium.com/pew-research-center-decoded/avoiding-technical-debt-in-social-science-research-54618194790a},
  urldate = {2022-11-11},
  abstract = {A lack of attention to detail in one research project can have compounding effects on future research projects.},
  langid = {english},
  organization = {{Pew Research Center: Decoded}},
  file = {/home/skynet3/Zotero/storage/9CEZI34L/avoiding-technical-debt-in-social-science-research-54618194790a.html}
}

@online{TopepoFESCode,
  title = {Topepo/{{FES}}: {{Code}} and {{Resources}} for "{{Feature Engineering}} and {{Selection}}: {{A Practical Approach}} for {{Predictive Models}}" by {{Kuhn}} and {{Johnson}}},
  shorttitle = {Topepo/{{FES}}},
  url = {https://github.com/topepo/FES},
  urldate = {2022-11-11},
  abstract = {Code and Resources for \&quot;Feature Engineering and Selection: A Practical Approach for Predictive Models\&quot; by Kuhn and Johnson - topepo/FES: Code and Resources for \&quot;Feature Engineering a...},
  langid = {english},
  organization = {{GitHub}},
  file = {/home/skynet3/Zotero/storage/EUMDRV8K/FES.html}
}

@online{TopicsEconometricsAdvances,
  title = {Topics in {{Econometrics}}: {{Advances}} in {{Causality}} and {{Foundations}} of {{Machine Learning}}},
  shorttitle = {Topics in {{Econometrics}}},
  url = {https://maxkasy.github.io/home/TopicsInEconometrics2019/},
  urldate = {2022-11-11},
  abstract = {Research on machine learning, experimental design, economic inequality, and optimal policy},
  langid = {english},
  organization = {{Maximilian Kasy}},
  file = {/home/skynet3/Zotero/storage/5S5ITUPS/TopicsInEconometrics2019.html}
}

@misc{torfiNaturalLanguageProcessing2021,
  ids = {torfiNaturalLanguageProcessing2021a},
  title = {Natural {{Language Processing Advancements By Deep Learning}}: {{A Survey}}},
  shorttitle = {Natural {{Language Processing Advancements By Deep Learning}}},
  author = {Torfi, Amirsina and Shirvani, Rouzbeh A. and Keneshloo, Yaser and Tavaf, Nader and Fox, Edward A.},
  date = {2021-02-27},
  number = {arXiv:2003.01200},
  eprint = {2003.01200},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2003.01200},
  url = {http://arxiv.org/abs/2003.01200},
  urldate = {2022-11-11},
  abstract = {Natural Language Processing (NLP) helps empower intelligent machines by enhancing a better understanding of the human language for linguistic-based human-computer communication. Recent developments in computational power and the advent of large amounts of linguistic data have heightened the need and demand for automating semantic analysis using data-driven approaches. The utilization of data-driven strategies is pervasive now due to the significant improvements demonstrated through the usage of deep learning methods in areas such as Computer Vision, Automatic Speech Recognition, and in particular, NLP. This survey categorizes and addresses the different aspects and applications of NLP that have benefited from deep learning. It covers core NLP tasks and applications and describes how deep learning methods and models advance these areas. We further analyze and compare different approaches and state-of-the-art models.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/skynet3/Zotero/storage/RLLHXUBU/Torfi et al. - 2021 - Natural Language Processing Advancements By Deep L.pdf;/home/skynet3/Zotero/storage/XAFM8INW/2003.html}
}

@online{TransformersScratch,
  title = {Transformers from {{Scratch}}},
  url = {https://e2eml.school/transformers.html},
  urldate = {2022-11-11},
  file = {/home/skynet3/Zotero/storage/YMFQJPV2/transformers.html}
}

@online{TransformersScratchPeterbloem,
  title = {Transformers from Scratch | Peterbloem.Nl},
  url = {https://peterbloem.nl/blog/transformers},
  urldate = {2022-11-11},
  file = {/home/skynet3/Zotero/storage/NQBV2NJJ/transformers.html}
}

@online{treadwayHowPlotXGBoost2021,
  title = {How to Plot {{XGBoost}} Trees in {{R}}},
  author = {Treadway, Andrew},
  date = {2021-04-28T00:02:48+00:00},
  url = {http://theautomatic.net/2021/04/28/how-to-plot-xgboost-trees-in-r/},
  urldate = {2022-11-11},
  abstract = {In this post, we cover how to plot XGBoost trees in R. We'll be using R's xgboost package to create these visualizations.},
  langid = {american},
  organization = {{Open Source Automation}},
  file = {/home/skynet3/Zotero/storage/KJIUY6LI/how-to-plot-xgboost-trees-in-r.html}
}

@misc{trisovicLargescaleStudyResearch2021,
  ids = {trisovicLargescaleStudyResearch2021a},
  title = {A Large-Scale Study on Research Code Quality and Execution},
  author = {Trisovic, Ana and Lau, Matthew K. and Pasquier, Thomas and Crosas, Mercè},
  date = {2021-03-23},
  number = {arXiv:2103.12793},
  eprint = {2103.12793},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2103.12793},
  url = {http://arxiv.org/abs/2103.12793},
  urldate = {2022-11-11},
  abstract = {This article presents a study on the quality and execution of research code from publicly-available replication datasets at the Harvard Dataverse repository. Research code is typically created by a group of scientists and published together with academic papers to facilitate research transparency and reproducibility. For this study, we define ten questions to address aspects impacting research reproducibility and reuse. First, we retrieve and analyze more than 2000 replication datasets with over 9000 unique R files published from 2010 to 2020. Second, we execute the code in a clean runtime environment to assess its ease of reuse. Common coding errors were identified, and some of them were solved with automatic code cleaning to aid code execution. We find that 74\textbackslash\% of R files crashed in the initial execution, while 56\textbackslash\% crashed when code cleaning was applied, showing that many errors can be prevented with good coding practices. We also analyze the replication datasets from journals' collections and discuss the impact of the journal policy strictness on the code re-execution rate. Finally, based on our results, we propose a set of recommendations for code dissemination aimed at researchers, journals, and repositories.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Digital Libraries,Computer Science - Software Engineering},
  file = {/home/skynet3/Zotero/storage/XMMAWS3K/Trisovic et al. - 2021 - A large-scale study on research code quality and e.pdf;/home/skynet3/Zotero/storage/2DJBTIL6/2103.html}
}

@article{trisovicLargescaleStudyResearch2022,
  title = {A Large-Scale Study on Research Code Quality and Execution},
  author = {Trisovic, Ana and Lau, Matthew K. and Pasquier, Thomas and Crosas, Mercè},
  date = {2022-02-21},
  journaltitle = {Scientific Data},
  shortjournal = {Sci Data},
  volume = {9},
  number = {1},
  pages = {60},
  publisher = {{Nature Publishing Group}},
  issn = {2052-4463},
  doi = {10.1038/s41597-022-01143-6},
  url = {https://www.nature.com/articles/s41597-022-01143-6},
  urldate = {2022-11-11},
  abstract = {This article presents a study on the quality and execution of research code from publicly-available replication datasets at the Harvard Dataverse repository. Research code is typically created by a group of scientists and published together with academic papers to facilitate research transparency and reproducibility. For this study, we define ten questions to address aspects impacting research reproducibility and reuse. First, we retrieve and analyze more than 2000 replication datasets with over 9000 unique R files published from 2010 to 2020. Second, we execute the code in a clean runtime environment to assess its ease of reuse. Common coding errors were identified, and some of them were solved with automatic code cleaning to aid code execution. We find that 74\% of R files failed to complete without error in the initial execution, while 56\% failed when code cleaning was applied, showing that many errors can be prevented with good coding practices. We also analyze the replication datasets from journals’ collections and discuss the impact of the journal policy strictness on the code re-execution rate. Finally, based on our results, we propose a set of recommendations for code dissemination aimed at researchers, journals, and repositories.},
  issue = {1},
  langid = {english},
  keywords = {Information technology,Research data,Software},
  file = {/home/skynet3/Zotero/storage/8DV6UH9Y/Trisovic et al. - 2022 - A large-scale study on research code quality and e.pdf;/home/skynet3/Zotero/storage/56PEZBYX/s41597-022-01143-6.html}
}

@article{trockmanPatchesAreAll2021,
  title = {Patches {{Are All You Need}}?},
  author = {Trockman, Asher and Kolter, J. Zico},
  date = {2021-11-23},
  url = {https://openreview.net/forum?id=TVHS5Y4dNvM},
  urldate = {2022-11-11},
  abstract = {Although convolutional networks have been the dominant architecture for vision tasks for many years, recent experiments have shown that Transformer-based models, most notably the Vision Transformer (ViT), may exceed their performance in some settings. However, due to the quadratic runtime of the self-attention layers in Transformers, ViTs require the use of patch embeddings, which group together small regions of the image into single input features, in order to be applied to larger image sizes. This raises a question: Is the performance of ViTs due to the inherently-more-powerful Transformer architecture, or is it at least partly due to using patches as the input representation? In this paper, we present some evidence for the latter: specifically, we propose the ConvMixer, an extremely simple model that is similar in spirit to the ViT and the even-more-basic MLP-Mixer in that it operates directly on patches as input, separates the mixing of spatial and channel dimensions, and maintains equal size and resolution throughout the network. In contrast, however, the ConvMixer uses only standard convolutions to achieve the mixing steps. Despite its simplicity, we show that the ConvMixer outperforms the ViT, MLP-Mixer, and some of their variants for similar parameter counts and data set sizes, in addition to outperforming classical vision models such as the ResNet. Our code is available at https://github.com/tmp-iclr/convmixer.},
  langid = {english},
  file = {/home/skynet3/Zotero/storage/5R83NYHN/Trockman and Kolter - 2021 - Patches Are All You Need.pdf}
}

@online{TroubleControllingBlocks,
  title = {The Trouble with \&lsquo;Controlling for Blocks\&rsquo; - {{DeclareDesign}}},
  url = {https://declaredesign.org/blog/biased-fixed-effects.html},
  urldate = {2022-11-11},
  file = {/home/skynet3/Zotero/storage/V86HWU5I/biased-fixed-effects.html}
}

@article{truongSelectiveReviewOffline2020,
  ids = {truongSelectiveReviewOffline2020a},
  title = {Selective Review of Offline Change Point Detection Methods},
  author = {Truong, Charles and Oudre, Laurent and Vayatis, Nicolas},
  date = {2020-02},
  journaltitle = {Signal Processing},
  shortjournal = {Signal Processing},
  volume = {167},
  eprint = {1801.00718},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  pages = {107299},
  issn = {01651684},
  doi = {10.1016/j.sigpro.2019.107299},
  url = {http://arxiv.org/abs/1801.00718},
  urldate = {2022-11-11},
  abstract = {This article presents a selective survey of algorithms for the offline detection of multiple change points in multivariate time series. A general yet structuring methodological strategy is adopted to organize this vast body of work. More precisely, detection algorithms considered in this review are characterized by three elements: a cost function, a search method and a constraint on the number of changes. Each of those elements is described, reviewed and discussed separately. Implementations of the main algorithms described in this article are provided within a Python package called ruptures.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computational Engineering; Finance; and Science,Statistics - Computation,Statistics - Methodology},
  file = {/home/skynet3/Zotero/storage/JYZHW2RZ/Truong et al. - 2020 - Selective review of offline change point detection.pdf;/home/skynet3/Zotero/storage/SPT4A6KK/1801.html}
}

@online{TrustLIMEYes,
  title = {Trust in {{LIME}}: {{Yes}}, {{No}}, {{Maybe So}}?},
  shorttitle = {Trust in {{LIME}}},
  url = {https://www.dominodatalab.com/blog/trust-in-lime-local-interpretable-model-agnostic-explanations},
  urldate = {2022-11-11},
  abstract = {Brief Overview of LIME (Local Interpretable Model-Agnostic Explanations)},
  langid = {english},
  file = {/home/skynet3/Zotero/storage/ZELGU3YN/trust-in-lime-local-interpretable-model-agnostic-explanations.html}
}

@online{TrustworthyMachineLearning,
  title = {Trustworthy {{Machine Learning}} by {{Kush R}}. {{Varshney}}},
  url = {http://www.trustworthymachinelearning.com/},
  urldate = {2022-11-11},
  file = {/home/skynet3/Zotero/storage/MBQDUHHW/www.trustworthymachinelearning.com.html}
}

@article{turnerSelectivePublicationAntidepressant2022,
  title = {Selective Publication of Antidepressant Trials and Its Influence on Apparent Efficacy: {{Updated}} Comparisons and Meta-Analyses of Newer versus Older Trials},
  shorttitle = {Selective Publication of Antidepressant Trials and Its Influence on Apparent Efficacy},
  author = {Turner, Erick H. and Cipriani, Andrea and Furukawa, Toshi A. and Salanti, Georgia and de Vries, Ymkje Anna},
  date = {2022-01-19},
  journaltitle = {PLOS Medicine},
  shortjournal = {PLOS Medicine},
  volume = {19},
  number = {1},
  pages = {e1003886},
  publisher = {{Public Library of Science}},
  issn = {1549-1676},
  doi = {10.1371/journal.pmed.1003886},
  url = {https://journals.plos.org/plosmedicine/article?id=10.1371/journal.pmed.1003886},
  urldate = {2022-11-11},
  abstract = {Background Valid assessment of drug efficacy and safety requires an evidence base free of reporting bias. Using trial reports in Food and Drug Administration (FDA) drug approval packages as a gold standard, we previously found that the published literature inflated the apparent efficacy of antidepressant drugs. The objective of the current study was to determine whether this has improved with recently approved drugs. Methods and findings Using medical and statistical reviews in FDA drug approval packages, we identified 30 Phase II/III double-blind placebo-controlled acute monotherapy trials, involving 13,747 patients, of desvenlafaxine, vilazodone, levomilnacipran, and vortioxetine; we then identified corresponding published reports. We compared the data from this newer cohort of antidepressants (approved February 2008 to September 2013) with the previously published dataset on 74 trials of 12 older antidepressants (approved December 1987 to August 2002). Using logistic regression, we examined the effects of trial outcome and trial cohort (newer versus older) on transparent reporting (whether published and FDA conclusions agreed). Among newer antidepressants, transparent publication occurred more with positive (15/15 = 100\%) than negative (7/15 = 47\%) trials (OR 35.1, CI95\% 1.8 to 693). Controlling for trial outcome, transparent publication occurred more with newer than older trials (OR 6.6, CI95\% 1.6 to 26.4). Within negative trials, transparent reporting increased from 11\% to 47\%. We also conducted and contrasted FDA- and journal-based meta-analyses. For newer antidepressants, FDA-based effect size (ESFDA) was 0.24 (CI95\% 0.18 to 0.30), while journal-based effect size (ESJournals) was 0.29 (CI95\% 0.23 to 0.36). Thus, effect size inflation, presumably due to reporting bias, was 0.05, less than for older antidepressants (0.10). Limitations of this study include a small number of trials and drugs—belonging to a single class—and a focus on efficacy (versus safety). Conclusions Reporting bias persists but appears to have diminished for newer, compared to older, antidepressants. Continued efforts are needed to further improve transparency in the scientific literature.},
  langid = {english},
  keywords = {Antidepressants,Depression,Drug interactions,Drug research and development,Drug therapy,Metaanalysis,Publication ethics,Randomized controlled trials},
  file = {/home/skynet3/Zotero/storage/HU96APDX/Turner et al. - 2022 - Selective publication of antidepressant trials and.pdf;/home/skynet3/Zotero/storage/KD3QNK3G/article.html}
}

@online{TutorialJAX101,
  title = {Tutorial: {{JAX}} 101 — {{JAX}} Documentation},
  url = {https://jax.readthedocs.io/en/latest/jax-101/index.html},
  urldate = {2022-11-11},
  file = {/home/skynet3/Zotero/storage/G9R6FTP3/index.html}
}

@article{tuvFeatureSelectionEnsembles2009,
  ids = {tuvFeatureSelectionEnsembles2009a},
  title = {Feature {{Selection}} with {{Ensembles}}, {{Artificial Variables}}, and {{Redundancy Elimination}}},
  author = {Tuv, Eugene and Borisov, Alexander and Runger, George and Torkkola, Kari},
  date = {2009-07-01},
  journaltitle = {Journal of Machine Learning Research},
  shortjournal = {Journal of Machine Learning Research},
  volume = {10},
  pages = {1341--1366},
  doi = {10.1145/1577069.1755828},
  abstract = {Predictive models benefit from a compact, non-redundant subset of features that improves interpretability and generalization. Modern data sets are wide, dirty, mixed with both numerical and categorical predictors, and may contain interactive effects that require complex models. This is a challenge for filters, wrappers, and embedded feature selection methods. We describe details of an algorithm using tree-based ensembles to generate a compact subset of non-redundant features. Parallel and serial ensembles of trees are combined into a mixed method that can uncover masking and detect features of secondary effect. Simulated and actual examples illustrate the effectiveness of the approach.}
}

@online{UjjwalkarnDataScienceRCurated,
  title = {Ujjwalkarn/{{DataScienceR}}: A Curated List of {{R}} Tutorials for {{Data Science}}, {{NLP}} and {{Machine Learning}}},
  shorttitle = {Ujjwalkarn/{{DataScienceR}}},
  url = {https://github.com/ujjwalkarn/DataScienceR},
  urldate = {2022-11-11},
  abstract = {a curated list of R tutorials for Data Science, NLP and Machine Learning  - ujjwalkarn/DataScienceR: a curated list of R tutorials for Data Science, NLP and Machine Learning},
  langid = {english},
  organization = {{GitHub}},
  file = {/home/skynet3/Zotero/storage/N83C6TN7/DataScienceR.html}
}

@online{UMPersonalWorld,
  title = {U-{{M Personal World Wide Web Server}}},
  url = {http://www-personal.umich.edu/},
  urldate = {2022-11-11},
  file = {/home/skynet3/Zotero/storage/PKBVGYGK/www-personal.umich.edu.html}
}

@online{UnconfrandomPdf,
  ids = {UnconfrandomPdfa},
  title = {8\_unconf-Random.Pdf},
  url = {https://drive.google.com/file/d/1nV8QMLxwXi-iWSqiwRN4KnMSWfoWJned/view?usp=embed_facebook},
  urldate = {2022-11-11},
  organization = {{Google Docs}},
  file = {/home/skynet3/Zotero/storage/KQHRSJFU/view.html}
}

@online{UnderstandingBiasVarianceTradeoff,
  title = {Understanding the {{Bias-Variance Tradeoff}}},
  url = {http://scott.fortmann-roe.com/docs/BiasVariance.html},
  urldate = {2022-11-11}
}

@incollection{UnderstandingChoosingRight2012,
  title = {Understanding and {{Choosing}} the {{Right Probability Distributions}}},
  booktitle = {Advanced {{Analytical Models}}},
  date = {2012},
  pages = {899--917},
  publisher = {{John Wiley \& Sons, Ltd}},
  doi = {10.1002/9781119197096.app03},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/9781119197096.app03},
  urldate = {2022-11-11},
  isbn = {978-1-119-19709-6},
  langid = {english},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/9781119197096.app03},
  file = {/home/skynet3/Zotero/storage/7CQ2L6L9/2012 - Understanding and Choosing the Right Probability D.pdf;/home/skynet3/Zotero/storage/6ER9VNKU/9781119197096.html}
}

@online{UnderstandingConvolutionalNeural2015,
  title = {Understanding {{Convolutional Neural Networks}} for {{NLP}}},
  date = {2015-11-07T00:00:00+00:00},
  url = {https://dennybritz.com/posts/wildml/understanding-convolutional-neural-networks-for-nlp/},
  urldate = {2022-11-11},
  abstract = {When we hear about Convolutional Neural Network (CNNs), we typically think of Computer Vision.},
  langid = {english},
  organization = {{Denny's Blog}}
}

@online{UnitsSTAT157,
  title = {Units — {{STAT}} 157, {{Spring}} 19 Documentation},
  url = {http://courses.d2l.ai/berkeley-stat-157/units/index.html},
  urldate = {2022-11-11}
}

@online{UniversityPittsburghError,
  title = {University of {{Pittsburgh}}: {{Error HTTP}} 403 - {{Forbidden}}},
  url = {https://sites.pitt.edu/},
  urldate = {2022-11-11},
  file = {/home/skynet3/Zotero/storage/YDBEUNXR/sites.pitt.edu.html}
}

@online{universityStatisticsDataScience,
  ids = {universityStatisticsDataSciencea},
  title = {Statistics \& {{Data Science}} - {{Statistics}} \& {{Data Science}} - {{Dietrich College}} of {{Humanities}} and {{Social Sciences}} - {{Carnegie Mellon University}}},
  author = {University, Carnegie Mellon},
  url = {http://www.cmu.edu/dietrich/statistics-datascience/index.html},
  urldate = {2022-11-11},
  abstract = {Statistics \& Data Science},
  langid = {english},
  file = {/home/skynet3/Zotero/storage/ERSHDVA7/index.html}
}

@online{universityTwoExperimentsPeer2022,
  title = {Two {{Experiments}} in {{Peer Review}}: {{Posting Preprints}} and {{Citation Bias}}},
  shorttitle = {Two {{Experiments}} in {{Peer Review}}},
  author = {University, Carnegie Mellon, Machine Learning Department},
  date = {2022-04-01T15:42:16-04:00},
  url = {https://blog.ml.cmu.edu/2022/04/01/peer_review_experiments_2022/},
  urldate = {2022-11-11},
  abstract = {There is increasing interest in computer science and elsewhere to understand and improve peer review (see here for an overview). With this motivation, we conducted two experiments regarding peer review which we summarize in this blog post.},
  langid = {american},
  organization = {{Machine Learning Blog | ML@CMU | Carnegie Mellon University}}
}

@online{UnofficialGuidanceVarious,
  title = {Unofficial Guidance on Various Topics by {{Social Science Data Editors}}},
  url = {https://social-science-data-editors.github.io/guidance/},
  urldate = {2022-11-11},
  abstract = {Guidance for authors wishing to create data and code supplements, and for replicators.},
  langid = {american},
  organization = {{Data and Code Guidance by Data Editors}},
  file = {/home/skynet3/Zotero/storage/35YZ7VYY/guidance.html}
}

@online{UsefulEquationsNonlinear,
  title = {Some Useful Equations for Nonlinear Regression in {{R}}},
  url = {https://www.statforbiology.com/nonlinearregression/usefulequations},
  urldate = {2022-11-11},
  file = {/home/skynet3/Zotero/storage/AEGQZFDN/usefulequations.html}
}

@online{UsersAaltoFi,
  title = {Users.Aalto.Fi},
  url = {https://users.aalto.fi/},
  urldate = {2022-11-11},
  file = {/home/skynet3/Zotero/storage/659TEIHT/users.aalto.fi.html}
}

@online{UsingJAXAccelerate,
  ids = {UsingJAXAcceleratea},
  title = {Using {{JAX}} to Accelerate Our Research},
  url = {https://www.deepmind.com/blog/using-jax-to-accelerate-our-research},
  urldate = {2022-11-11},
  abstract = {DeepMind engineers accelerate our research by building tools, scaling up algorithms, and creating challenging virtual and physical worlds for training and testing artificial intelligence (AI) systems. As part of this work, we constantly evaluate new machine learning libraries and frameworks.},
  langid = {english},
  file = {/home/skynet3/Zotero/storage/V3GZDXLH/using-jax-to-accelerate-our-research.html}
}

@online{UWComputerSciences,
  title = {{{UW Computer Sciences User Pages}}},
  url = {https://pages.cs.wisc.edu/},
  urldate = {2022-11-11},
  file = {/home/skynet3/Zotero/storage/RXDBTKSJ/pages.cs.wisc.edu.html}
}

@misc{valle-perezDeepLearningGeneralizes2019,
  title = {Deep Learning Generalizes Because the Parameter-Function Map Is Biased towards Simple Functions},
  author = {Valle-Pérez, Guillermo and Camargo, Chico Q. and Louis, Ard A.},
  date = {2019-04-21},
  number = {arXiv:1805.08522},
  eprint = {1805.08522},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1805.08522},
  url = {http://arxiv.org/abs/1805.08522},
  urldate = {2022-11-11},
  abstract = {Deep neural networks (DNNs) generalize remarkably well without explicit regularization even in the strongly over-parametrized regime where classical learning theory would instead predict that they would severely overfit. While many proposals for some kind of implicit regularization have been made to rationalise this success, there is no consensus for the fundamental reason why DNNs do not strongly overfit. In this paper, we provide a new explanation. By applying a very general probability-complexity bound recently derived from algorithmic information theory (AIT), we argue that the parameter-function map of many DNNs should be exponentially biased towards simple functions. We then provide clear evidence for this strong simplicity bias in a model DNN for Boolean functions, as well as in much larger fully connected and convolutional networks applied to CIFAR10 and MNIST. As the target functions in many real problems are expected to be highly structured, this intrinsic simplicity bias helps explain why deep networks generalize well on real world problems. This picture also facilitates a novel PAC-Bayes approach where the prior is taken over the DNN input-output function space, rather than the more conventional prior over parameter space. If we assume that the training algorithm samples parameters close to uniformly within the zero-error region then the PAC-Bayes theorem can be used to guarantee good expected generalization for target functions producing high-likelihood training sets. By exploiting recently discovered connections between DNNs and Gaussian processes to estimate the marginal likelihood, we produce relatively tight generalization PAC-Bayes error bounds which correlate well with the true error on realistic datasets such as MNIST and CIFAR10 and for architectures including convolutional and fully connected networks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/home/skynet3/Zotero/storage/NAU43KAR/Valle-Pérez et al. - 2019 - Deep learning generalizes because the parameter-fu.pdf;/home/skynet3/Zotero/storage/BL7VRJJR/1805.html}
}

@article{vandenbrouckeTestNegativeDesignsDifferences2019,
  title = {Test-{{Negative Designs}}: {{Differences}} and {{Commonalities}} with {{Other Case}}–{{Control Studies}} with “{{Other Patient}}” {{Controls}}},
  shorttitle = {Test-{{Negative Designs}}},
  author = {Vandenbroucke, Jan P. and Pearce, Neil},
  date = {2019-11},
  journaltitle = {Epidemiology},
  volume = {30},
  number = {6},
  pages = {838--844},
  issn = {1044-3983},
  doi = {10.1097/EDE.0000000000001088},
  url = {https://journals.lww.com/epidem/Abstract/2019/11000/Test_Negative_Designs__Differences_and.10.aspx},
  urldate = {2022-11-11},
  abstract = {Test-negative studies recruit cases who attend a healthcare facility and test positive for a particular disease; controls are patients undergoing the same tests for the same reasons at the same healthcare facility and who test negative. The design is often used for vaccine efficacy studies, but not exclusively, and has been posited as a separate type of study design, different from case–control studies because the controls are not sampled from a wider source population. However, the design is a special case of a broader class of case–control designs that identify cases and sample “other patient” controls from the same healthcare facilities. Therefore, we consider that new insights into the test-negative design can be obtained by viewing them as case–control studies with “other patient” controls; in this context, we explore differences and commonalities, to better define the advantages and disadvantages of the test-negative design in various circumstances. The design has the advantage of similar participation rates, information quality and completeness, referral/catchment areas, initial presentation, diagnostic suspicion tendencies, and preferences by doctors. Under certain assumptions, valid population odds ratios can be estimated with the test-negative design, just as with case–control studies with “other patient” controls. Interestingly, directed acyclic graphs (DAGs) are not completely helpful in explaining why the design works. The use of test-negative designs may not completely resolve all potential biases, but they are a valid study design option, and will in some circumstances lead to less bias, as well as often the most practical one.},
  langid = {american},
  file = {/home/skynet3/Zotero/storage/PUSCYE9X/Vandenbroucke and Pearce - 2019 - Test-Negative Designs Differences and Commonaliti.pdf;/home/skynet3/Zotero/storage/LLDVFGEH/Test_Negative_Designs__Differences_and.10.html}
}

@misc{vanlaarhovenL2RegularizationBatch2017,
  title = {L2 {{Regularization}} versus {{Batch}} and {{Weight Normalization}}},
  author = {van Laarhoven, Twan},
  options = {useprefix=true},
  date = {2017-06-16},
  number = {arXiv:1706.05350},
  eprint = {1706.05350},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1706.05350},
  url = {http://arxiv.org/abs/1706.05350},
  urldate = {2022-11-11},
  abstract = {Batch Normalization is a commonly used trick to improve the training of deep neural networks. These neural networks use L2 regularization, also called weight decay, ostensibly to prevent overfitting. However, we show that L2 regularization has no regularizing effect when combined with normalization. Instead, regularization has an influence on the scale of weights, and thereby on the effective learning rate. We investigate this dependence, both in theory, and experimentally. We show that popular optimization methods such as ADAM only partially eliminate the influence of normalization on the learning rate. This leads to a discussion on other ways to mitigate this issue.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/skynet3/Zotero/storage/D4IQC4JJ/van Laarhoven - 2017 - L2 Regularization versus Batch and Weight Normaliz.pdf;/home/skynet3/Zotero/storage/N9YEZQRP/1706.html}
}

@article{vannoordenScienceThatNever2017,
  title = {The Science That’s Never Been Cited},
  author = {Van Noorden, Richard},
  date = {2017-12-13},
  journaltitle = {Nature},
  volume = {552},
  number = {7684},
  pages = {162--164},
  publisher = {{Nature Publishing Group}},
  doi = {10.1038/d41586-017-08404-0},
  url = {https://www.nature.com/articles/d41586-017-08404-0},
  urldate = {2022-11-11},
  abstract = {Nature investigates how many papers really end up without a single citation.},
  issue = {7684},
  langid = {english},
  keywords = {Authorship,Databases,Publishing},
  annotation = {Bandiera\_abtest: a Cg\_type: News Feature Subject\_term: Databases, Authorship, Publishing},
  file = {/home/skynet3/Zotero/storage/KINI3NQE/Van Noorden - 2017 - The science that’s never been cited.pdf;/home/skynet3/Zotero/storage/9SFW2KG3/d41586-017-08404-0.html}
}

@article{vansmedenCriticalAppraisalArtificial2022,
  ids = {vansmedenCriticalAppraisalArtificial2022a},
  title = {Critical Appraisal of Artificial Intelligence-Based Prediction Models for Cardiovascular Disease},
  author = {van Smeden, Maarten and Heinze, Georg and Van Calster, Ben and Asselbergs, Folkert W and Vardas, Panos E and Bruining, Nico and de Jaegere, Peter and Moore, Jason H and Denaxas, Spiros and Boulesteix, Anne Laure and Moons, Karel G M},
  options = {useprefix=true},
  date = {2022-08-14},
  journaltitle = {European Heart Journal},
  shortjournal = {European Heart Journal},
  volume = {43},
  number = {31},
  pages = {2921--2930},
  issn = {0195-668X},
  doi = {10.1093/eurheartj/ehac238},
  url = {https://doi.org/10.1093/eurheartj/ehac238},
  urldate = {2022-11-11},
  abstract = {The medical field has seen a rapid increase in the development of artificial intelligence (AI)-based prediction models. With the introduction of such AI-based prediction model tools and software in cardiovascular patient care, the cardiovascular researcher and healthcare professional are challenged to understand the opportunities as well as the limitations of the AI-based predictions. In this article, we present 12 critical questions for cardiovascular health professionals to ask when confronted with an AI-based prediction model. We aim to support medical professionals to distinguish the AI-based prediction models that can add value to patient care from the AI that does not.},
  file = {/home/skynet3/Zotero/storage/NKNTAHJQ/van Smeden et al. - 2022 - Critical appraisal of artificial intelligence-base.pdf;/home/skynet3/Zotero/storage/AFPJ9F29/6593474.html}
}

@online{VariabilityModelSpecification2021,
  title = {The {{Variability}} of {{Model Specification}}},
  date = {2021-10-06T03:59:19+00:00},
  url = {https://deepai.org/publication/the-variability-of-model-specification},
  urldate = {2022-11-11},
  abstract = {10/06/21 - It's regarded as an axiom that a good model is one that compromises between bias and variance. The bias is measured in training co...},
  organization = {{DeepAI}},
  file = {/home/skynet3/Zotero/storage/VA2RQUQZ/the-variability-of-model-specification.html}
}

@misc{varmaLearningDependencyStructures2019,
  ids = {varmaLearningDependencyStructures2019a},
  title = {Learning {{Dependency Structures}} for {{Weak Supervision Models}}},
  author = {Varma, Paroma and Sala, Frederic and He, Ann and Ratner, Alexander and Ré, Christopher},
  date = {2019-03-14},
  number = {arXiv:1903.05844},
  eprint = {1903.05844},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/1903.05844},
  urldate = {2022-11-11},
  abstract = {Labeling training data is a key bottleneck in the modern machine learning pipeline. Recent weak supervision approaches combine labels from multiple noisy sources by estimating their accuracies without access to ground truth labels; however, estimating the dependencies among these sources is a critical challenge. We focus on a robust PCA-based algorithm for learning these dependency structures, establish improved theoretical recovery rates, and outperform existing methods on various real-world tasks. Under certain conditions, we show that the amount of unlabeled data needed can scale sublinearly or even logarithmically with the number of sources \$m\$, improving over previous efforts that ignore the sparsity pattern in the dependency structure and scale linearly in \$m\$. We provide an information-theoretic lower bound on the minimum sample complexity of the weak supervision setting. Our method outperforms weak supervision approaches that assume conditionally-independent sources by up to 4.64 F1 points and previous structure learning approaches by up to 4.41 F1 points on real-world relation extraction and image classification tasks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/skynet3/Zotero/storage/38Z5EXIZ/Varma et al. - 2019 - Learning Dependency Structures for Weak Supervisio.pdf;/home/skynet3/Zotero/storage/5B6VZHMK/1903.html}
}

@misc{vaswaniAttentionAllYou2017,
  title = {Attention {{Is All You Need}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  date = {2017-12-05},
  number = {arXiv:1706.03762},
  eprint = {1706.03762},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1706.03762},
  url = {http://arxiv.org/abs/1706.03762},
  urldate = {2022-11-11},
  abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/skynet3/Zotero/storage/7SLIUDYA/Vaswani et al. - 2017 - Attention Is All You Need.pdf;/home/skynet3/Zotero/storage/WSQWLNDS/1706.html}
}

@misc{vatsRevisitingGelmanRubinDiagnostic2020,
  title = {Revisiting the {{Gelman-Rubin Diagnostic}}},
  author = {Vats, Dootika and Knudson, Christina},
  date = {2020-09-16},
  number = {arXiv:1812.09384},
  eprint = {1812.09384},
  eprinttype = {arxiv},
  primaryclass = {stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1812.09384},
  url = {http://arxiv.org/abs/1812.09384},
  urldate = {2022-11-11},
  abstract = {Gelman and Rubin's (1992) convergence diagnostic is one of the most popular methods for terminating a Markov chain Monte Carlo (MCMC) sampler. Since the seminal paper, researchers have developed sophisticated methods for estimating variance of Monte Carlo averages. We show that these estimators find immediate use in the Gelman-Rubin statistic, a connection not previously established in the literature. We incorporate these estimators to upgrade both the univariate and multivariate Gelman-Rubin statistics, leading to improved stability in MCMC termination time. An immediate advantage is that our new Gelman-Rubin statistic can be calculated for a single chain. In addition, we establish a one-to-one relationship between the Gelman-Rubin statistic and effective sample size. Leveraging this relationship, we develop a principled termination criterion for the Gelman-Rubin statistic. Finally, we demonstrate the utility of our improved diagnostic via examples.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Computation,Statistics - Methodology},
  file = {/home/skynet3/Zotero/storage/ZGC3ZIR7/Vats and Knudson - 2020 - Revisiting the Gelman-Rubin Diagnostic.pdf;/home/skynet3/Zotero/storage/FC5PFB4I/1812.html}
}

@article{vehtariPracticalBayesianModel2017,
  ids = {vehtariPracticalBayesianModel2017a},
  title = {Practical {{Bayesian}} Model Evaluation Using Leave-One-out Cross-Validation and {{WAIC}}},
  author = {Vehtari, Aki and Gelman, Andrew and Gabry, Jonah},
  date = {2017-09},
  journaltitle = {Statistics and Computing},
  shortjournal = {Stat Comput},
  volume = {27},
  number = {5},
  pages = {1413--1432},
  issn = {0960-3174, 1573-1375},
  doi = {10.1007/s11222-016-9696-4},
  url = {http://link.springer.com/10.1007/s11222-016-9696-4},
  urldate = {2022-11-11},
  langid = {english},
  file = {/home/skynet3/Zotero/storage/9ZXAY437/Vehtari et al. - 2017 - Practical Bayesian model evaluation using leave-on.pdf}
}

@misc{veitchSenseSensitivityAnalysis2020,
  title = {Sense and {{Sensitivity Analysis}}: {{Simple Post-Hoc Analysis}} of {{Bias Due}} to {{Unobserved Confounding}}},
  shorttitle = {Sense and {{Sensitivity Analysis}}},
  author = {Veitch, Victor and Zaveri, Anisha},
  date = {2020-12-08},
  number = {arXiv:2003.01747},
  eprint = {2003.01747},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2003.01747},
  url = {http://arxiv.org/abs/2003.01747},
  urldate = {2022-11-11},
  abstract = {It is a truth universally acknowledged that an observed association without known mechanism must be in want of a causal estimate. However, causal estimation from observational data often relies on the (untestable) assumption of `no unobserved confounding'. Violations of this assumption can induce bias in effect estimates. In principle, such bias could invalidate or reverse the conclusions of a study. However, in some cases, we might hope that the influence of unobserved confounders is weak relative to a `large' estimated effect, so the qualitative conclusions are robust to bias from unobserved confounding. The purpose of this paper is to develop \textbackslash emph\{Austen plots\}, a sensitivity analysis tool to aid such judgments by making it easier to reason about potential bias induced by unobserved confounding. We formalize confounding strength in terms of how strongly the confounder influences treatment assignment and outcome. For a target level of bias, an Austen plot shows the minimum values of treatment and outcome influence required to induce that level of bias. Domain experts can then make subjective judgments about whether such strong confounders are plausible. To aid this judgment, the Austen plot additionally displays the estimated influence strength of (groups of) the observed covariates. Austen plots generalize the classic sensitivity analysis approach of Imbens [Imb03]. Critically, Austen plots allow any approach for modeling the observed data and producing the initial estimate. We illustrate the tool by assessing biases for several real causal inference problems, using a variety of machine learning approaches for the initial data analysis. Code is available at https://github.com/anishazaveri/austen\_plots},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning,Statistics - Methodology},
  file = {/home/skynet3/Zotero/storage/8WGFLGKP/Veitch and Zaveri - 2020 - Sense and Sensitivity Analysis Simple Post-Hoc An.pdf;/home/skynet3/Zotero/storage/3WHVY74N/2003.html}
}

@misc{veitchSenseSensitivityAnalysis2020a,
  title = {Sense and {{Sensitivity Analysis}}: {{Simple Post-Hoc Analysis}} of {{Bias Due}} to {{Unobserved Confounding}}},
  shorttitle = {Sense and {{Sensitivity Analysis}}},
  author = {Veitch, Victor and Zaveri, Anisha},
  date = {2020-12-08},
  number = {arXiv:2003.01747},
  eprint = {2003.01747},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2003.01747},
  url = {http://arxiv.org/abs/2003.01747},
  urldate = {2022-11-15},
  abstract = {It is a truth universally acknowledged that an observed association without known mechanism must be in want of a causal estimate. However, causal estimation from observational data often relies on the (untestable) assumption of `no unobserved confounding'. Violations of this assumption can induce bias in effect estimates. In principle, such bias could invalidate or reverse the conclusions of a study. However, in some cases, we might hope that the influence of unobserved confounders is weak relative to a `large' estimated effect, so the qualitative conclusions are robust to bias from unobserved confounding. The purpose of this paper is to develop \textbackslash emph\{Austen plots\}, a sensitivity analysis tool to aid such judgments by making it easier to reason about potential bias induced by unobserved confounding. We formalize confounding strength in terms of how strongly the confounder influences treatment assignment and outcome. For a target level of bias, an Austen plot shows the minimum values of treatment and outcome influence required to induce that level of bias. Domain experts can then make subjective judgments about whether such strong confounders are plausible. To aid this judgment, the Austen plot additionally displays the estimated influence strength of (groups of) the observed covariates. Austen plots generalize the classic sensitivity analysis approach of Imbens [Imb03]. Critically, Austen plots allow any approach for modeling the observed data and producing the initial estimate. We illustrate the tool by assessing biases for several real causal inference problems, using a variety of machine learning approaches for the initial data analysis. Code is available at https://github.com/anishazaveri/austen\_plots},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning,Statistics - Methodology},
  file = {/home/skynet3/Zotero/storage/BE589I22/Veitch and Zaveri - 2020 - Sense and Sensitivity Analysis Simple Post-Hoc An.pdf;/home/skynet3/Zotero/storage/2C6CHKBE/2003.html}
}

@misc{vexlerValidPValuesExpectations2020,
  title = {Valid P-{{Values}} and {{Expectations}} of p-{{Values Revisited}}},
  author = {Vexler, Albert},
  date = {2020-01-14},
  number = {arXiv:2001.05126},
  eprint = {2001.05126},
  eprinttype = {arxiv},
  primaryclass = {stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2001.05126},
  url = {http://arxiv.org/abs/2001.05126},
  urldate = {2022-11-11},
  abstract = {A storm of favorable or critical publications regarding p-values-based procedures has been observed in both the theoretical and applied literature. We focus on valid definitions of p-values in the scenarios when composite null models are in effect. A valid p-value (VpV) statistic can be used to make a prefixed level-decision. In this context, Kolmogorov Smirnov goodness-of-fit tests and the normal two sample problem are considered. In particular, we examine an issue regarding the goodness-of-fit testability based on a single observation. This article exemplifies constructions of new test procedures, advocating practical reasons to implement VpV-based mechanisms. The VpV framework induces an extension of the conventional expected p-value (EPV) tool for measuring the performance of a test. Associating the EPV concept with the receiver operating characteristic (ROC) curve methodology, a well-established biostatistical approach, we propose a Youden index based optimality principle to derive critical values of decision making procedures. In these terms, the significance level alpha=0.05 can be suggested, in many situations. In light of an ROC curve analysis, we introduce partial EPVs to characterize properties of tests including their unbiasedness. We also provide the intrinsic relationship between the Bayes Factor (BF) test statistic and the BF of test statistics. Keywords: AUC; Bayes Factor; Expected p-value; Kolmogorov Smirnov tests; Likelihood ratio; Nuisance parameters; P-value; ROC curve; Pooled data; Single observation; Type I error rate; Youden index},
  archiveprefix = {arXiv},
  keywords = {Statistics - Methodology},
  file = {/home/skynet3/Zotero/storage/4QDFPAHS/Vexler - 2020 - Valid p-Values and Expectations of p-Values Revisi.pdf;/home/skynet3/Zotero/storage/XSN9V44T/2001.html}
}

@article{vickersEverythingYouAlways2010,
  title = {Everything You Always Wanted to Know about Evaluating Prediction Models (but Were Too Afraid to Ask)},
  author = {Vickers, Andrew J. and Cronin, Angel M.},
  date = {2010-12},
  journaltitle = {Urology},
  shortjournal = {Urology},
  volume = {76},
  number = {6},
  eprint = {21030068},
  eprinttype = {pmid},
  pages = {1298--1301},
  issn = {0090-4295},
  doi = {10.1016/j.urology.2010.06.019},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2997853/},
  urldate = {2022-11-11},
  pmcid = {PMC2997853},
  file = {/home/skynet3/Zotero/storage/F3X2YJFW/Vickers and Cronin - 2010 - Everything you always wanted to know about evaluat.pdf}
}

@online{viewsGentleIntroductionTidymodels2019,
  title = {A {{Gentle Introduction}} to Tidymodels},
  author = {Views, R.},
  date = {2019-06-19T00:00:00+00:00},
  url = {https://rviews.rstudio.com/2019/06/19/a-gentle-intro-to-tidymodels/},
  urldate = {2022-11-11},
  abstract = {Recently, I had the opportunity to showcase tidymodels in workshops and talks. Because of my vantage point as a user, I figured it would be valuable to share what I have learned so far. Let’s begin by framing where tidymodels fits in our analysis projects. The diagram above is based on the R for Data Science book, by Wickham and Grolemund. The version in this article illustrates what step each package covers.},
  langid = {american},
  file = {/home/skynet3/Zotero/storage/UEDPZMD6/a-gentle-intro-to-tidymodels.html}
}

@online{VincentarelbundockMarginaleffectsPackage,
  title = {Vincentarelbundock/Marginaleffects: {{An R}} Package to Compute Marginal Effects, Adjusted Predictions, Contrasts, and Marginal Means for a Wide Variety of Models},
  shorttitle = {Vincentarelbundock/Marginaleffects},
  url = {https://github.com/vincentarelbundock/marginaleffects},
  urldate = {2022-11-11},
  abstract = {An R package to compute marginal effects, adjusted predictions, contrasts, and marginal means for a wide variety of models - vincentarelbundock/marginaleffects: An R package to compute marginal eff...},
  langid = {english},
  organization = {{GitHub}},
  file = {/home/skynet3/Zotero/storage/XF4YAD4F/marginaleffects.html}
}

@software{VipVariableImportance2022,
  title = {Vip: {{Variable Importance Plots}}},
  shorttitle = {Vip},
  date = {2022-10-11T13:39:25Z},
  origdate = {2017-08-01T12:30:12Z},
  url = {https://github.com/koalaverse/vip},
  urldate = {2022-11-11},
  abstract = {Variable Importance Plots (VIPs)},
  organization = {{koalaverse}},
  keywords = {interaction-effect,machine-learning,partial-dependence-plot,supervised-learning-algorithms,variable-importance,variable-importance-plots}
}

@online{VisualIntroductionMachine,
  title = {A Visual Introduction to Machine Learning, {{Part II}}},
  url = {http://www.r2d3.us/visual-intro-to-machine-learning-part-2/},
  urldate = {2022-11-11},
  abstract = {Learn about bias and variance in our second animated data visualization.},
  langid = {english},
  file = {/home/skynet3/Zotero/storage/W2S9K2XA/visual-intro-to-machine-learning-part-2.html}
}

@online{VisualIntroductionMachinea,
  title = {A Visual Introduction to Machine Learning},
  url = {http://www.r2d3.us/visual-intro-to-machine-learning-part-1/},
  urldate = {2022-11-11},
  abstract = {What is machine learning? See how it works with our animated data visualization.},
  langid = {english},
  file = {/home/skynet3/Zotero/storage/2JDSCHGG/visual-intro-to-machine-learning-part-1.html}
}

@online{VisualisingResidualsBlogR,
  ids = {VisualisingResidualsBlogRa},
  title = {Visualising {{Residuals}} • {{blogR}}},
  url = {https://drsimonj.svbtle.com/visualising-residuals},
  urldate = {2022-11-11},
  abstract = {Residuals. Now there’s something to get you out of bed in the morning! OK, maybe residuals aren’t the sexiest topic in the world. Still, they’re an essential element and means for identifying potential problems of any statistical model. For... | blogR | Walkthroughs and projects using R for data science.},
  langid = {english},
  organization = {{blogR on Svbtle}},
  file = {/home/skynet3/Zotero/storage/ZREEPNLW/visualising-residuals.html}
}

@online{Vtree,
  title = {Vtree},
  url = {https://nbarrowman.github.io/vtree.html},
  urldate = {2022-11-11},
  abstract = {a flexible R package for displaying nested subsets of a data frame},
  langid = {american},
  organization = {{nbarrowman.github.io}},
  file = {/home/skynet3/Zotero/storage/9YDUAVWJ/vtree.html}
}

@online{vuorreSometimesBayesianEstimation2017,
  title = {Sometimes {{I R}} - {{Bayesian Estimation}} of {{Signal Detection Models}}},
  author = {Vuorre, Matti},
  date = {2017-10-09},
  url = {https://mvuorre.github.io/posts/2017-10-09-bayesian-estimation-of-signal-detection-theory-models/},
  urldate = {2022-11-11},
  abstract = {Signal Detection Theory (SDT) is a popular theoretical framework for modeling memory and perception. Calculating point estimates of equal variance Gaussian SDT parameters is easy using widely known formulas. More complex SDT models, such as the unequal variance SDT model, require more complicated modeling techniques. These models can be estimated using Bayesian (nonlinear and/or hierarchical) regression methods, which are illustrated here.},
  langid = {english},
  file = {/home/skynet3/Zotero/storage/W8F6A233/2017-10-09-bayesian-estimation-of-signal-detection-theory-models.html}
}

@misc{vyasOutofDistributionDetectionUsing2018,
  title = {Out-of-{{Distribution Detection Using}} an {{Ensemble}} of {{Self Supervised Leave-out Classifiers}}},
  author = {Vyas, Apoorv and Jammalamadaka, Nataraj and Zhu, Xia and Das, Dipankar and Kaul, Bharat and Willke, Theodore L.},
  date = {2018-09-04},
  number = {arXiv:1809.03576},
  eprint = {1809.03576},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1809.03576},
  url = {http://arxiv.org/abs/1809.03576},
  urldate = {2022-11-11},
  abstract = {As deep learning methods form a critical part in commercially important applications such as autonomous driving and medical diagnostics, it is important to reliably detect out-of-distribution (OOD) inputs while employing these algorithms. In this work, we propose an OOD detection algorithm which comprises of an ensemble of classifiers. We train each classifier in a self-supervised manner by leaving out a random subset of training data as OOD data and the rest as in-distribution (ID) data. We propose a novel margin-based loss over the softmax output which seeks to maintain at least a margin \$m\$ between the average entropy of the OOD and in-distribution samples. In conjunction with the standard cross-entropy loss, we minimize the novel loss to train an ensemble of classifiers. We also propose a novel method to combine the outputs of the ensemble of classifiers to obtain OOD detection score and class prediction. Overall, our method convincingly outperforms Hendrycks et al.[7] and the current state-of-the-art ODIN[13] on several OOD detection benchmarks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/skynet3/Zotero/storage/CWXT87G5/Vyas et al. - 2018 - Out-of-Distribution Detection Using an Ensemble of.pdf;/home/skynet3/Zotero/storage/294HP68T/1809.html}
}

@online{w.d.ScikitlearnDefaultsAre2019,
  title = {Scikit-Learn’s {{Defaults}} Are {{Wrong}}},
  author = {{W.D.}},
  date = {2019-08-31T02:35:00+00:00},
  url = {https://ryxcommar.com/2019/08/30/scikit-learns-defaults-are-wrong/},
  urldate = {2022-11-11},
  abstract = {This recent Tweet erupted a discussion about how logistic regression in Scikit-learn uses L2 penalization with a lambda of 1 as default options. If you don’t care about data science, this sou…},
  langid = {english},
  organization = {{r y x, r}},
  file = {/home/skynet3/Zotero/storage/FKIVX5TS/scikit-learns-defaults-are-wrong.html}
}

@misc{wagenmakersBayesianThinkingToddlers2020,
  title = {Bayesian {{Thinking}} for {{Toddlers}}},
  author = {Wagenmakers, Eric-Jan},
  date = {2020-10-21T13:20:11},
  publisher = {{PsyArXiv}},
  doi = {10.31234/osf.io/w5vbp},
  url = {https://psyarxiv.com/w5vbp/},
  urldate = {2022-11-11},
  abstract = {Bayesian thinking is easy, and toddlers do it all the time. The fundamental principle is learning from experience: hypotheses that predict the data well receive a boost in plausibility, whereas hypotheses that predict the data poorly suffer a decline. For example, hypothesis A could state “At 6 am my parents will generally be asleep” and hypothesis B could state “At 6 am my parents will generally be awake”. When a toddler then wakes up at 6 am and notices that both parents are still sound asleep, this observation increases the plausibility of hypothesis A and decreases that of hypothesis B. Easy! Knowledgeable readers will discover that the dinosaur cover story hints at concepts such as Ockham’s razor, coherent knowledge updating, and probability as degree of reasonable belief. Statisticians may recognize Phil Dawid's prequential principle in action. Toddlers may just want to look at the dinosaurs.},
  langid = {american},
  keywords = {Dinosaurs,Knowledge updating,Prequential,Quantitative Methods,Social and Behavioral Sciences,Statistical Methods,Tutorial},
  file = {/home/skynet3/Zotero/storage/C3NSR8WP/Wagenmakers - 2020 - Bayesian Thinking for Toddlers.pdf}
}

@misc{wagerEstimationInferenceHeterogeneous2017,
  title = {Estimation and {{Inference}} of {{Heterogeneous Treatment Effects}} Using {{Random Forests}}},
  author = {Wager, Stefan and Athey, Susan},
  date = {2017-07-09},
  number = {arXiv:1510.04342},
  eprint = {1510.04342},
  eprinttype = {arxiv},
  primaryclass = {math, stat},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/1510.04342},
  urldate = {2022-11-11},
  abstract = {Many scientific and engineering challenges -- ranging from personalized medicine to customized marketing recommendations -- require an understanding of treatment effect heterogeneity. In this paper, we develop a non-parametric causal forest for estimating heterogeneous treatment effects that extends Breiman's widely used random forest algorithm. In the potential outcomes framework with unconfoundedness, we show that causal forests are pointwise consistent for the true treatment effect, and have an asymptotically Gaussian and centered sampling distribution. We also discuss a practical method for constructing asymptotic confidence intervals for the true treatment effect that are centered at the causal forest estimates. Our theoretical results rely on a generic Gaussian theory for a large family of random forest algorithms. To our knowledge, this is the first set of results that allows any type of random forest, including classification and regression forests, to be used for provably valid statistical inference. In experiments, we find causal forests to be substantially more powerful than classical methods based on nearest-neighbor matching, especially in the presence of irrelevant covariates.},
  archiveprefix = {arXiv},
  keywords = {Mathematics - Statistics Theory,Statistics - Machine Learning,Statistics - Methodology},
  file = {/home/skynet3/Zotero/storage/2795QMEV/Wager and Athey - 2017 - Estimation and Inference of Heterogeneous Treatmen.pdf;/home/skynet3/Zotero/storage/G62GPFHD/1510.html}
}

@article{wallaceAnalysisImperfectWorld2020,
  title = {Analysis in an Imperfect World},
  author = {Wallace, Michael},
  date = {2020},
  journaltitle = {Significance},
  volume = {17},
  number = {1},
  pages = {14--19},
  issn = {1740-9713},
  doi = {10.1111/j.1740-9713.2020.01353.x},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1740-9713.2020.01353.x},
  urldate = {2022-11-11},
  abstract = {When we observe the world, we sometimes make mistakes. Michael Wallace, on behalf of the measurement error topic group of the STRATOS Initiative, explains the potentially severe consequences of this often overlooked issue, and how statistics can help bring us back – or at least a little closer – to the truth},
  langid = {english},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1740-9713.2020.01353.x},
  file = {/home/skynet3/Zotero/storage/68G3A6UV/Wallace - 2020 - Analysis in an imperfect world.pdf;/home/skynet3/Zotero/storage/P85K5RTP/j.1740-9713.2020.01353.html}
}

@article{wangCovariateAdjustmentRandomized2020,
  ids = {wangCovariateAdjustmentRandomized2020a},
  title = {Covariate Adjustment for Randomized Controlled Trials Revisited},
  author = {Wang, Jixian},
  date = {2020},
  journaltitle = {Pharmaceutical Statistics},
  volume = {19},
  number = {3},
  pages = {255--261},
  issn = {1539-1612},
  doi = {10.1002/pst.1988},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/pst.1988},
  urldate = {2022-11-11},
  abstract = {Covariate adjustment for the estimation of treatment effect for randomized controlled trials (RCT) is a simple approach with a long history, hence, its pros and cons have been well-investigated and published in the literature. It is worthwhile to revisit this topic since recently there has been significant investigation and development on model assumptions, robustness to model mis-specification, in particular, regarding the Neyman-Rubin model and the average treatment effect estimand. This paper discusses key results of the investigation and development and their practical implication on pharmaceutical statistics. Accordingly, we recommend that appropriate covariate adjustment should be more widely used for RCTs for both hypothesis testing and estimation.},
  langid = {english},
  keywords = {average treatment effect,covariate adjustment,model misspecification,Neyman-Rubin model},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/pst.1988},
  file = {/home/skynet3/Zotero/storage/VTTRGZ7X/pst.html}
}

@misc{wangHeavyuserBiasTesting2019,
  title = {On {{Heavy-user Bias}} in {{A}}/{{B Testing}}},
  author = {Wang, Yu and Gupta, Somit and Lu, Jiannan and Mahmoudzadeh, Ali and Liu, Sophia},
  date = {2019-08-10},
  number = {arXiv:1902.02021},
  eprint = {1902.02021},
  eprinttype = {arxiv},
  primaryclass = {stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1902.02021},
  url = {http://arxiv.org/abs/1902.02021},
  urldate = {2022-11-11},
  abstract = {On-line experimentation (also known as A/B testing) has become an integral part of software development. To timely incorporate user feedback and continuously improve products, many software companies have adopted the culture of agile deployment, requiring online experiments to be conducted and concluded on limited sets of users for a short period. While conceptually efficient, the result observed during the experiment duration can deviate from what is seen after the feature deployment, which makes the A/B test result biased. In this paper, we provide theoretical analysis to show that heavy-users can contribute significantly to the bias, and propose a re-sampling estimator for bias adjustment.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Applications},
  file = {/home/skynet3/Zotero/storage/FNIL78JI/Wang et al. - 2019 - On Heavy-user Bias in AB Testing.pdf;/home/skynet3/Zotero/storage/629UYM2S/1902.html}
}

@misc{wangPermutationbasedCausalInference2017,
  title = {Permutation-Based {{Causal Inference Algorithms}} with {{Interventions}}},
  author = {Wang, Yuhao and Solus, Liam and Yang, Karren Dai and Uhler, Caroline},
  date = {2017-11-04},
  number = {arXiv:1705.10220},
  eprint = {1705.10220},
  eprinttype = {arxiv},
  primaryclass = {stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1705.10220},
  url = {http://arxiv.org/abs/1705.10220},
  urldate = {2022-11-15},
  abstract = {Learning directed acyclic graphs using both observational and interventional data is now a fundamentally important problem due to recent technological developments in genomics that generate such single-cell gene expression data at a very large scale. In order to utilize this data for learning gene regulatory networks, efficient and reliable causal inference algorithms are needed that can make use of both observational and interventional data. In this paper, we present two algorithms of this type and prove that both are consistent under the faithfulness assumption. These algorithms are interventional adaptations of the Greedy SP algorithm and are the first algorithms using both observational and interventional data with consistency guarantees. Moreover, these algorithms have the advantage that they are nonparametric, which makes them useful also for analyzing non-Gaussian data. In this paper, we present these two algorithms and their consistency guarantees, and we analyze their performance on simulated data, protein signaling data, and single-cell gene expression data.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Applications,Statistics - Methodology},
  file = {/home/skynet3/Zotero/storage/72I8TDQ8/Wang et al. - 2017 - Permutation-based Causal Inference Algorithms with.pdf;/home/skynet3/Zotero/storage/8A68KB63/1705.html}
}

@misc{wangSelfsemisupervisedLearningLearn2020,
  title = {Self-Semi-Supervised {{Learning}} to {{Learn}} from {{NoisyLabeled Data}}},
  author = {Wang, Jiacheng and Ma, Yue and Gao, Shuang},
  date = {2020-11-02},
  number = {arXiv:2011.01429},
  eprint = {2011.01429},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2011.01429},
  url = {http://arxiv.org/abs/2011.01429},
  urldate = {2022-11-11},
  abstract = {The remarkable success of today's deep neural networks highly depends on a massive number of correctly labeled data. However, it is rather costly to obtain high-quality human-labeled data, leading to the active research area of training models robust to noisy labels. To achieve this goal, on the one hand, many papers have been dedicated to differentiating noisy labels from clean ones to increase the generalization of DNN. On the other hand, the increasingly prevalent methods of self-semi-supervised learning have been proven to benefit the tasks when labels are incomplete. By 'semi' we regard the wrongly labeled data detected as un-labeled data; by 'self' we choose a self-supervised technique to conduct semi-supervised learning. In this project, we designed methods to more accurately differentiate clean and noisy labels and borrowed the wisdom of self-semi-supervised learning to train noisy labeled data.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/home/skynet3/Zotero/storage/P9UKAKTY/Wang et al. - 2020 - Self-semi-supervised Learning to Learn from NoisyL.pdf;/home/skynet3/Zotero/storage/SDGXBKIA/2011.html}
}

@article{wattenbergHowUseTSNE2016,
  ids = {wattenbergHowUseTSNE2016a},
  title = {How to {{Use}} T-{{SNE Effectively}}},
  author = {Wattenberg, Martin and Viégas, Fernanda and Johnson, Ian},
  date = {2016-10-13},
  journaltitle = {Distill},
  shortjournal = {Distill},
  volume = {1},
  number = {10},
  pages = {e2},
  issn = {2476-0757},
  doi = {10.23915/distill.00002},
  url = {http://distill.pub/2016/misread-tsne},
  urldate = {2022-11-11},
  abstract = {Although extremely useful for visualizing high-dimensional data, t-SNE plots can sometimes be mysterious or misleading.},
  langid = {english},
  file = {/home/skynet3/Zotero/storage/YZR4GR8K/misread-tsne.html}
}

@online{WebHostingFaculty,
  title = {Web Hosting - {{Faculty}} and {{Staff}} - {{University}} of {{Victoria}}},
  url = {https://www.uvic.ca/systems/services/web/webhosting-fac-staff/},
  urldate = {2022-11-11},
  file = {/home/skynet3/Zotero/storage/BLJK3YVT/webhosting-fac-staff.html}
}

@online{WeeklyPapers,
  title = {Weekly {{Papers}}},
  url = {https://papers.labml.ai/papers/weekly},
  urldate = {2022-11-11},
  file = {/home/skynet3/Zotero/storage/QUSXKT22/weekly.html}
}

@misc{weiImplicitExplicitRegularization2020,
  title = {The {{Implicit}} and {{Explicit Regularization Effects}} of {{Dropout}}},
  author = {Wei, Colin and Kakade, Sham and Ma, Tengyu},
  date = {2020-10-15},
  number = {arXiv:2002.12915},
  eprint = {2002.12915},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2002.12915},
  url = {http://arxiv.org/abs/2002.12915},
  urldate = {2022-11-11},
  abstract = {Dropout is a widely-used regularization technique, often required to obtain state-of-the-art for a number of architectures. This work demonstrates that dropout introduces two distinct but entangled regularization effects: an explicit effect (also studied in prior work) which occurs since dropout modifies the expected training objective, and, perhaps surprisingly, an additional implicit effect from the stochasticity in the dropout training update. This implicit regularization effect is analogous to the effect of stochasticity in small mini-batch stochastic gradient descent. We disentangle these two effects through controlled experiments. We then derive analytic simplifications which characterize each effect in terms of the derivatives of the model and the loss, for deep neural networks. We demonstrate these simplified, analytic regularizers accurately capture the important aspects of dropout, showing they faithfully replace dropout in practice.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/skynet3/Zotero/storage/QSC4RHRD/Wei et al. - 2020 - The Implicit and Explicit Regularization Effects o.pdf;/home/skynet3/Zotero/storage/XU26M48E/2002.html}
}

@article{weiRoleBalancedTraining2013,
  title = {The {{Role}} of {{Balanced Training}} and {{Testing Data Sets}} for {{Binary Classifiers}} in {{Bioinformatics}}},
  author = {Wei, Qiong and Jr, Roland L. Dunbrack},
  date = {2013-07-09},
  journaltitle = {PLOS ONE},
  shortjournal = {PLOS ONE},
  volume = {8},
  number = {7},
  pages = {e67863},
  publisher = {{Public Library of Science}},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0067863},
  url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0067863},
  urldate = {2022-11-11},
  abstract = {Training and testing of conventional machine learning models on binary classification problems depend on the proportions of the two outcomes in the relevant data sets. This may be especially important in practical terms when real-world applications of the classifier are either highly imbalanced or occur in unknown proportions. Intuitively, it may seem sensible to train machine learning models on data similar to the target data in terms of proportions of the two binary outcomes. However, we show that this is not the case using the example of prediction of deleterious and neutral phenotypes of human missense mutations in human genome data, for which the proportion of the binary outcome is unknown. Our results indicate that using balanced training data (50\% neutral and 50\% deleterious) results in the highest balanced accuracy (the average of True Positive Rate and True Negative Rate), Matthews correlation coefficient, and area under ROC curves, no matter what the proportions of the two phenotypes are in the testing data. Besides balancing the data by undersampling the majority class, other techniques in machine learning include oversampling the minority class, interpolating minority-class data points and various penalties for misclassifying the minority class. However, these techniques are not commonly used in either the missense phenotype prediction problem or in the prediction of disordered residues in proteins, where the imbalance problem is substantial. The appropriate approach depends on the amount of available data and the specific problem at hand.},
  langid = {english},
  keywords = {Deletion mutation,Human genomics,Machine learning,Missense mutation,Mutation,Mutation databases,Primates,Substitution mutation},
  file = {/home/skynet3/Zotero/storage/6V3IDXUS/Wei and Jr - 2013 - The Role of Balanced Training and Testing Data Set.pdf;/home/skynet3/Zotero/storage/2I64DL9G/article.html}
}

@online{weissteinArithmeticMean,
  type = {Text},
  ids = {weissteinArithmeticMeana},
  title = {Arithmetic {{Mean}}},
  author = {Weisstein, Eric W.},
  publisher = {{Wolfram Research, Inc.}},
  url = {https://mathworld.wolfram.com/},
  urldate = {2022-11-11},
  abstract = {The arithmetic mean of a set of values is the quantity commonly called "the" mean or the average. Given a set of samples \{x\_i\}, the arithmetic mean is  x\^\_=1/Nsum\_(i=1)\^Nx\_i.  (1)   It can be computed in the Wolfram Language using Mean[list]. The arithmetic mean is the special case M\_1 of the power mean and is one of the Pythagorean means. When viewed as an estimator for the mean of the underlying distribution (known as the population mean), the arithmetic mean of a sample is...},
  langid = {english},
  file = {/home/skynet3/Zotero/storage/HC275MID/ArithmeticMean.html}
}

@online{weissteinSequence,
  type = {Text},
  ids = {weissteinSequencea},
  title = {Sequence},
  author = {Weisstein, Eric W.},
  publisher = {{Wolfram Research, Inc.}},
  url = {https://mathworld.wolfram.com/},
  urldate = {2022-11-11},
  abstract = {A sequence is an ordered set of mathematical objects. Sequences of object are most commonly denoted using braces. For example, the symbol \{2n\}\_(n=1)\^infty denotes the infinite sequence of even numbers \{2,4,...,2n,...\}.},
  langid = {english},
  file = {/home/skynet3/Zotero/storage/UVPMJX4G/Sequence.html}
}

@online{Welcome,
  ids = {Welcomea},
  title = {Welcome},
  url = {https://stat.tamu.edu/},
  urldate = {2022-11-11},
  abstract = {Texas A\&M Department of Statistics offers undergraduate, masters and PH.D. eduction. Applicable for Data Science, Healthcare, Environmental Studies, and government},
  langid = {american},
  organization = {{Dept. of Statistics, Texas A\&M University}},
  file = {/home/skynet3/Zotero/storage/BCKNS7YR/stat.tamu.edu.html}
}

@online{WelcomeAdvanced,
  title = {Welcome | {{Advanced R}}},
  url = {https://adv-r.hadley.nz/index.html},
  urldate = {2022-11-11}
}

@online{WelcomeBayesianModeling,
  title = {Welcome — {{Bayesian Modeling}} and {{Computation}} in {{Python}}},
  url = {https://bayesiancomputationbook.com/welcome.html},
  urldate = {2022-11-11}
}

@online{WelcomeDataScience,
  title = {Welcome | {{R}} for {{Data Science}}},
  url = {https://r4ds.had.co.nz/},
  urldate = {2022-11-11},
  file = {/home/skynet3/Zotero/storage/2V5G8XQ2/r4ds.had.co.nz.html}
}

@online{WelcomeGeographicallyBased,
  title = {Welcome | {{Geographically}} Based {{Economic}} Data ({{G-Econ}})},
  url = {https://gecon.yale.edu/},
  urldate = {2022-11-11},
  file = {/home/skynet3/Zotero/storage/E582VDAZ/gecon.yale.edu.html}
}

@online{WelcomeLightGBMDocumentation,
  title = {Welcome to {{LightGBM}}’s Documentation! — {{LightGBM}} 3.3.2 Documentation},
  url = {https://lightgbm.readthedocs.io/en/v3.3.2/},
  urldate = {2022-11-11}
}

@online{WelcomePythonData,
  title = {Welcome — {{Python}} for {{Data Science}}},
  url = {https://aeturrell.github.io/python4DS/welcome.html},
  urldate = {2022-11-11}
}

@article{wenLowReproducibilityCancer2018,
  title = {On the Low Reproducibility of Cancer Studies},
  author = {Wen, Haijun and Wang, Hurng-Yi and He, Xionglei and Wu, Chung-I},
  date = {2018-09},
  journaltitle = {National science review},
  shortjournal = {Natl Sci Rev},
  volume = {5},
  number = {5},
  eprint = {31258951},
  eprinttype = {pmid},
  pages = {619--624},
  issn = {2095-5138},
  doi = {10.1093/nsr/nwy021},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6599599/},
  urldate = {2022-11-11},
  pmcid = {PMC6599599},
  file = {/home/skynet3/Zotero/storage/RUENIYC9/Wen et al. - 2018 - On the low reproducibility of cancer studies.pdf}
}

@inproceedings{wenTimeSeriesData2021,
  ids = {wenTimeSeriesData2021a},
  title = {Time {{Series Data Augmentation}} for {{Deep Learning}}: {{A Survey}}},
  shorttitle = {Time {{Series Data Augmentation}} for {{Deep Learning}}},
  booktitle = {Proceedings of the {{Thirtieth International Joint Conference}} on {{Artificial Intelligence}}},
  author = {Wen, Qingsong and Sun, Liang and Yang, Fan and Song, Xiaomin and Gao, Jingkun and Wang, Xue and Xu, Huan},
  date = {2021-08},
  eprint = {2002.12478},
  eprinttype = {arxiv},
  primaryclass = {cs, eess, stat},
  pages = {4653--4660},
  doi = {10.24963/ijcai.2021/631},
  url = {http://arxiv.org/abs/2002.12478},
  urldate = {2022-11-11},
  abstract = {Deep learning performs remarkably well on many time series analysis tasks recently. The superior performance of deep neural networks relies heavily on a large number of training data to avoid overfitting. However, the labeled data of many real-world time series applications may be limited such as classification in medical time series and anomaly detection in AIOps. As an effective way to enhance the size and quality of the training data, data augmentation is crucial to the successful application of deep learning models on time series data. In this paper, we systematically review different data augmentation methods for time series. We propose a taxonomy for the reviewed methods, and then provide a structured review for these methods by highlighting their strengths and limitations. We also empirically compare different data augmentation methods for different tasks including time series classification, anomaly detection, and forecasting. Finally, we discuss and highlight five future directions to provide useful research guidance.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Electrical Engineering and Systems Science - Signal Processing,Statistics - Machine Learning},
  file = {/home/skynet3/Zotero/storage/777GNMB4/Wen et al. - 2021 - Time Series Data Augmentation for Deep Learning A.pdf;/home/skynet3/Zotero/storage/LKD5Y7YT/2002.html}
}

@article{westfallStatisticallyControllingConfounding2016,
  title = {Statistically {{Controlling}} for {{Confounding Constructs Is Harder}} than {{You Think}}},
  author = {Westfall, Jacob and Yarkoni, Tal},
  date = {2016-03-31},
  journaltitle = {PLOS ONE},
  shortjournal = {PLOS ONE},
  volume = {11},
  number = {3},
  pages = {e0152719},
  publisher = {{Public Library of Science}},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0152719},
  url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0152719},
  urldate = {2022-11-11},
  abstract = {Social scientists often seek to demonstrate that a construct has incremental validity over and above other related constructs. However, these claims are typically supported by measurement-level models that fail to consider the effects of measurement (un)reliability. We use intuitive examples, Monte Carlo simulations, and a novel analytical framework to demonstrate that common strategies for establishing incremental construct validity using multiple regression analysis exhibit extremely high Type I error rates under parameter regimes common in many psychological domains. Counterintuitively, we find that error rates are highest—in some cases approaching 100\%—when sample sizes are large and reliability is moderate. Our findings suggest that a potentially large proportion of incremental validity claims made in the literature are spurious. We present a web application (http://jakewestfall.org/ivy/) that readers can use to explore the statistical properties of these and other incremental validity arguments. We conclude by reviewing SEM-based statistical approaches that appropriately control the Type I error rate when attempting to establish incremental validity.},
  langid = {english},
  keywords = {Behavior,Emotions,Forecasting,Personality,Research errors,Social sciences,Swimming,Test statistics},
  file = {/home/skynet3/Zotero/storage/VF6S7UZ2/Westfall and Yarkoni - 2016 - Statistically Controlling for Confounding Construc.pdf;/home/skynet3/Zotero/storage/QNKAV6KQ/article.html}
}

@online{wezerekHowGoodAre2019,
  title = {How {{Good Are FiveThirtyEight Forecasts}}?},
  author = {Wezerek, Gus, Jay Boice},
  date = {2019-04-04T07:00:00-04:00},
  url = {https://projects.fivethirtyeight.com/checking-our-work/},
  urldate = {2022-11-11},
  abstract = {We checked all of our sports and election predictions since 2008. They’re reliable.},
  langid = {english},
  organization = {{FiveThirtyEight}}
}

@online{WhatPointRobustness,
  title = {What’s the Point of a Robustness Check? | {{Statistical Modeling}}, {{Causal Inference}}, and {{Social Science}}},
  url = {https://statmodeling.stat.columbia.edu/2017/11/29/whats-point-robustness-check/},
  urldate = {2022-11-11},
  file = {/home/skynet3/Zotero/storage/5XSI7PWV/whats-point-robustness-check.html}
}

@online{WhatWrongMy,
  title = {What's {{Wrong With My Time Series}} | {{Stitch Fix Technology}} – {{Multithreaded}}},
  url = {https://multithreaded.stitchfix.com/blog/2017/02/28/whats-wrong-with-my-time-series/},
  urldate = {2022-11-11},
  abstract = {Time series modeling sits at the core of critical business operations such as supply and demand forecasting and quick-response algorithms like fraud and anom...}
}

@online{WhenAirForce2016,
  title = {When {{U}}.{{S}}. Air Force Discovered the Flaw of Averages},
  date = {2016-01-16},
  url = {https://www.thestar.com/news/insight/2016/01/16/when-us-air-force-discovered-the-flaw-of-averages.html},
  urldate = {2022-11-11},
  abstract = {In the early 1950s, a young lieutenant realized the fatal flaw in the cockpit design of U.S. air force jets. Todd Rose explains in an excerpt from his book, The End of Average.},
  langid = {english},
  organization = {{thestar.com}}
}

@online{WhyDataNever,
  title = {Why {{Data Is Never Raw}}},
  url = {https://www.thenewatlantis.com/publications/why-data-is-never-raw},
  urldate = {2022-11-11},
  abstract = {On the seductive myth of information free of human judgment},
  langid = {american},
  organization = {{The New Atlantis}}
}

@online{WhyPredictionMetrics,
  title = {Why {{Prediction Metrics}} Are {{Dangerous For Causal Models}} — {{Causal Inference}} for the {{Brave}} and {{True}}},
  url = {https://matheusfacure.github.io/python-causality-handbook/Prediction-Metrics-For-Causal-Models.html},
  urldate = {2022-11-11}
}

@online{WhyYouShould2022,
  title = {Why {{You Should}} (or {{Shouldn}}'t) Be {{Using Google}}'s {{JAX}} in 2022},
  date = {2022-02-15T14:28:08},
  url = {https://www.assemblyai.com/blog/why-you-should-or-shouldnt-be-using-jax-in-2022/},
  urldate = {2022-11-11},
  abstract = {Should you be using JAX in 2022? Check out our recommendations on using JAX for Deep Learning and more!},
  langid = {english},
  organization = {{News, Tutorials, AI Research}},
  file = {/home/skynet3/Zotero/storage/7NS3KLB3/why-you-should-or-shouldnt-be-using-jax-in-2022.html}
}

@book{wickhamWelcomeMasteringShiny,
  title = {Welcome | {{Mastering Shiny}}},
  author = {Wickham, Hadley},
  url = {https://mastering-shiny.org/},
  urldate = {2022-11-11},
  abstract = {A book created with bookdown.},
  langid = {english},
  file = {/home/skynet3/Zotero/storage/ICNB8856/mastering-shiny.org.html}
}

@book{wickhamWelcomeTidyverseStyle,
  title = {Welcome | {{The}} Tidyverse Style Guide},
  author = {Wickham, Hadley},
  url = {https://style.tidyverse.org/index.html},
  urldate = {2022-11-11},
  abstract = {A book created with bookdown.},
  langid = {english},
  file = {/home/skynet3/Zotero/storage/EYSV76WI/index.html}
}

@software{wildenBperBayesianPrediction2022,
  title = {Bper: {{Bayesian Prediction}} for {{Ethnicity}} and {{Race}}},
  shorttitle = {Bper},
  author = {Wilden, Bertrand},
  date = {2022-08-19T15:25:01Z},
  origdate = {2020-12-07T21:55:32Z},
  url = {https://github.com/bwilden/bper},
  urldate = {2022-11-11},
  abstract = {Bayesian Prediction for Ethnicity and Race}
}

@article{wilsonFiveStarGuideAchieving2021,
  ids = {wilsonFiveStarGuideAchieving2021a},
  title = {A {{Five-Star Guide}} for {{Achieving Replicability}} and {{Reproducibility When Working}} with {{GIS Software}} and {{Algorithms}}},
  author = {Wilson, John P. and Butler, Kevin and Gao, Song and Hu, Yingjie and Li, Wenwen and Wright, Dawn J.},
  date = {2021-07-29},
  journaltitle = {Annals of the American Association of Geographers},
  volume = {111},
  number = {5},
  pages = {1311--1317},
  publisher = {{Taylor \& Francis}},
  issn = {2469-4452},
  doi = {10.1080/24694452.2020.1806026},
  url = {https://doi.org/10.1080/24694452.2020.1806026},
  urldate = {2022-11-11},
  abstract = {The availability and use of geographic information technologies and data for describing the patterns and processes operating on or near the Earth’s surface have grown substantially during the past fifty years. The number of geographic information systems software packages and algorithms has also grown quickly during this period, fueled by rapid advances in computing and the explosive growth in the availability of digital data describing specific phenomena. Geographic information scientists therefore increasingly find themselves choosing between multiple software suites and algorithms to execute specific analysis, modeling, and visualization tasks in environmental applications today. This is a major challenge because it is often difficult to assess the efficacy of the candidate software platforms and algorithms when used in specific applications and study areas, which often generate different results. The subtleties and issues that characterize the field of geomorphometry are used here to document the need for (1) theoretically based software and algorithms; (2) new methods for the collection of provenance information about the data and code along with application context knowledge; and (3) new protocols for distributing this information and knowledge along with the data and code. This article discusses the progress and enduring challenges connected with these outcomes.},
  keywords = {application context knowledge,conocimiento del contexto de la aplicación,e-ciencia,e-science,GIS,replicabilidad,replicability,reproducibilidad,reproducibility,scientific workflow systems,SIG,sistemas de flujo de trabajo científico,可再现性,可重复性,地理信息系统,应用背景知识,数字科学,科学工作流程系统},
  annotation = {\_eprint: https://doi.org/10.1080/24694452.2020.1806026}
}

@inproceedings{winataHandlingImbalancedDataset2015,
  title = {Handling {{Imbalanced Dataset}} in {{Multi-label Text Categorization}} Using {{Bagging}} and {{Adaptive Boosting}}},
  booktitle = {2015 {{International Conference}} on {{Electrical Engineering}} and {{Informatics}} ({{ICEEI}})},
  author = {Winata, Genta Indra and Khodra, Masayu Leylia},
  date = {2015-08},
  eprint = {1810.11612},
  eprinttype = {arxiv},
  primaryclass = {cs},
  pages = {500--505},
  doi = {10.1109/ICEEI.2015.7352552},
  url = {http://arxiv.org/abs/1810.11612},
  urldate = {2022-11-11},
  abstract = {Imbalanced dataset is occurred due to uneven distribution of data available in the real world such as disposition of complaints on government offices in Bandung. Consequently, multi-label text categorization algorithms may not produce the best performance because classifiers tend to be weighed down by the majority of the data and ignore the minority. In this paper, Bagging and Adaptive Boosting algorithms are employed to handle the issue and improve the performance of text categorization. The result is evaluated with four evaluation metrics such as hamming loss, subset accuracy, example-based accuracy and micro-averaged f-measure. Bagging ML-LP with SMO weak classifier is the best performer in terms of subset accuracy and example-based accuracy. Bagging ML-BR with SMO weak classifier has the best micro-averaged f-measure among all. In other hand, AdaBoost MH with J48 weak classifier has the lowest hamming loss value. Thus, both algorithms have high potential in boosting the performance of text categorization, but only for certain weak classifiers. However, bagging has more potential than adaptive boosting in increasing the accuracy of minority labels.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/skynet3/Zotero/storage/EJZF9VEU/Winata and Khodra - 2015 - Handling Imbalanced Dataset in Multi-label Text Ca.pdf;/home/skynet3/Zotero/storage/SE57LPJY/1810.html}
}

@article{wittenMiniIntroductionInformationTheory2020,
  title = {A {{Mini-Introduction To Information Theory}}},
  author = {Witten, Edward},
  date = {2020-04},
  journaltitle = {La Rivista del Nuovo Cimento},
  shortjournal = {Riv. Nuovo Cim.},
  volume = {43},
  number = {4},
  eprint = {1805.11965},
  eprinttype = {arxiv},
  primaryclass = {hep-th, physics:quant-ph},
  pages = {187--227},
  issn = {0393-697X, 1826-9850},
  doi = {10.1007/s40766-020-00004-5},
  url = {http://arxiv.org/abs/1805.11965},
  urldate = {2022-11-11},
  abstract = {This article consists of a very short introduction to classical and quantum information theory. Basic properties of the classical Shannon entropy and the quantum von Neumann entropy are described, along with related concepts such as classical and quantum relative entropy, conditional entropy, and mutual information. A few more detailed topics are considered in the quantum case.},
  archiveprefix = {arXiv},
  keywords = {High Energy Physics - Theory,Quantum Physics},
  file = {/home/skynet3/Zotero/storage/GMMK8CSM/Witten - 2020 - A Mini-Introduction To Information Theory.pdf;/home/skynet3/Zotero/storage/X8G3CWW2/1805.html}
}

@article{woldCrossValidatoryEstimationNumber1978,
  title = {Cross-{{Validatory Estimation}} of the {{Number}} of {{Components}} in {{Factor}} and {{Principal Components Models}}},
  author = {Wold, Svante},
  date = {1978-11},
  journaltitle = {Technometrics},
  shortjournal = {Technometrics},
  volume = {20},
  number = {4},
  pages = {397--405},
  issn = {0040-1706, 1537-2723},
  doi = {10.1080/00401706.1978.10489693},
  url = {http://www.tandfonline.com/doi/abs/10.1080/00401706.1978.10489693},
  urldate = {2022-11-11},
  langid = {english}
}

@article{woodHowShouldVariable2008,
  ids = {woodHowShouldVariable2008a},
  title = {How Should Variable Selection Be Performed with Multiply Imputed Data?},
  author = {Wood, Angela M. and White, Ian R. and Royston, Patrick},
  date = {2008},
  journaltitle = {Statistics in Medicine},
  volume = {27},
  number = {17},
  pages = {3227--3246},
  issn = {1097-0258},
  doi = {10.1002/sim.3177},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.3177},
  urldate = {2022-11-11},
  abstract = {Multiple imputation is a popular technique for analysing incomplete data. Given the imputed data and a particular model, Rubin's rules (RR) for estimating parameters and standard errors are well established. However, there are currently no guidelines for variable selection in multiply imputed data sets. The usual practice is to perform variable selection amongst the complete cases, a simple but inefficient and potentially biased procedure. Alternatively, variable selection can be performed by repeated use of RR, which is more computationally demanding. An approximation can be obtained by a simple ‘stacked’ method that combines the multiply imputed data sets into one and uses a weighting scheme to account for the fraction of missing data in each covariate. We compare these and other approaches using simulations based around a trial in community psychiatry. Most methods improve on the naïve complete-case analysis for variable selection, but importantly the type 1 error is only preserved if selection is based on RR, which is our recommended approach. Copyright © 2008 John Wiley \& Sons, Ltd.},
  langid = {english},
  keywords = {multiple imputation,multiply imputed data,stacked data,stepwise,variable selection},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/sim.3177},
  file = {/home/skynet3/Zotero/storage/SQ2R7Q5B/sim.html}
}

@online{WorkshopInsightsNegative,
  title = {Workshop on {{Insights}} from {{Negative Results}} in {{NLP}}},
  url = {http://insights-workshop.github.io/},
  urldate = {2022-11-11},
  abstract = {Workshop on Insights from Negative Results in NLP},
  langid = {american},
  organization = {{Insights 2020}},
  file = {/home/skynet3/Zotero/storage/VVY4QSNL/insights-workshop.github.io.html}
}

@online{WrapVectorsMarkdown,
  title = {Wrap {{Vectors}} in {{Markdown Formatting}}},
  url = {https://kiernann.com/gluedown/},
  urldate = {2022-11-11},
  abstract = {Ease the transition between R vectors and markdown     text. With gluedown and rmarkdown, users can create traditional     vectors in R, glue those strings together with the markdown syntax,     and print those formatted vectors directly to the document. This     package primarily uses GitHub Flavored Markdown (GFM), an offshoot of     the unambiguous CommonMark specification by John MacFarlane (2019)     {$<$}https://spec.commonmark.org/{$>$}.},
  langid = {english},
  file = {/home/skynet3/Zotero/storage/Z6JYYTHQ/gluedown.html}
}

@online{WRDDocumentation,
  title = {{{WRD R}}\&{{D Documentation}}},
  url = {https://wrdrd.github.io/docs/},
  urldate = {2022-11-11},
  langid = {english},
  organization = {{WRD R\&D}},
  file = {/home/skynet3/Zotero/storage/YGB8LDEM/docs.html}
}

@article{wrightRangerFastImplementation2017,
  title = {Ranger: {{A Fast Implementation}} of {{Random Forests}} for {{High Dimensional Data}} in {{C}}++ and {{R}}},
  shorttitle = {Ranger},
  author = {Wright, Marvin N. and Ziegler, Andreas},
  date = {2017},
  journaltitle = {Journal of Statistical Software},
  shortjournal = {J. Stat. Soft.},
  volume = {77},
  number = {1},
  eprint = {1508.04409},
  eprinttype = {arxiv},
  primaryclass = {stat},
  issn = {1548-7660},
  doi = {10.18637/jss.v077.i01},
  url = {http://arxiv.org/abs/1508.04409},
  urldate = {2022-11-11},
  abstract = {We introduce the C++ application and R package ranger. The software is a fast implementation of random forests for high dimensional data. Ensembles of classification, regression and survival trees are supported. We describe the implementation, provide examples, validate the package with a reference implementation, and compare runtime and memory usage with other implementations. The new software proves to scale best with the number of features, samples, trees, and features tried for splitting. Finally, we show that ranger is the fastest and most memory efficient implementation of random forests to analyze data on the scale of a genome-wide association study.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Computation,Statistics - Machine Learning},
  file = {/home/skynet3/Zotero/storage/6JDQTU6U/Wright and Ziegler - 2017 - ranger A Fast Implementation of Random Forests fo.pdf;/home/skynet3/Zotero/storage/YBVH2MZY/1508.html}
}

@online{XGBoostDocumentationXgboost,
  title = {{{XGBoost Documentation}} — Xgboost 1.7.1 Documentation},
  url = {https://xgboost.readthedocs.io/en/stable/},
  urldate = {2022-11-11}
}

@misc{xiongIllusionCausalityVisualized2019,
  title = {Illusion of {{Causality}} in {{Visualized Data}}},
  author = {Xiong, Cindy and Shapiro, Joel and Hullman, Jessica and Franconeri, Steven},
  date = {2019-08-01},
  number = {arXiv:1908.00215},
  eprint = {1908.00215},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1908.00215},
  url = {http://arxiv.org/abs/1908.00215},
  urldate = {2022-11-11},
  abstract = {Students who eat breakfast more frequently tend to have a higher grade point average. From this data, many people might confidently state that a before-school breakfast program would lead to higher grades. This is a reasoning error, because correlation does not necessarily indicate causation -- X and Y can be correlated without one directly causing the other. While this error is pervasive, its prevalence might be amplified or mitigated by the way that the data is presented to a viewer. Across three crowdsourced experiments, we examined whether how simple data relations are presented would mitigate this reasoning error. The first experiment tested examples similar to the breakfast-GPA relation, varying in the plausibility of the causal link. We asked participants to rate their level of agreement that the relation was correlated, which they rated appropriately as high. However, participants also expressed high agreement with a causal interpretation of the data. Levels of support for the causal interpretation were not equally strong across visualization types: causality ratings were highest for text descriptions and bar graphs, but weaker for scatter plots. But is this effect driven by bar graphs aggregating data into two groups or by the visual encoding type? We isolated data aggregation versus visual encoding type and examined their individual effect on perceived causality. Overall, different visualization designs afford different cognitive reasoning affordances across the same data. High levels of data aggregation by graphs tend to be associated with higher perceived causality in data. Participants perceived line and dot visual encodings as more causal than bar encodings. Our results demonstrate how some visualization designs trigger stronger causal links while choosing others can help mitigate unwarranted perceptions of causality.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Human-Computer Interaction},
  file = {/home/skynet3/Zotero/storage/JVI5B5SV/Xiong et al. - 2019 - Illusion of Causality in Visualized Data.pdf;/home/skynet3/Zotero/storage/8YPIFE4F/1908.html}
}

@article{xSomethingGreat2005,
  title = {Something Great},
  author = {X, Mr.},
  date = {2005},
  publisher = {{nobody}}
}

@article{xuComprehensiveSurveyClustering2015,
  title = {A {{Comprehensive Survey}} of {{Clustering Algorithms}}},
  author = {Xu, Dongkuan and Tian, Yingjie},
  date = {2015-06},
  journaltitle = {Annals of Data Science},
  shortjournal = {Ann. Data. Sci.},
  volume = {2},
  number = {2},
  pages = {165--193},
  issn = {2198-5804, 2198-5812},
  doi = {10.1007/s40745-015-0040-1},
  url = {http://link.springer.com/10.1007/s40745-015-0040-1},
  urldate = {2022-11-12},
  abstract = {Data analysis is used as a common method in modern science research, which is across communication science, computer science and biology science. Clustering, as the basic composition of data analysis, plays a significant role. On one hand, many tools for cluster analysis have been created, along with the information increase and subject intersection. On the other hand, each clustering algorithm has its own strengths and weaknesses, due to the complexity of information. In this review paper, we begin at the definition of clustering, take the basic elements involved in the clustering process, such as the distance or similarity measurement and evaluation indicators, into consideration, and analyze the clustering algorithms from two perspectives, the traditional ones and the modern ones. All the discussed clustering algorithms will be compared in detail and comprehensively shown in Appendix Table 22.},
  langid = {english},
  file = {/home/skynet3/Zotero/storage/RYLYVTZA/Xu and Tian - 2015 - A Comprehensive Survey of Clustering Algorithms.pdf}
}

@misc{yadavColdCaseLost2019,
  title = {Cold {{Case}}: {{The Lost MNIST Digits}}},
  shorttitle = {Cold {{Case}}},
  author = {Yadav, Chhavi and Bottou, Léon},
  date = {2019-11-04},
  number = {arXiv:1905.10498},
  eprint = {1905.10498},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1905.10498},
  url = {http://arxiv.org/abs/1905.10498},
  urldate = {2022-11-11},
  abstract = {Although the popular MNIST dataset [LeCun et al., 1994] is derived from the NIST database [Grother and Hanaoka, 1995], the precise processing steps for this derivation have been lost to time. We propose a reconstruction that is accurate enough to serve as a replacement for the MNIST dataset, with insignificant changes in accuracy. We trace each MNIST digit to its NIST source and its rich metadata such as writer identifier, partition identifier, etc. We also reconstruct the complete MNIST test set with 60,000 samples instead of the usual 10,000. Since the balance 50,000 were never distributed, they enable us to investigate the impact of twenty-five years of MNIST experiments on the reported testing performances. Our results unambiguously confirm the trends observed by Recht et al. [2018, 2019]: although the misclassification rates are slightly off, classifier ordering and model selection remain broadly reliable. We attribute this phenomenon to the pairing benefits of comparing classifiers on the same digits.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/skynet3/Zotero/storage/MRRCCUKS/Yadav and Bottou - 2019 - Cold Case The Lost MNIST Digits.pdf;/home/skynet3/Zotero/storage/GVP3AAUT/1905.html}
}

@article{yanaiHypothesisLiability2020,
  ids = {yanaiHypothesisLiability2020a},
  title = {A Hypothesis Is a Liability},
  author = {Yanai, Itai and Lercher, Martin},
  date = {2020-09-03},
  journaltitle = {Genome Biology},
  shortjournal = {Genome Biology},
  volume = {21},
  number = {1},
  pages = {231},
  issn = {1474-760X},
  doi = {10.1186/s13059-020-02133-w},
  url = {https://doi.org/10.1186/s13059-020-02133-w},
  urldate = {2022-11-11},
  file = {/home/skynet3/Zotero/storage/LEQ5WQWF/Yanai and Lercher - 2020 - A hypothesis is a liability.pdf;/home/skynet3/Zotero/storage/NVYT3ITZ/s13059-020-02133-w.html}
}

@software{yanAppliedml2022,
  title = {Applied-Ml},
  author = {Yan, Eugene},
  date = {2022-11-12T15:15:20Z},
  origdate = {2020-07-04T18:57:47Z},
  url = {https://github.com/eugeneyan/applied-ml},
  urldate = {2022-11-12},
  abstract = {📚 Papers \& tech blogs by companies sharing their work on data science \& machine learning in production.},
  keywords = {applied-data-science,applied-machine-learning,computer-vision,data-discovery,data-engineering,data-quality,data-science,deep-learning,machine-learning,natural-language-processing,production,recsys,reinforcement-learning,search}
}

@misc{yangFastTreeSHAPAccelerating2022,
  title = {Fast {{TreeSHAP}}: {{Accelerating SHAP Value Computation}} for {{Trees}}},
  shorttitle = {Fast {{TreeSHAP}}},
  author = {Yang, Jilei},
  date = {2022-07-26},
  number = {arXiv:2109.09847},
  eprint = {2109.09847},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2109.09847},
  url = {http://arxiv.org/abs/2109.09847},
  urldate = {2022-11-11},
  abstract = {SHAP (SHapley Additive exPlanation) values are one of the leading tools for interpreting machine learning models, with strong theoretical guarantees (consistency, local accuracy) and a wide availability of implementations and use cases. Even though computing SHAP values takes exponential time in general, TreeSHAP takes polynomial time on tree-based models. While the speedup is significant, TreeSHAP can still dominate the computation time of industry-level machine learning solutions on datasets with millions or more entries, causing delays in post-hoc model diagnosis and interpretation service. In this paper we present two new algorithms, Fast TreeSHAP v1 and v2, designed to improve the computational efficiency of TreeSHAP for large datasets. We empirically find that Fast TreeSHAP v1 is 1.5x faster than TreeSHAP while keeping the memory cost unchanged. Similarly, Fast TreeSHAP v2 is 2.5x faster than TreeSHAP, at the cost of a slightly higher memory usage, thanks to the pre-computation of expensive TreeSHAP steps. We also show that Fast TreeSHAP v2 is well-suited for multi-time model interpretations, resulting in as high as 3x faster explanation of newly incoming samples.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/skynet3/Zotero/storage/GXXXJB25/Yang - 2022 - Fast TreeSHAP Accelerating SHAP Value Computation.pdf;/home/skynet3/Zotero/storage/54VIS2PU/2109.html}
}

@misc{yangWhatGPTKnows2022,
  title = {What {{GPT Knows About Who}} Is {{Who}}},
  author = {Yang, Xiaohan and Peynetti, Eduardo and Meerman, Vasco and Tanner, Chris},
  date = {2022-05-15},
  number = {arXiv:2205.07407},
  eprint = {2205.07407},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2205.07407},
  url = {http://arxiv.org/abs/2205.07407},
  urldate = {2022-11-11},
  abstract = {Coreference resolution -- which is a crucial task for understanding discourse and language at large -- has yet to witness widespread benefits from large language models (LLMs). Moreover, coreference resolution systems largely rely on supervised labels, which are highly expensive and difficult to annotate, thus making it ripe for prompt engineering. In this paper, we introduce a QA-based prompt-engineering method and discern \textbackslash textit\{generative\}, pre-trained LLMs' abilities and limitations toward the task of coreference resolution. Our experiments show that GPT-2 and GPT-Neo can return valid answers, but that their capabilities to identify coreferent mentions are limited and prompt-sensitive, leading to inconsistent results.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/skynet3/Zotero/storage/A77WLDCD/Yang et al. - 2022 - What GPT Knows About Who is Who.pdf;/home/skynet3/Zotero/storage/RZIL8DFW/2205.html}
}

@video{yannickilcherUnderspecificationPresentsChallenges2020,
  title = {Underspecification {{Presents Challenges}} for {{Credibility}} in {{Modern Machine Learning}} ({{Paper Explained}})},
  editor = {{Yannic Kilcher}},
  date = {2020-11-10},
  url = {https://www.youtube.com/watch?v=gch94ttuy5s},
  urldate = {2022-11-11},
  editortype = {director}
}

@online{YaohuiZengBiglassoBiglasso,
  title = {{{YaohuiZeng}}/Biglasso: Biglasso: {{Extending Lasso Model Fitting}} to {{Big Data}} in {{R}}},
  shorttitle = {{{YaohuiZeng}}/Biglasso},
  url = {https://github.com/YaohuiZeng/biglasso},
  urldate = {2022-11-11},
  abstract = {biglasso: Extending Lasso Model Fitting to Big Data in R - YaohuiZeng/biglasso: biglasso: Extending Lasso Model Fitting to Big Data in R},
  langid = {english},
  organization = {{GitHub}},
  file = {/home/skynet3/Zotero/storage/N74MLK3S/biglasso.html}
}

@online{YesDidIt,
  title = {Yes, but Did It Work? {{Evaluating}} Variational Inference | {{Statistical Modeling}}, {{Causal Inference}}, and {{Social Science}}},
  url = {https://statmodeling.stat.columbia.edu/2018/06/27/yes-work-evaluating-variational-inference/},
  urldate = {2022-11-11}
}

@misc{yinDemystifyLindleyParadox2020,
  ids = {yinDemystifyLindleyParadox2020a},
  title = {Demystify {{Lindley}}'s {{Paradox}} by {{Interpreting P-value}} as {{Posterior Probability}}},
  author = {Yin, Guosheng and Shi, Haolun},
  date = {2020-02-24},
  number = {arXiv:2002.10883},
  eprint = {2002.10883},
  eprinttype = {arxiv},
  primaryclass = {stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2002.10883},
  url = {http://arxiv.org/abs/2002.10883},
  urldate = {2022-11-11},
  abstract = {In the hypothesis testing framework, p-value is often computed to determine rejection of the null hypothesis or not. On the other hand, Bayesian approaches typically compute the posterior probability of the null hypothesis to evaluate its plausibility. We revisit Lindley's paradox (Lindley, 1957) and demystify the conflicting results between Bayesian and frequentist hypothesis testing procedures by casting a two-sided hypothesis as a combination of two one-sided hypotheses along the opposite directions. This can naturally circumvent the ambiguities of assigning a point mass to the null and choices of using local or non-local prior distributions. As p-value solely depends on the observed data without incorporating any prior information, we consider non-informative prior distributions for fair comparisons with p-value. The equivalence of p-value and the Bayesian posterior probability of the null hypothesis can be established to reconcile Lindley's paradox. Extensive simulation studies are conducted with multivariate normal data and random effects models to examine the relationship between the p-value and posterior probability.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Computation,Statistics - Methodology},
  file = {/home/skynet3/Zotero/storage/5HYXYXV8/Yin and Shi - 2020 - Demystify Lindley's Paradox by Interpreting P-valu.pdf;/home/skynet3/Zotero/storage/NEAWHTGZ/2002.html}
}

@online{YouNeed16,
  title = {You Need 16 Times the Sample Size to Estimate an Interaction than to Estimate a Main Effect | {{Statistical Modeling}}, {{Causal Inference}}, and {{Social Science}}},
  url = {https://statmodeling.stat.columbia.edu/2018/03/15/need-16-times-sample-size-estimate-interaction-estimate-main-effect/},
  urldate = {2022-11-11}
}

@article{youngChannelingFisherRandomization2019,
  ids = {youngChannelingFisherRandomization2019a,youngChannelingFisherRandomization2019b},
  title = {Channeling {{Fisher}}: {{Randomization Tests}} and the {{Statistical Insignificance}} of {{Seemingly Significant Experimental Results}}*},
  shorttitle = {Channeling {{Fisher}}},
  author = {Young, Alwyn},
  date = {2019-05-01},
  journaltitle = {The Quarterly Journal of Economics},
  shortjournal = {The Quarterly Journal of Economics},
  volume = {134},
  number = {2},
  pages = {557--598},
  issn = {0033-5533},
  doi = {10.1093/qje/qjy029},
  url = {https://doi.org/10.1093/qje/qjy029},
  urldate = {2022-11-07},
  abstract = {I follow R. A. Fisher'sThe Design of Experiments (1935), using randomization statistical inference to test the null hypothesis of no treatment effects in a comprehensive sample of 53 experimental papers drawn from the journals of the American Economic Association. In the average paper, randomization tests of the significance of individual treatment effects find 13\% to 22\% fewer significant results than are found using authors’ methods. In joint tests of multiple treatment effects appearing together in tables, randomization tests yield 33\% to 49\% fewer statistically significant results than conventional tests. Bootstrap and jackknife methods support and confirm the randomization results.},
  file = {/home/skynet3/Zotero/storage/7JMIMAVQ/Young - 2019 - Channeling Fisher Randomization Tests and the Sta.pdf;/home/skynet3/Zotero/storage/7PDC27RU/5195544.html;/home/skynet3/Zotero/storage/8UILCBY9/5195544.html}
}

@article{youngCHANNELLINGFISHERRANDOMIZATION,
  ids = {youngCHANNELLINGFISHERRANDOMIZATIONa},
  title = {{{CHANNELLING FISHER}}: {{RANDOMIZATION TESTS AND THE STATISTICAL INSIGNIFICANCE OF SEEMINGLY SIGNIFICANT EXPERIMENTAL RESULTS}}},
  author = {Young, Alwyn},
  pages = {47},
  langid = {english},
  file = {/home/skynet3/Zotero/storage/99FVEFTY/Young - CHANNELLING FISHER RANDOMIZATION TESTS AND THE ST.pdf}
}

@misc{yuNewFrameworkOnline2020,
  ids = {yuNewFrameworkOnline2020a},
  title = {A {{New Framework}} for {{Online Testing}} of {{Heterogeneous Treatment Effect}}},
  author = {Yu, Miao and Lu, Wenbin and Song, Rui},
  date = {2020-02-08},
  number = {arXiv:2002.03277},
  eprint = {2002.03277},
  eprinttype = {arxiv},
  primaryclass = {stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2002.03277},
  url = {http://arxiv.org/abs/2002.03277},
  urldate = {2022-11-11},
  abstract = {We propose a new framework for online testing of heterogeneous treatment effects. The proposed test, named sequential score test (SST), is able to control type I error under continuous monitoring and detect multi-dimensional heterogeneous treatment effects. We provide an online p-value calculation for SST, making it convenient for continuous monitoring, and extend our tests to online multiple testing settings by controlling the false discovery rate. We examine the empirical performance of the proposed tests and compare them with a state-of-art online test, named mSPRT using simulations and a real data. The results show that our proposed test controls type I error at any time, has higher detection power and allows quick inference on online A/B testing.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Applications,Statistics - Methodology},
  file = {/home/skynet3/Zotero/storage/UFIKAZE3/Yu et al. - 2020 - A New Framework for Online Testing of Heterogeneou.pdf;/home/skynet3/Zotero/storage/4HNYU3BC/2002.html}
}

@misc{yuTutorialVAEsBayes2020,
  ids = {yuTutorialVAEsBayes2020a},
  title = {A {{Tutorial}} on {{VAEs}}: {{From Bayes}}' {{Rule}} to {{Lossless Compression}}},
  shorttitle = {A {{Tutorial}} on {{VAEs}}},
  author = {Yu, Ronald},
  date = {2020-06-30},
  number = {arXiv:2006.10273},
  eprint = {2006.10273},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2006.10273},
  url = {http://arxiv.org/abs/2006.10273},
  urldate = {2022-11-11},
  abstract = {The Variational Auto-Encoder (VAE) is a simple, efficient, and popular deep maximum likelihood model. Though usage of VAEs is widespread, the derivation of the VAE is not as widely understood. In this tutorial, we will provide an overview of the VAE and a tour through various derivations and interpretations of the VAE objective. From a probabilistic standpoint, we will examine the VAE through the lens of Bayes' Rule, importance sampling, and the change-of-variables formula. From an information theoretic standpoint, we will examine the VAE through the lens of lossless compression and transmission through a noisy channel. We will then identify two common misconceptions over the VAE formulation and their practical consequences. Finally, we will visualize the capabilities and limitations of VAEs using a code example (with an accompanying Jupyter notebook) on toy 2D data.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/skynet3/Zotero/storage/5SQ7BJXF/Yu - 2020 - A Tutorial on VAEs From Bayes' Rule to Lossless C.pdf;/home/skynet3/Zotero/storage/5ZMFXICN/2006.html}
}

@software{zengAwesometextsummarization2022,
  title = {Awesome-Text-Summarization},
  author = {Zeng, Linghui},
  date = {2022-11-10T12:34:25Z},
  origdate = {2017-06-13T12:29:17Z},
  url = {https://github.com/mathsyouth/awesome-text-summarization},
  urldate = {2022-11-11},
  abstract = {A curated list of resources dedicated to text summarization},
  keywords = {abstractive-text-summarization,deep-learning,extractive-text-summarization,machine-learning,natural-language-processing,nlp,text-mining}
}

@misc{zengValidatingLabelConsistency2021,
  title = {Validating {{Label Consistency}} in {{NER Data Annotation}}},
  author = {Zeng, Qingkai and Yu, Mengxia and Yu, Wenhao and Jiang, Tianwen and Jiang, Meng},
  date = {2021-09-22},
  number = {arXiv:2101.08698},
  eprint = {2101.08698},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2101.08698},
  url = {http://arxiv.org/abs/2101.08698},
  urldate = {2022-11-11},
  abstract = {Data annotation plays a crucial role in ensuring your named entity recognition (NER) projects are trained with the right information to learn from. Producing the most accurate labels is a challenge due to the complexity involved with annotation. Label inconsistency between multiple subsets of data annotation (e.g., training set and test set, or multiple training subsets) is an indicator of label mistakes. In this work, we present an empirical method to explore the relationship between label (in-)consistency and NER model performance. It can be used to validate the label consistency (or catches the inconsistency) in multiple sets of NER data annotation. In experiments, our method identified the label inconsistency of test data in SCIERC and CoNLL03 datasets (with 26.7\% and 5.4\% label mistakes). It validated the consistency in the corrected version of both datasets.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/home/skynet3/Zotero/storage/6B43J4DJ/Zeng et al. - 2021 - Validating Label Consistency in NER Data Annotatio.pdf;/home/skynet3/Zotero/storage/BLCHAZ49/2101.html}
}

@article{zhangBayesianTimeSeries2018,
  title = {Bayesian {{Time Series Forecasting}} with {{Change Point}} and {{Anomaly Detection}}},
  author = {Zhang, Anderson Y. and Lu, Miao and Kong, Deguang and Yang, Jimmy},
  date = {2018-02-15},
  url = {https://openreview.net/forum?id=rJLTTe-0W},
  urldate = {2022-11-11},
  abstract = {Time series forecasting plays a crucial role in marketing, finance and many other quantitative fields. A large amount of methodologies has been developed on this topic, including ARIMA, Holt–Winters, etc. However, their performance is easily undermined by the existence of change points and anomaly points, two structures commonly observed in real data, but rarely considered in the aforementioned methods. In this paper, we propose a novel state space time series model, with the capability to capture the structure of change points and anomaly points, as well as trend and seasonality. To infer all the hidden variables, we develop a Bayesian framework, which is able to obtain distributions and forecasting intervals for time series forecasting, with provable theoretical properties. For implementation, an iterative algorithm with Markov chain Monte Carlo (MCMC), Kalman filter and Kalman smoothing is proposed. In both synthetic data and real data applications, our methodology yields a better performance in time series forecasting compared with existing methods, along with more accurate change point detection and anomaly detection.},
  langid = {english},
  file = {/home/skynet3/Zotero/storage/Z7L4F9VJ/Zhang et al. - 2018 - Bayesian Time Series Forecasting with Change Point.pdf;/home/skynet3/Zotero/storage/T2RF33KJ/forum.html}
}

@misc{zhangDiveDecisionTrees2021,
  title = {Dive into {{Decision Trees}} and {{Forests}}: {{A Theoretical Demonstration}}},
  shorttitle = {Dive into {{Decision Trees}} and {{Forests}}},
  author = {Zhang, Jinxiong},
  date = {2021-01-20},
  number = {arXiv:2101.08656},
  eprint = {2101.08656},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2101.08656},
  url = {http://arxiv.org/abs/2101.08656},
  urldate = {2022-11-11},
  abstract = {Based on decision trees, many fields have arguably made tremendous progress in recent years. In simple words, decision trees use the strategy of "divide-and-conquer" to divide the complex problem on the dependency between input features and labels into smaller ones. While decision trees have a long history, recent advances have greatly improved their performance in computational advertising, recommender system, information retrieval, etc. We introduce common tree-based models (e.g., Bayesian CART, Bayesian regression splines) and training techniques (e.g., mixed integer programming, alternating optimization, gradient descent). Along the way, we highlight probabilistic characteristics of tree-based models and explain their practical and theoretical benefits. Except machine learning and data mining, we try to show theoretical advances on tree-based models from other fields such as statistics and operation research. We list the reproducible resource at the end of each method.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/skynet3/Zotero/storage/INI4HFTH/Zhang - 2021 - Dive into Decision Trees and Forests A Theoretica.pdf;/home/skynet3/Zotero/storage/MWTWYDNC/2101.html}
}

@misc{zhangExploringTransformerBackbones2022,
  title = {Exploring {{Transformer Backbones}} for {{Heterogeneous Treatment Effect Estimation}}},
  author = {Zhang, Yi-Fan and Zhang, Hanlin and Lipton, Zachary C. and Li, Li Erran and Xing, Eric P.},
  date = {2022-02-02},
  number = {arXiv:2202.01336},
  eprint = {2202.01336},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2202.01336},
  urldate = {2022-11-11},
  abstract = {Previous works on Treatment Effect Estimation (TEE) are not in widespread use because they are predominantly theoretical, where strong parametric assumptions are made but untractable for practical application. Recent work uses multilayer perceptron (MLP) for modeling casual relationships, however, MLPs lag far behind recent advances in ML methodology, which limits their applicability and generalizability. To extend beyond the single domain formulation and towards more realistic learning scenarios, we explore model design spaces beyond MLPs, i.e., transformer backbones, which provide flexibility where attention layers govern interactions among treatments and covariates to exploit structural similarities of potential outcomes for confounding control. Through careful model design, Transformers as Treatment Effect Estimators (TransTEE) is proposed. We show empirically that TransTEE can: (1) serve as a general purpose treatment effect estimator that significantly outperforms competitive baselines in a variety of challenging TEE problems (e.g., discrete, continuous, structured, or dosage-associated treatments) and is applicable to both when covariates are tabular and when they consist of structural data (e.g., texts, graphs); (2) yield multiple advantages: compatibility with propensity score modeling, parameter efficiency, robustness to continuous treatment value distribution shifts, explainable in covariate adjustment, and real-world utility in auditing pre-trained language models},
  archiveprefix = {arXiv},
  version = {1},
  keywords = {Computer Science - Machine Learning},
  file = {/home/skynet3/Zotero/storage/TTLDD3KU/Zhang et al. - 2022 - Exploring Transformer Backbones for Heterogeneous .pdf;/home/skynet3/Zotero/storage/HY7RRLRM/2202.html}
}

@misc{zhangHowUsingMachine2021,
  title = {How {{Using Machine Learning Classification}} as a {{Variable}} in {{Regression Leads}} to {{Attenuation Bias}} and {{What}} to {{Do About It}}},
  author = {Zhang, Han},
  date = {2021-05-28T06:56:06},
  publisher = {{SocArXiv}},
  doi = {10.31235/osf.io/453jk},
  url = {https://osf.io/preprints/socarxiv/453jk/},
  urldate = {2022-11-11},
  abstract = {Social scientists have increasingly been applying machine learning algorithms to "big data" to measure theoretical concepts they cannot easily measure before, and then been using these machine-predicted variables in a regression.  This article first demonstrates that directly inserting binary predictions (i.e., classification) without regard for prediction error will generally lead to attenuation biases of either slope coefficients or marginal effect estimates. We then propose several estimators to obtain consistent estimates of coefficients. The estimators require the existence of validation data, of which researchers have both machine prediction and true values. This validation data is either automatically available during training algorithms or can be easily obtained. Monte Carlo simulations demonstrate the effectiveness of the proposed estimators. Finally, we summarize the usage pattern of machine learning predictions in 18 recent publications in top social science journals, apply our proposed estimators to two of them, and offer some practical recommendations.},
  langid = {american},
  keywords = {Econometrics,Economics,Methodology,Models and Methods,Political Science,Social and Behavioral Sciences,Social Statistics,Sociology},
  file = {/home/skynet3/Zotero/storage/EQUYZGSJ/Zhang - 2021 - How Using Machine Learning Classification as a Var.pdf}
}

@misc{zhangInductiveBiasMasked2021,
  title = {On the {{Inductive Bias}} of {{Masked Language Modeling}}: {{From Statistical}} to {{Syntactic Dependencies}}},
  shorttitle = {On the {{Inductive Bias}} of {{Masked Language Modeling}}},
  author = {Zhang, Tianyi and Hashimoto, Tatsunori},
  date = {2021-04-12},
  number = {arXiv:2104.05694},
  eprint = {2104.05694},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2104.05694},
  url = {http://arxiv.org/abs/2104.05694},
  urldate = {2022-11-11},
  abstract = {We study how masking and predicting tokens in an unsupervised fashion can give rise to linguistic structures and downstream performance gains. Recent theories have suggested that pretrained language models acquire useful inductive biases through masks that implicitly act as cloze reductions for downstream tasks. While appealing, we show that the success of the random masking strategy used in practice cannot be explained by such cloze-like masks alone. We construct cloze-like masks using task-specific lexicons for three different classification datasets and show that the majority of pretrained performance gains come from generic masks that are not associated with the lexicon. To explain the empirical success of these generic masks, we demonstrate a correspondence between the Masked Language Model (MLM) objective and existing methods for learning statistical dependencies in graphical models. Using this, we derive a method for extracting these learned statistical dependencies in MLMs and show that these dependencies encode useful inductive biases in the form of syntactic structures. In an unsupervised parsing evaluation, simply forming a minimum spanning tree on the implied statistical dependence structure outperforms a classic method for unsupervised parsing (58.74 vs. 55.91 UUAS).},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/skynet3/Zotero/storage/FCTFNFNB/Zhang and Hashimoto - 2021 - On the Inductive Bias of Masked Language Modeling.pdf;/home/skynet3/Zotero/storage/RBH32LDY/2104.html}
}

@online{zhangLightGBMQuantileRegression2020,
  title = {{{LightGBM}} for {{Quantile Regression}}},
  author = {Zhang, Jeremy},
  date = {2020-10-03T17:05:57},
  url = {https://towardsdatascience.com/lightgbm-for-quantile-regression-4288d0bb23fd},
  urldate = {2022-11-11},
  abstract = {Understand Quantile Regression},
  langid = {english},
  organization = {{Medium}}
}

@inproceedings{zhouMixedHamiltonianMonte2020,
  ids = {zhouMixedHamiltonianMonte2020a},
  title = {Mixed {{Hamiltonian Monte Carlo}} for {{Mixed Discrete}} and {{Continuous Variables}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Zhou, Guangyao},
  date = {2020},
  volume = {33},
  pages = {17094--17104},
  publisher = {{Curran Associates, Inc.}},
  url = {https://papers.nips.cc/paper/2020/hash/c6a01432c8138d46ba39957a8250e027-Abstract.html},
  urldate = {2022-11-11},
  abstract = {Hamiltonian Monte Carlo (HMC) has emerged as a powerful Markov Chain Monte Carlo (MCMC) method to sample from complex continuous distributions. However, a fundamental limitation of HMC is that it can not be applied to distributions with mixed discrete and continuous variables. In this paper, we propose mixed HMC (M-HMC) as a general framework to address this limitation. M-HMC is a novel family of MCMC algorithms that evolves the discrete and continuous variables in tandem, allowing more frequent updates of discrete variables while maintaining HMC's ability to suppress random-walk behavior. We establish M-HMC's theoretical properties, and present an efficient implementation with Laplace momentum that introduces minimal overhead compared to existing HMC methods. The superior performances of M-HMC over existing methods are demonstrated with numerical experiments on Gaussian mixture models (GMMs), variable selection in Bayesian logistic regression (BLR), and correlated topic models (CTMs).},
  file = {/home/skynet3/Zotero/storage/8B9U4HGC/Zhou - 2020 - Mixed Hamiltonian Monte Carlo for Mixed Discrete a.pdf}
}

@article{ziemannGeneNameErrors2016,
  ids = {ziemannGeneNameErrors2016a},
  title = {Gene Name Errors Are Widespread in the Scientific Literature},
  author = {Ziemann, Mark and Eren, Yotam and El-Osta, Assam},
  date = {2016-08-23},
  journaltitle = {Genome Biology},
  shortjournal = {Genome Biology},
  volume = {17},
  number = {1},
  pages = {177},
  issn = {1474-760X},
  doi = {10.1186/s13059-016-1044-7},
  url = {https://doi.org/10.1186/s13059-016-1044-7},
  urldate = {2022-11-11},
  abstract = {The spreadsheet software Microsoft Excel, when used with default settings, is known to convert gene names to dates and floating-point numbers. A programmatic scan of leading genomics journals reveals that approximately one-fifth of papers with supplementary Excel gene lists contain erroneous gene name conversions.},
  keywords = {Gene symbol,Microsoft Excel,Supplementary data},
  file = {/home/skynet3/Zotero/storage/7MW6J2SS/Ziemann et al. - 2016 - Gene name errors are widespread in the scientific .pdf;/home/skynet3/Zotero/storage/LAPJRD7N/s13059-016-1044-7.html}
}

@article{zotero-40213,
  url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0029744},
  file = {/home/skynet3/Zotero/storage/GWQYGT7U/_.pdf}
}

@article{zotero-40218,
  doi = {10.1371/journal.pone.0029744},
  url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0029744},
  file = {/home/skynet3/Zotero/storage/KICPHQQ7/_.pdf}
}

@online{zotero-41510,
  url = {https://nips.cc/Conferences},
  urldate = {2022-11-11},
  file = {/home/skynet3/Zotero/storage/G2IYPMAK/Conferences.html}
}

@online{zotero-41819,
  url = {https://probml.github.io/pml-book/book1.html},
  urldate = {2022-11-11},
  file = {/home/skynet3/Zotero/storage/B27URMBU/book1.html}
}

@online{zotero-43408,
  url = {https://exts.ggplot2.tidyverse.org/gallery/},
  urldate = {2022-11-11},
  file = {/home/skynet3/Zotero/storage/C2WZPANL/gallery.html}
}

@preamble{ "\ifdefined\DeclarePrefChars\DeclarePrefChars{'’-}\else\fi " }
